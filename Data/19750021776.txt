b'SURVEY OF DECENTRALIZED CONTROL METHODS*\nMichael Athans**\nMassachusetts Institute of Technology\nCambridge, Massachusetts\n\nINTRODUCTION\nThis paper presents an overview of the types of problems that are being considered by control\ntheorists in the area of dynamic large-scale systems with emphasis on decentralized control strategies. A similar paper by Varaiya (ref. 1) indicated the interplay between static notions drawn from\nthe mathematical economics, management, and programming areas and the attempts by control\ntheorists to extend the static notions into the stochastic dynamic case. In this paper, we shall not\nelaborate upon the dynamic or team aspects of large-scale systems. Rather we shall concentrate on\napproaches that deal directly with decentralized decision making for large-scale systems.\nAlthough a survey paper, the number of surveyed results is relatively small. This is due to the\nfact that there is still not a unified theory for decentralized control. What is available is a set of\nindividual contributions that point out both "blind alleys" as well as potentially fruitful\napproaches.\nWhat we shall attempt to point out is that future advances in decentralized system theory are\nintimately connected with advances in the soK;alled stochastic control problem with nonclassical\ninformation pattern. To appreciate how this problem differs from the classical stochastic control\nproblem, it is useful to briefly summarize the basic assumptions and mathematical tools associated\nwith the latter. This is done in section 2. Section 3 is concerned with certain pitfalls that arise when\none attempts to impose a decentralized structure at the start, but the mathematics "wipes out" the\noriginal intent. Hence, one can draw certain conclusions about the proper mathematical formulation\nof decentralized control problems. Section 4 surveys some research (primarily carried out by the\nauthor and his students) that attempts to circumvent some of the pitfalls discussed in sections.\nSection 5 presents some conclusions about future research.\nCLASSICAL STOCHASTIC CONTROL PROBLEM\nThis section reviews in an informal way the classical stochastic control problem or the problem\nof stochastic control with classical information structure. Our main purpose is to indicate the types\nof assumptions one makes in this class of problems, the nature of the mathematical tools available,\nand the general structure of the solution. This overview is necessary so that one can see that the\nsolution to the classical stochastic control problem leads to a completely centralized system.\n*This research has been conducted in the Decision and Control Sciences Group of the MIT Electronic Systems\nLaboratory with support from AFOSR grant 72-2273, NASA/AMES grant NGL-22-009-124, and NSF grant\nGK41647.\n**Professor of Electrical Engineering and Director MIT Electronic Systems Laboratory.\n101\n\nFrom a technical point of view, the best survey of the issues associated with classical stochastic\ncontrol is, in the opinion of the author, the paper by Witsenhausen (ref. 2). The classical stochastic\ncontrol problem is the subject of many standard texts and monographs (see, e.g., refs. 3 to 7).\nFigure 1 is an abstract block diagram of a centralized control system. One deals with a\nstochastic dynamical system (usually described by a set of stochastic difference or differential\nequations) and one makes possibly noisy measurements of certain of its variables. The time evolution of the system variables is influenced by stochastic disturbances and decisions (or control)\nvariables generated in closed-loop feedback form by a single controller or decision maker.\nThe assumptions made in the classical stochastic control problem are as follows:\nA2.1 There is a single controller or decision maker.\nA2.2 The controller knows the mathematical form of the system dynamics (i.e., the stochastic\ndifferential or difference equations).\nA2.3 The controller knows the relationship of the measurements to the system variables.\nA2.4 The controller knows a priori the probability densities of all underlying stochastic variables (i.e., exogenous disturbances, uncertain system parameters, measurement errors, etc.).\nA2.5 The controller wishes to minimize a well-defined scalar deterministic cost functional.\nA2.6 The controller at any time t has instant and perfect recall of all past applied inputs or\ndecisions and all past and present measurements.\nUnder these assumptions, classical stochastic control provides a well-defined rule that translates all\ninformation available to the controller at time t (i.e., the contents of assumptions A2.1 to A2.6) to\nan optimal control or decision at time t.\nFrom a technical point of view the state variable (causal) description of the dynamical system,\nthe use of a Bayesian rule to deal with the stochastic elements, and the use of stochastic dynamic\nprogramming blend well to yield the optimal stochastic control in a relatively straightforward\nconceptual manner. Actual calculations are generally very complex with the exception of the\nso-called Linear-Quadratic-Gaussian (LQG) problem (refs. 4-8).\nDECENTRALIZED CONTROL: PITFALLS\nThis section presents the types of issues that arise when the basic assumptions of section 2\nassociated with classical centralized stochastic control are modified. There are several ways to\ndepart from the basic framework of classical stochastic control. In this paper, we shall adopt the\nviewpoint of examining the issues when we wish to analyze some sort of hierarchical multilevel\ndecentralized system (ref. 9).\n\n102\n\nOne does not have to examine a complex hierarchical structure to understand the issues\nassociated with decentralized control. Figure 2 presents the simplest possible case involving a\ntwo-level structure. We shall elaborate upon the structure implied in figure 2 to point out its general\ncharacteristics.\n\nCase 3.1\nImagine for the time being that the "interaction" channel and the "coordinator" did not\nappear in figure 2. We are left with two "uncoupled" dynamical systems. If the framework of\nsection 2 is adopted, we can postulate that each controller solves a classical stochastic control\nsystem.\nCase 3.2\nNext, let us still leave the "coordinator" out of figure 2, but restore the "interaction" channel.\nWhat we mean by "interaction" is that certain decision and/or state and/or output variables of each\nsystem influence the dynamic evolution of the other system. If this interaction is "weak," then it is\npossible for both systems to operate non-optimally but still satisfactorily without altering the basic\ncontrol strategy of Case 3.1, because the interactions are viewed as exogenous unknown disturbances and the inherent use of feedback tends to make the overall system response somewhat\ninsensitive to weak, unmodeled disturbances.\nThis situation, namely, with weak interaction and the absence of coordination, has been\nanalyzed by Chong, Kwong, and Athans (refs. 10-12). This research attempted to replace the weak\ninteraction disturbances, which are actually correlated in time, with equivalent "fake white noise"\ninputs which are uncorrelated in time, and to evaluate techniques by which, in the LQG context,\nthe convariance of the "fake white noise" can be selected.\n\nCase 3.3\nIf the dynamic interaction between the systems is not negligible, then the performance of each\nsystem in figure 2 can be expected to deteriorate severely. To "cure" this performance degradation,\none introduces the "coordinator" in figure 2.\nIntuitively, in any physical large-scale system, the role of the coordinator is to receive some\nsort of information from the local subsystems and make some decisions to improve the performance\ncompared to that under Case 3.2. The crucial question then is to make precise the role of the\ncoordinator as a function of postulated strategies for the lower level subsystems.\nIt is possible, in any given physical situation, to specify the task of the coordinator in a\nreasonably good, but ad hoc way, so that the overall system performance is satisfactory for the\nspecific application. However, the heart of decentralized control theory research is to formulate\nprecise mathematical problems whose solution defines the optimal task of the coordinator, without\ndestroying the intuitively appealing decentralized structure in figure 2.\n\n103\n\nSome Blind Alleys\nThe following assumptions sound reasonable from a physical point of view, but when they are\nincorporated in a mathematical framework, the mathematical solution destroys the decentralized\nstructure. The pitfalls that the assumptions lead to are easily seen without resorting to complex\nmathematics, and the appreciation of the pitfalls provides valuable knowledge on how not to\nformulate a decentralized control problem.\nWe start with a list of assumptions, again keeping the structure of figure 2 in mind:\nA3.1 Each local system neglects the interaction from the other system.\nA3.2 Under A3.1, each local controller knows the dynamics and probabilistic information\nassociated with his own system, has his own performance index, and has perfect recall of his own\npast measurements and controls. It follows (see section 2) that each local system can solve its own\nwell-formulated classical stochastic control problem, and we assume that each local system applies\nin real time the optimal stochastic control obtained under these assumptions.\nA3.3 The coordinator knows the dynamics of both subsystems, including the interaction, as\nwell as all prior probabilistic information available to each local subsystem.\nA3.4 The coordinator\'s cost functional (performance index, utility function) is a well-defined\nfunction of the cost functional of each local subsystem (e.g., a weighted sum).\nA3.5 At each instant of time, the coordinator can apply a dynamic control to each local\nsystem of the same nature of the local control.\nA3.6 At each instant of time, each local subsystem transmits instantly and without error its\nmeasurements and controls to the coordinator; furthermore, the coordinator has perfect recall.\nThe key question is then: Under assumptions A3.1 to A3.6, what is the optimal decision rule\nfor the coordinator? The answer is exceedingly simple. Under assumptions A3.3 to A3.6, the\ncoordinator has a classical stochastic control problem for the entire system. Hence, so far as the\ncoordinator is concerned, he must solve the overall optimal stochastic control problem (see section\n2) and his optimal strategy is to (i) cancel the locally computed controls (see assumption A3.2) and\n(ii) substitute the global optimal controls.\nThus, the essential decentralized nature of the problem is destroyed. This points out that, even\nin the stochastic case, one cannot allow the coordinator full knowledge of everything because the\nmathematically optimal solution allows the coordinator to completely take over. This problem is\neven more serious if a complete deterministic framework for decentralized control is adopted\n(ref. 9).\n\nControl-Sharing Strategies\nSo that the coordinator does not take over completely, some of assumptions A3.1 to A3.6\nmust be modified to deny the coordinator full knowledge of everything. Needless to say, there are\n104\n\nmany ways to modify assumptions A3.1 to A3.6 and to attempt an analysis of the role of the\ncoordinator.\nIn this section, we shall examine one variation because it has received some attention in the\ncontrol literature, although not precisely in the context of this paper. Hence our remarks represent\na reinterpretation of the research of Aoki (ref. 13) and Sandell and Athans (ref. 14).\nOne can argue that the coordinator can take over under assumptions A3.1 to A3.6 because\nassumption A3.6 allows the coordinator to have instantaneous access to all measurements of the\nlocal systems. This allows him (see assumption A3.3) to calculate the local controls to be used by\nthe subsystems (see assumption A3.2) and cancel them (see assumption A3.5). Hence one may\nthink that one way to prevent the coordinator from taking over is to deny to him the actual\nmeasurements of the local systems. Thus, we seek to modify assumption A3.6.\nAssumption A3.6, however, deals not only with measurements but with controls. We can\nadopt the intuitive philosophy "do not flood your boss with day-to-day occurrences, but let him\nknow your day-to-day decisions." If we adopt this framework, we can replace assumption A3.6\nwith the following:\nA3.6(M) At each instant of time, each local subsystem transmits instantly and without error\nONLY its controls, but not its measurements to the coordinator; furthermore, the coordinator has\nperfect recall.\nOne can then pose a mathematical problem under assumptions A3.1 to A3.5 and A3.6(M) to\nfind the optimal decision rule for the coordinator. The answer (refs. 13, 14) is both surprising and\ninteresting: (i) the stochastic control problem for the coordinator is not well defined, in the sense\nthat an optimal solution does not exist; and (ii) although an optimal solution does not exist, one\ncan find e optimal solutions in the sense that one can approach the unattainable optimal solution\narbitrarily closely.\nThe way these e optimal solutions are obtained is interesting and instructive because they\nindicate once more how not to formulate a decentralized control problem. We shall attempt to\nexplain how this happens by a simple example.\nFrom figure 2, let us suppose that the system operates in discrete time so that measurements\nand decisions are made at the values of the time index t \xe2\x80\x94 0, 1, 2, 3, . . . . Letz^t) denote the\nmeasurement and let ujt) denote the control of system 1; for simplicity, assume that both Z j ( f )\nand u t ( t ) are scalars. Suppose that the following sequence of measurements has been made by\nsystem 1:\nf = 0, z (0) = 6\n/ - 1, z ( l ) = 7\n(1)\n\nt =2,z(2)= 8\nt = 3 , z (3)= 9\n\n105\n\nUnder assumption A3.2, system 1 has a well-defined rule for generating its own optimal control.\nSuppose that, at the basis of the measurements of equation (1), the optimal local control for\nsystem 1 at the time / = 3 is\nw,*(3) = 1.234 .\n\n(2)\n\nUnder assumption A3.6(M), the control in equation (2) can be transmitted instantly and without\nerror to the coordinator. However, the nature of the e optimal solution indicates that system 1\nshould not transmit the control (eq. 2) to the coordinator. Rather, it should transmit and apply to\nthe system a control of the following form:\nd i (3) = 1.234000000 . . . 00006789.\n\n(3)\n\nThe information conveyed to the coordinator when he receives the control (eq. 3) without error is\nvery different from that contained in equation (2). Examination of equations (1) to (3) indicates\nthat the past measurements (6, 7, 8, 9) have been coded in \xc2\xabi(3). From the front part of equation (3), the coordinator knows the control Hj*(3) of equation (2); from the tail end of equation (3), the coordinator knows exactly the past measurements of system 1. The string of zeros\nbetween the control (1.234) and the coded measurements (6789) is simply to guarantee that the\napplication of u 1 (3\\ rather than MJ *(3), to the system results in an infinitesimal loss in system\nperformance (i.e., the 000 ... 006789 part of the control is wiped out by the system uncertainty).\nHence, assumption A3.6(M) is not strong enough to prevent the coordinator from obtaining all\nthe information he needs to take over for all practical cases.\n\nConclusions\nThe above discussion points out that, in stochastic decentralized problems, instantaneous\nerror-free transmissions of either both controls and measurements or controls alone is not a realistic\nmathematical assumption because this allows the coordinator to take over and destroy the decentralized nature of the problem.\nDECENTRALIZED CONTROL: PROMISING AVENUES\nIn this section, we discuss some recent results that appear to be useful toward building some\nelements of a theory for decentralized control. Once more the reader is referred to Variaya (ref. 1)\nfor additional concepts and discussion.\n\nDecentralization with Fixed Structure\nFigure 3 depicts a specific decentralized structure somewhat different from that discussed in\nsection 3. One is given an nth order, linear stochastic dynamic system, with two sets of measurements (z j and z2) and two sets of controls (u, and u2 ). It is decided a priori to select two dynamic\ncontrollers that generate stochastic controls in the manner illustrated in figure 3. It is assumed that\n106\n\nthere is a single cost functional to be minimized, and one can view the job of the coordinator as\ndefining the characteristics of the two controllers. This represents a variation on the dynamic team\nproblem (see, e.g., Ho and Chu (refs. 15, 16)).\nBecause of the nonclassical information pattern and the general lack of knowledge-for solving\ndynamic team problems, some additional assumptions have to be made so that the problem of\ndesigning the two controllers in figure 3 can be solved.\nFor the LQG continuous-time case, Chong and Athans (refs. 10, 17) fixed the structure of\neach controller to be linear and of the same dimension as the order of the dynamic system that was\ncontrolled. Furthermore, each controller was constrained so that its internal Kalman-Bucy filter\nwould produce unconditional zero mean estimates of the state, ignoring the actions of the other\ncontroller. The parameter matrices of each dynamic controller could then be globally optimized by\nsolving a deterministic matrix optimal control problem through the use of the matrix minimum\nprinciple (ref. 18). The discrete-time version of this problem was considered by Carpenter (ref. 19).\nTwo basic conclusions can be drawn from the above studies (see also Variaya (ref. 1)):\n(i) The off-line computational effort for solving such decentralized problems is greater than\nthat required for the centralized case.\n(ii) Even in the LQG context, the separation theorem or certainty equivalence principle fails\nto hold.\n\nPeriodic Coordination\ni\nThe discussion in section 3 indicates that for decentralized systems (fig. 3), one cannot provide\nthe coordinator with instantaneous and error-free transmission of the local subsystem measurements\nand/or decisions to the coordinator; otherwise, the coordinator takes over and substitutes the\nglobally optimal stochastic controls, thus overriding the decisions of the local controller.\nOne way to bypass this problem is to assume that the coordinator is allowed to "interfere"\nonly occasionally. This notion of periodic coordination has been considered by Chong and Athans\n(refs. 20-22). To understand the intuitive notion of periodic coordination, suppose that assumptions\nA3.1 to A3.4 of section 3 are still valid, but assumptions A3.5 and A3.6 are replaced by the\nfollowing (informal) one.\nPeriodic coordination structure \xe2\x80\x94 Suppose that the entire system operates in discrete time. For\nconcreteness, we assume that the basic time unit is a day. Then the basic system operation is\n(i) Assume that each lower-level system makes its measurements and generates its controls\n(decisions) once a day.\n(ii) Once a month, all lower-level system measurements and controls are "mailed" without\nerror to the coordinator.\n\n107\n\nThe basic question is: what is the job of the coordinator at the beginning of each month? The\nmathematical approach adopted and the results obtained (refs. 20-22) have the following interpretation which we feel has certain intuitively appealing aspects.\nOnce a month, the coordinator has a threefold task with respect to each lower-level system:\n(i) Set it straight. In a technical context, he corrects the estimates generated by the lower-level\nKalman filters because these estimates are in error because each lower-level system neglects the\ninteractions from the other lower-level systems.\n(ii) Change its directives, in the sense that new time paths for the lower-level controls are given\nin an open-loop sense.\n(iii) Change its incentives, in the sense that additional terms are added to each lower-level\nsystem cost functional to compensate for the fact that the global cost functional differs from each\nlower-level cost functional.\nThe main advantage of these results is that the mathematical theory itself suggests the tasks that\nmust be performed periodically by the coordinator. The main disadvantage is that the coordinator\nmust still solve a very large-scale stochastic optimization problem, although not as often as in the\nbasic time frame of the lower level. Although for certain applications this approach may be feasible,\nit lacks the capability of somehow aggregating the information flow from the lower-level systems to\nthe coordinator.\nNonetheless, because the theory itself suggests this mode of coordination (by changing directives and incentives), it provides strong motivation to postulate a specific framework for operating\nthe lower-level systems. A preliminary formulation along these lines can be found in a recent paper\nby Athans (ref. 23).\n\\\nThe notion of delaying the information exchanged between different portions of a hierarchical\nsystem is intuitively appealing. Much more research is needed to understand its impact on decentralized control theory. However, the results of Sandell and Athans (ref. 14), in which it was shown\nthat LQG problems with a unit-time step delay of information exchange admit a linear optimal\ndecision rule, which can be calculated explicitly, appear to be promising so far as their applicability\nto decentralized control theory is concerned.\nRemarks\nMost of the results surveyed attempt in one way or another to present to both the coordinator\nand the local subsystems a classical stochastic control problem. Although research along these lines\nis useful, there is no definitive theory that deals directly with issues of aggregated information,\ndecision making with partial information, or decision making with finite memory.\nTo adequately deal with these issues in the context of decentralized control, much additional\nresearch is needed in the area of stochastic control with nonclassical information patterns. The\nfamous Witsenhausen counter-example (ref. 24), in which a simple LQG problem with nonclassical\ninformation pattern was shown to have a nonlinear optimal decision rule, points out the immense\n108\n\ndifficulties associated with this class of problems. Witsenhausen (refs. 25, 26) has continued his\nfundamental investigations in this class of problems, but their implications in the context of\ndecentralized control theory remain largely unexplored. The work on finite-state, finite-memory\ncontrol of Sandell and Athans (refs. 27, 28) may be useful to aggregate the flow of information\nbetween the different levels of a hierarchy and to limit the computational complexity available to\nthe coordinator. In addition, the recent results surveyed by Ho (ref. 29) pertaining to approaches in\ninformation structures when many decision makers are involved is of direct importance to\ndecentralized control problems.\nCONCLUSIONS\n\nThe main conclusions that one can draw are:\n(1) Any purely deterministic approach to multilevel hierarchical dynamic systems is not apt to\nlead to realistic theories or designs.\n(2) The flow of measurements and decisions in a decentralized system should not be\ninstantaneous and error-free.\n(3) Delays in information exchange in a decentralized system lead to reasonable approaches to\ndecentralized control.\n(4) A mathematically precise notion of aggregating information is not yet available.\n(5) Research in nonclassical information structures is directly relevant to problems of\ndecentralized control.\n\n109\n\nREFERENCES\n\n1. Varaiya, P. P.: Trends in the Theory of Decision-Making in Large Systems, Annals of Economic and Social\nMeasurement, vol. 1, no. 4, Oct. 1972, pp. 493-500.\n2. Witsenhausen, H. S.: Separation of Estimation and Control for Discrete Time Systems. Proceedings IEEE, vol.\n59,1971.\n3. Bellman, R.: Dynamic Programming, Princeton University Press, 1957.\n4. Aoki, M.: Optimization of Stochastic Systems, Academic Press, 1967.\n5. Hinderer, K.: Foundations of Non-stationary Dynamic Programming with Discrete Time Parameter. SpringerVerlag, Berlin, 1970.\n6. Astrom, K.: Introduction to Stochastic Control Theory. Academic Press, 1970.\n7. Kushner, H. J.: Introduction to Stochastic Control. Holt, Rinehart, and Winston, 1971.\n8. Athans, M.: The Discrete Time Linear-Quadratic-Gaussian Stochastic Control Problem. Annals of Economic and\nSocial Measurement, vol. 1, October 1972, pp. 449-491.\n9. Mesarovic, M. D.; Macko, D.; and Takahara, Y.: Theory of Hierarchical Multilevel Systems. Academic Press,\n1970 (see also the review of this book in IEEE Transactions on Automatic Control, vol. AC-17, 19,72, pp.\n280-281).\ni\n10. Chong, C. Y.: On the Stochastic Control of Coupled Linear Systems, S. M. Thesis, MIT, 1972.\n11. Kwong, R. H.: On a Stochastic Control Method for Weakly Coupled Systems. S. M. Thesis, MIT, 1972.\n12. Kwong, R. H.; Chong, C. Y.; and Athans, M.: On Stochastic Control System Design Methods for Weakly\n. Coupled Large Scale Systems. Proceedings 1972 Joint Automatic Control Conference, pp. 220-229.\n13. Aoki, M.: On Decentralized Linear Stochastic Control Problems with Quadratic Cost. IEEE Transactions on\nAutomatic Control, vol. AC-18, June 1973, pp. 243-250.\n14. Sandell, Jr., N. R.; and Athans, M.: Solution of Some Nonclassical LQG Stochastic Decision Problems. IEEE\nTransactions on Automatic Control, vol. AC-19, April, 1974, pp. 108-116.\n15. Ho, Y. C.; and Chu, K. C.: Team Decision Theory and Information Structures in Optimal Control Problems;\nPart I. IEEE Transactions on Automatic Control, vol. AC-17, 1972.\n16. Chu, K. C.: Team Decision Theory and Information Structures in Optimal Control Problems; Part II. IEEE\nTransactions on Automatic Control, vol. AC-17, 1972.\n17. Chong, C. Y.; and Athans, M.: On the Stochastic Control of Linear Systems with Different Information Sets.\nIEEE Transactions on Automatic Control, vol. AC-16, October, 1971, pp. 423-430.\n\n110\n\n18. Athans, M.: The Matrix Minimum Principle. Information and Control, vol. 11, November 1967, pp. 592-606.\n19. Carpenter, Jr., C. H.: Some Control and Communications Aspects of Stochastic Systems. S. M. Thesis, MIT,\n1972.\n20. Chong, C. Y.: On the Decentralized Control of Urge-Scale Systems. Ph.D. Thesis, MIT, 1972.\n21. Chong, C. Y.; and Athans, M.: On the Periodic Coordination of Linear Stochastic Systems. July 1974 (to\nappear).\n22. Chong, C. Y.; and Athans, M.: Hierarchical Decomposition for a Stochastic Optimization Problem. Proceeding\n1973 Princeton Conference on Information Sciences and Systems, Princeton, N.J., March 1973.\n23. Athans, M.: Theoretical and Practical Problems in the Design of Stochastic Large Scale Systems. Proceedings\nFourth Iranian Conference on Electrical Engineering, Shiraz, Iran, May 1974.\n24. Witsenhausen, H. S.: A Counterexample in Stochastic Optimal Control. SIAM Journal on Control, vol. 6, 1968.\n25. Witsenhausen, H. S.: On Information Structures, Feedback, and Causality. SIAM Journal on Control, vol. 9,\n1971.\n26. Witsenhausen, H. S.: A Standard Form for Sequential Stochastic Control. J. Mathematical System Theory, vol.\n7,1973.\n27. Sandell, Jr., N. R.; and Athans, M.: Control of Finite State Finite Memory Stochastic Systems. Proceedings\n1974 Princeton Conference on Information Sciences and Systems, Princeton, N J., March 1974.\n28. Sandell, Jr., N. R.: Control of Finite-State, Finite-Memory Stochastic Systems. Ph.D. Thesis, MIT, May 1974.\n29. Ho, Y. C.: Information Structures in Many-Person Optimization Problems. Annals of Economic and Social\nMeasurement, (submitted).\n\nIll\n\nDYNAMIC\nCONTROLLER\n\n*1\n\nN-th ORDER\nCONTROLS\n\nLINEAR STOCHASTIC\nDYNAMIC SYSTEM\n\nNOISY\nMEASUREMENTS\n\nDYNAMIC\nCONTROLLER #2\n\nFigure 1.\xe2\x80\x94 Centralized control system.\n\nCOORDINATOR\nt Disturbances\nINTERACTION\n\nSYSTEM *1\n\nDecisions\n\nSYSTEM\n\nMeasurements\n\nSYSTEM #2\nCONTROLLER\n\nCONTROLLER\n#1\n\nCONTROLLER\n#2\n\nFigure 2.\xe2\x80\x94 Two level hierarchical structure.\n\n112\n\nFigure 3.\xe2\x80\x94 Decentralized structure.\n\n'