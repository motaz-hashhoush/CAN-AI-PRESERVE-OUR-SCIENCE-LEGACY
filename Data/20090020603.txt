b'Information Sciences\nAlgorithm Optimally Orders Forward-Chaining Inference Rules\nRequirements for exhaustive data-flow analysis are relaxed.\nNASA\xe2\x80\x99s Jet Propulsion Laboratory, Pasadena, California\nPeople typically develop knowledge\nbases in a somewhat ad hoc manner by\nincrementally adding rules with no specific organization. This often results in\na very inefficient execution of those\nrules since they are so often order sensitive. This is relevant to tasks like Deep\nSpace Network in that it allows the\nknowledge base to be incrementally developed and have it automatically ordered for efficiency.\nAlthough data flow analysis was first\ndeveloped for use in compilers for producing optimal code sequences, its usefulness is now recognized in many software systems including knowledge-based\nsystems. However, this approach for ex-\n\nhaustively computing data-flow information cannot directly be applied to inference systems because of the ubiquitous\nexecution of the rules. An algorithm is\npresented that efficiently performs a\ncomplete producer/consumer analysis\nfor each antecedent and consequence\nclause in a knowledge base to optimally\norder the rules to minimize inference\ncycles.\nAn algorithm was developed that optimally orders a knowledge base composed of forwarding chaining inference\nrules such that independent inference\ncycle executions are minimized, thus, resulting in significantly faster execution.\nThis algorithm was integrated into the\n\nJPL tool Spacecraft Health Inference Engine (SHINE) for verification and it resulted in a significant reduction in inference cycles for what was previously\nconsidered an ordered knowledge base.\nFor a knowledge base that is completely\nunordered, then the improvement is\nmuch greater.\nThis work was done by Mark James of Caltech for NASA\xe2\x80\x99s Jet Propulsion Laboratory.\nFurther information is contained in a TSP\n(see page 1).\nThe software used in this innovation is\navailable for commercial licensing. Please contact Karina Edmonds of the California Institute of Technology at (626) 395-2322. Refer\nto NPO-42003.\n\nProject Integration Architecture\nAll information of technological processes can be readily originated, manipulated, shared,\npropagated to other processes, and viewed by man or machine.\nJohn H. Glenn Research Center, Cleveland, Ohio\nThe Project Integration Architecture\n(PIA) is a distributed, object-oriented, conceptual, software framework for the generation, organization, publication, integration, and consumption of all information\ninvolved in any complex technological\nprocess in a manner that is intelligible to\nboth computers and humans. As used here,\n\xe2\x80\x9call information\xe2\x80\x9d signifies, more specifically, all information that has been or could\nbe coded in digital form. This includes not\nonly experimental data, design data, results\nof simulations and analyses, organizational\nand financial data, and the like, but also sets\nof rules, computer programs, processes,\nand methods of solution.\nIn the development of PIA, it was recognized that in order to provide a single computational environment in which all information associated with any given complex\ntechnological process could be viewed, reviewed, manipulated, and shared, it is necessary to formulate all the elements of such\na process on the most fundamental level.\nIn this formulation, any such element is regarded as being composed of any or all of\n\nNASA Tech Briefs, January 2008\n\nthree parts: input information, some transformation of that input information, and\nsome useful output information.\nAnother fundamental principle of PIA\nis the assumption that no consumer of information, whether human or computer,\ncan be assumed to have any useful foreknowledge of an element presented to it.\nConsequently, a PIA-compliant computing system is required to be ready to respond to any questions, posed by the consumer, concerning the nature of the\nproffered element. In colloquial terms, a\nPIA-compliant system must be prepared\nto provide all the information needed to\nplace the element in context.\nTo satisfy this requirement, PIA extends the previously established objectoriented-programming concept of selfrevelation and applies it on a grand\nscale. To enable pervasive use of selfrevelation, PIA exploits another previously established object-oriented-programming concept \xe2\x80\x94 that of semantic\ninfusion through class derivation. By\nmeans of self-revelation and semantic\n\ninfusion through class derivation, a\nconsumer of information can inquire\nabout the contents of all information\nentities (e.g., databases and software)\nand can interact appropriately with\nthose entities.\nOther key features of PIA include the\nfollowing:\n\xe2\x80\xa2 Encapsulation of dimensionality and\nother semantically appropriate functionality;\n\xe2\x80\xa2 Enforcement of the dimensional nature\nof information (that is, something that is\ndimensional in nature cannot be accessed\nin a dimensionally-unaware manner);\n\xe2\x80\xa2 Exploitation of the object-identification\nfacilities of the Common Object Request Broker Architecture (CORBA) to\nprovide an object \xe2\x80\x9caddress space\xe2\x80\x9d\n(defining the quantity of information\nthat can be stored) that reaches to a\npractical infinity;\n\xe2\x80\xa2 Use of the object-etherealization and incarnation facilities of the CORBA to\nmake feasible the serving of a practically infinite number of objects;\n\n35\n\n'