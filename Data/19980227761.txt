b'NASA-GB-001-96\n\nSoftware\nProgram\nSoftware Management Guidebook\n\nNovember 1996\n\nNATIONAL AERONAUTICS AND\nSPACE ADMINISTRATION\nWASHINGTON, DC\n\nSoftware\nProgram\nSoftware Management Guidebook\n\nNovember 1996\n\nNATIONAL AERONAUTICS AND\nSPACE ADMINISTRATION\nWASHINGTON, DC\n\nCSC 10034618\n\nNASA-GB-00 1-96\n\n..\n\n11\n\nForeword\nThis document is a product of the National Aeronautics and Space Administration (NASA)\nSoftware Program, an Agency-wide program to promote continual improvement of software\nengineering within NASA. The goals and strategies for this program are documented in the\nNASA Software Strategic Plan, July 13, 1995.\nAdditional information is available from the NASA Software IV&V facility on the World Wide\nWeb site http:Ilwww.ivv.nasa.govl.\n\n...\n\n111\n\nNASA-GB-001-96\n\nTable of Contents\nPage\nForeword ........................................................................................................................................\n1. Introduction..................................................................................................................................\n1.1 Background ..........................................................................................................................\n1.2 Purpose.................................................................................................................................\n1.3 Scope....................................................................................................................................\n1.4 Overview..............................................................................................................................\n. .\n1.4.1 Organization.............................................................................................................\n1.4.2 Terminology .............................................................................................................\n1.4.3 Notation....................................................................................................................\n\n...\n\n11\n1\n\n1\n1\n1\n2\n2\n2\n2\n2\n\n2. Software Engineering Process Requirements and Infrastructure................................................. 5\n2.1 General Requirements.......................................................................................................... 5\n2.2 Specific Required Activities and Products ..........................................................................\n6\n2.2.1 Management Activities ............................................................................................ 8\n2.2.2 Technical Activities ................................................................................................. 8\n2.2.3 Software Process Improvement Activities ............................................................... 8\n2.2.4 System-Level Considerations................................................................................... 8\n2.3 Software Process Responsibilities ..................................................................................... 10\n2.3.1 Level 1: NASA Headquarters, IV&V Facility, and Software Working Group......11\n2.3.2 Level 2: Center and Intra-Center Elements............................................................ 12\n2.3.3 Level 3: Branches and Software Projects............................................................... 12\n2.4 Process Assets .................................................................................................................... 13\n3. The Software Project\xe2\x80\x99s Process..................................................................................................\n3.1 The Five-Step Project Process ...........................................................................................\n3.2 Documenting the Project\xe2\x80\x99s Process-The Software Plan ..................................................\n4. Beginning to Plan the Project: Understanding the Scope of Work ............................................\n4.1 Ascertaining Customer Requirements and Constraints .....................................................\n\n15\n15\n17\n21\n21\n\n4.2 Ascertaining Customer Goals and Objectives ...................................................................\n4.3 Understanding Management\xe2\x80\x99s Risk Tolerance ..................................................................\n4.4 Understanding Products to be Delivered and Their Characteristics..................................\n4.4.1 Documentation.......................................................................................................\n4.4.2 Software Product Releases .....................................................................................\n4.4.3 Milestone Reviews.................................................................................................\n\n22\n23\n23\n23\n23\n24\n\n5 . Defining the Technical Approach .............................................................................................. 25\n5.1 Selecting an Appropriate Life-Cycle Model ......................................................................\n26\n\nV\n\nNASA-GB-00 1-96\n\n5.1.1 Waterfall Development Life-Cycle Model.............................................................\n5.1.2 Incremental Development Life-Cycle Model .........................................................\n5.1.3 Evolutionary Development Life-Cycle Model .......................................................\n5.1.4 Package-Based Development Life-Cycle Model ...................................................\n5.1.5 Legacy System Maintenance Life-Cycle Model ....................................................\n5.2 Selecting Appropriate Activities, Methods, and Products .................................................\n5.2.1 Software CI Requirements Definition and Analysis ..............................................\n5.2.2 Software CI Design ................................................................................................\n5.2.3 Software CI Implementation and Testing ..............................................................\n5.2.4 Software CI Qualification Testing .........................................................................\n5.2.5 Preparing for Software Delivery ............................................................................\n5.2.6 Software Product Validation and Verification.......................................................\n5.2.7 Software Configuration Management ....................................................................\n5.2.8 Software Quality Assurance...................................................................................\n5.2.9 Milestone Reviews .................................................................................................\n6. Finishing the Software Plan-Defining the Management Approach ........................................\n6.1 Establishing the Software Project\xe2\x80\x99s Organizational Structure...........................................\n\n28\n30\n32\n34\n36\n38\n40\n45\n47\n53\n54\n56\n64\n67\n68\n75\n76\n\n6.2 Estimating and Scheduling the Work ................................................................................ 77\n...\n6.3 Planning Other Activities................................................................................................... 80\n6.4 Reviewing the Software Plan .............................................................................................\n\n83\n\n7. Running the Project....................................................................................................................\n7.1 Managing the Project .........................................................................................................\n7.1.1 Preparing the Software Team .................................................................................\n7.1.2 Monitoring and Controlling the Project .................................................................\n7.1.3 Communicating with Stakeholders........................................................................\n7.1.4 Maintaining the Software Plan ...............................................................................\n7.1.5 Keeping Project Records........................................................................................\n7.2 Closing Out the Project ......................................................................................................\n\n85\n86\n86\n87\n88\n90\n91\n92\n\nAppendix A . Glossary ....................................................................................................................\n\n95\n\nAppendix B . Building for Reuse..................................................................................................\nAppendix C. COTS, GOTS, Reused, and Other NDI Software Products ...................................\nC.1 COTS Software Products ................................................................................................\n\n101\n103\n103\n\nC.2 Evaluating COTS, GOTS, Reused, and Other NDI Software Products .......................... 105\nC.3 Guidelines for Performing Required Activities Involving COTS, GOTS, Reused, and\nOther NDI Software Products ....................................................................................... 106\nAppendix D. System-Level Considerations.................................................................................\nD.1 System Requirements Analysis .......................................................................................\nD.2 System Design.................................................................................................................\nD.3 Software CI and Hardware CI Integration and Testing...................................................\nD.4 System Qualification Testing ..........................................................................................\nNASA-GB-00 1-96\n\nvi\n\n109\n109\n109\n110\n110\n\n. .\n\nAbbreviations and Acronyms.......................................................................................................\nReferences ....................................................................................................................................\n\n..\n\nvii\n\n113\n115\n\nNASA-GB-001-96\n\nFigures\nPage\n\n. Required Software Process Activities .......................................................................... 6\nFigure 2-2 . System Life Cycle ......................................................................................................... 9\nFigure 2-3 . Software Development Context.................................................................................. 10\nFigure 2 4. Software Maintenance or Enhancement Context ....................................................... 10\nFigure 3-1 . The Five-Step Project Process.................................................................................... 16\nFigure 3-2 . Planning the Software Project .................................................................................... 18\nFigure 3-3 . Tailoring the Project\xe2\x80\x99s Software Process....................................................................\n20\nFigure 5-1 . Phases and Activities .................................................................................................. 26\nFigure 5-2 . Waterfall Development Life-Cycle Model ................................................................. 29\nFigure 5-3 . Incremental Development Life-Cycle Model ............................................................. 31\nFigure 5 4. Evolutionary Development Life-Cycle Model ........................................................... 33\nFigure 5-5 . Package-Based Development Life-Cycle Model ........................................................ 35\nFigure 5-6 . Legacy System Maintenance Life-Cycle Model ......................................................... 36\nFigure 5-7 . Primary Software Engineering Activities ................................................................... 39\nFigure 5-8 . Software Engineering Support Activities ................................................................... 39\nFigure 6-1 . Typical Software Project Organization.......................................................................\n76\nFigure 7-1 . Running the Project .................................................................................................... 85\nFigure 7-2 . Product Handovers ..................................................................................................... 89\nFigure B-1 . High-Reuse Life-Cycle Model ................................................................................. 102\nFigure 2\n1\n.\n\nNASA-GB-00 1-96\n\n...\n\nVlll\n\nPage\n\n. Use of Icons ...................................................................................................................\nTable 2 . Required Activities, Products, and Roles ......................................................................\n1\n.\nTable 2-2 . Sampling of Software Products at Each Organizational Level....................................\nTable 3-1 . Mapping the Five-Step Project Process to This Guidebook........................................\nTable 4-1 . Sample Project Objectives ...........................................................................................\nTable 5-1 . Defining a Life Cycle ...................................................................................................\nTable 5-2 . Summary of Waterfall Development Life-Cycle Model ..............................................\nTable 5-3 . Products and Milestone Reviews for the Waterfall Development Life-Cycle\nModel ..........................................................................................................................\nTable 5 4. Summary of Incremental Development Life-Cycle Model ..........................................\nTable 5-5 . Products and Milestone Reviews for the Incremental Development Life-Cycle\nModel ..........................................................................................................................\nTable 5-6 . Summary of Evolutionary Development Life-Cycle Model ........................................\nTable 5-7 . Products and Milestone Reviews for the Evolutionary Development Life-Cycle\nModel ..........................................................................................................................\nTable 5-8 . Summary of Package-Based Development Life-Cycle Model ....................................\nTable 5-9 . Major Products and Milestone Reviews for the Package-Based Development LifeCycle Model ...............................................................................................................\nTable 5-1 0. Summary of Legacy System Maintenance Life-Cycle Model ...................................\nTable 5-1 1. Products and Milestone Reviews for the Legacy System Maintenance Life-Cycle\nModel ..........................................................................................................................\nTable 5-1 2. Structured Requirements Analysis Method ...............................................................\nTable 5-1 3. Object-Oriented Requirements Analysis Method ......................................................\nTable 5-14 . Prototyping Technique...............................................................................................\nTable 5-1 5 . JAD Workshop Technique.........................................................................................\nTable 5-1 6. Structured Design Method .........................................................................................\nTable 5-1 7. Object-Oriented Design Method ................................................................................\nTable 5-18 . Top-Down Method ....................................................................................................\nTable 5-1 9. Bottom-Up Method ....................................................................................................\nTable 5-20 . Functional Path Method .............................................................................................\nTable 5-2 1. Software Product V&V Summary .............................................................................\nTable 1\n.\n\nix\n\n3\n7\n11\n15\n22\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n41\n42\n43\n44\n46\n46\n51\n51\n52\n57\n\nNASA-GB-00 1-96\n\n. Inspection Method ..................................................................................................... 5 8\nTable 5-23 . Walkthrough Method ................................................................................................. 58\nTable 5-24 . Document Review Method ........................................................................................ 59\nTable 5-25 . Demonstration Method .............................................................................................. 59\nTable 56 . Functional Testing Method ........................................................................................ 60\n2\n.\nTable 5-27 . Structural or Coverage Testing Method .....................................................................\n61\nTable 5-28 . Statistical Testing Method ......................................................................................... 61\nTable 5-29 . Regression Testing Method ....................................................................................... 62\nTable 5-30 . Testing Methods vs . Testing Levels ..........................................................................\n62\nTable 5-3 1. The Cleanroom Method ............................................................................................. 63\nTable 5-32 . Candidate Milestone Reviews ................................................................................... 70\nTable 5-33 . Meetings..................................................................................................................... 71\nTable 5-34 . Presentations .............................................................................................................. 71\nTable 5-3 5 . Demonstrations.......................................................................................................... 72\nTable 6-1 . Three Levels of Estimates and Plans ...........................................................................\n77\nTable 6-2 . Mini-Waterfall ............................................................................................................. 78\nTable 6-3 . Timeboxes.................................................................................................................... 79\nTable 6 4. Required Activities and Related Measures .................................................................. 81\nTable 6-5 . Process Studies ............................................................................................................ 82\nTable 7-1 . Recommended Status Reports and Meetings............................................................... 88\nTable C-1 . Guidelines for Using COTS, GOTS, Reused, and Other NDI Software Products....107\nTable 5 2\n.\n2\n\nNASA-GB-00 1-96\n\nX\n\n1 Introduction\n\n1. Background\nI\n\nT\n\nhe objective of every National Aeronautics and Space Administration (NASA) software\nengineering project is to provide, to the customer, a software product that is engineered to\nsatisfj the customer\xe2\x80\x99s requirements, within determined cost, schedule, and quality\nguidelines.\nThe term software engineering encompasses new development, modification, reuse, reengineering, maintenance, and all other activities resulting in software products. Throughout\nNASA, organizations engineer software products that cover a wide spectrum of characteristics\n(Reference 1):\nApplication domains include flight and embedded software, mission ground support\nsoftware, general support software, science analysis software, research software, and\nadministrative and Information Resources Management (IRM) software.\nTarget operating environments encompass PC-based, mainframe-based, workstationbased, and clientherver-based solutions.\nProduct sizes range from only a few thousand lines of code to more than a million.\nCost and cycle-time requirements vary.\nDesired end-product qualitieereusability, commercialization, and consequences of\nsoftware failure (from minor inconvenience to loss of a mission or loss of life)-also\nvary.\nOne significant lesson learned from many years of software engineering throughout NASA is that\nno single solution can solve every problem. No one life-cycle model, analysis and design method,\ntesting method, product evaluation method, or degree of formality for documents and reviews is\nappropriate for all NASA software projects. To accommodate these variations, each project must\ntailor its software process to acknowledge customer requirements and constraints; goals and\nobjectives for cost, cycle time, and product qualities; and management\xe2\x80\x99s tolerance for risk. Such\ntailoring is the responsibility of the project\xe2\x80\x99s software manager.\n\n1.2 Purpose\nThe purpose of this NASA Software Management Guidebook is twofold. First, this document\ndefines the core products and activities required of NASA software projects. It defines life-cycle\nmodels and activity-related methods but acknowledges that no single life-cycle model is\nappropriate for all NASA software projects. It also acknowledges that the appropriate method for\naccomplishing a required activity depends on characteristics of the software project.\nSecond, this guidebook provides specific guidance to software project managers and team leaders\nin selecting appropriate life cycles and methods to develop a tailored plan for a software\nengineering project.\n\n1\n\nNASA-GB-001-96\n\n1.3 Scope\nThis handbook addresses the engineering of software products, where those products either\n(1) comprise a software system for which this handbook governs the overall engineering effort or\n(2) are part of a hardware-software system for which this handbook governs only the software\nportion.\nSystem engineering management issues are outside of the scope of this guidebook. Section 2.2.4\nplaces the software life cycle in the context of the system life cycle, and Appendix Appendix D.\ndiscusses the system-level considerations required of the software manager and team members.\nThis book also does not cover acquisition of software products; it covers development and\nmaintenance.\n\n1.4 Overview\n1.4.1 Organization\nChapter 2 of this guidebook summarizes the common elements of the overall NASA software\nengineering process. Chapters 3 through 6 describe the NASA software engineering process in\nsomewhat more detail, including summary descriptions of required activities and products, and\nrecommended methods for performing those activities. Chapter 7 discusses running and then\nclosing out software projects.\nAppendix Appendix A. is a glossary of software engineering terms that every software project\nmanager should understand. Appendix Appendix B. provides guidance for building software\ncomponents with reusability in mind. Appendix Appendix C. provides more detailed guidance\nfor incorporating non-developed items (NDIs) into software products. Appendix Appendix D.\nlists additional considerations when the software under development is only part of a larger\nsystem.\n1.4.2 Terminology\nSeveral terms have specific meanings in this document. These terms are used consistently to\nemphasize and ensure understanding of the flexibility built into the software process common\nrequirements. Please refer to the glossary in Appendix Appendix A. for definitions and\ndiscussion of the following key sets of terms:\nDevelopment, maintenance, enhancement\nLife-cycle models, phases, activities, methods\nSoftware configuration items (CIS),systems\nDocumentation, record\n1.4.3 Notation\nTable 1-1 explains the use of icons in this guidebook.\n\nNASA-GB-00 1-96\n\n2\n\nTable 1-1. Use of Icons\nA double exclamation mark highlights related tips that have\nbeen shown effective on NASA programs.\n\nJ\n\nQB<\n\nA check mark highlights required software engineering\nactivities.\nA pair of scissors highlights software process tailoring\ninformation; that is, methods and techniques proven to be\neffective on NASA programs and recommended for use in\nperforming a particular activity.\n\n3\n\nNASA-GB-00 1-96\n\n2. Software Engineering Process Requirements and\nInfrastructure\n\nT\n\nhis chapter summarizes the common aspects of the overall NASA software engineering\nprocess. The first two sections discuss key activities and products that are expected of\nNASA software projects. The third section discusses the roles and responsibilities of\npersonnel at various organizational levels. The final section introduces the concept of a process\nasset library (PAL).\n\n2.1 General Requirements\nEvery software project must meet a number of general requirements in carrying out the detailed\nrequired activities, for example\nThe software team uses systematic, documented methods for all software engineering\nactivities. These methods are described or referenced in the project\xe2\x80\x99s software plan.\nThe software team applies standards for representing requirements; design; code; test\nplans, procedures, and results; and other software products. These standards are\ndescribed or referenced in the software plan.\nDuring the course of the project, the software team identifies and evaluates NDIs,\nincluding commercial-off-the-shelf (COTS), government-off-the-shelf (GOTS), and\nreusable software products, as well as software products not created by project\npersonnel (for example, by other NASA or contractor personnel), for use in fulfilling\nthe project requirements. The scope of the search and the criteria to be used for\nevaluation are as described in the software plan. Software products that meet the\ncriteria are used where practical. Incorporated software products must meet applicable\ndata rights and licensing requirements. Appendix Appendix C. discusses an approach\nfor developing systems that comprise predominantly NDI components.\nDuring the course of the project, the software team identifies opportunities for\ndeveloping software products for reuse and evaluates the benefits and costs of these\nopportunities. Opportunities that provide cost benefits and are compatible with the\ncustomer\xe2\x80\x99s objectives are identified to the customer. The software requirements might\nalso state that the software team develop software products specifically for reuse.\nThe software team identifies critical software CIS (or portions thereof) and develops\nand implements a strategy that addresses the following critical issues:\nSystem resource utilization: critical system resource capacities or constraints are\nimposed on the final product\nSafety: failure of the software could lead to a hazardous state\nSecurity: failure could lead to a breach of system security\nPrivacy: failure could lead to a breach of system privacy\nOther critical characteristics\n\n5\n\nNASA-GB-001-96\n\nThe software team develops an appropriate strategy for such software, including tests\nand analyses, to ensure that the requirements, design, implementation, and operating\nprocedures for the identified software minimize or eliminate the potential for\nhazardous or compromising conditions. The software team records the strategy in the\nsoftware plan, implements the strategy, and produces evidence, as part of required\nsoftware records, that the defined strategy has been carried out successfully.\nThe software team records rationale that will be useful to the software operations and\nmaintenance (O&M) organization for key decisions made in specifying, designing,\nimplementing, and testing the software. The rationale includes trade-offs considered,\nanalysis methods, and other criteria used to make the decisions. The rationale is\nrecorded in documents, code comments, or other media that are transferable to the\nsoftware O&M organization.\n\n2.2 Specific Required Activities and Products\nThe software manager establishes a project software engineering process that is based on the\nNASA software process and consistent with the software requirements. The NASA software\nprocess comprises three categories of activities:\n1. Management\n2. Technical\n3. Software process improvement\nFigure 2-1 illustrates the relationships among the activities; Table 2-1 summarizes the activities\nand the primary products generated as a result of performing each activity.\nMonitor and control software project\n(maintain project software plan\nand records as necessary)\n0\n1\n\n<\n\nPrepare software team\n\n%\n\nIndependently assure software products and activities (SQA)\n\n-.\n-.\n\nManage configuration (SCM)\n\nU\n\n7\n\nm\nT\n2.\nm\n:\n:\n\nParticipate in milestone reviews\nValidate and verib (V&V) software products\n\ncn\n\n%\n4\n\ns\n\no_\nIu\n\n3\n\nPerform required technical activities\n(i.e., software CI requirements definition and\nanalysis through qualification testing,\nincluding interim deliveries)\nuntil final product is delivered\n\nI ime\n\nFigure 2-1. Required Software Process Activities\n\nNASA-GB-001-96\n\n6\n\nI\n\nTable 2-1. Required Activities, Products, and Roles\nActivity\n\nPrimary Products\n\nSoftware project\nplanning\n\nSoftware plan\nProject planning review\n\nSoftware CI\nrequirements\ndefinition and\nanalvsis\nSoftware CI design\n\nSoftware CI requirements specification\nV&V records for requirements definition and analysis products\nSoftware requirements milestone review\n\nSoftware CI\nimplementation\nand testing\n\nSoftware CI\nqualification testing\nPreparation for\nsoftware delivery\n\nSoftware project\nclose-out\n\nSoftware product\nvalidation and\nverification (V&V)\nSoftware\nconfiguration\nmanagement\nSoftware quality\nassurance\nMilestone reviews\n\nSoftware CI design specification\nV&V records for design products\nSoftware design milestone review\nUnit-level design\nImplementation test plans, procedures\nImplemented, integrated, tested software\nV&V records for implementation and testing products\nQualification test readiness milestone review\nQualification test plan, procedures\nQualification tested software\nV&V records for qualification testing products\nExecutable software\nSoftware source files\nVersion description\nAs-built software description\nSoftware user\'s guide\nSoftware project history\nSoftware project lessons learned and recommendations for\nimprovement\nSoftware Droiect close-out data\nSoftware product V&V records\n\nSoftware design architect\nSoftware detail designer\nSoftware QA representative\nSoftware implementer\nSoftware unit tester\nSoftware integrator and tester\nSoftware QA representative\n\nSoftware configuration management plan (part of the software\nPlan)\nControlled software products\nSoftware configuration management records\nSoftware quality assurance plan (part of the software plan)\nSoftware quality assurance records\nMilestone reviews\n\nSoftware configuration\nmanager\nRest of software team\nSoftware QA representative\nSoftware QA representative\n\nSoftware team\npreparation\n\nTraining records\n\nProject monitoring\nand controlling\n\nManagement indicators\nProject status reviews\nSoftware technology study plans and study results\nDefect causal analysis recommendations\nSystem and operations concept, operational scenarios\nSystem requirements specification\nSystem design specification\nHardware and software CI integration and test plan,\nprocedures\nSystem qualification test plan, procedures\nQualification tested system\nV&V records for qualification testing products\n\nSoftware process\nimprovement\nSystem-level\nconsiderations\n\nKey Roles\nSoftware manager\nSoftware team leader\nSoftware QA representative\nSoftware requirements analyst\nSoftware QA representative\n\n7\n\nSoftware qualification tester\nSoftware QA representative\nSoftware configuration\nmanager\nRest of software team\nSoftware QA representative\nSoftware manager\nSoftware QA representative\n\nSoftware team\nSoftware QA representative\n\nSoftware manager\nRest of software team\nSoftware QA representative\nSoftware manager\nRest of software team\nSoftware QA rewesentative\nSoftware manager\nSoftware QA representative\nEntire software team\nSoftware QA representative\nEntire software team\nSoftware QA representative\n\nNASA-GB-00 1-96\n\n2.2.1 Management Activities\nThe following are required management-related activities:\nSoftware project planning\nSoftware team preparation\nSoftware project monitoring and control\nSoftware project close-out\n\n2.2.2 Technical Activities\nThe following are required technical activities, each of which produces one or more specific\nsoftware products. They may overlap, may be applied iteratively, may be applied differently to\ndifferent elements of software, and are not necessarily performed in the order listed. (Remember\nthat activities are not synonymous with phases. Refer to the Glossary (Appendix Appendix A. )\nfor definitions and discussion regarding activities versus phases.)\nSoftware CI requirements definition and analysis\nSoftware CI design\nSoftware CI implementation and testing\nSoftware CI qualification testing\nPreparation for software delivery\nThe following are required support activities that are performed in conjunction with each of the\nabove technical activities:\nSoftware product validation and verification (V&V)\nMilestone reviews\nSoftware configuration management (SCM)\nSoftware quality assurance (SQA)\n\n2.2.3 Software Process Improvement Activities\nEvery software project presents an opportunity to study and improve the software process. One\nmechanism used in the NASA software process improvement program is to study the application\nof new technologies.\' Process studies are conducted any time an unproven life-cycle or activityrelated method is selected by the software manager. The NASA Software Process Improvement\nGuidebook (Reference 2 ) describes the approach NASA uses to study and understand the effects\nof new technologies on software products and processes.\n\n2.2.4 System-Level Considerations\nFigure 2-2 illustrates the concept of a system life cycle. This section clarifies some terms related\nto the system life cycle.\n1\n\nA "new" technology is one that has not been proven to be effective in practice in a particular NASA application\narea or domain. It may have been proven effective elsewhere, however.\n\nNASA-GB-001-96\n\n8\n\nDEVELOP SYSTEM\nWITH INITIAL\nOPERATIONAL\n\nFigure 2-2. System Life Cycle\n\nIn this guidebook, the term system refers to the operational entity that the organization is\nresponsible for developing, maintaining, or enhancing. That is, if the organization is responsible\nfor developing several software and hardware CISand is responsible for integrating them into an\noperational entity, then the collection of those CIS is the system. If, however, the organization is\nresponsible for developing a single software CI, which may be integrated into (for example) a\nground support system by a different NASA organization, then the software CI itself is the\nsystem referred to in this guidebook.\nSome of the systems that NASA organizations develop, or maintain and enhance, include\nmultiple CIS. There may be a mix of software CIS and hardware CIS, or the system may be only\nhardware or only software. This guidebook discusses the activities associated with the\ndevelopment and with the maintenance and enhancement of the software CI elements of a\nsystem, but also includes very high-level summaries of the relevant system-level activities to help\nthe reader understand the context in which the software effort may take place. When the system\ncomprises only software CIS, then most of the system- and CI-level products are one and the\nsame.\n\nDevelopment (see Figure 2-3) is the creation and installation of an operational system that meets\nan initially defined set of system requirements. Once the system is operational, subsequent\nchanges are considered maintenance or enhancement (see Figure 2 4 ) . Many of the maintenance\nand enhancement activities are the same or similar to those used in development. The NASA\nsoftware process applies equally to development and to maintenance and enhancement efforts.\n(The figures reflect the software emphasis of this guidebook.)\n\n9\n\nNASA-GB-001-96\n\nREUSABLE OR\n\nREUSABLE OR\n\nFigure 2-3. Software Development Context\n\nFigure 2-4. Software Maintenance or\nEnhancement Context\n\nWhen the software being engineered is a part of a larger system, additional system-level\nconsiderations may need to be taken into account. Appendix Appendix D. addresses those\nconsiderations.\n\n2.3 Software Process Responsibilities\nThe subsections that follow summarize the basic responsibilities for maintaining and using the\nNASA software process at the various organizational levels. Table 2-2 provides examples of\nsoftware process-related products at each level.\n\nNASA-GB-001-96\n\n10\n\nTable 2-2. Sampling of Software Products at Each Organizational Level\n\nLevel\n\n1\nHeadquarters,\nIV&V Facility, and\nSoftware Working Group\n\n2\nCenter and\nIntra-Center Elements\n(Directorates, Divisions)\n\n3\nBranches and\nSoftware Projects\n\nProducts\nNASA Software Strategic Plan\nThis Software Management Guidebook\nOther agency-wide software engineering\n(management, development, and assurance)\nguidebooks, plans, policies, and standards\nDomain guidance\nIndependent in-progress assessments of highprofile development projects\xe2\x80\x99 activities and\nproducts\nSoftware ena ineerina train ina\nSoftware quality assurance plan (to include SQA\nprocedures), which is referenced by the software\nplan and is administered by a center-level\nsoftware quality assurance organization that is\nindependent of the individual projects\nApproach for reviewing and approving projects\xe2\x80\x99\nsoftware plans\nApproach for developing and approving lower\nlevel standards. Drocedures. and Dlans\nSoftware engineering (management and\ndevelopment) standards and processes\nSoftware plan\nPAL and other related software assets\nProject-related software training\nSoftware products resulting from applying the\nprocess defined bv the plan\n\n2.3.1 Level 1: NASA Headquarters, IV&V Facility, and Software Working Group\nThe NASA Software Strategic Plan (Reference 3 ) , completed in July 1995, complements the\nAgency-wide strategic vision and mission statements while focusing on software within NASA.\nThis plan was developed by the NASA Software Working Group (SWG) under the auspices of\nthe NASA Software Independent Validation and Verification (IV&V) Facility at Fairmont, West\nVirginia, sponsored by the Office of Safety and Mission Assurance (OSMA) at Headquarters\n(HQ). The goals and implementation strategies of the NASA Software Strategic Plan address\n(1) defining and improving software engineering processes (including processes for management,\ndevelopment, and quality assurance), (2) transferring software product and process technologies,\nand ( 3 ) maintaining a core competency in software. Those three elements comprise the NASA\nSoftware Program.\nThe NASA Software Working Group is the implementation vehicle for the NASA Software\nProgram. The role of the SWG is as follows:\nDefine, refme, and implement the goals of the NASA Software Strategic Plan\nProvide guidance to all NASA software-related activities\n\n11\n\nNASA-GB-001-96\n\nEnsure that available software processes are disseminated\nThe SWG has one or two representatives from each center and is chaired by a member of the\nIV&V Facility.\nA 1993 survey of NASA Centers (Reference 1) found that NASA does not have a common set of\nsoftware standards that is used across the Agency in the manner of the Department of Defense\xe2\x80\x99s\nmilitary standard (Reference 4). One focus of the NASA SWG is to produce guidebooks and\nsupporting training on the basis of NASA-wide experience in key areas such as project\nmanagement, assurance, risk management, and software process improvement (see Section 2.4\nfor examples).\n2.3.2 Level 2: Center and Intra-Center Elements\nFor the most part, individual centers have given directorates, divisions, and offices the\nresponsibility for developing their own standards and common processes. According to the 1993\nsurvey (Reference l), two NASA centers (the Marshall Space Flight Center (MSFC) and the Jet\nPropulsion Laboratory (JPL)) have written software development standards that are baselined at\nthe center level and include a formal waiver process. The other centers have software standards\nand processes implemented at a lower organizational level. Center-level activities typically focus\non defining approaches for plan review and approval. For critical mission systems, however, the\nSQA organization at each center has the responsibility to ensure that, throughout the life cycle of\nthe project, software engineering activities are performed and software products are prepared in\naccordance with the software project\xe2\x80\x99s software plan.\n2.3.3 Level 3: Branches and Software Projects\nBranch managers and software project managers share responsibility for developing and\napproving lower level standards, procedures, and plans for implementing the software process in\ntheir areas. They are responsible for the following:\nIdentifjing, developing, and maintaining those lower level standards, procedures, and\nplans that are unique to a particular development effort and those that are common to\nfamilies of software products\nProviding training for their development efforts\nEstablishing and maintaining local PALSthat contain locally specific process assets\nLower level software engineering process functions define, develop, and implement needed\nsoftware process assets that are not provided in a higher level NASA PAL. They are also\nresponsible for defining, developing, and implementing a software process improvement\nprogram that supports the needs of the projects and branches and for providing software-related\ninformation required by higher level measurement and process improvement programs.\nEach project must plan its own specific approach for accomplishing the software work assigned\nto it. The approach must be documented in a software plan and must comply with the local and\nhigher level process requirements.\n\nNASA-GB-001-96\n\n12\n\n2.4 Process Assets\nThe software team uses process assets from an experience-based PAL. A PAL is a compilation of\nNASA estimating and planning models, historical data, life-cycle models, activity definitions,\nproduct standards and templates, and examples of good practices that are available to a software\nproject for developing, maintaining, and implementing its defined process. A PAL may be\nimplemented at any and all organizational levels, as appropriate, within a NASA organization\nresponsible for systems development.\n\nAn organization\xe2\x80\x99s PAL typically contains the following products:\nHigher level NASA process assets\nAll of the local organization\xe2\x80\x99s software process definition documents\nSoftware-related guidebooks, handbooks, and white papers, as they become available\nRecommended activity and method definitions and product standards applied within\nthe organization\nTraining material related to the organization\xe2\x80\x99s software process\nPlans and results from software process studies\nOther assets that the organization determines to be applicable\nThis Software Management Guidebook and other NASA headquarters-level software-related\nproducts (for example, the NASA Software Measurement Guidebook (Reference 5 ) and the\nNASA Software Process Improvement Guidebook (Reference 2)) are included in every NASA\nPAL. Every PAL must also include copies of NASA Management Instruction @MI) 2410.1OB\nand other software-related NMIs.\nNASA\xe2\x80\x99s Software Management Guidebook is a primary source of guidance to the manager in\npreparing the project\xe2\x80\x99s software plan (see Section 3.2) and selecting appropriate elements from\nthe PAL:\nLife-cycle models (for example, waterfall, iterative refinement, spiral)\nMilestone reviews (for example, requirements reviews, design reviews)\nProducts and product formats (for example, degree of formality, packaging)\nEngineering methods (for example, structured approach, object-oriented approach, the\nCleanroom method)\nProduct V&V methods (for example, peer review methods, testing methods)\nProduct control methods (for example, degree of formality)\nUsing these assets, each software manager prepares a project-specific software plan that defines\nthe selected life-cycle model, methods, tools, and product standards the software team will use.\nManagers may also use their organizations\xe2\x80\x99 PALS (of varying degrees of formality and structure)\nas sources of more detailed software process assets that have been tailored for specific\napplication domains. This common tailoring approach allows each software project to draw on\nproven, successful methods appropriate to satisfy their customers\xe2\x80\x99 needs in a predictable,\nefficient, and cost-effective manner.\n\n13\n\nNASA-GB-001-96\n\n3. The Software Project\xe2\x80\x99s Process\n\nT\n\nhis chapter summarizes the software project\xe2\x80\x99s process, which includes required activities\nfrom developing a software plan that meets the unique needs of each customer through\ndelivery of the final software product and close-out of the project. This chapter also\nbriefly discusses the project software plan, which documents the project\xe2\x80\x99s process.\n\n3.1 The Five-Step Project Process\nFigure 3-1 is a summary of the five key steps required to take a software engineering project\nfrom inception through final delivery and close-out. Subsequent chapters of this guidebook\nexpand on each of the steps. Table 3-1 maps the five steps to the applicable section of this\nguidebook\nTable 3-1. Mapping the Five-Step Project Process to This Guidebook\nProject\nProcess Step\n\nHandbook Section\n\nSTEP 1\n\nChapter 4: Beginning to Plan the Project: Understanding the Scope of Work\n\nSTEP 2\n\nChapter 5: Defining the Technical Approach\n\nSTEP 3\n\nChapter 6: Finishing the Software Plan-Defining\n\nSTEP 4\n\nSection 7.1: Managing the Project\n\nSTEP 5\n\nSection 7.2: Closing Out the Project\n\n15\n\nthe Management Approach\n\nNASA-GB-00 1-96\n\nFigure 3-1. The Five-Step Project Process\n\nNASA-GB-00 1-96\n\n16\n\n3.2 Documenting the Project\xe2\x80\x99s Process-The\n\nSoftware Plan\n\nActivity Requirements\n\n17\n\nNASA-GB-00 1-96\n\nMonitor and control software project\n(maintain project software plan\nand records as necessary)\nPrepare software team\n\nI ime\n\nFigure 3-2. Planning the Software Project\n\nEvery software project has its own software process. The project\xe2\x80\x99s software plan documents the\nprocess and provides a disciplined approach to organizing and managing a software project. The\nexistence of a plan does not guarantee project success; the key to successful software\nmanagement is generating and maintaining a realistic, usable plan and then following it.\nFollowing the plan involves not only maintaining the plan itself, but also performing activities to\nmeasure progress and performance against the plan. Use the plan to assist in recognizing danger\nsignals, and take early and appropriate actions to solve problems.\nThis guidebook does not mandate a specific format for a software plan; however, every PAL\nshould include good examples of software management plans and a reusable template for such a\nplan. An excellent example of such a template is the Reusable Software Management Plan\ndeveloped by the Software Assurance Technology Center at the Goddard Space Flight Center\n(GSFC) (Reference 6). It includes an on-line help tool for tailoring the text to an individual\nsoftware project. Another excellent example is provided in the NASA Software Engineering\nLaboratory\xe2\x80\x99s Manager \xe2\x80\x99s Handbook for Software Development (Reference 7). Depending on the\nspecific development environment, items may be arranged differently or new material may be\nadded. In nearly all cases, a project\xe2\x80\x99s software plan will be contained in more than one physical\ndocument, because many of the plan\xe2\x80\x99s components will be common to multiple projects (for\nexample, a QA or CM plan). However, the project\xe2\x80\x99s software plan must contain references to all\nrequired topics that are packaged separately.\nBy completing the initial plan early in the life cycle, the manager becomes familiar with the\nessential aspects of the specific software engineering effort:\nScope and requirements of the project\nOverall schedule and milestones\nStaff requirements\n\nNASA-GB-001-96\n\n18\n\nEach manager defines the project\xe2\x80\x99s technical approach, driven by the customer\xe2\x80\x99s requirements,\nconstraints, goals, and objectives; management\xe2\x80\x99s risk tolerance level; and the target and\ndevelopment or maintenance environments. The technical approach is described in the software\nplan and should concentrate on information unique to, or tailored for, a specific project. Simply\nreference applicable documents that contain standard policies, guidelines, standards, and\nprocedures to be applied; do not restate that information in detail. Begin to write the plan as soon\nas information about the project\xe2\x80\x99s definition and scope is available. The plan should be available\nwithin the first 30 to 60 days of the project, except for information that will not be available until\nlater in the life cycle (indicate who will supply any missing information and when it will be\nprovided). Distribute copies of the plan to all levels of project management, to the client, and to\nthe software team.\nThe following additional points are included here for completeness and to ensure common\nunderstanding. While developing the plan and running the project\nIncorporate lessons learned from other similar or related software projects. Read\nsoftware project history reports from those projects.\nUse sound judgment. One significant lesson learned from performing software\nengineering on NASA projects is that no single life-cycle model, analysis and design\nmethod, testing method, degree of formality of documents and reviews, etc. is\nappropriate for every NASA software project. Use of sound, professional judgment is\nexpected with respect to decisions related to such topics.\nYou can always do more. This guidebook defines the minimum required software\nactivities and products, not the precise set of things to do. If a manager or project\nteam feels that something additional would be useful, they are empowered to do so.\nRemember that you are not alone in this planning effort; you are not the first nor will\nyou be the last to plan a new or different software project. Consult with other software\nmanagers for their opinions, advice, and ideas.\nFigure 3-3 illustrates the inputs to the process of preparing a tailored software plan:\nThe customer\xe2\x80\x99s requirements and constraints as well as his or her goals and objectives\nThe risk tolerance levels of the project manager and of the project\xe2\x80\x99s organizational\nmanagement\nReusable process assets from local and higher level PALS\nThe NASA Software Management Guidebook (this document)\nThe NASA Software Measurement Guidebook (Reference 5 )\nThe NASA Software Process Improvement Guidebook (Reference 2 )\n\n19\n\nNASA-GB-001-96\n\nFigure 3-3. Tailoring the Project\xe2\x80\x99s Software Process\n\nSoftware Measurement Guidebook presents information on the purpose and importance\nof measurement for\n1. Understanding and modeling the software engineering process\n2. Aiding in the management of software projects\n3 . Guiding improvement in software engineering processes\nIt discusses the specific procedures and activities of an organizational measurement program and\nthe roles of the people involved. The guidebook also clarifies the role that measurement can and\nmust play in the goal of continual, sustained improvement for all software production and\nmaintenance efforts.\nThe Software Process Improvement Guidebook is a companion document that describes how a\nsoftware project uses a new technology (that is, one that has not yet been proven anywhere within\nthe organization) in a controlled fashion and quantitatively assesses the effect of the technology\non the products generated and the process used.\n\nNASA-GB-001-96\n\n20\n\n4. Beginning to Plan the Project: Understanding the\nScope of Work\n\nT\n\nhis chapter describes activities that are required to begin planning the project. Software\nproject planning need not wait, indeed must not wait, for receipt of the final software\nrequirements to begin the project planning process. It should begin as soon as project\npersonnel become aware of the intent of the customer to initiate a new software effort, and it\nshould be coordinated, as much as possible, with other NASA groups responsible for related\nrequirements.\nActivity Requirements\n\nThe rest of this chapter provides guidance in accomplishing this required activity.\n\n4.1 Ascertaining Customer Requirements and Constraints\nAscertain the customer\xe2\x80\x99s requirements and constraints from the software requirements\nspecifications, discussions with the customer, and any other appropriate means. The customer\nmay have only a general idea of the software requirements; determination of detailed\nrequirements may follow later or may even be part of the project. (The same applies to\ncharacteristics of products to be delivered.)\nNASA experience has been that the more the end user of the software product is involved\nthroughout the product\xe2\x80\x99s engineering life cycle, the more likely the expected product will result.\nThe customer, however, is not always the end user of the software product. When this is the case,\nwork with the customer to encourage that the end user be as involved throughout as much of the\nproduct\xe2\x80\x99s life cycle as possible. This involvement is particularly crucial during the requirements\nanalysis and design activities early in the life cycle. It is also important to try to get end-user\ninvolvement at milestone reviews.\n\n21\n\nNASA-GB-001-96\n\n4.2 Ascertaining Customer Goals and Objectives\nDuring meetings with the customer, ascertain the customer\xe2\x80\x99s organizational goals and specific\nproject objectives regarding overall cost, schedule, and product qualities. Also understand the\ngoals of higher levels of the organization. The software manager and customer use the goals to\ndetermine and agree on the appropriate robustness desired of product, formality and level of\ndetail and polish in documents, milestone review presentations, and so on. The software manager\ndefines, in the project\xe2\x80\x99s software plan, the specific approach that best addresses the application\ndomain, the objectives established for the project, and the size of the effort. Table 4-1 provides\nexamples of the types of objectives that might be established for a project.\nTable 4-1. Sample Project Objectives\nObjective\n\nExamples\n\nMinimize cost to develop\nMinimize cost to maintain\nDeliver by fixed date (for example, 90 days before\nlaunch)\nMaximize reusability\nMaximize robustness\nMaximize freedom from defects\nMaximize performance (for example, response\ntime)\n\ncost\nSchedule\nProduct\nQuaIities\n\nI\n\nNASA-GB-001-96\n\nI\n\n0\n\nMaximize maintainability or extensibility\n\n22\n\n4.3 Understanding Management\xe2\x80\x99s Risk Tolerance\nTo minimize costs or cycle time, it is sometimes necessary to introduce risk. Understand what\nlevel of risk tolerance is acceptable to the management of both the development or maintenance\norganization and the customer\xe2\x80\x99s organization, and include a risk management approach in the\nsoftware plan that accommodates that level of risk. (See Section 6.3.)\nAn approach for continuous risk management is explained in detail in the Software Engineering\nInstitute\xe2\x80\x99s (SEI\xe2\x80\x99S) technical report Continuous Risk Management Guidebook (Reference S ) ,\nrecently adopted by the NASA Software Program for use throughout the Agency.\n\n4.4 Understanding Products to be Delivered and Their Characteristics\nReach agreement with the customer regarding the products to be delivered and the goals for each\nproduct with respect to its cost, schedule, and qualities. Remember that initially the customer\nmay have only a general idea of the product requirements; determination of specific products\nmay follow later or may even be part of the project.\n\n4.4.1 Documentation\nThe amount of time and effort expended in producing a document can vary considerably. (Refer\nto the glossary for the definition of documentation.) Factors that affect document production\ninclude the following:\nAudience for the document-points of view and levels of experience of the readers\nGeneral format\nLevel of detail\nDegree of formality\nFor deliverable documents, the following additional factors apply:\nPaper vs. electronic delivery (if paper, the number of copies per delivery)\nNumber of interim deliverable versions\n\n4.4.2 Software Product Releases\nUse interim software releases (as opposed to the \xe2\x80\x9cbig bang\xe2\x80\x9d approach) either to satisfj early\noperational needs of the customer or as a risk mitigation technique (for example, to ensure the\nfeasibility of high-risk requirements). Integrating and testing the system in parts helps to localize\n\n23\n\nNASA-GB-001-96\n\nerros and reduce debugging time. Early releases also help to build the customer\xe2\x80\x99s confidence\nlevel, as well as that of management, that the project is on track.\n\n4.4.3 Milestone Reviews\nMilestone reviews and associated review material should also be treated as deliverable products.\nThat is, discuss their goals with the customer in terms of responsibilities, cost, schedule, and\nqualities. Many of the factors that affect producing documentation-oriented products also apply\nto milestone reviews. (See Section 5.2.9 for details.)\n\nNASA-GB-00 1-96\n\n24\n\n5. Defining the Technical Approach\n\nC\n\nhapter 4 described the first step in the planning process: understanding the scope of the\nwork to be performed. The next step is to define a technical approach that best\naccomplishes the work, which is the subject of this chapter. It includes selecting an\nappropriate life-cycle model along with corresponding activities, methods, and products.\nActivity Requirements\n\nAlthough the following descriptions of process requirements and tailoring guidelines are\nextensive, a new project manager should not feel overly intimidated by the length of this chapter.\nIf your new project is similar to one that another project manager has managed previously, then\nvisit your PAL or seek out that manager and reuse the appropriate tailoring from the earlier\nproject. Complete the applicable additional process tailoring and you will be ready to estimate\nand plan your project based on the specifics of your process. After you have defmed the technical\napproach for your project, everything you need to know to complete the management approach\nand then run the project is provided in the following two chapters, which are much shorter.\n\n2\n\n\xe2\x80\x9cProven\xe2\x80\x9d means used successfully within NASA.\n\n25\n\nNASA-GB-001-96\n\n5.1 Selecting an Appropriate Life-Cycle Model\nThis section identifies the life-cycle models recommended for use on NASA software projects. A\nlife-cycle model comprises one or more phases (for example, a requirements definition phase, a\ndesign phase, a test phase). Each phase is defined as the time interval between two scheduled\nevents. For example, in the waterfall life-cycle model, the design phase is defined as the period\nbetween the software specification review and the critical design review (CDR).\nWithin each phase, one or more activities are executed. For example, during the waterfall\nmodel\xe2\x80\x99s design phase, the design activity is performed; the test planning activity for qualification\ntesting may be done at the same time. In most cases, activities neither begin nor end precisely at\nthe phase boundaries; rather, they overlap adjacent phases, as illustrated in Figure 5-1.\nPhases.\n\nI\nIY\n\n2\nLL\n\nW\n\nFigure 5-1. Phases and Activities\n\nVarious methods (or techniques) may be used in the performance of an activity. For example,\nobject-oriented design is one proven design method; structured design is another.\nThis document does not mandate any particular software life-cycle model, and the order of\nactivities described here is not intended to conform to any particular model. Few specific\nmethods are mandated for required activities. These decisions are left to the software manager,\nwho selects an appropriate life-cycle model and activity-related methods and defines them in the\n\nNASA-GB-001-96\n\n26\n\nproject\xe2\x80\x99s software plan. This chapter contains guidance on selecting an appropriate,\nrecommended life-cycle model and methods for many activities.\nFor convenience, Table 5-1 provides the definitions (see the Glossary) of several important terms\nused extensively in this section.\nTable 5-1. Defining a Life Cycle\nTerm\n\nDefinition\n\nSoftware life cycle\n\n\xe2\x80\x9cThe period of time that begins when a software product is conceived and ends\nwhen the software is no longer available for use\xe2\x80\x9d (Reference 9). A life cycle is\ntypically divided into life-cycle phases.\n\nLife-cycle model\n\nA framework on which to map activities, methods, standards, procedures, tools,\nproducts, and services (for example, waterfall, spiral).\n\nLife-cycle phase\n\nA division of the software effort into non-overlapping time periods. Life-cycle\nphases are important reference points for the software manager. Multiple\nactivities may be performed in a life-cycle phase; an activity may span multiple\nphases.\n\nActivity\n\nA unit of work that has well-defined entry and exit criteria. Activities can usually\nbe broken into discrete steps.\n\nMethod\n\nA technique or approach, possibly supported by procedures and standards, that\nestablishes a way of performing activities and arriving at a desired result.\n\nQB<\n\nFive life-cycle models are summarized in the following subsections. These models\nare recommended on the basis of NASA\xe2\x80\x99s experience applying them successfully at\nvarious centers. Development is addressed before maintenance, and the\ndevelopment life-cycle models are ordered from the simplest and most familiar to\nwhat may be the most complex and least familiar.\nWaterfall development life-cycle model\nIncremental development life-cycle model\nEvolutionary development life-cycle model\nPackage-based development life-cycle model\nLegacy system maintenance life-cycle model\n\n27\n\nNASA-GB-001-96\n\n5.1 . Waterfall Development Life-Cycle Model\nI\nTable 5-2 summarizes the life cycle defmed by the waterfall development model.\n\nSummay description\nand discussion\n\nThe waterfall (single-build) life-cycle model is essentially a oncethrough-do-each-step-once approach. Simplistically, determine user\nneeds, define requirements, design the system, implement the\nsystem, test, fix, and deliver the system (Reference I O ) .\nThis model is illustrated in Figure 5-2. Major products and milestone\nreviews for this life-cvcle model are summarized in Table 5-3.\n\nAdvantages\n\nWell-studied, well-understood, and well-defined\nEasy to model and understand\n\nuct is not available for initial use until the project is nearly\n\nThe design and technology are proven and mature\n\nNASA-GB-00 1-96\n\n28\n\n~\n\nRequirements analysis\n~\n\n&\nArchitectural design\n\\\n\n1\n\nDetailed design\n\nt\nImplementation & testing\n\nt\nQualification testing\n\n1\nDelivery\n\nFigure 5-2. Waterfall Development Life-Cycle Model\nTable 5 3 Products and Milestone Reviews for the Waterfall Development Life-Cycle Model\n-.\nLifecycle phase\n\nProject planning\nRequirements definition\nand analysis\nArchitectural design\n\nDetailed design\nImplementation and\ntesting\n\nQualification testing\n\nMajor products\n\nSoftware plan\nSoftware requirements\nspecification (SWRS)\nSoftware design specification\n(SWDS), preliminary\nQualification test plan\nPreliminarv user\xe2\x80\x99s guide\nSoftware design specification\n(SWDS), detailed\nUnit-level design\nImplemented, tested software\nQualification test procedures\nDraft user\xe2\x80\x99s auide\nQualification-tested software\nQualification test report\nFinal user\xe2\x80\x99s guide\nAs-built software description\n\n29\n\nMilestone reviews\n\nNone\nSoftware Specification Review (SSR)\nPreliminary Design Review (PDR)\n\nCritical Design Review (CDR)\nQualification Test Readiness Review\n(QTRR)\n\nAcceptance Test Readiness Review\n(ATRR)\n\nNASA-GB-00 1-96\n\n5.1.2 Incremental Development Life-Cycle Model\nTable 5 4 summarizes the life cycle defined by the incremental development model.\nTable 5-4. Summary of Incremental Development Life-Cycle Model\nSummary description\nand discussion\n\nThe incremental (multi-build) life-cycle model determines user needs\nand defines a subset of the system requirements, then performs the\nrest of the development in a sequence of builds. The first build\nincorporates part of the planned capabilities, the next build adds\nmore capabilities, and so on, until the system is complete (Reference\nIO).\nThis model is illustrated in Figure 5-3. Major products and milestone\nreviews for this life-cycle model are summarized in Table 5-5.\n\nAdvantages\n\nDisadvantages\n\nNASA-GB-00 1-96\n\nReduces risks of schedule slips, requirements changes, and\nacceptance problems\nIncreases manageability\nInterim builds of the product facilitate feeding back changes in\nsubsequent builds\nInterim builds may be delivered before the final version is done;\nthis allows end users to identify needed changes\nBreaks up development for long lead time projects\nAllows users to validate the product as it is developed\nAllows software team to defer development of less well\nunderstood requirements to later releases after issues have been\nresolved\nAllows for early operational training on interim versions of the\nproduct\nAllows for validation of operational procedures early\nIncludes well-defined checkpoints with customer and users via\nreviews\nLike the waterfall life-cycle model, most if not all requirements\nmust be known up front\nSensitive to how specific builds are selected\nPlaces products (particularly requirements) under configuration\ncontrol early in the life cycle, thereby requiring formal change\ncontrol procedures that may increase overhead, particularly if\nrequirements are unstable\nProject is similar to one done successfully before\nMost of the requirements are stable and well-understood; but\nsome TBDs may exist\nThe design and technology are proven and mature\nTotal project duration is greater than one year or customer needs\ninterim release(s)\n\n30\n\n1\n1\nDefine or Derive\nSoftware CI\nRequirements\n\nDesign Software CI\n\nI\n\nI\n\nRelease 1\nImplement and Test Software CI\n\nI\n\nQualification\nTest\n\nI\n\nI\n\nRelease 2\nRequirements\nAnalysis and\nDesign\n\nImplement and\nTest Software CI\n\nQualification Test\n\na\na\na\nFinal Release\n\nI\n\nRequirements\nAnalysis and\nDesign\n-\n\nImplement and\nTest Software CI\nI\n\nI\n\nI\n\nFigure 5 3 Incremental Development Life-Cycle Model\n-.\nTable 5-5. Products and Milestone Reviews for the Incremental Development Life-Cycle Model\nLifecycle phase\n\nProject planning\nRequirements definition\nand analysis\nArchitectural design\n\nDetailed design\n\nImplementation and\ntesting\n\nQualification testing\n\nMajor products\n\nSoftware plan\nSoftware requirements\nspecification (SWRS)\nSoftware design specification\n(SWDS), preliminary\nQualification test plan\nPreliminarv user\xe2\x80\x99s guide\nSoftware design specification\n(SWDS), detailed through at least\nthe first build\nSoftware design specification\n(SWDS), detailed through at least\nthe next build\nUnit-level design\nImplemented, tested software\nQualification test procedures\n-Draft user\xe2\x80\x99s guide\nQualification-tested software\nQualification test report\nFinal user\xe2\x80\x99s guide\nAs-built software descriDtion\n\n31\n\nMilestone reviews\n\nNone\nSoftware Specification Review (SSR)\nPreliminary Design Review (PDR)\n\nCritical Design Review (CDR)\n\nBuild Design Review (BDR)\n\nQualification Test Readiness Review\n(QTRR)\n\nAcceptance Test Readiness Review\n(ATRR)\n\nNASA-GB-00 1-96\n\n5.1.3 Evolutionary Development Life-Cycle Model\nTable 5-6 summarizes the life cycle defined by the evolutionary development model.\nTable 5-6. Summary of Evolutionary Development Life-Cycle Model\nSummary description\nand discussion\n\nLike the incremental development model, the evolutionary life-cycle model\nalso develops a system in builds, but differs from the incremental model in\nacknowledging that the user needs are not fully understood and not all\nrequirements can be defined up front. In the evolutionary approach, user\nneeds and system requirements are partially defined up front, then are\nrefined in each succeeding build. The system evolves as the\nunderstanding of user needs and the resolution of issues occurs.\nPrototyping is especially useful in this life-cycle model. (The evolutionary\ndevelopment life-cycle model is sometimes referred to as a spiral\ndevelopment model, but it is not the same as Boehm\xe2\x80\x99s spiral model\n(Reference 11). This model is also sometimes referred to as a prototyping\nlife-cycle model, but it should not be confused with the prototyping\ntechnique defined in Section 5.2.1.)\nThis life-cycle model is illustrated in Figure 5-4. Major products and\nmilestone reviews for this model are summarized in Table 5-7.\n\nAdvantages\n\nDisadvantages\n\nNASA-GB-00 1-96\n\nNot all requirements need be known up front\nAddressing high risk issues (for example, new technologies or unclear\nrequirements) early may reduce risk\nLike the incremental life-cycle model, interim builds of the product\nfacilitate feeding back changes in subsequent builds\nUsers are actively involved in definition and evaluation of the system\nPrototyping techniques enable developers to demonstrate functionality\nto users with minimal of effort\nEven if time or money runs out, some amount of operational capability\nis available\nBecause not all requirements are well-understood up front, the total\neffort involved in the project is difficult to estimate early. Therefore,\nexpect accurate estimates only for the next cycle, not for the entire\ndevelopment effort.\nLess experience on how to manage (progress is difficult to measure)\nRisk of never-ending evolution (for example, continual \xe2\x80\x9cgold plating\xe2\x80\x9d)\nMay be difficult to manage when cost ceilings or fixed delivery dates\nare specified\nWill not be successful without user involvement\nRequirements or design are not well-defined, not well-understood, or\nlikely to undergo significant changes\nNew or unproved technologies are being introduced\nSystem capabilities can be demonstrated for evaluation by users\nThere are diverse user groups with potentially conflicting needs\n\n32\n\nFigure 5-4. Evolutionary Development Life-Cycle Model\nTable 5-7. Products and Milestone Reviews for the Evolutionary Development Life-Cycle Model\nLife-cycle phase\n\nConcept definition\nRequirements and\narchitecture\ndefinition\n\nImplementation\n\nIntegration and test\n\nInstallation and\nacceptance\nOperations and\nmaintenance\n\nMajor products\n\nInitial System Development Plan to be\nupdated in later phases\nPreliminary requirements document\nArchitectural design document containing\nthe infrastructure plus the architecture of\neach release as it evolves\nRequirements traceabilitv map\nEvolutionary Implementation Plan\nTimebox plan for each timebox (see Table\n\nMilestone reviews\n\nSystem Concept Review (SCR)\nCombined System Requirements\nReview (SRR) and System\nDesign Review (SDR)\n\nTimebox assessments\nQualification testing after all\ntimeboxes for the release have\n6-3)\nbeen completed\nSoftware product baseline combining new,\nreused, and off-the-shelf products\nUpdated requirements traceability map\nDraft user documentation\nAcceptance Test Readiness\nSystem test procedures\nReview (ATRR)\nIntegrated, tested software\nQualification test report\nFinal user documentation\n\n(Although these system life-cycle phases are shown in Figure 5-4 for completeness, they are not discussed here, or in\nthe corresponding tables for the other life-cycle models, because they are out of the scope of the software life cycle.)\n\n33\n\nNASA-GB-00 1-96\n\n5.1.4 Package-Based Development Life-Cycle Model\nTable 5-8 summarizes the life cycle defined by the package-based development model.\nTable 5-8. Summary of Package-Based Development Life-Cycle Model\nSummary description\nand discussion\n\nThe package-based development life-cycle model is used for system\ndevelopment based largely on the use of commercial-off-the-shelf\nand Government off-the-shelf products and reusable packages\n(Reference 12). Typically, some custom software development is\nneeded to provide interfaces among the NDls.\nThis model is illustrated in Figure 5-5. Major products and milestone\nreviews for this life-cycle model are summarized in Table 5-9.\n\nAdvantages\n\nDisadvantages\n\nMost appropriate\nwhen\n\n...\n\nNASA-GB-00 1-96\n\nLower cost than developing equivalent functionality from scratch\nCycle time also often lower than developing equivalent\nfunctionality from scratch\nImproves confidence in quality of the end product (since quality of\nNDls is already known)\nMay result in compromises between desired functionality and\nf unctionaIity provided by NDIs\nMaintainability may be more of a challenge because source of\nNDls may not be the same NASA organization (for example,\nrequires third party to make changes, raises SCM issues when\nNDI vendor releases uDdated versions)\nA significant portion of the functionality of a system can be\nprovided bv NDls\n\n34\n\nRefined requirements,\nhstomer\'s requirements\n/\n\nt\n\nRequiredents\n\nPrototyping\n\nrechnology breakthrough ,\nSystem\n\\lew requirements,\nMaintenance\n\\lew products\n\nFigure 5-5. Package-Based Development Life-Cycle Model\nTable 5-9. Major Products and Milestone Reviews for the Package-Based Development Life-Cycle\nModel\nLifecycle phase\n\nRequirements Analysis\nand Package\nIdentification\nArchitectural Definition\nand Package\nSelection\nSystem Integration and\nTest\nTechnology Update and\nSystem Maintenance\n\nMajor products\n\nSystem Development Plan\nRequirements\nStrawman high-level architecture\nCandidate packages\nModified requirements\nSystem architecture\nFinal packages\nDelivered system\nEnhanced system\n\n35\n\nMilestone reviews\n\nSystem Requirements Review (SRR)\n\nSystem Design Review (SDR)\n\nUser demonstrations\nOperational Readiness Review (ORR)\nUser demonstrations\n\nNASA-GB-00 1-96\n\n5.1.5 Legacy System Maintenance Life-Cycle Model\nTable 5-1 0 summarizes the life cycle defined by the legacy system maintenance model.\nTable 5-10. Summary of Legacy System Maintenance Life-Cycle Model\nSummary description\nand discussion\n\nThe legacy system maintenance release life-cycle model is used to\napply fixes or minor enhancements to an operational system. (Use a\nwaterfall or incremental life-cycle model for major enhancements.)\nSelected and sometimes abbreviated activities performed in the\nsoftware development life cycles are also performed during\nmaintenance. The legacy system maintenance life-cycle model is\nsimilar in nature to the waterfall life-cycle model; the primary\ndifference is that the architectural design has already been\nestablished (Reference I O ) .\nThis model is illustrated in Figure 5-6. Major products and milestone\nreviews for this life-cycle model are summarized in Table 5-1 1.\n\nMost appropriate\nwhen\n\n...\n\nMaintenance release comprises only fixes and minor enhancements.\n\nRequirements analysis\n\ne\n\\\n\nDetailed design\n\nImplementation & testing\n\nQualification testing\n\nt\nDelivery\n\nFigure 5-6. Legacy System Maintenance Life-Cycle Model\n\nNASA-GB-00 1-96\n\n36\n\nTable 5-1 1. Products and Milestone Reviews for the Legacy System Maintenance Life-Cycle Model\nLife-cycle phase\n\nI\n\nRelease planning\nRequirements definition\nand analvsis\nDesign\nImplementation and\ntesting\n\nI\n\nQualification testing\n\nMajor products\n\nI\n\nMilestone reviews\n\nI\n\nRelease contents agreement\nRelease Contents Review (RCR)\nRelease requirements specification Release Requirements Review (RRR)\nRelease design specification\nUnit-level design\nImplemented, tested software\nQualification test plan and\nproced ures\nDraft user\xe2\x80\x99s guide updates\nQualification-tested software\nQualification test report\n\nRelease Design Review (RDR)\nRelease Qualification Test Readiness\nReview (RQTRR)\n\nAcceptance Test Readiness Review\n(ATRR)\n\nFinal user\xe2\x80\x99s guide updates\nAs-built software description\nuwdates\n\n37\n\nNASA-GB-00 1-96\n\n5.2 Selecting Appropriate Activities, Methods, and Products\nAfter an appropriate life-cycle model has been selected, it must be populated with software\nengineering activities, methods, techniques, and products that will help achieve the goals and\nobjectives established for the project. The following subsections present implementation\nguidance for each required activity (see Table 2-1) and identifj recommended methods for\nperforming those activities.\nTwo logical groups of activities are described in this section:\n1. Activities that are performed to produce a specific software product (shaded in Figure\n5-7, Primary Software Engineering Activities)\n- Software CI requirements definition and analysis (Section 5.2.1)\n- Software CI design (Section 5.2.2)\n- Software CI implementation and testing (Section 5.2.3)\n- Software CI qualification testing (Section 5.2.4)\n- Preparation for software delivery (Section 5.2.5)\n2. Activities that are performed in support of each of the first group of activities (shaded\nin Figure 5-8, Software Engineering Support Activities and described beginning in\nSection 5.2.6)\n- Software product validation and verification (Section 5.2.6)\n- Software configuration management (Section 5.2.7)\n- Software quality assurance (Section 5.2.8)\n\n- Milestone reviews (Section 5.2.9)\n\nNASA-GB-001-96\n\n38\n\nMonitor and control software project\n(maintain project software plan\nand records as necessary)\n0\ne\n\nPrepare software team\n\nEl\n\nIndependently assure software products and activities (SQA)\n\n-.I\n\nManage configuration (SCM)\n\n0\n3\n\n..\n-.\n\nEl\n\n:.I\n\nParticbate in milestone reviews\nValidate and verib (V&V) software products\n\nI ime\n\nFigure 5-7. Primary Software Engineering Activities\nMonitor and control software project\n(maintain project software plan\nand records as necessary)\n0\n1\n\nPrepare software team\n\nI\n\nPerform required technical activities\n(i.e., software CI requirements definition and\nanalysis through qualification testing,\nincluding interim deliveries)\nuntil final product is delivered\n\nI ime\n\nFigure 5-8. Software Engineering Support Activities\n\n39\n\nNASA-GB-00 1-96\n\n5.2.1 Software CI Requirements Definition and Analysis\nActivity Requirements\n\nRecommended methods\nStructured requirements analysis (Table 5-1 2)\nObject-oriented requirements analysis (Table 5-1 3 )\n\nNASA-GB-00 1-96\n\n40\n\nTable 5-12. Structured Requirements Analysis Method\nSummary Description\nand Discussion\n\nStructured requirements analysis is a method of analyzing and specifying the\nrequirements of a product from a functional point of view. Structured analysis\nincludes the use of data flow diagrams, data dictionaries, structured English,\ndecision tables, and decision trees to develop a structured requirements\nspecification.\n\nAdvantages\n\nFamiliar to most software reauirements analvsts\n\nDisadvantages\n\nIt is sometimes difficult to transform data flow diagrams from this activity into\nstructure charts in the Software CI Design activity\n\nMost appropriate\nwhen ..\n.\nKey products\n\nStructured techniques will be used throughout the effort\nThe problem to be solved is well-understood\nSoftware requirements specification (SWRS), including the following:\nClassification of requirements by clarity level (fully defined, needs\nclarification, or ambiguous) and by category (functional, performance,\noperations, or programmatic)\nData flow diagrams (DFDs)\nFunction specifications\nData dictionaries\nIdentification and definition of external interfaces\nTraceability matrices (maps software requirements to higher level system\nrequirements, if applicable)\n\n41\n\nNASA-GB-00 1-96\n\nTable 5-13. Object-Oriented Requirements Analysis Method\nSummary Description\nand Discussion\nAdvantages\n\nDisadvantages\n\nObject-oriented requirements analysis is a method of analyzing and specifying\nthe requirements of a product in terms of the objects that the system is\nmodeling and operations that pertain to those objects.\nThere is empirical evidence within NASA that use of object-oriented\ntechnology facilitates reuse\nFelt by some NASA practitioners to be a more natural, intuitive approach\nthan traditional structured approaches, resulting in products that are more\nmaintainable and modifiable\nCan be difficult for personnel with a structured analysis background\nUnless a recognized object-oriented method is chosen (for example,\nBooch, OMT), computer-aided software engineering (CASE) tool support\nis limited or nonexistent\nObject-oriented techniques will be used throughout the effort\nReusability, maintainability, or modifiability of the products developed is an\nimportant objective\n(The consensus of NASA personnel who have applied object-oriented\nmethods is that object-oriented technology is applicable in most situations.)\n\nKey products\n\nSoftware requirements specification (SWRS), including the following:\nClassification of requirements by clarity level (fully defined, needs\nclarification, or ambiguous) and by category (functional, performance,\noperations, or programmatic)\nEntity relationship diagrams, data flow diagrams, and state transition\ndiagrams\nIdentification and definition of external interfaces\nTraceability matrices (maps software requirements to higher level system\nreauirements. if anv)\n\nRecommended techniques\nPrototyping (Table 5-14)\nJoint application development (JAD) workshops (Table 5-1 5 )\n\nNASA-GB-00 1-96\n\n42\n\nTable 5-14. Prototyping Technique\nSummary Description\nand Discussion\n\nAdvantages\n\nDisadvantages\n\nKey products\n\nA prototype is an early experimental model of a system, system component,\nor system function that contains enough capabilities for it to be used to\nestablish or refine requirements, or to validate critical design concepts. A\nprototype is not an early operational version of a system. It does not contain\nall required system support functions; is not meant to be as reliable or robust\nas an operational system; and is seldom constrained by stringent\nperformance, safety, security, or operational requirements.\nPrototyping is useful to clarify unclear requirements, to obtain buy-in on\nuser interface characteristics, to gain experience when a new technology is\nbeing applied, or to evaluate alternative designs when major performance\nor reliability issues are unresolved.\nUsers get to see early versions of product functionality and provide\nfeedback without a lot of time and effort on the part of the developers and\ntesters.\nPrototyping is sometimes inappropriately used in an attempt to avoid\nperforming proven software engineering activities like peer reviews,\ntesting, configuration management, and documentation.\nUsers may interpret a prototype to be a finished product and not recognize\n(or accept) that additional work is required to develop an operational\nproduct.\nPrototypes, even when used appropriately, have a tendency to become\noperational products.\nRequirements are unclear, when the user interface is crucial, when a new\ntechnology is being applied, or when major performance or reliability\nissues are unresolved.\nPrototype development plan\nA prototype of the product to be developed\nPrototype summary report\n\n43\n\nNASA-GB-00 1-96\n\nTable 5-15. JAD Workshop Technique\nSummary Description\nand Discussion\n\nAdvantages\n\nDisadvantages\n\nJAD is a facilitated workshop technique designed to bring together principal\nstakeholders to solve a well-defined problem (for example, producing a product of\none of the recommended requirements analysis methods) in order to produce a\nwell-defined product or set of products (for example, a traceability matrix).\nIncludes people with authority to make decisions\nIncreases visibility into needs of customer and users, and concerns of\ndevelopers\nFacilitates discussing alternatives, and advantages and disadvantages of each\nReduces risk that decisions made by workshop participants will be changed\nlater, thus reducing costs and cycle time due to less rework\nRelies on effective group dynamics, resulting in increased synergism among\nstakeholders\nImpossible if principal stakeholders are not available, not given the authority, or not\nbacked by their management\nCycle time must be shortened\nThere are a number of interfacing organizations that are all stakeholders\nParticipants with knowledge and authority are available (or will be made\navailable)\nThere is adequate time for preparation, execution, and follow up\nAn exDerienced facilitator and amrotxiate facilities are available\n\nKey products\n\nNASA-GB-00 1-96\n\nWorkshop notes\nAction item lists\nIntended end products of JAD workshop (requirements specifications in this\ncase)\nPost-JAD management briefing\n\n44\n\n5.2.2 Software CI Design\nActivity Requirements\n\nRecommended methods\nStructured design (Table 5-16)\nObject-oriented design (Table 5-1 7)\n\n45\n\nNASA-GB-00 1-96\n\nTable 5-16. Structured Design Method\nSummary Description\nand Discussion\n\nStructured design is a method of designing a product from a functional point\nof view. Structured design includes the use of structure charts, structured\nEnglish, data flow diagrams, data dictionaries, decision tables, and decision\ntrees to develop a structured design specification.\n\nAdvantages\n\nFamiliar to most software designers\n\nDisadvantages\n\nThere is empirical evidence within NASA that components designed using\nstructured methods are less reusable and less maintainable than those using\nobject-oriented methods\nIt is sometimes difficult to transform data flow diagrams from the Software CI\nRequirements Definition and Analysis activity into structure charts in this\nactivity\n\nMost appropriate\nwhen ..\n.\n\nProduct being engineered is one-of-a-kind and is expected to have a relatively\nshort life time (for example, up to a few years)\n\nKey products\n\nSoftware design specification (SWDS), including the following:\nData flow diagrams (DFDs)\nStructure charts\nFunction specifications\nData dictionaries\nTraceability matrices (maps software design components to software\nrequirements)\n\nTable 5-1 7. Object-OrientedDesign Method\nSummary Description\nand Discussion\n\nObject-oriented design is a method of designing a product in terms of the\nobjects that the system is modeling and operations that pertain to those\nobjects.\n\nAdvantages\n\nDisadvantages\n\nKey products\n\nI\n\nThere is empirical evidence within NASA that use of object-oriented\ntechnology facilitates reuse\nFelt by some NASA practitioners to be a more natural, intuitive approach\nthan traditional structured approaches, resulting in products that are more\nmaintainable and modifiable\nCan be difficult for personnel with a structured design background\nUnless a recognized obiect-oriented method is chosen (for example,\n\nSoftware design specification (SWDS), including the following:\nRefined object diagrams\nTraceability matrices (maps software design components to software\nreauirements)\n\nNASA-GB-00 1-96\n\n46\n\nQB<\n\nRecommended techniques\nThe same techniques recommended in Section 5.2.1 (Software CI Requirements\nDefinition and Analysis) are often useful in defining and evaluating alternative\nsoftware CI designs.\n\nTailoring Guidancefor the Design Activity\nWhen there is a high level of design reuse, the architectural design probably already exists and\nneed not be considered a separate element of the design activity. However, be sure to highlight\nchanges from the reused architectural design.\n\n5.2.3 Software CI Implementation and Testing\nThis activity consists of two essential elements:\n1. Implementation and unit testing\n2. Integration and testing\n\n47\n\nNASA-GB-00 1-96\n\n5.2.3.1 Software Implementation and Unit Testing\nActivity Requirements\n\nNASA-GB-00 1-96\n\n48\n\nRecommended testing methods\nBecause software testing is a key element in several required software engineering\nactivities and because different testing methods are appropriate in different\nsituations, the proven testing methods are summarized in one section of this\nhandbook, Section 5.2.6, Software Product Validation and Verification.\n\nTailoring Guidancefor Unit Testing\nThe formality and rigor of unit testing will vary depending on the unit\xe2\x80\x99s complexity and\ncriticality. Units that are especially complex or critical may need to be tested in isolation, using\ntest drivers and stubs. Otherwise, the testing may be conducted on a collection of related units,\nperhaps in conjunction with integration testing. Select the level of formality and rigor that is most\nappropriate and cost-effective for the project as a whole or for various parts of the system.\n\n49\n\nNASA-GB-001-96\n\n5.2.3.2 Software Integration and Testing\nActivity Requirements\n\nr/\n\nNASA-GB-00 1-96\n\n50\n\nRecommended integration methods\nTop-down method (Table 5-1 8)\nBottom-up method (Table 5-1 9)\nFunctional path method (Table 5-20)\nTable 5-18. Top-Down Method\nSummary Description\nand Discussion\n\nIntegration follows a top-down approach, where lower level modules are\nadded to the top-level driver, level by level, until all components have been\nintegrated and tested.\n\nAdvantages\n\nIntegrated, tested system can be used as drivers (does not need drivers for\nhigher level calling routines)\n\nDisadvantages\n\nNeeds stubs for lower-level called components until the real components\nbecome available\n\nMost appropriate\nwhen ...\n\nArchitectural design is broad and shallow (for example, transaction\nprocessing)\nEmphasis is on high-level interfaces\nUsing high-level NDls\nBeginning to integrate\nThere is risk or uncertainty regarding the architectural design\n\nTable 5-19. Bottom-Up Method\nSummary Description\nand Discussion\n\nIntegration follows a bottom-up approach, where low-level components are\nintegrated and tested with other low-level components, which are then\nintegrated and tested with other low-level components, and so on until all\ncomponents have been integrated and tested.\n\nAdvantages\n\nDoes not need stubs for lower level called components\n\nDisadvantages\n\nNeeds drivers for higher level calling routines until the real components\nbecome available\n\nMost appropriate\nwhen\n\n...\n\nArchitectural design is narrow and deep (for example, scientific systems)\nWorking on utility components\nWorking on standalone algorithmic components\nUsing low-level NDls\nBeginning to integrate\n\n51\n\nNASA-GB-001-96\n\nTable 5-20. Functional Path Method\nSummaW Description\nand Discussion\n\nAn end-to-end functional path (or thread) is constructed, to which other\nmodules are then added, until all components have been integrated and\ntested.\nTests functions or objects in context\nHelps assure that functional requirements are being addressed\nRequires fewer drivers, since the only driver required is the one needed to\ninitiate the thread\n\nAdvantages\n\nDisadvantages\n\nHarder to ensure complete testing coverage\nMaking enhancements to an existing system\nVerifying end-to-end data flows\nIntegration has progressed to the point where there is an existing structure\nto build into\nArchitectural desian is obiect-oriented\n\nQB<\n\nRecommended testing methods\nRefer to Section 5.2.6, for a discussion of recommended testing methods.\n\nTailoring Guidancefor Integration Testing\nJust as described for unit testing, the formality and rigor of integration testing will vary\ndepending on the module\xe2\x80\x99s complexity and criticality. Modules that are especially complex or\ncritical may need to be tested in isolation, using test drivers and stubs. Otherwise, the testing may\nbe conducted on a collection of related modules. Select the level of formality and rigor that is\nmost appropriate and cost-effective for the project as a whole or for various parts of the system.\n\ntt\n\nNASA-GB-00 1-96\n\nTIPS\nFor high-reuse systems, concentrate integration\nactivities on new and modi tied components.\n\n52\n\ntesting\n\n5.2.4 Software CI Qualification Testing\nActivity Requirements\n\nQB<\n\nRecommended methods\nRefer to Section 5.2.6 for a discussion of recommended testing methods.\n\n53\n\nNASA-GB-00 1-96\n\n5.2.5 Preparing for Software Delivery\nActivity Requirements\n\nNASA-GB-00 1-96\n\n54\n\n55\n\nNASA-GB-00 1-96\n\n5.2.6 Software Product Validation and Verification\nNote: Recall that this is the first of four sections describing the group of support engineering\nactivities shown in Figure 5-8. (See the beginning of Section 5.2.)\nActivity Requirements\n\nNASA-GB-00 1-96\n\n56\n\nTable 5-21. Software Product V&V Summary\nSoftware product ...\n\nSoftware requirements\nspecification (SWRS)\nSoftware design\nwecification (SWDS)\nUnit-level design\nsDecifications\nUnit-level code\n\nIntegrated sets of units\n(modules) up to the CI\nlevel\nSoftware CI build\nSoftware CI release\n\nSoftware delivery to\ncustomer\n\n... Is evaluated against (at least) the\nfollowing requirements or design\nspecification ...\nSystem requirements allocated to the\nsoftware CI\n\nI\n\n... Usually using the following product\nWalkthroughs; document reviews;\ninspection\nWalkthroughs; document reviews;\ninmection\nInspection (per unit design\ncertification criteria)\nInspection (per unit code certification\ncriteria)\nUnit testing (per unit test plan)\nIntegration testing (also known as\nmodule, string, thread testing) (per\nintegration test plan)\nBuild qualification testing (BQT) (per\nBQT plan)\nFormal (release) qualification testing\n(FQT) (per FQT plan)\n\nSWRS\nSWDS (detailed design)\nUnit-level design specifications\n\nSWDS (detailed design)\n\nInitially, based on SWDS; later,\nevolving toward SWRS\nInitially, based on SWRS; later,\nevolving toward system\nrequirements allocated to the\nsoftware CI\nSystem requirements allocated to the\nsoftware CI\n\nV&V method@)\n\nI\n\nAcceptance testing (AT) (per AT plan;\nwhen the system is a software\nsystem, FQT is the AT)\n\nRecommended methods\nPeer review methods\n- Inspection method (Table 5-22)\n- Walkthrough method (Table 5-23)\n- Document review method (Table 5-24)\n\n- Demonstration method (Table 5-25)\nTesting methods\n- Functional (black box) testing (Table 5-26)\n- Structural (white or clear box) or coverage (statement, branch, or\npath) testing (Table 5-27)\n- Statistical testing (Table 5-28)\n- Regression testing (Table 5-29)\n- Cleanroom (Table 5-3 1)\n\n57\n\nNASA-GB-00 1-96\n\nTable 5-22. Inspection Method\nSummary Description\nand Discussion\n\nInspections are well-defined peer reviews that are intended to verify\ncorrectness, quality, and compliance with requirements and standards. There\nare two types of inspection: One-on-one and team. A one-on-one inspection\nrelies on a single inspector; team inspections include two or more. (However,\nstudies conducted within NASA as well as other parts of the software industry\nclearly show that two or more inspectors are far more effective in discovering\ndefects than a single one (Reference 14).) The primary use of inspections is\nto find defects; secondary uses include exposing team members to details of\nother parts of the system, helping increase awareness of the importance of\npaying attention to details when creating products, helping more junior\npersonnel identify more subtle defects, and increasing the skills of junior\npersonnel by exposing them to the techniques used by more senior staff\nmembers.\nProduct certification is used as input to the project\xe2\x80\x99s progress measurement\nprocess. Defects found by inspections (which are recorded on inspection and\ncertification records) are used as input to the defect causal analysis process.\n\nAdvantages\n\nVery thorough\n\nDisadvantages\n\nSometimes misapplied-either focuses only on minor defects, such as\nformatting issues, or done as a formality to certify a product after informal\npeer reviews have already been performed and no further defects are known.\n\nKey products\n\nOverall development time is tight\nTime allocated to higher level testing is tight\nProduct quality goals are high\nInspection and certification records (software product V&V records)\nInspection data collection forms\n\nTable 5-23. Walkthrough Method\nSummary Description\nand Discussion\n\nWalkthroughs are primarily a method for communicating information to team\nmembers. Walkthroughs are not intended to be a defect-finding tool; however,\nobvious problems are sometimes identified during a walkthrough.\nQuick, effective, relatively informal method for sharing information with peers.\n\nDisadvantages\n\nI Key products\n\nNASA-GB-00 1-96\n\nSometimes used in place of inspection method (walkthroughs are not nearly\nas thorough as inspections; defects are likely to escape detection until later in\nthe life cycle, when effort to repair is greater).\n\nComments from team members\nQuestions to be answered\nAction items\n\n58\n\nTable 5-24. Document Review Method\nSummary Description\nand Discussion\n\nDocument reviews are peer reviews of a finished document that are intended\nto verify completeness, correctness, consistency, quality, and compliance with\nsta ndards\n\nAdvantages\n\nEnables review of all parts of a document in context with each other\n\nTable 5-25. Demonstration Method\nSummary Description\nand Discussion\nAdvantages\n\nDisadvantages\n\nMost appropriate\nwhen\n\n...\n\nDemonstrations are most frequently used in conjunction with prototypes. The\npurpose of a demonstration is to solicit feedback from another group (for\nexample, end users of the product).\nFacilitates early evaluation of product characteristics in terms of product\xe2\x80\x99s\nlook and feel\nEncourages customer and end user participation\nEffective demonstrations require careful preparation (that is, they take\nextra time and effort)\nMay lead customer and end users to believe that product is (almost) done\nEvaluation may occur later in phase than would occur using other methods\nCan lead to requirements growth in terms of nice-to-have features\nAlternative approaches need to be evaluated (for example, user interface\ntechniaues)\n\n~\n\nKey products\n\nPrototypes\nDemonstration summary, results, or agreements\n\n59\n\nNASA-GB-00 1-96\n\nTable 5-26. Functional Testing Method\nSummary Description\nand Discussion\n\nIn functional testing, sometimes referred to as black box testing, verifying\nfunctionality and correct interfacing of software components is the focus. The\ntester does not need to understand the internal design of the software\ncomponent(s) under test, nor how it is implemented-simply what the\nsoftware is expected to do.\nFunctional testing is often organized based on threads of software\ncomponents that accomplish a higher level function, and is sometimes based\non operational scenarios. NASA software testers have found tests based on\noperational scenarios to be very effective at uncovering errors.\n\nAdvantages\n\nDisadvantages\n\nMost appropriate\nwhen\n\n...\n\nKey products\n\nNASA-GB-00 1-96\n\nNot overly time-consuming\nEnsures that functional requirements are tested\nCan be performed by testers without detailed knowledge of design of the\nsoftware\nLess likely to detect defects in parts of software that are not frequently\nexecuted\nIdentifies symptoms of defects, not necessarily their causes\nRobustness of product is not established\nA primary objective of the testing is to verify\nSatisfaction of requirements\nCorrect interfacing of components\nTest plans and procedures, including traceability to design or requirements\nspecification (see Table 5-21)\nTest records (software product V&V records)\n\n60\n\nTable 5-27. Structural or Coverage Testing Method\nSummary Description\nand Discussion\n\nStructural testing, sometimes referred to as clear box or white box testing, is\ntesting in which verification of correct implementation of the software design is\nthe focus. The tester must understand the internal design of the software\ncomponent(s) under test.\nCoverage testing is a specific form of structural testing that is designed to\nensure that each statement or logic path or software component is executed\nat least once.\n\nAdvantages\n\nIncreases confidence in structure of design and correct implementation of\nthe design\nMore likely (than functional testing) to identify causes of problems\n\nDisadvantages\n\nVery time-consuming\nMay miss aspects of functionality and of the \xe2\x80\x9cbig picture\xe2\x80\x9d\nRequires in-depth understanding of software internals\n\nsoftware paths\nLower level component testing is being performed (that is, unit level or\n\nKey products\n\nTest plans and procedures, including traceability to design or requirements\nspecification (see Table 5-21)\nTest records (software product V&V records)\n\nTable 5-28. Statistical Testing Method\nSummary Description\nand Discussion\n\nStatistical testing is testing based on a detailed assessment of expected\nusage profiles of characteristics of the software that are especially important\nto the customer. Such characteristics include assessing which software\ncomponents will be executed most often, which components can cause\ncatastrophic results if defects are present, etc. Statistical testing is usually\nbased on operational scenarios.\n\nAdvantages\n\nConcentrates testing effort on parts of the software that are most important to\nthe customer\n\nDisadvantages\n\nRequires an in-depth understanding of how the software will be used\noperationally\nIncreases possibility that defects will remain undiscovered in less\nfreauentlv exercised parts of the software\n\nKey products\n\nTest plans and procedures, including traceability to design or requirements\nspecification (see Table 5-21)\nTest records (software product V&V records)\n\n61\n\nNASA-GB-00 1-96\n\nTable 5-29. Regression Testing Method\nSummary Description\nand Discussion\n\nRegression testing is retesting previously tested software after some kind of\nchange has been made. Changes may have been made in the software itself,\nor in other software or hardware with which the software interfaces. The\npurpose of regression testing is to verify that the changes have not adversely\naffected previously tested software.\nRegression testing does not usually include rerunning all test cases that were\noriginally used, but instead a predefined subset of the test cases that are\nselected based on criteria established by the project team and the customer.\nPromotes confidence in an evolving product\nEnsures that obvious defects have not been introduced into the Droduct\n\nAdvantages\nDisadvantages\n\nIf regression test sets are not chosen carefully,\nConsiderable effort can be expended if testing is overly exhaustive\nDefects can go undetected if testing is too superficial\n\nMost appropriate\nwhen ..\n.\n\nChanges are made to software that has undergone higher levels of testing\n(that is, integration level or qualification level)\n\nKey products\n\nTest records (software product V&V records)\n\nOne large NASA organization has found over the past few years that the methods summarized in\nTable 5-3 0 have been used most often for unit-level, integration-level, and qualification-level\ntesting.\nTable 5-30. Testing Methods vs. Testing Levels\nTesting\nMethod\n\nUnit Level\nTesting\n\nIntegration Level\nTesting\n\nJ\n\nFunctional\nStructuraI\nCoverage\n\nQualification Level\nTestina\n\nI\n\nJ\n\nI\n\nJ\n\nStatistical\n\nJ\n\nJ\n\nRegression\n\nJ\n\nJ\n\nThe Cleanroom method (Reference 15) is an alternative approach that has been used successfully\non some NASA projects (Reference 16).\n\nNASA-GB-001-96\n\n62\n\nSummaW Description\nand Discussion\n\nThe Cleanroom method provides an alternative to traditional testing, with a\ngoal of preventing software errors rather than detecting them. Developed at\nIBM in the late 1980s, Cleanroom relies on human discipline and intellectual\ncontrol to build quality into the final product instead of on computer-aided\nprogram debugging to detect and remove errors\n\nAdvantages\n\nSuccessful application of the Cleanroom methodology can significantly\nincrease software quality and reliability, decrease test and debug time, and\nminimize rework efforts\n\nDisadvantages\n\nMay not be applicable to large projects\n\nTailoring Guidance for Software Product V&V\nSelect or adjust the product V&V methods based on the customer\xe2\x80\x99s goals and objectives for cost,\nschedule, and product qualities. For example,\nIf high robustness or reliability is called for\n- Increase the amount of peer review and testing\n- Increase the degree of independence of the evaluators from the developer\nIf less robustness or reliability is acceptable\n- Have peer reviews and testing focus on those portions of the system that are\nexpected to have high use or are especially critical\nRefer to Appendix Appendix C. for guidelines for evaluating NDIs. (Examples of procedures for\nV&V can be found in Reference 17.)\n\n63\n\nNASA-GB-001-96\n\n5.2.7 Software Configuration Management\nActivity Requirements\n\n4\n\nNASA-GB-00 1-96\n\n64\n\nTailoring Guidancefor the SCM Activity\nLevels o Control-There are two fundamental levels of software product change control:\nf\nbaseline control (or configuration management) and local control. Apply the appropriate level\nand type of control to verified products-intermediate as well as delivered, new as well as\nchanged from previously controlled versions.\n\nBaseline Control. Some software products, for example, the software requirements, design, and\ncode, should have baselines established at predetermined points. These baselines are reviewed\nand agreed on with the customer, and serve as the basis for further development. Baselines are\ntypically established in conjunction with milestone reviews, such as SSR (software\nrequirements), CDR (software design), and following software CI qualification testing and\nphysical configuration audit (PCA) or functional configuration audit (FCA) (code). Apply a\nrigorous change control process to baselined items.\nLocal Control. Some software products, such as the software plan, may not need to be placed\nunder baseline control, but still need to be controlled locally. This implies that the version of the\nproduct in use at a given time (past or present) is known (that is, version control), and changes\nare incorporated in a controlled manner (that is, change control). For example, software plans are\ntypically placed under local control, but not baseline control.\nThe specific change control approach used by a software project will vary based on many factors.\nOn some projects, the customer is responsible for some or all SCM functions; in other cases, the\nsoftware team is responsible; and sometimes the responsibilities are split (for example,\nrequirements documents might be controlled by the customer, but design documents might be\nunder local control until the CI is delivered). Similarly, the configuration review function for a\nlarge project might require a formal configuration control board (CCB) while the CCB for a four-\n\n65\n\nNASA-GB-001-96\n\nperson development project might be simply the software project manager and a customer\nrepresentative.\nEach project\xe2\x80\x99s product control approach is defined in the SCM portion of its software plan and\nspecifies which products are to be placed under baseline control and which will be under local\ncontrol. The approach defines what products will be controlled, under what conditions each kind\nof product is initially placed under control, who controls it and where, and what has to occur to\nchange it.\nConfiguration Audits-It may not be necessary to perform FCAs or PCAs at CI level, they may\nbe done at higher level (for example, system or release). Nor is it necessary to perform FCAs and\nPCAs separately, they may be done together on a system level. (Reference 17 includes examples\nof procedures for FCAs and PCAs.)\nRequirements Management-While a software product\xe2\x80\x99s requirements are only one of several\nsubordinate entities that must be controlled, they are emphasized here because everything that the\nsoftware team does is based on those requirements. Poor management of requirements has\ncaused problems in the past.\nIf the customer does not require or maintain any form of software requirements specification, the\nsoftware team should document and control (to whatever degree of formality is deemed\nappropriate) the product\xe2\x80\x99s requirements as they understand them and as they are being\nimplemented. The customer should be given a copy of these software requirements to help ensure\nthat the software team and the customer have a common understanding of the basis for the end\nsoftware product. The requirements should be kept up to date in a controlled fashion.\n\nNASA-GB-001-96\n\n66\n\n5.2.8 Software Quality Assurance\nActivity Requirements\n\nRecommended Methods\nAuditing, monitoring, and assessing performance of activities and\nqualities of products\n\nIn some environments, the term SQA is used to refer to more than \xe2\x80\x9cindependent assurance of software products and\nprocess\xe2\x80\x9d; that is, it sometimes includes peer reviews, testing, etc. This guidebook, however, restricts the SQA\nactivity to include only independent assurance; peer reviews, testing, etc. are included under the Software Product\nV&V activity.\n\n67\n\nNASA-GB-001-96\n\n5.2.9 Milestone Reviews\nActivity Requirements\n\nNASA-GB-00 1-96\n\n68\n\n69\n\nNASA-GB-00 1-96\n\nTable 5-32. Candidate Milestone Reviews\nReview\n\nObjective\nThese reviews are held to provide management with the information\nnecessary to assess progress and execute appropriate corrective action,\nif required, regarding:\n\nConcept or contents reviews\n(SCRs, RCRs)\n\nThe operational concept for a software system or the content of a\nrelease.\n\nRRRs)\n\nThe specified requirements for a software system, subsystem, or\nrelease and to establish the functional baseline.\n\nSystem design reviews (SDRs)\n\nOne or more of the following:\n\nI\n\n0\n0\n\nThe system- or subsystem-wide design decisions\nThe architectural design of a software system or subsystem\n\nand to establish the system design baseline.\nI\n\nSoftware specification reviews\n(SSRs)\n\nThe specified requirements (that is, SWRS) for a software CI and to\nestablish the CI-allocated baseline. (SSRs may also be referred to as\nSRRs when the svstem is essentiallv all software.)\n\nSoftware design reviews (PDRs,\nCDRs, BDRs, RDRs)\n\nOne or more of the following:\n0\n0\n0\n\nThe software CI-wide design decisions\nThe architectural design of a software CI\nThe detailed design (that is, SWDS) of a software CI or portion\nthereof (such as a database or an upcoming build)\n\nand to establish the CI development baseline.\n\nI\n\nTest readiness reviews (QTRRs,\n\nOne or more of the following:\n0\n0\n\n0\n\nThe status of the software test environment\nThe test cases and test procedures to be used for software CI\nqualification testing or system qualification testing\nThe status of the software to be tested\n\nThe reviews that follow software CI qualification testing, PCA, and FCA,\nbut precede the next stage of testing (that is, either system-level\nintegration and testing or system installation and acceptance testing)\nare held to establish the CI product baseline.\nI\n\nOperational readiness reviews\n(ORRs)\n\nOne or more of the following:\n0\n0\n0\n0\n0\n0\n\nThe readiness of the software for installation at operational sites\nThe user and operator manuals\nThe software product specifications\nThe software version descriptions\nThe status of installation preparations and activities\nThe status of transition preparations and activities, including\ntransitioning the software development environment (if applicable) to\nthe maintenance organization\n\nand to establish the operational baseline.\n\nNASA-GB-00 1-96\n\n70\n\nQ/\n\nRecommended Methods\nMeetings (Table 5-33), for example, the milestone review may be held\nas the final element of a JAD workshop (see Table 5-15)\nPresentations (Table 5-34)\nDemonstrations (Table 5-3 5 )\nTable 5-33. Meetings\n\nSummary Description\nand Discussion\n\nAdvantages\n\nDisadvantages\n\nMost appropriate\nwhen ..\n.\nKey products\n\nMeetings come in a wide variety of sizes (for example, length, depth of\ncoverage, number of attendees) and degrees of formality. This variation can\nbe of considerable advantage to the software manager in that he or she can,\nin conjunction with the customer, establish the most cost-effective structure\nfor the meeting.\nUsually most efficient and least costly method\nEasily scheduled and planned\nMore conducive to greater interaction of participants than presentations\nDegree of formality may not be acceptable to the customer\nSometimes difficult to control\nAgreeable to the customer\nAction items\nApproved products(s)\nApproval to proceed to the next phase\n\nTable 5-34. Presentations\nSummary Description\nand Discussion\n\nPresentations, like meetings, come in a wide variety of sizes and degrees of\nformality. Usually, however, additional effort is expended in preparing\nmaterials designed expressly for the presentation.\n\nAdvantages\n\nInvolve a larger audience than meetings\n\nDisadvantages\n\nMost appropriate\nwhen ..\n.\nKey products\n\nInvolve a larger audience than meetings\nUsually more formal than meetings and, therefore, require more\npreparation effort; for example, may require producing presentat ion-quaIity\nslides, holding dry runs\nLess conducive to interaction of audience than meetings\nRequired by the customer\nAction items\nApproved products(s)\nApproval to proceed to the next phase\n\n71\n\nNASA-GB-001-96\n\nSummaW Description\nand Discussion\n\nDemonstrations are used to display the current state of the software product\nor prototypes of the product\n\nAdvantages\n\nInstill level of confidence that the project is on the right track\n\nDisadvantages\n\nIn early life-cycle phases, users may interpret the product to be finished and\nnot recognize (or accept) that additional work is required to develop an\noperational product\n\nI Key products\n\nI\n\nI\n\nAction items\nApproved products(s)\nApproval to proceed to the next phase\n\nTailoring Guidancefor the Milestone Reviews Activity\nMilestone reviews may be conducted incrementally, dealing at each review with a subset of the\nlisted items or a subset of the system or software CI(s) being reviewed.\nNMI 7 120.4 (Reference 18) establishes management policies and responsibilities for major\nsystem program and projects. The companion NASA Handbook (NHB) 7120.5 (Reference 19)\nindicates that only PDRs and CDRs are required of all major program^.^ The candidate milestone\nreviews discussed in this guidebook are recommended, but not required, for all software\ndevelopment and maintenance projects, not only for major programs.\n\n4\n\nNMI 7120.4 defines the scope of a \xe2\x80\x9cmajor\xe2\x80\x9d program or project.\n\nNASA-GB-001-96\n\n72\n\n73\n\nNASA-GB-00 1-96\n\n6. Finishing the Software Plan-Defining\nManagement Approach\n\nthe\n\nT\n\nhe preceding two chapters described how to understand the scope and characteristics of\nthe work to be performed, and how to define an appropriate technical approach to perform\nthe work. This chapter describes the steps needed to define the management approach for\nthe project and to document and review the project\xe2\x80\x99s software plan. (As recommended in Chapter\n4, planning should be coordinated, as much as possible, with other groups responsible for related\nsoftware efforts.)\nActivity Requirements\n\nThe rest of this chapter provides guidance in accomplishing this required activity.\n\n75\n\nNASA-GB-00 1-96\n\n6.1 Establishing the Software Project\xe2\x80\x99s Organizational Structure\nActivity Requirements\n\nLJ\nIndependent\nSoftware Quality\nAssurance\n\ni\n,\nI_\n\n--------\n\nProject Manager\n(if appropriate) or\nSenior Manager\n\n.____----\n\n[Project] Software\nManager\n\nSoftware\nConfiguration\nManager\n\nConfiguration (or\nChange) Review\n\nRequirements\nAnalyst(s)\n\nTester(s)\n\nQualification\nTester(s)\n\nFigure 6-1. Typical Software Project Organization\n\nTailoring guidance\nWhile Figure 6-1 illustrates a typical NASA software engineering organization, it does not imply\nthat different persons are responsible for each role. Quite the contrary, very often a single\nindividual may assume two or more roles on a typical software project (though care must be\ntaken when assigning personnel to roles in which independence is required). These roles may\nalso be filled by personnel from outside of the software team\xe2\x80\x99s own organization (for example,\ncustomer personnel may be responsible for certain roles).\n\nNASA-GB-00 1-96\n\n76\n\n6.2 Estimating and Scheduling the Work\nActivity Requirements\n\nTable 6-1. Three Levels of Estimates and Plans\nPeriod planned\n\nPurpose of estimate and plan\n\nProject, start to finish\n\nProvide customer with an estimate and plan for the full software effort.\nEnsure thorough high-level analysis of the full software effort.\n\nUpcoming build\n\nEnsure thorough detailed analysis of the work to be accomplished in the next\nbuild.\n\nFiscal year\n\nDevelop plans that reflect the customer\xe2\x80\x99s budget for the fiscal year.\n\nTailoring Guidance\nLocal standards for estimating size, effort, and duration are based on models derived from\nhistorical records of similar projects developed by the same organization. The basis of a size\nestimate for software projects, for example, may be actual counts of lines of codes, function\npoints, database transactions, or whatever unit is appropriate for the application domain.\nAn example of local standards for estimating can be found in the Manager\xe2\x80\x99s Handbook for\nSoftware Development (Reference 7) of the NASA Software Engineering Laboratory (SEL). The\nSEL derived local estimation standards for all software developed or maintained within the Flight\nDynamics Division at GSFC. The SEL\xe2\x80\x99s estimation models are based on the Constructive Cost\nModel (COCOMO) (Reference 20) but are tailored to the local environment and based on\nanalysis of measurement data collected from software projects since 1976. A detailed explanation\nof the derivation of the SEL estimation models can be found in the Cost and Schedule Estimation\nStudy Report (Reference 21). Further examples are available in the NASA Software\nMeasurement Guidebook (Reference 5).\n\ntt\n\nTIPS\nThe best estimate is still just an estimate. Expect to have to reestimate as a result of monitoring the status of the pro-ject (see\nSection 7.1.2).\n\n77\n\nNASA-GB-001-96\n\nActivity Requirements\n\nRecommended techniques\nMini-Waterfall\nTimeboxes\nTable 6-2. Mini-Waterfall\nSummaW Description\nand Discussion\n\nAdvantages\nDisadvantages\n\nNASA-GB-00 1-96\n\nThis is the traditional technique for planning builds. That is, each build\nincludes some emphasis on analyzing the requirements allocated to the build,\nusually includes detailed design of the portion of the software that will be\nimplemented in the build, implementation and testing of the software, and\nqualification testing of the build.\n\nI Technique is familiar to most managers\nBasically the same as those described under Waterfall Life-Cycle Model\n(Table 5-2) but not as significant in impact; that is, the requirements to be\nimplemented in the build must be known before beginning the build.\n\n78\n\nTable 6-3. Timeboxes\nSummaW Description\nand Discussion\n\nA timebox refers to a subdivision of a build created to achieve a unit of\nproduction that is manageable in size, complexity, staffing, and duration. It is a\nplanning construct that controls functionality delivered by establishing fixed\nresource and time budgets. That is, when the allocated time and effort have\nbeen expended, it is time to move on to the next timebox.\n\nAdvantages\n\nHelps avoid \xe2\x80\x9cgold plating\xe2\x80\x9d\n\nDisadvantages\n\nThe product resulting from the timebox might not be complete in its own right\nFunctionality is not well-understood or is likely to change considerably\nUsing the evolutionary development life-cycle model\nDeveloping prototypes\nCvcle time is critical\n\nTailoring Guidancefor Planning Builds\nAvoid treating all software CIS as though they must be developed in lock-step,\nreaching key milestones at the same time. Allowing software CIS to be on different\nschedules can result in more optimal development.\nSimilarly, avoid treating software components within a CI as though they must be\ndeveloped in lock-step, all designed by a certain date, implemented by a certain date,\netc. Flexibility in the scheduling of software components can also be effective.\nThe required software engineering activities need not be performed sequentially.\nSeveral may be taking place at one time, and an activity may be performed continually\nor as needed throughout a build or across multiple builds. The activities in each build\nshould be laid out in the manner that best suits the work to be done.\n\n79\n\nNASA-GB-001-96\n\n6.3 Planning Other Activities\nActivity Requirements\n\nNASA-GB-00 1-96\n\n80\n\n. - l c ~ i i . reqirirements continired\ni~.\n\nPrimary products. The primary products from this activity are as follows:\n0\n0\n0\n0\n\nTraining plan\nRisk management plan\nTPM plan\nProcess improvement plan\nManagement measurement plan\nTable 6-4. Required Activities and Related Measures\nActivity\n\ncost\n\nSchedule\n\nDefects\n\nOther\n\nSoftware project planning\n\nJa\n\nSoftware CI requirements definition and\nanalysis\n\nJ\n\nJ\n\nJ\n\nRisks, TPMC\n\nSoftware CI design\n\nJ\n\nJ\n\nJ\n\nRisks, TPMC\n\nSoftware CI implementation and testing\n\nJ\n\nJ\n\nJ\n\nRisks, TPMC\n\nSoftware CI qualification testing\n\nJ\n\nJ\n\nJ\n\nRisks, TPMC\n\nPreparing for software delivery\nSoftware project close-out\nSoftware product V&V\n\nI\n\nI\n\nJ\n\nSoftware configuration management\n\nI\nI\n\nJb\n\nI\n\nI\n\nI\n\ndb\n\nSoftware quality assurance\n\nI\n\nI\nJ\n\nMilestone reviews\nSoftware team preparation\nProject monitoring and controlIing\n\nJa\n\nSoftware process improvement\n\nJ\n\nJ\n\nSystem-level considerations\n\nJ\n\nJ\n\nRecommended process improvement method\nProcess studies (Table 6-5) (Reference 2)\n\n81\n\nNASA-GB-00 1-96\n\nTable 6-5. Process Studies\nSummary Description\nand Discussion\n\nA process study is a method by which the software manager can objectively\ndetermine the impact of introducing a new technology into the software\nengineering process. The study consists of identifying the objective of the\nchange (for example, reduce cost of development), identifying the new\ntechnology, recording the baseline characteristic to be improved (for example,\ndevelopment cost of like products), and identifying the measures to\ndemonstrate whether the objective has been attained. The project team\napplies the new technology to its work. The resulting measures are compared\nwith the baseline characteristics, thus quantifying the effect of the new\ntechnology on the product or process. The results are recorded and made\navailable for use by others.\n\nAdvantages\n\nProvides an objective means of assessing a new software technology.\n\nI Disadvantages\n\nI None\n\nI\n\nI\n\nKey products\n\nNASA-GB-00 1-96\n\nProcess study plan\nProcess studv reDort\n\n82\n\n6.4 Reviewing the Software Plan\nActivity Requirements\n\n83\n\nNASA-GB-00 1-96\n\n7. Running the Project\n\nC\n\nhapters 3 through 6 discussed how to develop a plan to facilitate running the project. This\nchapter focuses on the software manager\xe2\x80\x99s activities during the execution of the project.\nThose activities are shown in the shaded portions of Figure 7-1 :\nUsing the software plan to guide the project in its engineering efforts\nClosing out the project.\n\nPerform required technical activities\n(i e , software CI requirements definition and\nanalysis through qualification testing,\nincluding interim deliveries)\nuntil final product is delivered\n\nI ime\n\nFigure 7-1. Running the Project\n\n85\n\nNASA-GB-00 1-96\n\n7.1 Managing the Project\nActivity Requirements\n\n7.1 . Preparing the Software Team\nI\nActivity Requirements\n\nNASA-GB-00 1-96\n\n86\n\n7.1.2 Monitoring and Controlling the Project\nActivity Requirements\n\n87\n\nNASA-GB-00 1-96\n\n7.1.3 Communicating with Stakeholders\nActivity Requirements\n\nTable 7-1. Recommended Status Reports and Meetings\n\nI\n\nMechanism\nStatus reports\n\nWeekly progress reports\nMonthlv status reports\n\nI\n\nPurpose\n\nTo ensure regular communication of recent activities and outstanding issues\nTo ensure regular review and reporting of. progress and cost\n-\n\nMeetings\n\nPeriodically with the\nproject team members\nPeriodically with the\ncustomer\nWith interfacing groups as\nneeded\n\nNASA-GB-00 1-96\n\nTo communicate or discuss issues and changes\nTo provide a forum for questions and answers\nTo strengthen team cohesiveness\nTo communicate or discuss issues and changes\nTo maintain insight into customer\xe2\x80\x99s \xe2\x80\x9chot buttons\xe2\x80\x9d\nTo communicate or discuss issues and changes\n\n88\n\nI\n\nI\n\nI\n\nSystem requirements allocated to SW CI\n\nSoftware CI requirements definition team\n\nSoftware CI requirements\n\nDesigners\n\nSoftware design\n\nlmplementers\n\nTested code components\n\nSW CI integrators and testers\n\nTested software CI\n\nSystem integrators and testers\n\nTested system\n\nCustomer\n\nFigure 7-2. Product Handovers\n\n89\n\nNASA-GB-00 1-96\n\n7.1.4 Maintaining the Software Plan\nActivity Requirements\n\nNASA-GB-00 1-96\n\n90\n\n7.1.5 Keeping Project Records\nActivity Requirements\n\n91\n\nNASA-GB-00 1-96\n\n7.2 Closing Out the Project\nActivity Requirements\n\nNASA-GB-00 1-96\n\n92\n\n93\n\nNASA-GB-00 1-96\n\nAppendix A. Glossary\nAcceptance. An action by the customer (or an authorized representative) by which the customer\nassumes ownership of software products as partial or complete fulfillment of software\nrequirements.\nActivity. A unit of work that has well-defined entry and exit criteria. Activities can usually be\nbroken into discrete steps.\nAdapted unit. An existing unit that changes substantially (more than 25 percent of its content is\nchanged, added, or deleted). Its origin is usually external to the project. (Contrast with new unit,\nconverted unit, transported unit.)\nArchitecture. The organizational structure of a system or software CI, identifLing its\ncomponents, the component interfaces, and a concept of execution among the components.\nBuild. A version of software that meets a specified subset of the requirements that the completed\nsoftware will meet. (See also release.)\nCertification. Written confirmation that a work product has been evaluated (for example,\ninspected or tested) and any defects found by that evaluation process have been satisfactorily\nresolved.\nConfiguration item (CI). System component (for example, hardware CI or software CI) that is\ndeveloped or purchased, controlled, accepted, and maintained separately from other system\ncomponents. In practice, it is a component that is convenient and sensible to document and\ncontrol as an entity. (See software configuration item.)\nConverted unit. Existing unit that changes slightly (up to 25 percent of its content is changed,\nadded, or deleted). Its origin is usually external to the project. (Contrast with new unit, adapted\nunit, transported unit.)\nCOTS (or GOTS) software. Commercial (or government), off-the-shelf software. COTS and\nGOTS software includes (1) unique components that must be delivered with the product to\nexecute the operational software and (2) development tools that must be delivered with the\nproduct to support maintenance of the software.\nCustomer. The organization that procures software products for itself or another organization.\nCycle time. Elapsed calendar time (not effort) required to complete a given piece of work. For\nsoftware efforts, cycle time usually refers to either (1) the full product engineering period (from\ninitial receipt of product requirements through acceptance by the customer of the fully functional\ndelivered product) or (2) a release cycle (from agreement on the requirements for the release\nthrough acceptance by the customer of the delivered product). The latter case, the release cycle,\ncan pertain to new development releases as well as maintenance and enhancement releases. (See\nalso software life cycle.)\nDesign. Those characteristics of a system or software CI that are selected by the software team in\nresponse to the requirements (see requirements). Some will match the requirements; others will\n\n95\n\nNASA-GB-001-96\n\nbe elaborations of requirements, such as definitions of all error messages in response to a\nrequirement to display error messages; still others will be implementation related, such as\ndecisions about what software units and logic to use to satisfj the requirements.\n\nDevelopment. Production of a new product that leads to delivery of all initially planned\nfunctional capability. The new product may be built from newly created components, reused\ncomponents (with or without adaptations), GOTS components, and COTS components. (Contrast\nwith enhancement, maintenance; see also software engineering.)\nDocumentation. A collection of data, regardless of the medium (or media) on which it is\nrecorded, that generally has permanence and can be read by humans or machines. The\nsignificance of this definition is that documents do not necessarily have to be separately bound\nentities; they may comprise data in several media (for example, plans, CASE tools, databases).\nEnhancement. A major addition or change in the functionality of an operational system; often\nincludes many of the same activities as new development. (Contrast with new development,\nmaintenance; see also software engineering.)\nFirmware. The combination of a hardware device, computer instructions, and computer data that\nreside as read-only software on the hardware device.\nGOTS software. (See COTS/GOTS software.)\nInspection. A process whereby products are reviewed for correctness, completeness, quality, and\ncompliance with requirements and standards. The process is carried out by one or more peers of\nthe product\xe2\x80\x99s developer. (Contrast with walkthrough.)\nJoint application design (JAD). A facilitated workshop technique that brings together principal\nstakeholders to solve a well-defined problem to produce a well-defined product or set of\nproducts .\nLife cycle. (See software life cycle.)\nLife-cycle model. A framework on which to map activities, methods, standards, procedures,\ntools, products, and services (for example, waterfall, spiral).\nLife-cycle phase. A division of the software effort into non-overlapping time periods. Life-cycle\nphases are important reference points for the software manager. Multiple activities may be\nperformed in a life-cycle phase; an activity may span multiple phases. (Contrast with activity.)\nMaintenance. Implementation of problem fixes and minor enhancements to an operational\nproduct. (Contrast with new development, enhancement; see also software engineering.)\nMethod. A technique or approach, possibly supported by procedures and standards, that\nestablishes a way of performing activities and arriving at a desired result.\nMethodology. \xe2\x80\x9cA collection of methods, procedures, and standards that defines an integrated\nsynthesis of engineering approaches to the development of a product.\xe2\x80\x9d (Reference 22)\nMilestone review. A process or meeting involving representatives of both the customer and the\nsoftware team, during which project status, software products, and project issues are examined\nand discussed.\nModule. A cohesive group of software units that performs a software function.\nNASA-GB-001-96\n\n96\n\nNew development. (See development.)\nNew technology. A technology that has not yet been proven to be effective in practice in a\nparticular NASA application area or domain. It may have been proven elsewhere, however. (See\nalso technology.)\nNon-developed item (NDI). A component that is not newly created by the software team. This\nincludes reused components, COTS and GOTS components, and components developed by other\nNASA groups or contractor personnel.\nOrganization. \xe2\x80\x9cA unit within [NASA] within which many projects are managed as a whole. All\nprojects within an organization share a common top-level manager and common policies.\xe2\x80\x9d\n(Reference 22)\nPhase. (See life-cycle phase.)\nPolicy. \xe2\x80\x9cA guiding principle, typically established by senior management, which is adopted by an\norganization or project to influence and determine decisions.\xe2\x80\x9d (Reference 22) (Contrast with\nprocedure, standard.)\nProcedure. A written description of the roles, responsibilities, and steps required for performing\nan activity or a subset of an activity. (Contrast withpolicy, standard.)\nProcess. \xe2\x80\x9cA sequence of steps performed for a given purpose; for example, the software\nengineering process.\xe2\x80\x9d (Reference 9)\nProcess asset library (PAL). A NASA library, including a life-cycle methodology description,\nstandards and procedures, guidebooks and handbooks, style guides, software profiles, key\nlessons, study results, tailored processes, examples of acceptable products, etc.\nProduct. (See software product.)\nProject. (See software project.)\nProject process. A tailored version of the organization\xe2\x80\x99s standard process, defining and\nintegrating specific life-cycle models, activities, methods, procedures, standards, and tools used\nto accomplish delivery of the project\xe2\x80\x99s required products and services. It is defined in the\nPrototype. An early experimental model of a system, system component, or system function that\ncontains enough capabilities for it to be used to establish or refine requirements, or to validate\ndesign concepts.\nQualification testing. Testing performed to verify and demonstrate (often to the customer) that a\nsoftware CI or a system meets its specified requirements.\nRecord. To record information means to set down in a manner that can be retrieved and viewed.\nThe result may take many forms, including but not limited to hand-written notes, hard-copy or\nelectronic documents, and data recorded in CASE and project management tools. Information to\nbe delivered to the customer must be in the form, format, and medium agreed to with the\ncustomer. For information not to be delivered to the customer, the software manager selects the\nappropriate form, format, and medium.\nRelease. A build that is delivered to the customer. (See build.)\n97\n\nNASA-GB-001-96\n\nRequirement. A characteristic that a system or software CI must possess in order to be\nacceptable to the customer.\nReused code. Code that has undergone no more than 25 percent change; that is, converted code\nand transported code. (See converted unit, transported unit.)\nSoftware configuration item (CI). An aggregation of software that satisfies an end use function\nand is designated for separate configuration management by the customer or the maintenance\nteam. Software CIS are selected on the basis of tradeoffs among software function, size, host or\ntarget computers, developer, support concept, plans for reuse, criticality, interface considerations,\nneed to be separately documented and controlled, and other factors.\nSoftware engineering. A set of activities that results in software products. Software engineering\nincludes new development, modification, reuse, reengineering, maintenance, or any other\nactivities that result in software products.\nSoftware engineering environment. The facilities, hardware, software, firmware, procedures,\nand documentation needed to perform software engineering. Elements may include but are not\nlimited to CASE tools, compilers, assemblers, linkers, loaders, operating systems, debuggers,\nsimulators, emulators, documentation tools, and database management systems.\nSoftware engineering process. An organized set of activities performed to translate user needs\ninto software products.\nSoftware life cycle. \xe2\x80\x9cThe period of time that begins when a software product is conceived and\nends when the software is no longer available for use.\xe2\x80\x9d (Reference 9) A life cycle is typically\ndivided into life-cycle phases. (See life-cycle phase.)\nSoftware process. (See software engineeringprocess.)\nSoftware product. Software or associated information created, modified, or incorporated to\nsatisfy the software requirements. Examples include plans, requirements, design, code, databases,\ntest information, and manuals.\nSoftware product evaluation. Activities performed by the software team to ensure that inprocess and final software products meet criteria established for those products. (Contrast with\nsoftware quality assurance.)\nSoftware project. An undertaking requiring a concerted effort, which is focused on developing\nor maintaining a specific software product. Typically a software project has its own funding, cost\naccounting, and delivery schedule. That is, the project is the work to be done and the personnel\nassigned to perform the work; it is not the same as the product (for example, system) that they are\nto produce.\nSoftware quality assurance (SQA). Activities performed by independent QA personnel to\n(1) ensure that each activity described in the software plan is performed in accordance with the\nsoftware plan, and (2)ensure that each software product required by the organization or by\nsoftware requirements exists and has undergone software product evaluations, testing, and\ncorrective action as required. (Contrast with software product evaluation.)\nSoftware system. A system consisting solely of software and possibly the computer equipment\non which the software operates.\n\nNASA-GB-001-96\n\n98\n\nSoftware team. The group that engineers software products (including new development,\nmodification, reuse, reengineering, maintenance, or any other activity that results in software\nproducts).\nSoftware test environment. The facilities, hardware, software, firmware, procedures, and\ndocumentation needed to perform qualification, and possibly other, testing of software. Elements\nmay include but are not limited to simulators, code analyzers, test plan generators, and path\nanalyzers, and may also include elements used in the software engineering environment.\nSoftware transition. The set of activities that enables responsibility for software engineering to\npass from one organization, usually the organization that performs initial software development,\nto another, usually the organization that will perform sustaining engineering.\nSoftware unit. Smallest physical element of software processed by source code translators,\ncompilers, and assemblers.\nStandard. Written criteria used to develop and evaluate a product or to provide and evaluate a\nservice. (Contrast with policy, procedure.)\nSustaining engineering. Maintenance and enhancement of an operational product. (See\nmaintenance, enhancement.)\nSystem. Operational entity composed of a set of interrelated and cohesive configuration items\n(CIS). (See configuration item (Cr).)\nTechnology change management. \xe2\x80\x9c... [Ildentifling, selecting, and evaluating new technologies,\nand incorporating effective technologies into the organization. The objective is to improve\nsoftware quality, increase productivity, and decrease cycle time for product engineering.\xe2\x80\x9d\n(Reference 22)\nTechnology infusion. (Used synonymously with technology transfer; see technology transfer.)\nTechnology management. (See technology change management.)\nTechnology transfer. The successful importing or exporting of technology from lab to practice\nor from practice to practice.\nTechnology utilization. The application and integration of appropriate technologies in software\nefforts.\nTimebox. A subdivision of a release created to achieve a unit of production that is manageable in\nsize, complexity, staffing, and duration. It is a planning construct that controls functionality\ndelivered by fixing resources and time.\nTransported unit. Existing unit that is used verbatim (except for possible changes to the\ndevelopment history for traceability). Its origin is usually external to the project. (Contrast with\nnew unit, adapted unit, converted unit.)\nUnit. (See software unit.)\nValidation. \xe2\x80\x9cThe process of evaluating software during or at the end of the software\ndevelopment process to determine whether it satisfies specified requirements.\xe2\x80\x9d (Reference 9)\n(Contrast with verification.)\n\n99\n\nNASA-GB-00 1-96\n\nVerification. \xe2\x80\x9cThe process of evaluating software to determine whether the products of a given\ndevelopment phase [or activity] satisfj the conditions imposed at the start of that phase [or\nactivity].\xe2\x80\x9d (Reference 9) (Contrast with validation.)\nWalkthrough. Detailed technical presentation of a limited aspect or portion of a product. These\npresentations are usually made by the product\xe2\x80\x99s developer. (Contrast with inspection.)\n\nNASA-GB-00 1-96\n\n100\n\nAppendix B. Building for Reuse\neuse can apply to people, code, processes, requirements, and design, as well as to plans,\nstandards, and test scripts. Although a recent study of NASA software (Reference 1)\n\'ndicated that most reuse at the Agency focuses solely on code, reusing any of these\nresources in the development of a new system is appropriate when the result will be improved\nreliability, productivity, and maintainability without a negative impact on performance or\nrequirements satisfaction. Planning for reuse maximizes its benefits; therefore, design software\nprojects for reuse from the outset. During the requirements definition and analysis and design\nphases, for example, developers need to identifj potentially reusable architectures, designs, code,\nand approaches. To facilitate this process, provide developers with the structure and tools that\nwill assist them in finding and reusing existing components and architectures rather than\ndeveloping new software from scratch. Several activities can be performed throughout the life\ncycle to enable reuse:\nPerform domain analysis by examining the application domain to identifj common\nrequirements and functions. The result is a standard, general architecture or model\nthat incorporates the common functions of a specific applications area and can be\ntailored to accommodate differences among individual projects.\nDomain analysis enables requirements generalization, which involves preparing\nrequirements and specifications so that they cover a selected family of projects or\nmissions.\nDuring the design phase, explicitly design for reuse by providing modularity, standard\ninterfaces, and parameterization.\nPlace reusable source code in a reuse library, along with the associated requirements,\nspecifications, design documentation, and test data. Such a library should also contain\na search facility that enables varied access to the software (for example, by keyword\nor by name).\nA final activity that enables reuse on future projects involves the application of reuse\npreservation techniques during the maintenance and operations phase. The\nmaintenance team should avoid making quick fixes, which can negatively affect the\nreusability of the system. Rather, they should apply many of the same design practices\nthat promote reuse during software development. Alternatively, a separate\nmaintenance team might be established to maintain only the software in a reusable\nsoftware library. The team would notifj projects that incorporate software from the\nlibrary of any changes to that software.\n\nR,\n\nI\n\nSystems employing a high percentage of reused software typically still undergo each phase of the\nfull development life cycle, but certain reviews and documents may be consolidated and phase\nschedules collapsed or overlapped (see Figure Appendix B. -1). Reusing design, documentation,\nand code written for a previous project (and perhaps adapting it to some degree for use in the\nsystem to be developed) requires less effort than creating entirely new products.\n\n101\n\nNASA-GB-001-96\n\nReuse Production\ncantext\n\nExisting Systems\n\nA"alySK\n\nReuse Preservation\n\nTest\n\nApplication\nEngineering\n\nFigure Appendix B. -1. High-Reuse Life-Cycle Model\n\nNASA-GB-00 1-96\n\n102\n\nAppendix C. COTS, GOTS, Reused, and Other NDI\nSoftware Products\n\nT\n\nhis appendix provides guidance to the software manager for satisfying the requirements of\nthis guidebook when applied to incorporating COTS, GOTS, reused, and other NDI\nproducts.\n\nAppendix C. . COTS Software Products\nI\nUsing COTS products involves cultural implications. For example, in making a decision to\nincorporate COTS software, stakeholders must first be willing to accept the capabilities it\nprovides. Although it is possible to negotiate with a vendor to obtain and modi@ the source code,\nin so doing vendor support will be lost, and the system will be inconsistent with future product\nreleases.\nWhen considering the use of a COTS product, conduct the following activities during the\nactivities indicated:\nPrototype during requirements definition and analysis to evaluate the product\xe2\x80\x99s\ncapabilities and performance. Whereas prototyping typically is conducted to clarify\nrequirements or define a user interface, here it is performed to assess whether the\nproduct meets already defined functionality and performance requirements. Prototype\nto analyze the performance of the product, to assess its capabilities in relation to the\nrequirements, and to evaluate its ease of use. Conduct this activity as early as possible\nin the life cycle, because the resulting decision on whether to use a COTS product\nmay affect the overall system architecture. Defining the system architecture thus\nbecomes an iterative step that evolves based on the results of the prototyping efforts,\nbecause only a certain requirement or set of requirements may be satisfied by COTS\ncapabilities.\nAs soon as prototyping results indicate that use of a COTS product is appropriate,\nplan for its interfaces to other COTS products or to the rest of the system. The\ninput/output requirements of the COTS product must be carefully and completely\ndefined and understood, so that its interfaces to other components, custom as well as\nCOTS, can be created correctly. At this point, assess the following:\n- The effort required to develop the interface software versus the effort required to\ncustom develop all the code\n- The maintainability of the COTS software\n- The reliability of the COTS software\n- The quality and reliability of vendor support (which will be crucial to successful\nimplementation)\n\n103\n\nNASA-GB-00 1-96\n\nDuring design, address the connections to COTS products in the same way as other system\ninterfaces. Because no design products exist for COTS software, devise artifacts that provide the\ntraceability from COTS products to requirements.\nDuring implementation, no code will be written for the COTS products themselves; however,\ntesting becomes an issue because a COTS product cannot be tested at the unit level (that is, as in\nunit testing of custom software). The test plan must thus focus on the interfaces to COTS\nproducts. Apply additional rigor in analyzing the test output. Do not assume that less testing\neffort will be required for a COTS product; rather, the effort typically applied at the unit and\nmodule levels will simply be applied to testing at a higher level for COTS interfaces. If test\nresults indicate that a COTS portion of the system is not performing as required, obtain vendor\nsupport in troubleshooting and correcting the problem.\nThroughout the development cycle, reassess the selection of a COTS product when requirements\nchanges affect a part of the architecture that is satisfied by that product.\nContinually stay abreast of and reassess new versions of COTS products. Conduct additional\nprototyping to evaluate their use and enable the project to take advantage of new capabilities or\nto recognize possible changes in system architecture. However, at some point in the life cycle, it\nwill be necessary to finalize the version selection and proceed with development, regardless of\nnew capabilities that may become available. If using a multi-release approach, allow flexibility in\nthe design to facilitate the upgrade of COTS products assigned to later releases.\nDuring operations, weigh the need for capabilities provided by new versions of the COTS\nsoftware against a possible lack of vendor support for older versions. For operational systems\nwith multi-site installations, apply rigorous configuration management to ensure version control.\nManaging a project that comprises predominantly COTS products is similar to managing a\ncustom-development project. All of the same good management practices apply here as well.\nHowever, package vendors are a source of risk that needs special attention. When a project\nselects a package to be part of a system, it is buying a long-term relationship with a vendor.\nUnderstanding a vendor\'s financial stability, track record, and long-term strategy can be as\nimportant as understanding the vendor\'s product. Not only must the project develop a partnership\nwith its vendors, but it must also develop a partnership with the contracting and procurement side\nof its own organization. Software project managers must acquire a level of business\nunderstanding well beyond that needed in conventional development to successfully manage the\nproject.\nThis life-cycle concept requires the formation of a well-integrated team consisting of end-users,\ndomain experts, software engineers, independent testers, a system administrator, and a\nprocurement official, all coordinated by a project leader. The team roles must remain intact for\nthe entire system life cycle, and team members must be empowered to make decisions as to\nwhich system requirements are critical. Many of the traditional roles change slightly or require\ndifferent skills. In addition, the project team includes two new roles, the system administrator and\na procurement official, who play significant roles in comparison to the minimal support needed\nin these areas on traditional software development projects. A capable and interested\nprocurement official and systems administrator will be great assets to the project. If possible,\narrangements should be made to have a single procurement official and a system administrator\n\nNASA-GB-001-96\n\n104\n\nassigned to the project. Every effort should be made to make them aware of project needs and to\nwelcome them as members of the team.\nThe primary project roles are as follows:\n\nEnd-User. The end-users are the people who will be using the system operationally. They have a\nclear understanding of the requirements and the operational environment and are empowered to\nnegotiate requirements changes and represent the end-user organization.\nDomain expert. Domain experts have extensive experience in the problem domain and are aware\nof existing packages that are available within the domain. They have experience with, or are at\nleast aware of, other package-based systems within the domain from which architectures can be\nreused.\nSoftware engineer. Software engineers or developers are responsible for developing glueware\nand integrating the packages. They are responsible for engineering a solution that meets the\ncustomer\xe2\x80\x99s and end-users\xe2\x80\x99 quality expectations. It is best if the software engineers have some\nexperience with COTS integration or have specific experience with the packages being used.\nIndependent tester. Independent testers are responsible for verifjing that the system meets its\nrequirements. Experience with the application domain, incremental testing, and black-box testing\nis helpful.\nProcurement o f f e r . Procurement officers are responsible for obtaining demonstration copies for\nevaluation; purchasing selected products and negotiating for extensions of demonstration copies\nuntil official receipt of product; monitoring and extending license expiration dates. They are\nresponsible for keeping the project point-of-contact informed of expected product arrival dates\nand the terms of the contracts.\nSystem administrator. System administrators are responsible for installing COTS products as\nthey are received and setting up accounts as they are needed. They can also help troubleshoot\nproblems with hardware/package compatibility. It is critical that a system administrator be\navailable to provide services immediately upon request, so it is best if one is dedicated to the\nproject.\n\nAppendix C. .2 Evaluating COTS, GOTS, Reused, and Other NDI\nSoftware Products\nThe software manager specifies in the software plan the criteria for evaluating COTS, GOTS,\nreused, and other NDI software products for use in fulfilling software requirements. General\ncriteria are the software product\xe2\x80\x99s ability to meet specified requirements and cost-effectiveness\nover the life of the system. Examples of specific criteria include but are not limited to the\nfollowing:\nAbility to provide required capabilities and meet required constraints\nAbility to provide required safety, security, and privacy\nReliability and maturity, as evidenced by an established track record\nTestability\nInteroperability with other system and system-external elements\n\n105\n\nNASA-GB-001-96\n\nFielding issues, including:\n\n- Restrictions on copying and distributing the software or documentation\n- License or other fees applicable to each copy\nMaintainability\n- Likelihood the software product will need to be changed\n- Feasibility of accomplishing that change\n- Availability and quality of documentation and source files\n- Likelihood that the supplier will continue to support the current version\n- Impact on the system if the current version is not supported\n- The customer\xe2\x80\x99s data rights to the software product\nWarranties available\nShort- and long-term cost impacts of using the software product\nTechnical, cost, and schedule risks and tradeoffs in using the software product\n\nAppendix C. .3 Guidelines for Performing Required Activities\nInvolving COTS, GOTS, Reused, and Other NDI Software\nProducts\nThe following guidelines are provided to help interpret and satisfj the requirements to perform\nlife-cycle activities:\nAny software product required by this document may be a COTS, GOTS, reused, or\nother NDI software product as long as it meets the criteria established in the software\nplan. The software product may be used as is or modified.\nWhen COTS, GOTS, reused, or other NDI software has been selected to be\nincorporated into the delivered software product, some requirements in this document\nmust have special interpretation. Table Appendix C. -1 provides this interpretation.\nKey issues are whether the software will be modified, whether the unmodified\nsoftware constitutes an entire software CI or only one or more software units, and\nwhether the unmodified software has a positive performance record. The table is\npresented in a conditional manner: If an activity in the left column is required for a\ngiven type of software, the table tells how to interpret the activity for COTS, GOTS,\nreused, or other NDI software of that type.\n\nNASA-GB-00 1-96\n\n106\n\nTable Appendix C. -1. Guidelines for Using COTS, GOTS, Reused, and Other NDI Software\nProducts\n(1 of 2)\n\nSoftware project\nplanning\n\nInclude the activities in this table in project plans.\n\nSoftware CI\nrequirements\ndefinition and\nanalysis\n\nSpecify the project-specific\nrequirements the software CI must\nmeet; verify through records or retest\nthat the software CI can meet them.\n\nConsider the component\'s capabilities and characteristics in\nspecifying the requirements for the software CI of which it is a part.\n\nSoftware CI-wide\ndesign\n\nNo requirement: the software CI-wide\ndesign decisions have already been\nmade.\n\nConsider the component\'s capabilities and characteristics in\ndesigning software CI behavior and making other software CI-wide\ndesign decisions.\n\nSoftware CI\narchitectural\ndesign\n\nNo requirement: the software Cl\'s\narchitecture is already defined.\n\nInclude the component in the software CI architecture and allocate\nsoftware CI requirements to it.\n\nSoftware CI\ndetailed design\n\nNo requirement: the software Cl\'s\ndetailed design is already defined.\n\nNo requirement: the component is\nalready designed.\n\nModify the component\'s\ndesign as needed.\n\nSoftware\nimplementation\n\nNo requirement: the software for the\nsoftware Cl\'s components is already\nimplemented.\n\nNo requirement: the software for the\ncomponent is already implemented.\n\nModify the software for\nthe component.\n\nUnit testing\n\nNo requirement:\nthe software Cl\'s\nunits are already\ntested.\n\nPerform\nselectively if in\nquestion and\nunits are\naccessible.\n\nNo requirement:\nthe unit is\nalready tested.\n\nPerform this testing.\n\nIntegration and\ntesting\n\nNo requirement:\nthe software Cl\'s\ncomponents are\nalready\nintegrated.\n\nPerform\nselectively if in\nquestion and\ncomponents are\naccessible.\n\nPerform except\nwhere integration\nis already tested\nor proven.\n\nPerform this testing\n\nSoftware CI\nqualification\ntesting\n\nNo requirement:\nsoftware CI is\nalready tested\nand proven.\n\nPerform this\ntesting.\n\nInclude the component in software CI qualification testing.\n\n107\n\nNASA-GB-00 1-96\n\nTable Appendix C. -1. Guidelines for Using COTS, GOTS, Reused, and Other NDI Software\nProducts\n(2 of 2)\n\nPreparation for\nSoftware Delivery\n\nSoftware project\nclose-out\n\nApply to activities performed and software products prepared, modified, or used in incorporating this\nsoftware.\n\nSoftware product\nV&V\n\nApply to software products prepared or modified in incorporating this software; for software products used\nunchanged, apply unless a positive performance record or evidence of past evaluations indicates that\nsuch an V&V would be duplicative.\n\nSoftware\nconfiguration\nmanagement\n\nApply to all software products prepared, modified, or used in incorporating this software.\n\nSoftware quality\nassurance\n\nApply to all activities performed and all software products prepared, modified, or used in incorporating this\nsoftware.\n\nMilestone reviews\n\nCover the software products prepared or modified in incorporating this software; explicitly discuss COTS,\nGOTS, reused, and other NDI products.\n\nSoftware process\nimprovement\n\nApply to all activities performed in engineering this software.\n\nSystem\nrequirements\nanalysis\n\nI\n\nInclude the software for the software CI or component in the executable software; prepare source files for\nthe software CI or component, if available; include version descriptions; handle any license issues;\nprepare or provide as-built design descriptions for software whose design is known; cover use of the\nsoftware CI or component, as appropriate, through existing, new, or revised user or operator manuals;\ninstall the software CI or component at the support site; demonstrate regenerability if source is available;\ninclude the training offered.\n\nConsider software\'s capabilities in defining the system and operations concept and system requirements.\n\nUse test or\nperformance\nrecords to\nconfirm ability to\nmeet needs.\n\nTest to confirm\nability to meet\nneeds.\n\nUse test or\nperformance\nrecords to\nconfirm ability to\nmeet needs.\n\nTest to confirm\nability to meet\nneeds.\n\nUse tests or records to\ndetermine potential to\nmeet needs.\n\nSystem-wide\ndesign\n\nConsider the software\'s capabilities and characteristics in designing system behavior and in making other\nsystem-wide design decisions.\n\nSystem\narchitectural\ndesign\n\nInclude the software CI in the system\narchitecture; allocate system\nrequirements to it.\n\nSoftware CI and\nhardware CI\nintegration and\ntesting\n\nwhere\nintegration is\n\nsoftware CI in\nsoftware CI and\n\nproven.\n\nintegration and\n\nSystem\nqualification\ntesting\n\nInclude the software CI in system\nqualification testing.\n\nNASA-GB-00 1-96\n\nConsider the component\'s capabilities and characteristics in\ndesignating software CIS and allocating system requirements to\nthem.\nInclude the component in software CI and hardware CI integration\nand testing.\n\nInclude the component in system qualification testing\n\n108\n\nI\n\nAppendix D. System-Level Considerations\n\nW\n\nhen the software CI is part of a larger hardware-software system for which the\norganization has system-level responsibilities, a number of additional considerations\nmust be taken into account. In the following paragraphs regarding system-level\nactivities, if the software covered by this document is part of a hardware-software system for\nwhich this document covers only the software portion, participate means take part in, as\ndescribed in the software plan. If the software (and the computers on which it executes) is\nconsidered to constitute a system, participate means be responsible for.\n\nAppendix D. . System Requirements Analysis\nI\nThe software requirements analysts participate in system requirements analysis in accordance\nwith the requirements discussed in the subsections that follow.\nAnalysis of User Input\nThe software requirements analysts participate in analyzing user input provided by the customer\nto gain an understanding of user needs. This input may take the form of need statements, surveys,\nproblem reports and change requests, feedback on prototypes, interviews, or other user input or\nfeedback. This input is used to formulate the system and operations concept and the system\nrequirements.\nSystem and Operations Concept\nThe software requirements analysts participate in defining and recording the operational concept\nfor the system. The result includes all applicable items in the system and operations concept\ndocumentation standard, including the preparation of any required operational scenarios.\nSystem Requirements\nThe software requirements analysts participate in defining and recording the requirements to be\nmet by the system and the methods to be used to ensure that each requirement is met. The result\nincludes all applicable items in the system requirements specification (SRS) documentation\nstandard.\nIf a system consists of subsystems (or CIS), the activity in this subsection is intended to be\nperformed iteratively with the system design activities to define system requirements, design the\nsystem and identify its subsystems, define the requirements for and interfaces among those\nsubsystems, design the subsystems, identify their components, and so on.\n\nAppendix D. .2 System Design\nThe software requirements analysts and software design architects participate in system design in\naccordance with the requirements discussed in the subsections that follow.\n\n109\n\nNASA-GB-00 1-96\n\nSystem-Wide Design Decisions\nThe software requirements analysts and software design architects participate in defining and\nrecording system-wide design decisions (that is, decisions about the system\xe2\x80\x99s behavioral design\nand other decisions that affect the selection and design of system components). The result\nincludes all applicable items in the system-wide design section of the system design specification\n(SDS) documentation standard.\nDesign decisions remain at the discretion of the software requirements analysts and software\ndesign architects unless formally converted to requirements. The software team is responsible for\nfulfilling all requirements and demonstrating this fulfillment through qualification testing.\nDesign decisions act as software team-internal \xe2\x80\x9crequirements,\xe2\x80\x9d to be implemented, imposed on\ncontractors (if applicable), and confirmed by software team-internal testing; but their fulfillment\nneed not be demonstrated to the customer.\nSystem Architectural Design\nThe software requirements analysts and software design architects participate in defining and\nrecording the architectural design of the system (identifjing the components of the system, their\ninterfaces, and a concept of execution among them) and the traceability between the system\ncomponents and system requirements. The result includes all applicable items in the architectural\ndesign and traceability sections of the SDS documentation standard.\n\nAppendix D. .3 Software CI and Hardware CI Integration and Testing\nSoftware CI and hardware CI integration and testing means integrating software CIS with\ninterfacing hardware CIS and software CIS, testing the resulting groupings to determine whether\nthey work together as intended, and continuing this process until all software CIS and hardware\nCIS in the system are integrated and tested. The software qualification testers participate in\ndeveloping and recording test plans (in terms of inputs, expected results, and V&V criteria), test\nprocedures, and test data for conducting software CI and hardware CI integration and testing. The\ntest plans cover all aspects of the system-wide and system architectural design. The software\nqualification testers participate in software CI and hardware CI integration and testing in\naccordance with the software CI and hardware CI integration test plans and procedures. The\nsoftware team participates in analyzing the results of software CI and hardware CI integration\nand testing. Software-related analysis and test results are recorded in appropriate product V&V\nrecords files. The software team makes necessary revisions to the software, participates in\nretesting, and updates other software products as needed, based on the results of software CI and\nhardware CI integration and testing.\n\nAppendix D. .4 System Qualification Testing\nSystem qualification testing is performed to demonstrate (often to the customer) that system\nrequirements have been met. It covers the SRS. This testing contrasts with software team-internal\nsystem testing, performed as the final stage of software CI and hardware CI integration and\ntesting.\n\nNASA-GB-00 1-96\n\n110\n\nThe persons responsible for fulfilling the requirements in this section are not the persons who\nperformed detailed design or implementation of software in the system, although those persons\nmay participate, for example, by contributing test plans that rely on knowledge of the system\xe2\x80\x99s\ninternal implementation.\nThe software qualification testers participate in developing and recording the test preparations,\ntest plans, and test procedures to be used for system qualification testing and the traceability\nbetween the test plans and the system requirements. For software systems, the results include all\napplicable items in the software CI qualification test plan documentation standard. The software\nqualification testers participate in preparing the test data needed to carry out the test plans and in\nproviding the customer advance notice of the time and location of system qualification testing.\nThey participate in system qualification testing in accordance with the system test plans and\nprocedures. The software team participates in analyzing and recording the results of system\nqualification testing. For software systems, the result includes all applicable items in the software\nCI qualification test report documentation standard. The software team makes necessary\nrevisions to the software, provides the customer advance notice of retesting, participates in\nretesting, and updates other software products as needed, based on the results of system\nqualification testing.\n\n111\n\nNASA-GB-00 1-96\n\nAbbreviations and Acronyms\nAT\n\nacceptance test or testing\n\nATRR\n\nacceptance test readiness review\n\nBDR\n\nbuild design review\n\nBQT\nCASE\n\nbuild qualification testing\n\nCDR\n\ncritical design review\n\nCI\n\nconfiguration item\n\nCM\n\nconfiguration management\n\nCOCOMO\n\nConstructive Cost Model\n\nCOTS\n\ncommercial-off-the-shelf\n\nCCB\n\nconfiguration control board\n\nDFD\n\ndata flow diagram\n\nDR\n\ndiscrepancy report\n\nFCA\n\nfunctional configuration audit\n\nFQT\nGOTS\n\nformal qualification testing\ngovernment-off-the-shelf\n\nGSFC\n\nGoddard Space Flight Center\n\nHQ\nIDR\n\nheadquarters\n\nIRM\n\nInformation Resources Management\n\nIV&V\n\nindependent validation and verification\n\nJAD\n\njoint application development\n\nJPL\n\nJet Propulsion Laboratory\n\nMSFC\n\nMarshall Space Flight Center\n\nNASA\n\nNational Aeronautics and Space Administration\n\nNDI\n\nnon-developed item\n\nNMI\n\nNASA Management Instruction\n\nO&M\n\noperations and maintenance\n\nORR\n\noperational readiness review\n\ncomputer-aided software engineering\n\ninternal DR\n\n113\n\nNASA-GB-001-96\n\nOSMA\n\nOffice of Safety and Mission Assurance\n\nPAL\n\nprocess asset library\n\nPCA\n\nphysical configuration audit\n\nPDR\n\npreliminary design review\n\nQA\nQTRR\n\nquality assurance\n\nRCR\n\nrelease contents review\n\nRDR\n\nrelease design review\n\nRQTRR\n\nrelease qualification test readiness review\n\nRRR\n\nrelease requirements review\n\nSCM\n\nsoftware configuration management\n\nSCR\n\nsystem concept review\n\nSDR\n\nsystem design review\n\nSDS\n\nsystem design specification\n\nSEI\n\nSoftware Engineering Institute\n\nSEL\n\nSoftware Engineering Laboratory\n\nSPR\n\nsystem or software problem report\n\nSQA\n\nsoftware quality assurance\n\nSRR\n\nsystem requirements review\n\nSRS\n\nsystem requirements specification\n\nSSR\n\nsoftware specification review\n\nSTR\n\nsystem or software trouble report\n\nSWDS\n\nsoftware design specification\n\nSWG\n\nSoftware Working Group\n\nSWRS\n\nsoftware requirements specification\n\nTBD\n\nto be determined\n\nTPM\n\nTechnical Performance Measurement\n\nV&V\n\nvalidation and verification (also validate and verify)\n\nNASA-GB-001-96\n\nqualification test readiness review\n\n114\n\nReferences\n1.\n\nProfie o Software at the National Aeronautics and Space Administration (NASA),\nf\nSoftware Engineering Program, NASA-RPT-004-95, March 1995.\n\n2.\n\nJeletic, K, R. Pajerski, C. Brown, Software Process Improvement Guidebook, Software\nEngineering Program, NASA-GB-00 1-95, January 1996.\n\n3.\n\nNASA Software Strategic Plan, NASA Software Program, Fairmont, West Virginia, July\n1995.\n\n4.\n\nMIL-STD-498, Software Development and Documentation, Department of Defense,\nDecember 5 , 1994.\n\n5.\n\nBassman, M., F. McGarry, R. Pajerski, Software Measurement Guidebook, Software\nEngineering Program, NASA-GB-001-94, August 1995. Also published as SEL-94-102,\nSoftware Engineering Laboratory, NASA/GSFC, June 1995.\n\n6.\n\nReusable Software Management Plan (SMP) and On-line Help Tool, Software Assurance\nTechnology\nCenter,\nNASA/GSFC,\nhttp://satc.gsfc.nasa.gov/Documents/smp/\nsmppage.html .\n\n7.\n\nLandis, L., F. McGarry, S. Waligora, et al., Manager\xe2\x80\x99s Handbook for Software\nDevelopment (Revision I), SEL-84- 101, Software Engineering Laboratory, NASA/GSFC,\nNovember 1990, http :// fdd.gsfc.nasa.gov/mgr-hand/mnghnbk. html .\n\n8.\n\nAlberts, C. J., et al., Continuous Risk Management Guidebook (DRAFT version 0.3),\nSoftware Engineering Institute, Camegie Mellon University, January 1996.\n\n9.\n\nANSVIEEE-STD-610.12-1990, \xe2\x80\x9cIEEE Standard Glossary of Software Engineering\n\n10.\n\nLandis, L., S. Waligora, F. McGarry, et al., Recommended Approach to Software\nDevelopment (Revision 3), SEL-81-305, Software Engineering Laboratory, NASA/GSFC,\nJune 1992.\n\n11.\n\nBoehm, B., \xe2\x80\x9cA Spiral Model of Software Development and Enhancement,\xe2\x80\x9d IEEE\nComputer, May 1988.\n\n12.\n\nWaligora,. S., SEL Package-Based System Development Process, Software Engineering\nLaboratory, NASA/GSFC, February 1996, http://fdd.gsfc.nasa.gov/cotsweb.pdf.\n\n13.\n\nNASA Software Formal Inspections Guidebook, NASA-GB-A302, August 1993,\naccessible from http://www.ivv.nasa.gov/SWG/.\n\n14.\n\nWeller, E., \xe2\x80\x9cLessons from Three Years of Inspection Data,\xe2\x80\x9d IEEE Software, September\n1993.\n\n15.\n\nCurrit, P. A., M. Dyer, and H. D. Mills, \xe2\x80\x9cCertifLing the Reliability of software,\xe2\x80\x9d IEEE\nTransactions on Software Engineering, Vol. SE- 12, No. 1, January 1986, pp. 3-1 1.\n\n115\n\nNASA-GB-00 1-96\n\n16.\n\nBasili, V, and S. Green, \xe2\x80\x9cSoftware Process Evolution at the SEL,\xe2\x80\x9d IEEE Software, Vol.\n11, NO.4, July 1994, pp. 58-66.\n\n17.\n\nSoftware Engineering Evaluation System Technical Assessment Procedures and\nWorkshops, NASA Headquarters, Office of Safety and Mission Assurance, U.S. Army\nMissile Command, Redstone Arsenal, Alabama, 1994.\n\n18.\n\nNASA Management Instruction 7120.4, \xe2\x80\x9cManagement of Major System Programs and\n\n19.\n\nNASA Handbook 7120.5, \xe2\x80\x9cManagement of Major System Programs and Projects,\xe2\x80\x9d\nNovember 1993.\n\n20.\n\nBoehm, B. Software Engineering Economics, Prentice-Hall, Inc., Englewood Cliffs, NJ,\n1981.\n\n21.\n\nCondon, S., M. Regardie, M. Stark, and S. Waligora, Cost and Schedule Estimation\nReport, SEL-93-002, NASNGSFC, November 1993.\n\n22.\n\nf\nPaulk, M, et al., Key Practices o the Capability Maturity Model, Version 1.1, Software\nEngineering Institute, Camegie Mellon University, CMU/SEI-93-TR-25, February 1993.\n\nNASA-GB-00 1-96\n\n116\n\n'