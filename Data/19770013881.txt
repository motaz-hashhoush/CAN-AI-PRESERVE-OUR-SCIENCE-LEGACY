b'General Disclaimer\nOne or more of the Following Statements may affect this Document\n\nThis document has been reproduced from the best copy furnished by the\norganizational source. It is being released in the interest of making available as\nmuch information as possible.\n\nThis document may contain data, which exceeds the sheet parameters. It was\nfurnished in this condition by the organizational source and is the best copy\navailable.\n\nThis document may contain tone-on-tone or color graphs, charts and/or pictures,\nwhich have been reproduced in black and white.\n\nThis document is paginated as submitted by the original source.\n\nPortions of this document are not fully legible due to the historical nature of some\nof the material. However, it is the best reproduction available from the original\nsubmission.\n\nProduced by the NASA Center for Aerospace Information (CASI)\n\nOptimal Estimation for Discrete\nTime Jump Processes\n\nMarco V. Vaca and Steven A. Tretter\n\nMarch 25, 1977\n\n\t\n\nV\n\n^,^ 19\n3\nQpp 1 `,\'\n\nC-0\t\nLr\n\n`r N\n\nf ,\nNSN S1)Bf^\t\nIN)\'^j\n\nc-^\t\n\ns^\n\ni\n\n.^ ^, Z 129\tC\n\n~\t\n\nI\n\n>\n\n11AENT OF\nLA.\t 9\nELECTRICAL ENGINEERING\nUNIVERSITY OF MARYLAND\nCOLLEGE PARK. MARYLAK^? 20742\n(NASA-CR-152620)\t\n\nOPTTMAL ESTIMATION FOR\n\nN77-20925\n\nDISCPETE TIME JUMP PROCESSES (Maryland\n\'\xe2\x96\xba niv.)\t\n\n30 P HC A03/M v A01\t\n\nCSCL 124\n\nOnclas\nG3/64 22805\n\nf\n\nOptimal Estimation for Discrete\n^J\n\nTime Jump Processes\n\nEl\n\nMarco V. Vaca* and Steven A. Tretter*"r\n\n`I\nr\'\n\nk\n\nABSTRACT\nIn this paper we obtain optimum estimates of nonobservable random\nvariables or random processes which influence the rate \'=\'unctions of a\ndiscrete time jump process (DTJP).\n\n\t\n\nau\t\n\nThe approach we follow is based on the a posteriori probability of a\nnonobservable event expressed in terms of the a priori probability of that\nevent and of the sample function probability of the DTJP. Thus, we obtain\na general representation for optimum estimates and recursive equations\nfor MMSE estimates.\n\nIn general, MMSE estimates are nonlinear functions of the observations.\nWe examine the problem of estimating the rate of a DTJP when the rate is\na random variable with a probability density function of the form cxk(1-x)rn\nand show that the MMSE estimates are linear in this case. This class of\ndensity functions is rather rich and explains why there are insignificant\ndifferences between optimum unconstrained and linear MMSE estimates in\na variety of problems.\nt,\n\nf This work was partially supported by NASA Grant NSG5048.\nDepartment of Electrical Engineering, University of Maryland, College Park,\nMaryland, 20742.\nDepartment of Electrical Engineering, University of Maryland, College Park,\nMaryland, 20742.\nL.1\n\nli\n\nI\'i\n\nQ\n\n\xc2\xa2+\n\nI. INTRODUCTION\n\nI\'\nr\n\nr\n\nt\n\nEstimation and decision problems arising in communications and control\nit\n\nhave been studied in detail for continuous time observations.\t However, not much\n\nc\nit\n\nhas been published for the case in which the observation process is a discrete\n\ntf\n\nI\'.\n\nt\n\nr\n\n^\n\ntime jump process (DTJP).\t We define a DTJP as a process having arbitrary\n\n.\n\njumps at times tit tZ ,....\t A more precise definition is given below.\t Segall [1]\n3\n\nobtained some optimum estimates for the special case where the jumps are\ni\tt s\n\nrestricted tobe unity by using_discrete time martingale techniques. \t In this\npaper we derive optimal estimates for more general cases.\n\nf\n\nIn Section 2 we define discrete time jump processes precisely, present\nr\n\nsome representations, and derive the likelihood function for an observed realization.\t In Section 3 we derive the a posteriori probability measure for a\n\'\n\n\'I\n\nnonobservable random process we wish to estimate given an observed realization\ni\n\nof the DTJP.\t Recursive optimum estimation equations are derived in Section 4.\ni\n\nThe problem of optimum linear estimation is briefly discussed in Section 5. \t An\ninteresting example in which the optimum estimates turn out to be linear is\n\n`\n\npresented in Section 6.\nli\nC^\n\nv\n\n4\n\nj^\nY\n\n1\n\n.\'j\n\nef^\nlk\nI\'\ni\t\n\nZ. DEFINITION, REPRESENTATIONS, AND LIKELIHOOD FUNCTION\nFOR DISCRETE TIME JUMP PROCESSES\nWe wish to describe an arbitrary discrete time jump process, taking\nvalues on a .f-dimensional Euclidean space R^ by means of discrete time counting\n\n1,L\t\n\nprocesses. This approach has been used in the context of processes with\nindependent increments and in general continuous time jump process.\nLet T be the countable set\n\ni\n(1.\nr\n\nwhere t i is a real number, i, e. t. CR, for i\n\n= 0,\n\n1, 2, .... Let Q be the set of all\n\nPossible piecewise constant right continuous functions defined on R, taking\n\nvalues on R n, and having jumps in T only. An element weft will be called a sample\nfunction. Define the variables Y. Y(t,) and y, ^ y(t,) as\nY i (w ) = value of w at time\t\n\nt = ti e T for w CO,\n\nfj\nt\n\n1\'^\n\nI\nd\n\nYO(w)\n\n6\n\n0;\n\nYi ( w ) Y. (w)- Yi-1(w)= lump size of w at time t= ti s i\n^ 0\n\nj\n\nYo (W)\n\n4\n\nt^\n\n2\n\nLet U be the minimal sigma-algebra of subsets of 0 such that all functions\n(Y i (w), ti e T) are measurable. Denote by P any probability measure on a. The\ntriple (1),;Y, P) will be called the discrete time jump process and will be denoted\ni]\n\nby Y. Since yi is a 3 -measurable function for all i 2 0, we define IF k to be the subsigma algebra of a generated by (y i (s)), ie(0, 1, \xe2\x80\xa2 \xe2\x80\xa2 -, k)). For any Borel set A of\n\n^ef\n\nR\nR \',, with 04A, define the random variables N k (A) and n k (A) as\nll\xc2\xab\n\nNk(w,A)^ E\t\n\nI(Y i (w) - Y i-I (w)eA)\t\n\n(1)\n\n0<isk\n\nnk(w,A) ^ Nk ( w ,A) - N k-l ( w , A ) = I(yk ( w )eA)\t\n\n(2)\n\nwhere I( \xe2\x80\xa2 eA) is the indicator set function of the set A. In accordance with\nuu77u\n\naccepted usage, we shall drop the symbol w and write Y k , Nk , nk , etc. , for\nY k (w), Nk (w), nk (w), etc., respectively. Note that N k (A) represents the number\n\nU\n\nof jumps of the process Yttatfall inA during the time interval (t 0 <ti stk ] . Thus,\n\n^I\n\nNk (A) is a finite, nondecreasing, ak -measurable function of k. ThereJe\nfore, (Nk (A), k=0, 1.... ) is a submartingale for any Borel set ACR . The Doob\n\n8.\'s\n\ndecomposition for submartingales, [ 2, Chapter VII] ,implies that there exists a\nunique decomposition of N k (A) in terms of a (a k ,P)-marting\'ale Qk(A) and a\n\nn(\n\n0k-1 measurable, increasing process II k (A) withII O (A) = 0\t\nNk ( A ) = Q k (A)+ 11 k ( A ) > k=0,1,2,...\t\n\nH1\n\nsuch that\n(\'.\'3)\n\nFrom (1) - (3) we obtain, for k = 0, 1, 2, ... , and any Borel set A\nn\nnk (A ) = g k (A) + TTk (A )\n\n\t\n(4)\n\nU\nwhere\t\n\t\nu\nf\n\nt !\t\n.1\t\n\nqk(A) Q k (A ) Qk-1(A)\n\nnk(A) p\n\nIIk(A) -\n\nIIk_1(A)\n3\n\nIX:\nE\ne\n\nI\n\n,f\n\nNote that (qk (A), k= 0,1,...) is a martingale difference sequence (MD).\n,Bemark J,,,, The random variable rr k(A) has a simple interpretation in terms\nof the condit\xc2\xb1onal probability of a jump ai time t k . By taking the conditional\nexpectation with respect to k-1 of both side: of ( 4) we obtain\nnk(A) = P(yk e Al a,k _ I )\t\n\n(5)\n\nThe Doob decomposition (4) has been defined her-- i) to model the\nprocess Y, ii) to guaranty the existance of P(ykeAp k-1 ), and iii) for obtaining\nestimates of nonobservable events (Section 5).\nIt is possible to represent the process (y k , tk s T) by means of the process\nn defined in (2). The following lemma is a special case of a result given in\nGikhman and Skoroklnd[ 3, Chapter VI] and the proof will be omitted.\nLemma l Let yk(A) D yk I(ykCA) = yknk (A) for any Borel set A C:kwith 0 dA.\n=\nR\n\t\n\nThen\n\n`I\n\ny (A) _ xnk (dx) = J xg k (dx) + xirk (dx)\t\nA\t\n\nA\t\n\n(6)\n\nA\n\nfork= 1,2, ...\nij\n\t\nS\n\n\t\n\nNote that yk (A)is the jump size of Yk provided that Y k Yk-1 sA- If\nle\n1\nA = R - 0, Yk (A) becomes yk (R - 0) = yk, with\ny\n\nC\n\n\t\n\n\t\n\n+ J xrrk(dx)\t\nj xnk (dx) = J xgk(dx)\n\n(7)\n\nwhere the integration is on the space R with thevector 0 excluded. The :integrals\nin (6) and (7) are defined in the sense of Gikhman and Skorokhod [ 3, Section 3,\nChapter VII].\nRemark 2 If the space of all possible jumps of Y is countable, say \t , with\n24_ {U l ,U 2 , \xe2\x80\xa2 \xe2\x80\xa2 - ], the above representation reduces to\n4\n\n\'\n\nI\'\n\nub\'\n\nnk(U)\t\n\nI(yk U)\t\n\nfor Ue ?4\t\n\n(g)\n\nnk(U) = qk ( U ) +\t Xk(U)\t\n\nj\n\n(9)\n\n^\'\t I\nand\t\n\n^i\n\nyk = \xc2\xa3 U. c([Ti ) _\t\ni=1\n\nm\n\nwhere\t\n\nU i g k ( Ui ) + E\t Ui ^c\n(U i )\t\n\n(10)\n\nXk(U) = P ( nk (U ) = l I ^k _ 1)\t\n\n(11)\n\nIn the estimation problem we will study later on, we will assume, for\n\n^ I ^u\n1\n\nsimplicity, a countable jump space Z!.\nThe likelihood function.\t The likelihood function is a quantity proportional to\nthe probability of observing a particular realization of the jump process\nk\n(Y i\' tOstistk) for ti,t\n\nET and plays a fundamental role in estimation and\n\ndecision problems [4].\nWe wish to find the likelihood function for a discrete time discrete\namplitude jump process. Denote by Pk\t\n\nfi\n\n"\n\np(tk ) the probability of having a\n\nparticular realization of Y, i. e.\n;r\n\nPk = P(y i = i,\t i= 0, 1, ... , k)\t\n\n(lZ)\n\niI\nP\n\nwhere\t\n^.i\n\ni EV\t\n\nThen\n\nPk = P ( Yk\t\n\nk\'\n\n= 0,1, ... , k-1) Pk-1\nYi= $ i , i\n\nkJ]\n\ny\n[i\n\nk\n\n!(\n\n=\t "T\t P ( Y .\t\ni=1\n\nIf\n\n\'\n\n(\n\n1\n\nit\nf\nf\n\n$i^ Y.\t\n\n^i . i = 0, . , ... , i - 1 ) P ( Y 0 =\t 0)\t\n\n(13)\n\nwhere\nP(Yk= g k lyi = ^i, i= 0.1, ... ,k-1) = P (Yk = \xc2\xa7kk_l)\n= P (nk O = 1 Uk_1 ) \xc2\xb0 ak (gk )\t\n\n(14)\n\n5\n\n1\n\n\t\n\nLet t\xe2\x80\xa2\n\n,\n\nt.\n\n\xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2 , be the jump times of the random process Y with jump\n\n^ ^2 ,\n1\n\namplitudes E j , E j , \xe2\x80\xa2 \xe2\x80\xa2 , Then nj (E j ) = 1, i = 1, 2, \xe2\x80\xa2 \xe2\x80\xa2 . , The probability of no\n1\t\n\n2\t\n\ni\t\n\ni\n\njump,nk 0 is given by\nP( n k = 0ly i = S i r i=k,2,....k-1)=I-\'k\n\nwhere\t\n\nk\n?k = \xc2\xa3 ai(Ei)\ni=1\n\nTherefore, the likelihood function (13) becomes, with p0^P(yO = E0)\'\nk\t\np\nPk = PO 77 (ai(Ei))"i(\'i)\t\n\n1-n i\n\ni-1\t\n\n(15)\n\n1\n\nwhich can also be written as\n,- k\n\np k = ex pL l\xc2\xa3 1 (Pm ( d i T) n i (E i ) f (2\'n(1-1.1))(1-ni) J\n\n(16)\nk\t\n-\n\nm\t\n\n%(U )\t\n\nn\n\n+ (1 - i )) -J\n1 - Xm\xe2\x80\x94 n i (U rill )\n\nexp LEI ( m\xc2\xa3 1\t\n\nwhere we have used the fact that E O 0., n i =1F ni (U m ), and\n1\nk\nk\t\nm\n\xc2\xa3 k X. ( U ) n. (U ) _ \xc2\xa3 2n a. (E.) n.(E.)\ni m i m\ni=1 m=1\t\n\xc2\xa3\t\n\nRemark 3\t\n\nIf the set of all jump amplitudes of Y is uncountable,\n\nthen, by assuming that the limit\nX. (X) = lim\t\n\nE\n(TT (Ax.) ,rrl(^x,xf6x)]\n\nmaxlA x i^ O\nexists, where x = (x i , xz, - \xe2\x80\xa2 \xe2\x80\xa2 , x )e R , then the likelihood function a;\n\n6\n\nr\ni\t\n\nr\n\nI LLLLLL...\t\n\nk\t\n\nS\n\n6\t\n\nt\t\nP\n\ny^\t\n\nr\n\t\n\nI\n\n1_ 1\npk = expLF. 1 ^^ RPm( ^ )ni(dx)+2n(1_Xi)^\nR\nwhere ai J a i (x)m(dx) and m is a measure on R .\nR\n\nR\n\n1\n\n(\n\nii\n\nI\n\n(\n\ni\n\nl\n\nI\n\nj^\n\n^i\ni\n\nI\'\n\nIj\n\n8\n\na\n\nC\n\nIf\n\n1\nI\n1t\n\n7\n\ni\n\n(17)\n\nn\n\nJr\n\nI\n1\t\n\nt\n\n\'\n\ny\n\nY\n\n3. A POSTERIORI PROBABILITIES FOR ESTIMATION\nE l\n\n"\t\n\nWe shall formulate the estimation problem in a manner motivated by a\nproblem in communication theory [ 51, [ 61.\t Let X(t) be a nonobservable\n"signal" which is applied to the input of a general channel. \t Let y(t) be the\noutput of the channel at time to T.\t We will assume that the observation record\n\ni^\n\nIf\n\nof the channel output at times t 0 , tl , It\t \xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2 is a sample function of the discrete\ntime jump process described in Section 2.\t Based on the observation record\n(y(Q), 0 s a St; o, teT) we wish to find an estimate of X, in particular, the\nminimum mean square error estimate.\t We formulate the problem in some\nconvenient probability spaces in such a manner that the observations y can\ns\n\ninfluence the "signals " X.\nLet (ns\t,^ , P )s be a probability space called the "signal space" where\ns\t\n\nas\n\nthe events B e SI\t s\t are nonobservable.\t Let (f)\t m,Si\t P\t m\t (W ,\t\nrri\t\ns\t\ns\t\n\nI!\nI+\n\nw Co , be a\ns\t s\nprobability space called the "transfer space" where the probability measure is\nparameterized by the elements W 6 .\t The transfer space models the channel\nbehavior for each W s CC)\t s . We want to obtain statistical inferences about the\n3\nfl)\n\ni`\n\'\t 6)\n\n\t eQ\t\nWe will assume that the elements W m m are the sample functions of\nthe discrete time jump process described in Section 2 .\n\nif\n\nI\nl\n\n^I\n\nS3 =\t\n\ns\t\n\n$ Si\t\n\nm\n\njl\n\nZ\n\nE\t\n\n`5\n\nand\n\nJ\t\n\nPm (W S , Bm)P s (dWS )\t\n\n(1$)\n\nFor example, let Em be the event [yl = U I, y2= U 2\' \xe2\x80\xa2\t\n\n9\n\nyk= Uk) CO.\t This event\nI\t\n\nrepresents a particular realization of the discrete time jump process from t = t0\nS\n\n(yo\t\nI\t\n\n^?\n\nz\nj\n\nIt is convenient to construct the product space ( 0,6, P) where 0=0 sxD\n\nBs\n\n^(\n\nIf\n\nnonobservable events B e B by observing events B\t m\t e 13 m\ns\t s\t\n\nP(B s x Bm)\t\n\nfi\n\ni\n\n) ),\n\n1) up to t = tk.\t Then, from (16)\ns\n\n7\n\n\t\n\nPm(ws \' E\n\nm ) = P( w s , Xi , 1e [ O , - -\n\no k]) p\n\nk)\nPk(ws\'W r\ni-1\n\nkm\t\n\nexp\t\n\n(\n\n^l-1\n\n\xc2\xa3\n\n(\n\nEN\t\n\n\t\n\n^i Un\'ws\'wm\n\ni_1\n\n)\t\n\n\t\n\n) n (U \t" );wZn\n\n1-^i(tus, wrnl )\t\n\ni n\n\nn\n\n1\n\n1-Xi(w$,wm 1 )^ i\n\n(19)\n\nwhere we have indicated, explicitly, that the rates X.1 and X i n ) depend on the\n.(U\n"signal" element w s and the sample path Wm from t o up to ti-11 i. e. wm l .\nLet us define a new probability measure Pm on the transfer space,\nfunctionally independent of W and mutually absolutely continuous with respect\nto P m (w s ,-). For the event E m , we define\n-k\t\nd\n\n!\t\n\nY.(U ;1- m\nw l-1)\n\nm\t\n\nPm(E m ) = exp L\xc2\xa3 ^ \xc2\xa3 En / i n\ni-1 n=1 `\n\n1-Y l (w m )\t\n\n\\\n\nnl\n\n( U n) - gn 1 i-1\t\n\ni\t\n\n(ZO)\n\n1-yi(w1- )\n\nwhere the rates y i ( U n) and y i are not functions of ws . We define the likelihood\n\ni5\n\nratio L k ( ws , w m ) as\n\nPm(Ws,Em)\ns\'\nLk (\' s\'\t m\t\n\nPm (Em)\n\n= ex ri\xc2\xa3 (\xc2\xa3 2n \\ (Xi(Un;ws,1\t\np\ni-\t\n\n= 1 n=1\t\n\nY i (U n\' w m)\n\n1-Yi(wm 1)\t\n\n\\\n\n1-?,i(ws, m\n\nt.\n\n9\nLJ\n\nw1l ) ( -Yi ( w ml ) )\nm\n(1-\n\nni (L\'.\ni-1 ))\ni (w w\ns\'m\n\na\n\nThe probability of any event B s CP\t\n\ngiven a sample path realization W m of the\n\nobservationrocess can be calculated in terms of the likelihood ratio L \tk given\np\t\n\n^Y\nii\n\nin (21). In fact, we have\nF ^L\n\nTheorem 1.\t (Prior-to-posterior probability)\n^EI\n\nLet ( 0, Ss\t , Ps\t) and (fl\tm\t , S\t m\t , P\t m\t (W\t s ,\t )) be the signal and transfer spaces\ns\t\ndefined priviously.\t Let Lk be a likelihood ratio between Pm (W S , \xe2\x80\xa2) and Pm.\n\nt ^.\ni\n\nThen\n(ws,W)P(dWs)\nJ L k\tm\t 5\n\n1\n\'I\n\nP(fisxQ\t\n\n-\n\nIs\t \xc2\xaea k ) =\t\n\nx\n\ndP\n\nBs\nLk(WS, W m) Ps(dw\n\ns (B s IS k)\t\n\n(22)\n\n^s\ngg\n\n9\na-\n\n(4 ,fl\t s }\n\nfor every B es , where S\t\nProof\t\n\nThe proof of this theorem, in a more general context, is given in\n\nr\n^y\n\n[6, section IV]\n\ni\n\nJ\n\nNote\t\n\n}\nO\t\nthat the right-hand side of (22) does not depend on P m because\n\nit can be written, alternatively, using (21), as\nP\t (W ,B\t )Ps\t(dWs )\nm s\t m\t\nB\ts\nIs\t a U m (t k )) = (.\nP(BS x0\t\nm\t\ns\t\nP(W s\t \' B)P(d ws )\nm\t\n\nti\n\n\t\n\n!.\nPs\n\n(Bs\n\nl k)\t\n(23)\n\n,\n\nf2s\nf\n\nG\n\n3\n\nwhere Bm = Em and 5\t\n\n\'\n\n-\n\nS O 0 a\t m (tk ), therefore Pm is a \'fictitious" probability\n\nk\nmeasure used to prove the above theorem.\n\ntj\n^\n\n^\n\nji\n\nt+\n\nC\n\n!..\n\nRemark 4\t\n\n(Conditional probability density) \t Let the nonubservable random\n\nI\n\nvariable X be defined as X(UJ\t\n\n= W5 e r whe e Q= Rn.\t We want to obtain the\ns\ns\nr or that purpose, let\nconditional probability density function of X given t7\nB = [X,X+6X).\t Then (23) becomes\ns\n\n^\n\n.\n\nI(\n\ni\n\n^^\n\n10\n\n1,\n\n+A X\n91\n\n(x, B )P (dx)\ns _\t\n\n\tm\nI\nPsl[X, X+AX)I k) = Jpm\n\n(24)\n\nPM ,x, Bm)P5(dx)\nR\nn\n\nDividing both sides of (24) by Tr (AX i ), taking the limit when max I AX i I + 0,\ni\n\ni=1\t\n\nand assuming that P s (dx) = p s (x)dx, we obtain\nf^\n\nf\t\n\nrr (AXi)-1\t P s ([X, X+AX) I Zyk)= \t\nlim\t\n10 i=1\t\nmaxIAXiI\n\n(X,\nPm Bm)\n\nJ Pm(x,Bm)ps(x)dx\n\nPs(X)\n\nR\nps(XI\n\n^l\n\nr\t\n\n0\n\nwhere Em = B and Pm (X, Bm ) is given by (19) with W s = X.\n\n^^\n\n^J\n\n^I\n\n9k)\n\n11\n\n(25)\n\nI\t\n\ni\t\n\nI\t\n\nI\t\n\nI\t\n\ni\t\n\nI\t\n\nI\n\n4. RECURSIVE OPTIMUM ESTIMATES\nLet (X k , k = 0, 1, 2.... ) be an integrable random process defined on the\nproduct spare (0, f3). We want to obtain the best estimate\n\nx\n\nby observing a\n\nsample path wm from t 0 up to t n . The criteria is the minimum mean square\nerror.\nIt is well known that the conditional mean minimizes the mean square\nerror. Therefore, the best estimate is\n\nPS (dw sn )\t\nXk(WS, Wm)\n\nX k (w m )\t\n\n(26)\n\ns\nUsing (23), we can write (26) as\nL\nslu\n\nXk m\t ) =\t\n(w n\n\nr\n\nE (p\n\n( W 0 Wn\n\n)X (w , wn))\n\nm\t\n\nEs(pn(WS. Wm) )\n\n^a\n\nwhere, for simplicity we have defined\n^v\n\nn 6\npn ( W S , Wm ) = Pm (W S , B m ) B = {E n, i\n\nEquation (27) is the best estimate of Xl\n^o\n\nwm from t0 up to tn , and we have the followinf\n\ni) smoothing estimate, if n> k (i. e. t\n\ni.\n\nii) filtering estimate, if n = k (i, e, tn\ni71\n\niii) prediction estimate, if n < k (i. e. t\nn\ni\n\nE\n\nFor simplicity, we shall write (27) as\nEs(pnXk)\nXkln Es(pn)\n\nEl.\n12\nEll\n\n(27)\n\n\t\n\nu\n\nNote that X ma\t y not be A \xc2\xae tn) measurable for k> n, which implies that\ns m(\nk\t\nX k may not be B s - measurable and (27) does not a p ply. However, if X k is\n\n^tl\n\nz\t\n\n^^\ni\ni\n\nconstant on Q n , then, it is N s measurable and (27) applies,\n\nr\n\nRemark 5. Note that the random process X depends on both the signal Ws and\ni\n\nthe observations W which implies that feedback is allowed, and (27) is the best\nm\n\nS\n\nf\n\nestimate of X.\nE\n\n^\t\n\nr\n\nRecursive filtering estimate We wish to find a recursive formula for X gives\n\nk\n\nin (27) for n=k.\n4\n\nFrom (19) we see that\nr m(^\n\n(U n )\t\n\n1\n\nP k Pk-1 exp Ln 1 \\ 1 -a\t\n-\n\na\n\n(29)\n\nnk(Un) - \t 1- J\n\nwhere we have droppdd W s and W for simplicity. We prove now that the\ndenominator of (27) satisfies\n\\\n( W ( x i1 i\xe2\x80\xa21 (Un )\t1\t\n\xe2\x80\x94 A\t\nPk E 5 ( P k) = P k-1 exp \\ \xc2\xa3 `\t\nni(Un) - Ln\t\nn=1\t\n1-xi1i-1\n(Un)\t\n\nI\n\nwhere Xil i-1 (Un)\t\n\nEs(x.(11 ) - P\xe2\x80\xa2\ti-1 )\ni n\t\n\n(30)\n\nk=1, 2, .. .\n\nEs\t ( P i\t -1)\n\nFor k=0 we have p 0 = 1 and\ni\n\nn\n\npl - (1-al)\t\n\n\t\nn = l\t\n\nnl(Un) _ 1-^ 1\tif n 1 = 0\n\n(U n\n\\ ^1\n\n))\n1\n\nI\n^1(F)\t\n\ni\n\n(301)\n\nif n l ($) = 1\n\nThe last equality ,follows because n l= 1 if and only if there is a single jump of\n\nr;\n\n1\n\nsize X04 at t=t l . Notice that (30 1 ) can be written as\n\n}4\t\n\nP1 = P O (X1 (9) n1 (^) + (1-X1 ) (1-n 1))\t\n\nJI\nl\n\n13\n\n(31)\n\n\t\n\nr\n\'\t\n\nTaking the expectation with respect to E s , dividing both sides of (31) by E s (p o )\t\n\n)\n\n(which is equal to 1), and using (27), we have\t\n\np\n\nl J\t\n\nE (P)\t\n\n(5))\t\n\nE (P\t\nEs(P0)\t\n\nE s ( p 0)\t\n\nE (P (1-x ) )\n\n" l ( g ) +\t\n\nE s ( p0 )\t 1 (1 n1)\n\nThen\t\n\nE s ( P l ) = E s ( P 0 ) a l l 0 (^) n l (^) + (1-)\'1^0(^)) (1- n1 )]\t\n\nor\t\n\n,.l = p 0 (X110(\xc2\xa7)nl(\xc2\xa7) + (1- a^ o (^) (1-n1))\n\n( 32)\n\nl;\n\nSince ( 31) and (32) satisfy the name type of difference equation we conclude that\npl satisfies (30) with k = 1. Using mathematical induction, it is easy to verify\n11\t\n\n(30). The filtering estimate X I4 k becomes\nX, k =\n\nE (P X )\nEs (\nPk) = E s ( A kX k)\t\n\n(33)\n\n)\n)\nX . ( U\t \t (1-\t ,\'..^i-1 n\n(U ) _ pm\t 1-1 I\nexp r E F. k i k\n\t\n(Pk\t)-1P\t k\t\nAk\t\n\t (1-^ i ) i n\t\n1-Xi =\nLi=1`n=1\t \'i^i:Ii\n\nwhe re\t\n\n_\t\n\n)\t\n(U ) (1-X\t k^k-) n (U ) -\t\nL n=1 \'k!k -1(Un) (1-a k \t k n\t\n\n= A\t k-1\t exp r\t E Zn\t\n\n^d\t\nu.\t\n\nA\n\nk\t\n\n\'\n\nk-Y k\n\nk n\n\n1-X k^k -1 I\n1-Xk\n\n(34)\n\nk=1, 2, \xe2\x80\xa2 ..\t\n\nwhere .2 k is the exponential formula, and A 0 = 1.\n\xe2\x80\xa2-^\t\n\nC\t\n^\t\n\nTheorem 2 (Optimal filtering e;stimate). The optimum filtering estimate X^k\n\nN N\t. N N M\n\ngiven in (33) satisfies the stochastic difference equation\n^\t\nA\t\n\nX\t\n\nA\t\n\nk\t\n\n- X\t\n\nn\t\n\nA\t\n\nn\n\n(U)\n(U.)\nm Ek -l( X k a k ( U i)) - X\t lak-1 k^k-1 i\t +h\t k^k-1 iA\nA\t\n+ E\t\nq (U )\n\n^k-1 i = 1\t\n\nXo k_l (U i ) (1 - XA k-1)\t\n\nk i\n\n(35)\nfor k= 1, 2, \xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2 , where\n14\n\ne\t\n\nff\n11\n\nh\t\n\n^^\t\n\nf\n\nEs(XkXk(Ui)Pk_1)\n{\t\n\n(36)\n\nEk-1(Xkx ( U i )) -\t\n\n`\t\n\nE s k-1 )\t\n(P\t\n\n`\t\n\nklk-1(Ui)\n\n`I\n\nj\xc2\xa31 Ek-\n\nll\n\n.,\t\n\n..\t\n\n1 (U j )T k (U i) J\n1 C Xk(\'^^k-1(Ui)\'k(Uj) - aklk-\n\nit\n\n(37)\n\n- Ek-11V X( Wk-1 (U i )Xk - Ok-1 k(Ui))^ \t\n\nt\t\nli\t\n\nand\n\nII\n\ns\t\n\nok(U1)\t\n\nx\n\n(38)\n\nnk (U.) - xl^k_1(U)\t\n\nProof. From ( 33) and (34) we have\n(39)\n\nX k = E s (,k A k ) = E s (Xk Ak-1 1 k)\t\nBut\t\n\n1-x k\n\nRk =\t\n\n, n\t i = 0\n\n1-aklk-1\n\nY\t\nr\n\nak i\n(U.)\nf\n\nnk(Ui) = 1 , \t\n\nU i e 4, i = 1, 2, .. .\n\n\'klk- 1(Ui)\nA (U.)\n1-x\t\nm\t\n^k 1\nk (1_ \xc2\xa3 n(U.)) + 5 )\nk\tl - n (U\ni=1 \'l k-1(Ui)\ni= 1\t\n1-^lclk-1\t\n\n^Y\t\n\n1-2 k\tak(Ui) - Nk_1(Ui)\nnk(Ul)\n+ \xc2\xa3\t\n_\t\n1-X kl k_1 i=1 ^klk-I) (1-Xkk-1)\n\n+\xc2\xa3\n\nk k-1 i k\t\n\nk i klk-1\n(40)\n\nnk(U)\t\n\ng\t\n\n\' 1 k- 1 ( Ui ) (1-a klk-1)\n\ni = 1\t\nf\n\nThus, using (39) we have:\nI\t\n\n,\n\n\t-\n\n^t\n\nl\t\n\n(X a (U.)) ^\nm\t\nX\n\t E k-1(X k a k )\t + \xc2\xa3\t E k-1 k k i\t - X\tidk-1 klk\t 1(U. )\nXl^k = klk-1\n1 nk(Ui)\n\'q k-1 (Ui) (1-\'1 k-1)\ni\xc2\xb01\t\n1-aldk_1\t\nEk-1 LXk(alclk_1(Ui)Xk-aklk-1%k(Ui))J\n\ni\t+ \t\n<!\n\n( )\t\nnkU i\n\ni=1\t\ntQ\t\ni\n\nflak-1(Ui) (1-%-1)\n15\n\n(41)\n\nP\n\n"t\n\n,\n\t\n\nVIII\t\nE\n\n(U.),\nBy noting that \t k\t = E ]k i\t and\t idk-1\t _ \xc2\xa3\ti idk-1 i the first two terms on\ni (U.)\nthe right hand side of (41) are equal to\n\ns\t\nc\t\n!\t\n(\t\n\nX\t\n\nIj\t\n\n(U.)\n(X (U.^)\nE\t k-1 kxk ^\'\t - X\t i k-1 Ok-1 1\nn\t\n(nk(Ui) - x^ k-I ( U i ))\t\nn\t\n\nm\t\n\n+\xc2\xa3\t\n1^k-1 i-1\t\n\n(42)\n\n\'Ok-Pi) (1-^^k_I\n\n\'j\n\nand, after some manipulation, the third term on the right hand side of (41) is\n\nI\t\nC\t\n\nequal to\n\nm m\t E k-1L X k ( ^k Ik -1(Ui) ^ k (U ) ) n\t\n\xc2\xa3 T\t\ni = 1 j=1\t\n^Ak-I(Ud (1-Xk k-1)\n\n\tD\t\nI\t\n\nji\n\nm\t\nn\t\n\nhWk\n\n-I(Ui)\n\n^Igic_1(U.j1I\'k(U\n\ni)1,\ngk(Ui)\n\n(43)\n\ngk(Ui)\t\n\nn\t\n\nk k-1 i\n\nI `^\n\n(U i ) is defined in (37). Combicing (42) and (43) and using (38)we\n\t^ j\twhere h\t\nk^k-1\nii\n\nobtain (35).\n\n\t^\'\t\n\nExample I - hi the observation process is a discrete counting process, i. e. ,\n2(_ [U l ] _ [ 1 ] , then ak(U I ) = ak and (32) reduces to\n\nt_\t\n^\t\n\nn\t\n\nn\n\nEk _I (Xk^k) - Nk-lXkIk-1 ^\n\nl\nt\t\n\nN\t\n\nX^k-1 +\t\n\nqk\n\ni\n^ k - 1 (1-\n\nThis equation has been obtained by Segall [1 ].\n\nLI\n\nI\n\n(\nt\n\t` 3\tExample 2 - Let us assume that the nonobservable random process Xk\'can be\nl^\n\n\xe2\x80\xa2\t\n\njI\t\n^\nIf\t\n\n((\n\nf\t\n\n\t\n\nrepresented as\n\nu\nXk = f (k \' Xk-1\' k-1) + wk\nwhere f is a known function of Xk-1 and of a 3 k _I measurable\n\n16\n\n\t\n\n1!\t\n\n1\t\n\n1\t\n\n1\t\n\n1\t\n\n1\n\n1\t\n\ncontrol u k ; w k is a MD on the signal space, and is rat a function of Wm then\nwk can be interpreted as noise in the dynamics of Xk . Then, the one step\nprediction X k k-1 is\nXk^k-1 = Es(XkAk-1) = E s (f ( k. Xk-l . Uk-1 ) A k-1) +Es(wkAk-1)\n= fk k-1 + E s (wkAk l)\nWe will assume that wk is a MD with respect to some sigma-algebra Ps(tk)e8so\nand that Ak-1 is 6 s (t k-1) measurable, then\nE s ( wk A k-1 ) = E s ( E s ( w k A k-I Is s ( tk-1 )) = 0\nThus\t\n\nXklk-1 - fk{k-1\n\nExample 3 - Let us assume that the rate parameter Xk is a fixed random variable\nX defined on the signal space, i. e. ak = X = X(W S ) and that X is uniformly\ndistributed on [ 0, 1]. The best estimate X at ,-c a t=t k is given by:\nz\n\nE\t\n\nk-1)\nX = Xk-1 + k-1\t (X Z ) - ( X\t)Z (nk - Xk-1)\t\nX k-1 - (Xk-1)\n\nk=l, Z, ...\t\n\n(44)\n\nwhere No= z\nNote that in order to solve (44), we need to know E k-1 (X Z ), which can be obtained\n3\nfrom another difference equation involving a term E k-1 (X) and so on up to\ninfinity. Therefore (44) is not a closed form solution for the best estimate.\nHowever, this is a general characteristic of nonlinear estimation.\n1r\n\nMotivated by the problems of solving (44), we develop below a recursive\nformula for the conditional probability densitypk(X).\n\nL\n17\n\n\t\nk\n\nIr\nsfC\n\nTheorem 3 - Let p k (x) be the conditional probability density of the random\nvariable X = X(w s ) given a k . L \xe2\x80\xa2et a k (U i , X), the rate of the jump process, for\nUie \' , be a know function of X. Then\nm xk(Uip x) - x k-1 (U i ) + gk-1(Ui\' x)\n\np k (}d = Pk_1(x)[1 + \xc2\xa3 \t\n\ni = 1\t\n\nwhere gk-1 (U i , x)\t\n\n^J\t\n\ni\n. ,.\n\n\t\n\n4k(Ui)\t\n\nl\n\n(45)\n\nxk-1(Ui) (1-Xk k-1)\n\nXk^k-i(Ui, x) X k (x) - Xkl k _ 1 Xk (Ui , x)\n\nProof Let us consider the random variable y = ejvx, then\n\n^ h!\t\n\nN N .v\n\nE k (Y)\n\nJ\n\nnc\n\n(U.)+h^k-i(Ui) ^\nm E k-1 (Ya\t k (U)) - Y k-1^k-1 t\ni\n\ngk(Ui)\t\n\n= Ek_1(Y) +i\xc2\xa3i \t\n\n(46)\n\n)k^k-1(Ui) (1-1)\n-1)\n\n.Since p k (\xe2\x80\xa2) is the conditional probability density, then\n\n^yl\t\n\nfor veR\t\n\nEk(y) _ exp(jvx)p k (x)dx\t\n\n(47)\n\nC:a\tTherefore, from 146) and (47) we get\neX P(j vx ) P k ( x ) dx\n\n= ! exp(j\'vx)pk-1(x)dx\n\n(U.)Jexp(j)Pk-1 (x)dx\n1\t\n\t\n+ \xc2\xa3\t1 XOk-1(Ui)(1-Xok-1)\t L f e x P( jvx )X (U.)p\t (x) dx - ]\t k-1 i\t\nk i k-1\t\ni=\nt\n\n+J\n\n9\t\n\nm a k (U i ) Pk(x) = P k _l ( x ) + P k _ 1 ( x ) \xc2\xa3\t\ni = 1\t\n\n\'\t\n\nL\n\n)`k^lc-1(Ui) + ^lc^k-1\n\n(U.)(1-)\t\n\nlak-1 i\t\n18\n\nt\n\nk-l\'k(Ui)Ipk-1(x)dx\'\n\nfor any veR. Thus\n\n!\t\n\nI\t\n\neX P( jvx ) ( X ^ k-1 ( U d k- \' i\n\nk^k-1 )\n\n(Ui)^k-\'^k- l\'k(Ui) ^\n\ngk(Ui)\n\n\t\n\nI\t\n\n^\t\n\nI\t\n\nI\t\n\nI\t\n\ni\t\n\n1"\n\n1\t\n\nR\nL\n\nwhere p 0 (x) is the initial probability density function for the random variable X.\n\nExample 3(cont.) Let a k (U I ,X) = ak (X) = X be a uniformly distributed random\n}\t\n\nvariable, then (45) reduces to\n^\n\nPk (x) = Pk-1(x) L 1 + x -Xk 1 \t\n2 ( nk - Xk-I)"\t\nXk-I - (Xk-I)\n^\t\n\nwhere\t\n\nk= 1. 2, ...\t\n\n(48)\n\n^\n\nXk-1 J xpk-l(x)dx, and\n1\t\n\n0 5 X:5 1\n\n0\t\n\nPo(x) _\t\n\n\' otherwise\n^\n\n^\n\nNotice that knowing Pk-I (x) we can find X k-I which in turn allows us to find\nPk (x), and so on. Thus we obtain a close form solution for all the conditional\ni\n\nmoment of X. It is straightforward to verify from (48) that X satisfies (44)\nRecursive Smoothiny,,Fsrtimate, - We wish to find a recursive formula for the\noptimum smoothing estimate X An of the random variable X given 9 n for k<n.\nTheorem 4 (Optimal smoothing estimate). The optimum estimate X n, for\n\nN N \xe2\x80\xa2V N N N\n\nk< n and k fixed, satisfies the stochastic equation\n\nX, _\ni n\t\n\n^\t\n?X\t\n\nn-1 m\t\n)\nEi (X a. (U )) X x. \t (U ) +h (U m ^\n+ \xc2\xa3\t \xc2\xa3\t\nlai i+l^i m\t\nlc^i\nk i+1 m\t\nAk i = k m=1\t\n^\t\nqi+l(Um) (49)\n(Um ) (1-Xi+1d\nXi+lji\n\nwhe re\nhldi(Urn) j l EiC Xk( ^i+lli (Um )Xi+1 (Uj ) - Xi+Ai(Uj)Xi+1(Um)_1\n(50)\n\nF\nEiLX0( i+lli\n\n(Um)1 i+1 Ii+lji)i+l(Um^)\'I\n19\n\n\t\n\n,1\t !\t\n\nI\t\n\nI \t I\t\n\n_I..\n\nI\t I\t i\t\n\nY\n\n(U\nand q i+1 m\t ) n\ti+1 (U ) - x\t i (U m)\nmi+1^\nj\na\t\nProof The proof is very similar to that of Theorem 2. In fact, the smoothing\nN N N\nestimate is given by\nt\t\n\nXj n = E s ( X I, n; = Es(Xknn- l^n)\t(51)\n\nl )\t\nwhere\n\nXn(Um) - \'An-1(Um) nn(Um)\n= 1-Xn + \xc2\xa3\t\nA\nn 1- ^\t\nm-1\t\n(U )\nnln-1 m\t (1-^\t 4n-1 )\nr1n-1\t\n\n7\n\n0\t\n\n+ \xc2\xa3 \'^n-I(Um)\'m - ^ n(Um\'dn-1 n (U j\t\nn rn\n\'r,In-I(Um) (1-\' n-1)\t\nm=1\t\n\nrr\t\n\n(52)\n\n9i\ns^\n\nUpon supstitution of (52) in (51), we obtain\nX^n= X\n\nEn-l(XkXn(Um)) -Xkn-1\'nln-l(Um) +hkn- 1(Urn)\nqn(U ) ( 53 )\nm\n^n-1 m l\t\n\'4n-I(Um) (1-\'n n-1)\t\n\nfor n= k+1, k+2, \xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2\t\n\nWriting the stochastic equations for X^ n-l\' Xk n-2\'\n\netc., we deduce (49).\nRecursive Prediction Estimate - We wish to find a recursive equation for optimum\nprediction estimate Xk In of the random variable Xk given & n for n < k. \'We\n^i\n\nassume here that X is Pi s measurable. A sufficient condition for the Rs\nmeasurability of X is that X be constant on\n\ny\t\n^\n\nTheorem\nN N N N N5\n\nr\t\nit?\n\nm.\n\n(optimum prediction estimate). Assume that the random variable\n\nXk is a s measurable. The optimum prediction estimate X An for k> n and k\nfixed, satisfies the stochastic equation\n\ni\n\n0\n\n20\n\n\t\n\nn n\t\n\nn\t\n\n\tn-1\nm\t\n\t\xc2\xa3 X\t = X t\xc2\xa3\n\t\nkJ0\t i=0 m=1\t\nn\n\nE\t i\t (X\t k^. (U ))\ni+1 m\t\n\nn\n\n(U ) th (U )\n- X^ x\t\ni m\ni i+Ai m\t\n\nxitlli(Um) (1-Xi +4i)\n\nq (\t\n)\ni+l Um\n\n(54)\n\nwhere hO i (U m ) is defined in (50)\nProof - The proof of this theorem is identical to that of Theorem 4 and will be\nomitted.\n\n7\n\nf\'\n\nA special case of Theorem 4 is for U=\n\n= (1]. In this case, the\n{U 1 ]\n\nrecursive formula for XA becomes\nAn\nn\n\nn\t\n\nn-1 E. i k Xi +l) - N i l .t+l^i\t\n(X i+1\t\nkli\n+\xc2\xa3\t\nX\t\n= X\t\nlcln\t\n^k i=k\t\nxi+ll (1-^i +lei)\n\nt\n\n(55)\n\nwhich has been derived by Segall [ 1] The prediction estimate XA n, for\nU = {1 ] , becomes\nA\n\nn-1\nX^\n\nn\n\n= X,\n\n10\n\nKI\t\n\n+\xc2\xa3\t\n\ni=0\t\n\nE i (Xk xi+l ) - Xklixi\xe2\x80\xa2\n^\n\nxi+l i(1-Xitlli)\n\n21\n\n\t\n\nEJ\nEi\n\n^f\n\n5. COMMENTS ON OPTIMUM LINEAR ESTIMATION\n\nkl\n\nIn this section we indicate how to obtain the best linear estimate X\nn(Um)\nof the intensity function % k(U m), m=1, 2, , , . , in the sense of minimizing the error\ncovariance function E((Um) - \'kln(Um))2 by observing a sample path realization\n\n^G\n\n(U ))\n(n (U ), i = 1, 2, .... n), n a \t k, of a discrete time point process (n m which is\ni\ni\n\nm\n\nobtained (see Section 2) from an arbitrary discrete time, discrete amplitude jump\nr\n\nlt\nY\n\nprocess (yi , i= 1, 2, ... , n). As we discuss in Section 2, the Doob submartingale\n(U\ndecomposition of n i m) gives\n\nj\n\nni (U m ) _ % i (Um) + g i ( U m) ,\n\n(57)\n\n9\n\n@i\n\nfor i = 1, 2, ... , n; m= 1, 2, ... , where g i (U m ) in a MD sequence, therefore\nE(gi (Um)q. (U m )) = 0 for all i, j = 1, 2, ... , n.\n77\nThe best linear estimate Xk l n (U m ) is of the form\n\n^I\n\n\xe2\x80\x9e k\t\n\nn\n\n+1\xc2\xa31 H ki (U m)(ni (Um ) - E(Xi(Um)))\n\'kJ n(U m ) = E (^k (U m))\n\n(58)\n\ni\n\nwhere the unit response Hki (Um ) is obtained from the orthogonality principle\n\n(1^\t\n\nE IVUm ) - ak l n (U m)I( nj (U m) - E(k j (U m ))] = 0\t\n\nfor k, j sn, and m = 1, 2, ... .\nliWhen U m ) is the state of a linear dynamical system, the Kalman filter\n(\ntil ^\'\',\t\ncan be used to recursively compute the optimum linear estimates.\nIS\n\n11\n\n22\n\n(59)\n\n6. A CLASS OF PROBLEMS IN WHICH THE OPTIMUM ESTIMATES ARE\nLINEAR\nIt is well known that the unconstrained minimum mean-square error\nestimates of one set of random variables from another set are linear when the\ntwo sets are jointly normal. Few other examples are known where the optimum\nestimates are linear. In this section we present a problem for discrete time\npoint processes in which the optimq,m estimates are linear.\niNe will examine the problem of estimating the rate parameter X for a\nbinary discrete time point process when X is a random variable with the probability density function\n\n(km! k ) I\n\nxk (1- x) m\n\nfor 0\n\ns\n\nxs\n\n1\n\nP X ( x ) =\t\n\n( 60)\n\nelsewhere\n\n0\t\n\nwhere k and m are non-neg^tive integers. Let us assume that the observed\ndiscrete time point process yn, n= 1, 2.... is a sequence of binary numbers with\n\n(61)\n\nP ( yn = "X) = 1 - P ( yn = 01X) = X \t\nand that it is an independent sequence conditioned on X, that is,\n\nP(yi =\n\t\n\nn\ni= 1, ... n IX) =i rr\t 1\n=\n\nP(yi =\n\n9i IX)\n\n= XS (1- X)n-S\t(62)\n23\n\nwhere\nn\nS= E\t\ni=1\n\ny,\n\nFrom (25) it follows that\nP(y,=E\xe2\x80\x9ei=1,...,njx=x)\np X ( x lY i = ^i,i=1,...rn\t\n\nXx\t\n\n1,\nr\n\nP(yi=,i=1,...,nJX=x)pX(x)dx\n\n0\n\nxk+S\n\nx) m+n - S\n(11j k+S (1- x) m+n - S dx\n\nfor 0 5 x\n\n5\n\n(63)\n\n1\t\n\n0\n\nUsing the fact that\n1\n\nxm (1-x) n dx= m n! /(m+n+1) I\n0\nyields\n(k+m+n+l) I\t\n\np X {x ^yi , i= 1, ... , n) _ ( k +S) I (m+n - S) !\t\n\nk+S\t\n\nx\t\n\n(1-x)\t\n\nm+n-S for 0 !9x!51\n\t\n\nTherefore, the minimum mean-square error estimate of X given y i , ....\n1\xe2\x80\xa2\n\nE (X l yi .... , yn } = J x PX (x I yi, i = 1, ... , n) dx\nn =\n0\nn\n_ (k+l+ E yi)/(n+k+m+ 2)\ni=1\n\n24\n\nY is\n\n(64)\n\nwhich is a linear estimate. This result is not at all obvious from the recursive\nestimation formula of Example 1 in Section 4. Notice that as n becomes large,\nthe optimum estimate converges to the proportion of onus in the observed\nsequence.\nThe optimum estimate is unbiased. This follows since\nE(Xn } \t (k+1+nE(yi ) )/(n+k+m+2)\t\n\n(66)\n\nE(yi } = E(E (yi lX}} = E(X) = (k+l)/(k+m+2)\t\n\n(67)\n\nand\n\nso that\n(68)\n\nE(X = E(X}\t\nn\n\nThe linear minimum mean-square error estimate can also be derived by\nappealing to the Doob decomposition and expressing the observations as\nyi = X + qi . The sequence q = y - X is a martingale difference sequence with\nE(giq. }= E(E((yi -X)2 1XI } 6 i = E,X(1-X)} 6 i\nJ\n\n(m+l)(k+l)\t\n\n6\n\n(m+k+2)(m+ k+3)\t\n\nij\n\n(69)\n\nThe observations can be arranged in the matrix form\n1\n1\n\nYl\n\nY2\n\nql\nq2\n(70)\n\nX+\n1\n\nYn\n\n\t\n\n\'z\n\nq\n\nor\nY\n\n=A\n\nX +Q\n\n(701)\n25\n\ns\nw\n\n\t\n\ni\n1!/\n\nThen the optimum linear estimate is [7, Ch. 13]\nXR = E [X} + (A t R - \'A + V-1)-1 A t R- 1 (Y - E[Y})\t\n\xe2\x80\x94 \xe2\x80\x94\t\n\xe2\x80\x94\xe2\x80\x94\nn\t\n\n(1 :W\t\'\t\n\n(71)\n\nwhere R_ = cov Q and V = var X. This reduces to the conditional mean Xn\nderived above. The corresponding mean-square error is\n\nE [(X - Xn\t) 2 } _ (A t R -1\xe2\x80\x94\t + V\n\xe2\x80\x94\xe2\x80\x94 A\n\n-1 ) -1 =\t\n\n(m+l) (k+1)\t\n(k+m+2)(k+m+3)(n+k+m+2)\n\n(72)\n\nThe Kalman filter [ 81 can be used to obtain the optimum linear estimates\n\n111\t\n\nrecursively. If we consider X to be the state of the dynamical system\n\nCy\t\n\nn+l = X with X0 = X, then the observations are y n = Xn +qn and the Kalman\nX\nfilter equations become\nXn= Xn-1 + ( n/var q n ) (Yn - Xn-1 )\t\n\n(73)\n\n= n-1 (var qn)/ (rn-1+var q n)\t\nn\n\n(74)\n\nand\n\n\tF1 \'\t\n\nwith the initial conditions\nX 0 = E (X } and 0 = var X\t\n\n(75)\n^n\n\nThe mean-square error at time n is r\t\n\nn\n\n9\n\nIn computer simulations of the optimum nonlinear recursive estimator given\t\n\nn\n1\n\nby (44) and (48) and the optimum linear recursive estimator of previous paragraph\nwith probability density functions for X not belonging to the class in this section,\n\n`\t\n\nwe found only small differences in the estimates. This can be explained by the\t\n\nEj\t\n\n26\n\n(\t )\nkI\n\n\t\n\ni\ni t Eli\nG\t\n\nr\nrl\na\'\n\nfact that after a few steps the a posteriori probability density function becomes\npeaked about the true value of X and can be closely approximated by a density\nJJ.\'\t\nS\n\n^\t\n\n}\n\nof the class in this section. Then the two estimates become near; identical.\n3\n\n,\ni\n4\n\n{\n\ni\n\nF\n\nIj\n\nS^\n\n^.i\n\nR\n\nD\n\n1.\n\n-5555\n\ne\n\nj.\n\nl: \t\n\n..\t\n\n0044\n\nl\n\np\xe2\x80\x9e\nL\n\nis\n\n^I\nf5\n\n27\n\nREFERENCES\n1.\n\nA. Segall, "Recursive Estimation for Discrete-Time Point Processes",\nIEEE Trans. on Information Theory, Vol. IT-22, No. 44, pp. 422-431,\nJuly 1976.\n\n2.\n\nP. A. Meyer, Probability and Potentials, Blaisdell Publishing Company,\nWaltham, 1966.\n\n3.\n\nI. I. Gikhman and A. Y. Skorokhod, Introduction to the Theory of Random\nProcesses, W. B. Saunders, Philadelphia, 1965.\n\n4.\n\nD. L. Snyder, Random Point Processes, New York: Wiley, 1975.\n\n5.\n\nT. Kadota, "A New Look at the Mathematical Foundations of Communication\nTheory", Proc. of 9th Annual Allerton Conf, on Circuits and Systems\nTheory, pp. 485-496, University of Illinois, Urbana, October 1971.\n\n6.\n\nM. V. Vaca and D. L. Snyder, "Estimation and Decision for Observations\nDerived from Martingales: Part 1, Representations", IEEE Trans. on\nInformation Theory, Vol. IT-22, No. 6, pp. 691-707, November 1976.\n\n7.\n\nS. A. Tretter, Introduction to Discrete-Time Signal Processing, John\nWiley & Sons, 1976.\n\nS.\t\n\nR. E. Kalman, "A New Approach to Linear Filtering and Prediction,\nTransactions of the ASME, Journal of Basic Engineering, Vol. 82D,\npp. 34-45, March 1960.\n\nACKNOWLEDGEMENTS\nThis research was partially supported by NASA Grant NSG5048.\n\n28\n\n'