b'NASA Contractor Report 177511\n\ne\n\nMachine Characterization\nand Benchmark Performance\nPrediction\nRafael H . Saavedra-Barrera\n\n(brdSA-CB- 1775 11 1\nl A C E I b i E CBlkACTEEIZATXOI\nAh-6 BEICBBALiK PkR PCbW AlDCE P B P Z l C l l l C b i\n\n( C a l i f o r a i a Criv.)\n\n80\n\nCONTRACT NCA2-128\nDecember 1988\n\nNational Aeronautics and\nSpace Administration\n\nE\n\nCSCL I2A\n\n889-15648\n\nNASA Contractor Report 17751 1\n\nMachine Characterization\nand Benchmark Performance\nPrediction\nRafael H. Saavedra-Barrera\nUniversity of California,\nBerkeley, California\n\nPrepared for\nAmes Research Center\nCONTRACT NCA2-128\nDecember 1988\n\nNASA\nNationalAeronautics and\nSpace Administration\n\nAmes Research Center\n\nMoffett Feu,California 94035\nI\n\nTable of Contents\n\n.\n.....................................................................\n2 . L i m i t a t i o n s of Benchmarking ................................................\n2.1. System Characterization versus Performance Evaluation ...................\n2.2. The Role of Experimentation in System Characterization ..................\n3. A Model for P e r f o r m a n c e E v a l u a t i o n ......................................\n3.1. A Common Representation for Programs and Computer Systems ..........\n1 Introduction\n\n...............................................\n3.3. A Linear Model for Program Execution .....................................\n3.4. Limitations of the Linear Model .............................................\n4 . The S y s t e m Characterirer ....................................................\n4.1. Machine Implementation of Data Types .....................................\n3.2. The Class of Systems Studied\n\n......................\n4.2.1. Data Type Declarations ................................................\n4.2.2. Expressions .............................................................\n4.2.3. Statement-Level Control Structures ...................................\n4.2.4. Additional Parameters .................................................\n4.3. Experiment Design ...........................................................\n4.4. Test Structure and Measurement ............................................\n4.4.1. Direct Tests, Composite Tests, and Indirect Tests .....................\n4.5. Experimental Errors and Confidence Intervals ........ .. .. .. .. .........\n4.5.1. Reducing the Variance ...........................\n.. .. .. .. .........\n4.6. Is the Minimum Better than the Average? ............ .. .. .. .. .........\n4.7. Results Obtained with the System Characterizer ............................\n5 . The P r o g r a m Analyzer ........................................................\n4.2. Description of the System Characterizer\xe2\x80\x99s Parameters\n\niii\n\n1\n\n2\n2\n\n3\n4\n4\n\n6\n7\n\n8\n9\n\n10\n11\n12\n13\n\n15\n19\n\n20\n21\n23\n23\n25\n27\n29\n37\n\nPRECEDING PAGE BLANK NOT FILMED\n\n...........................................................\n5.2. Static and Dynamic Statistics ...............................................\n5.2.1. Description of the Test Programs ......................................\n5.3. Output from the Program Analyzer .........................................\n5.4. Program Statistics ...........................................................\n6 . T h e Execution P r e d i c t o r ......................................................\n5.1. Execution Profilers\n\n..................\n0.2. Execution Prediction and System Characterizers ............................\n6.3. Model 1-alidation .............................................................\n6.4. Execution Predictions and Actual Running Times ..........................\n0.1. Computing Execution Estimates and Experimental Errors\n\n.\n\n7 A n a l y s i s of Results a n d S u m m a r y\n\n...........................................\n\n...........................................................\n7.2. Future Improvements t o Our System ........................................\n7.3. Summary .....................................................................\n7.4. Acknowledgements ...........................................................\n8 . References ......................................................................\n9 . Appendix .......................................................................\n7.1. Analysis of Results\n\niv\nI\n\n37\n38\n\n39\n43\n43\n47\n\n47\n47\n48\n49\n\n56\n50\n59\n\n64\n64\n05\n69\n\nList of Figures\nFigure 3.1: System characterization and performance analysis\n\n......................\n\n...................\n......................\n\n5\n\nFigure 4.1: The same system charactrrizer is used in all systems\n\n10\n\nFigure 4.2: Assembler code of DO loop with step equal t o one\n\n17\n\nFigure 4.3: Assembler code of DO loop with step different from one\n\n................\n\n....................................\nFigure 4.5: Confidence intervals for ten different parameters .......................\n\nFigure 4.4: The basic structure of an experiment\n\nFigure 1.6: Characterization results (C\'R.\\J\'. CYBER, and IBM 3090)\n\n..............\n\n.............\nFigure 4.8: Characterization results (VXY 785. VAX 780. and SUN 3/50) ..........\nFigure 4.9: Characterization results (IBhl RT-PC/125) .............................\nFigure 4.10: Characterization results relative to the VAX-11/780 (I) ...............\nFigure 4.11: Characterization results relative to the VAX-11/780 (11) ...............\nFigure 5.1: Static and dynamic analysis of programs ................................\nFigure 4.7: Characterization result.s (Amdahl. Convex, and VAX 8600)\n\n..................................\nFigure 6.1: Execution time estimate for the Mandelbrot program ..................\nFigure 5.2: Static statistics for the Mandelbrot set\n\n..........................\nFigure 6.3: Predicted times versus real execution times (11) .........................\nFigure 6.4: Predicted times versus real execution times (111) ........................\nFigure 6.5: Predicted times versus real execution times (IV) ........................\nFigure 6.2: Predicted times versus real execution times (I)\n\nFigure 7.1: Parameters normalized against the VAX-11/780 (I)\n\n.....................\n\n....................\nFigure 7.3: Parameters normalized against the VAX-11/780 (111) ...................\n\nFigure 7.2: Parameters normalized against the VAX-11/780 (11)\n\nV\n\n17\n\n22\n28\n31\n32\n33\n34\n35\n36\n38\n\n42\n48\n52\n53\n54\n55\n\n60\n61\n\n62\n\nList of Tables\n\n...........................................\nTable 4.2: A4rithmeticparameters w i t h local operands ..............................\nTable 4.3: Arithmetic parameters w i t h global operands .............................\nTable 4.1: Characteristics of the machines\n\nTable 4.4: Conditional and logical parameters\n\n.......................................\n\n..........................\nTable 4.6: Parameters for intrinsic functions ........................................\nTable 4.i: Definitions of terms used in the time analysis ............................\nTable 4.5: Execution control and array access parameters\n\nTable 4.8: Sources of experimental error and minimum execution time\nTable 4.9: Mean and standard deviation\n\n...............................\n\n12\n14\n\n15\n15\n19\n20\n23\n\n.............\n\n25\n\n.............\n\n26\n\n.......... .............\nTable 4.11: Regions in figures 4.6-4.11 ................................. .............\nTable 5.1: Static and Dynamic statistics of kernel (Livermore Loops) ...............\nTable 5.2: Characteristics of the test programs ......................................\n\nTable 4.10: Estimates taking the average and the minimum\n\n.............................................\nTable 5.4: Distribution of statements in the Livermore Loops .......................\n\nTable 5.3: The Livermore Loops kernels\n\n29\n30\n39\n\n40\n41\n44\n\n...................................\nTable 5.6: Dynamic statistics of test programs (11) ..................................\nTable 6.1: Execution estimates and actual running times (I) ........................\nTable 6.2: Execution estimates and actual running times (11) .......................\n\n51\n\n......\n\n57\n\nTable 5.5: Dynamic statistics of test programs (I)\n\nTable 7.1: Relative performance between the SUN 3/50 and the IBM RT-PC\nTable 7.2: Accuracy of the prediction estimates\n\n.....................................\n\n...............\n..................................\n..................................\n\n45\n46\n\n50\n\n57\n\nTable 7.3: Distribution of execution time for Los Alamos benchmark\n\n58\n\nTable 9.1: Characterization results for regions 1-3\n\n69\n\nTable 9.2: Characterization results for regions 4-7\n\nvi\n\n70\n\nTable 9.3: Characterization results for regions 8-12\n\nVii\n\n.................................\n\n71\n\nMachine Characterization and Benchmark\nPerformance Prediction?\nR a j a el H . Sa av edr a-Barr e t a\n\nDepartment of Electrical Engineering and Computer Science\nComputer Science Division\n\nUniversity of California\nBerkeley, California 94720\nand\nDepartamento de Ingenieria Electrica\nDivisi6n de Ciencias BBsicas e Ingenieria\nUniversidad A u t h o m a Metropolitana, Iztapalapa\nMexico D.F., Me\'xico\n\nABSTRACT\n\nFrom runs of standard benchmarks or benchmark suites, it is not possible to characterize the machine nor to predict the running time of other\nbenchmarks which have not been run. In this paper, we report on a new\napproach t o benchmarking and machine characterization. We describe the\ncreation and use of a machine analyzer, which measures the performance of\na given machine on Fortran source language constructs. The machine\nanalyzer yields a set of parameters which characterize the machine and\nspotlight its strong and weak points. We also describe a program analyzer,\nwhich analyzes Fortran programs and determines the frequency of execution of each of the same set of source language operations. We then show\nthat by combining a machine characterization and a program characterization, we are able t o predict with good accuracy the running time of a given\nbenchmark on a given machine. Characterizations are provided for the\nCray X-MP/48, Cyber 205, IBM 3090/200, Amdahl 5840, Convex C-1,\nVAX 8600, VAX 11/785, VAX 11/780, SUN 3/50 and IBM RT-PC/125,\nand for the following benchmark programs or suites: Los Alamos\n(BMK8A1), Baskett, Linpack, Livermore Loops, Mandelbrot Set, NAS Kernels, Shell Sort, Smith, Whetstone and Sieve of Erathostenes.\nJune 30, 1988\n\nt The material presented here i based on research supported in part by NASA under consortium agreement\ns\nNCA2-128, the Mexican Council for Science and Technology (CONACXT) under contract 49992, the National\nScience Foundation under grants CCR-82025Ql and MIP-8713274, by the St,ate of California under the MICRO\nprogram and by the International Business Machines Corporation, Digital Equipment Corporation, Tandem,\nHewlettPackard, and Signetin.\n\n1\nIntroduction\nMachine performance is described by specifying, in the case of the CPU, the timings\nfor all instructions, instruction interactions within the pipeline. storage delays and delay\nprobabilities, etc. This approach for estimating performance is commonly used by computer architecture performance experts in the course of designing a new architecture or\nimplementing an existing one [PEU77, MAC84, W E 8 2 , EME84. CLA851. This type of\ncharacterization makes it very difficult t o compare machines with different instructions\nsets. The standard method of evaluating computers consists of selecting some \xe2\x80\x9ctypical\xe2\x80\x9d\nexisting programs and running them on the new machine(s), i.e. benchmarking. There are\na number of known problems with this approach [DON87b, WOR84]:\n(a) Unless the existing programs are modified, they may not take advantage of the\nnew architect we.\n\n(b)\n\nI t is not reasonable t o expect that a single figure of merit can meaningfully\ncharacterize a computer system.\n\n(c)\n\nEach benchmark is itself a mixture of characteristics, and doesn\xe2\x80\x99t relate t o a\nspecific aspect of machine performance.\n\nIt is very difficult to infer the performance of the N+l\xe2\x80\x99st benchmark as a function of benchmarks 1,...,N.\n(e) It is very difficult to predict the behavior of existing benchmarks on a new\n\n(d)\n\nmachine, even given the characteristics of the new machine, without running\nthe benchmarks.\n\nIn this work we propose a new method to characterize the performance of computer\nsystems at the level at which applications are written. This description can be used to\npredict the behavior of real workloads. The characterization is via experimental measurement of individual components of performance. We argue that by evaluating machines\naccording t o a number of (somewhat) independent paramet,ers, it is possible t o estimate\nperformance for a wide range of workloads. This will permit much more valid comparisons\nbetween machines, and expose machine weaknesses or strong points for use by both the\ncustomer (in purchase, and in job assignment among various machines), and by the\nmanufacturer who can then work t o improve the next version of the product.\nThis report is organized as follows. In section 2 we discuss the limitations of existing\nbenchmarks and identify the characteristics that must be taken into account in the design\nof programs that can be used in the future as industry standards for system characterization. Section 3 describes our model for system characterization and performance prediction. In sections 4-6 we present the main modules of our system: the system characterizer,\nthe program analyzer, and the performance predictor, and describe their principal components. In Section 7 we make an analysis of our results, discuss future improvements,\nand give a summary of this report.\n\n1\n\n2\n\nLimitations of Benchmarking\nIn the past few years there has been great interest in performance evaluation caused\nmainly by an increase in the number of different new architectures. The number of benchmarks currently used to evaluate these systems is growing day by day, and new studies of\nthe performance of these machines appear either in technical journals or in popular magazines. Every new benchmark is created with the expectation that it will become the standard of the industry and that manufacturers and customers will use it as the definitive\ntest to evaluate the performance of computer systems with a similar computer architecture. Sooner or later every major user of computer resources publishes its own benchmark\nt h a t characterized the workload of that scientific institution [BAI85a, BUC85, CLJR76,\nMCM861. Most of the time these benchmarks provide useful information only t o the particular set of users that are represented by the programs. The limitations of benchmarks\nmentioned in the introduction are well known, but those objections do not show the fundamental problems Kith benchmarking. T o understand why the results obtained with\nthese programs are inadequate for system characterization, w e must discuss in more detail\nhow to characterize a computer system, and its relation to performance evaluation.\n\n2.1. S y s t e m Characterization versus P e r f o r m a n c e Evaluation\nBenchmarks, whether they are real programs, synthetic benchmarks, or kernels, have\nthe problem that they confuse two different things: system characterization and performance evaluation. We define system characterization as an n-value vector where each of\nthe components represent the performance of a particular primitive operation. This vector fully describes the whole system a t some level of abstraction. From the designer\xe2\x80\x99s\npoint of view the primitive operations could be the fetching of an instruction stream into\nthe instruction buffer, a translation buffer miss, branching t o a microinstruction, and so\non. Users see the system a t the level a t which they write their applications, and for them,\nthe primitive functions are the set of operations supported by the language they use. Here\nthe execution time of each primitive function depends not only on the hardware but also\non the code produced by the compiler, and sometimes on the libraries and the operating\nsystem.\nThe performance evaluation of a computer system is the measurement of some\nnumber of properties of the system during the execution of a particular workload. The\nproperties measured may be the total execution time to complete some job steps, the\nmemory used during each of the different steps, etc. The important thing t o note here is\nt h a t the evaluation depends on the set of programs executed. This means t h a t the estimate is valid only for that particular suite. All of the existing benchmarks evaluate the\nperformance of the system when this system is running that specific benchmark, and the\nresults thus obtained cannot be extrapolated t o other benchmarks or real workloads. This\ndoes not mean that benchmarks are not useful; they provide a good first approximation of\nthe expected performance of the systems they measure.\nStandard benchmarking can be described as an experimental evaluation of alternatives. Each experiment represents a point in the performance space of the system. Only\n\n2\n\nLimitations of Benchmarking\n\n3\n\nby decomposing these measurements and relating them to the characteristics of the benchmarks can we use them to predict the performance of different workloads. We must start\nwriting benchmarks that measure basic individual components that affect the performance\nof computer systems, and then use these results to evaluate their behavior. This implies\nthat system characterization and performance evaluation must be seen as two independent\nactivities. In this model, benchmarking is part of the characterization process. We will\nrefer to this new kind of benchmarks as system characterizers t o distinguish them from\nnormal benchmarks. The results obtained with a system characterizer represent different\naspects of the architecture and the software. It is clear that these parameters cannot be\ncombined to produce a single figure of merit. With only one number it is not possible to\nisolate the effects of hardware and software components during the execution of a variety\nof applications.\n2.2. The R o l e of Experimentation in System Characterization\nExperimentation plays an important role in science and especially in fields like Physics, Biology and Chemistry. Practitioners of these fields use experiments to collect information about a phenomenon, and then analyze the data to sustain or refute a hypothesis\nabout the phenomenon. Experimentation plays the role of predictor-corrector tool in the\ndevelopment of models and theories. Unfortunately the concept of experimentation in\ncomputer science is not a well defined activity, [DEN80, DEN81, MCC79, FEL791 especially in benchmarking where experimentation is confused with running programs and\ntiming their execution times. A hypothesis or model t o validate is almost never present.\n\nAn experimental performance evaluation of a computer system must satisfy several\nconditions t o be considered experimentally sound. First, the results must be reproducible.\nThis means that independent researchers must be able t o produce the same results using\ndifferent experiments. The results must also be consistent; the repetition of an experiment must produce the same results. The experiments must be performed in a controlled\nenvironment and the effect of extraneous variables must be quantified. And lastly, a\nmodel of t<heexecution of the system must exist so the results of an experiment can be\nrelated to previous and future experiments.\nThe last condition is especially a weak point in benchmarking. \\Ve see experimentation as the only way in which disputes can be settled regarding the comparative performance of different systems and the effect of particular components on the performance of\nthe system, as the only way of verifying system improvements, and as the only way of\nestablishing a cumulative tradition in which improvements can be introduced and new\narchitectures can be evaluated.\n\n3\n\nA Model for Performance Evaluation\nBefore we present our model for performance evaluation, we will first summarize our\ndiscussion of the last sections. \\Ve know that every performance evaluation is relative t o\nthe workload used t o make the measurements, and also depends on the characteristics of\nthe computer. It is not possible to evaluate a system without knowing the structure of\nthe workload and the behavior of the different components of the system when the computer executes t.hat workload. In an ideal world, we will expect t h a t for any two machines\nthere exists an order on their performance. If machine A executes program X faster than\nmachine B , then if we replace X by any other program we obtain the same relationship.\nUnfortunately, this is almost never the case. and in some cases the difference can be significant. For this reason some of the goals of performance evaluation (prediction) should be\nt o help us answer some of the following questions:\n\n-\n\nFor which set of programs will machine A execute faster than machine B\n(without having to run the programs on both machines)?\n\n-\n\nWhat are the portions of the programs that will consume more resources in different machines?\n\n-\n\nWhat are the components that have the potential of being the bottlenecks in\nthe execution of some programs?\n\nLooking at these three questions we can identify different. subproblems that we must\nsolve in order t o answer them satisfactorily. The first involves the measurement of the\nperformance of the individual components. This is what we called in the last section system characterization. Second, we need to decompose the workload using the same set of\nparameters. We should be able to make an analysis of the workload in terms of the\nparameters used t o characterize the systems. Lastly, in order t o solve the first question,\nwe need t,o combine the characterization of the system with the analysis of the workload.\nThis last phase we will call it execution prediction, and is part of the performance prediction of the system. In figure 3.1 we show the different stages of this process.\n3.1. A C o m m o n Representation for P r o g r a m s a n d C o m p u t e r Systems\n\nIf we want t,o make predictions at the level of user programs and at the same time to\nquantify the behavior of the different components of the systems, we need t o represent the\nsystems and the programs using the same model. This model could be the machine code\nproduced by the compiler, but this approach has several problems. First we need to know\nfor every machine (in fact for every compiler) the code produced for each of the language\nconstructs. Second the representation thus obtained is only valid for the machines that\nhave the same instruction set. I t becomes necessary for the program analyzer t o know the\ninner workings of each possible compiler, or t o compile each program in each machine and\nthen to analyze the object code. IVhat we want is a flexible common representation for all\nthe systems that is independent of the architecture and the compiler.\n\n4\n\n.A Model for Performance Evaluation\n\n\xe2\x80\x98\n\nh\n\n/-,\n-.\nSystem\nI\nCharacterizer\n,a\n\n\xe2\x80\x98 - \xe2\x80\x99\n1 1\n\n7i\n\nA\n\n,/\xe2\x80\x99- Machine\n\nA\n\n,\n\n(\n\n(Chararacterizatiod\n\n\xe2\x80\x98w.\xe2\x80\x99\n\nProgram\n\nStatistics\n\ni\n\n5Predictor\n\n(Pedormanee\nEstimate\n\n\xe2\x80\x98\nU\n\xe2\x80\x98\nFigure 8.1: System characterization and performance analysis. On the upper left part we see the\nsystem characterization represented as a program (benchmark) executed in a computer and\nproducing the characterization of the system. On the upper right side we have the program\nanalyzer, which takes as input an application program and decomposes it (statically and\ndynamically) using the same set of parameters used in the system characterizer. The lower\npart represents the synthesis of both the characterization and the analysis producing the\nperformance evaluation of the machine relative to the workload.\n\nT h e execution time of a program depends on the code produced by the compiler, and\nthis code is a function of the operations and control statements that a particular language\nprovides. For this reason the set of parameters t o use as a base in our decomposition of\nprogram and systems must be an abstraction of the operations supported by most of the\nmachines. and these parameters must be identified with a particular operation or control\nstatement of the high-level language.\n\n.A Model for Performance Evaluation\n\n6\n\nI t should be clear that the number of parameters depends not only on the number of\noperations and control statements that a programming language has, but also on the accuracy of the estimates that we want to obtain.\n3 2 T h e Class of Systems Studied\n..\nA general model that produces estimates of the possible performance for all the different architectures and modes of computations is unlikely t o exist in the near future.\nNevertheless this does not mean that it is not possible to obtain a model of the performance for a significant number of the architectures, those which have a common mode of\noperation. >loreover once w e have a good model for this class of system, it is possible t o\nextend it t o include more complex architectures. The approach that we use in this\nresearch is to choose a model of computation and obtain a model of the performance for\nthe machines that share that mode of operation; we will later extend i t to include more\ncomplex systems.\n\nIn this study we will restrict ourselves to a particular model of computation, in\nwhich we have in each system a single processor running in scalar mode, and the code generated by the compiler is not optimized. We will assume that the uniprocessor system\ndoes not support vector operations or, more precisely, that the compiler does not produce\nvector operations. We will also assume that the programs are compiled with the optimization switch turned off.\nThe machines described previously correspond to the SISD (single instruction\nstream/single data stream) model in the classification made by Flynn [FLYi2]. Although\nthere is only one processor executing a single stream of data, this does not imply t h a t the\nprocessor does not have parallelism a t the level of the execution of machine instructions.\nThe execution of an instruction can occur at the same time that the next instruction is\ndecoded and the last instruction is executed.\nThe characterization of systems with rector operations and/or including optimization\nhas several problems not present in scalar processing without optimization. Although it is\nnot difficult to extend the characterizer to include vector operations, using this information t o predict the execution time of programs requires also the characterization of the\ncompiler. Only by knowing which DO loops the compiler is capable of vectorizing can we\nmake an acceptable prediction of the expected execution time. Even if it were possible t o\nrun experiments and detect when a particular compiler will generate vector code and\nwhich vector operations will be executed, we still have the problem of detecting in arbitrary programs the occurrence of possible vectorizations. This requires a program\nanalyzer as \xe2\x80\x98smart\xe2\x80\x99 as any vectorizing compiler; in fact \xe2\x80\x98smarter\xe2\x80\x99, because it has t o vectorize the same loops for any arbitrary compiler and program.\nOptimization is even more difficult to handle. In addition t o the problems mentioned in the last paragraph, w e also find that there is not always a clear boundary on\nwhen to apply one optimization instead of another. In fact, applying one set of optimizations may prevent the compiler from detecting others. The decision of which optimization\nto try first normally depends on the order in which the optimizations are tried. In some\ncases optimization eliminates redundant or/and \xe2\x80\x98dead\xe2\x80\x99 code, especially inside DO loops\nand this not only affects user programs but also benchmarks, so validating the measurements is even more difficult. Lubeck et al. reported that vector optimization had t o be\ndisabled in order to obtain meaningful measurements on the Fujitsu VP-200 [LUB85]. If\n\n-\n\nX llodel for Performance Evaluation\n\n4\n\nwe add that most \xe2\x80\x98optimizing\xe2\x80\x99 compilers can only perform certain optimizations on some\ndata types and not in others [LIN86a, LIN86b1, we understand why it requires a \xe2\x80\x98superoptimizer\xe2\x80\x99 to know how a program will be modified in order to make accurate prediction\nof the expected execution time of optimized programs. I t is outside the scope of this\nresearch to write such \xe2\x80\x98super-optimizer\xe2\x80\x99: we will try t o develop other techniques to characterize vectorization and optimization in the future.\n3.3. A Linear M o d e l for Program Execution\n\nIf we want to produce estimates of the time a program or set of programs will take\nto execute in some machines, w e will need to produce a model of how the total execution\ntime is obtained from the individual parameters. One approach used by machine\ndesigners is to obtain the mean execution rate of a system while executing a particular\nworkload. To do this we decompose the mean instruction execution time I into the sum\nof three basic components (MAC841\nI =E+D+S\n\n(3.1)\n\nwhere D is the mean pipeline delay per instruction, caused by path conflicts, register\ndependencies, and taken branch delays; S is the mean storage access delay per instruction\ncaused by a cache miss for instructions and operands; and E is the mean nominal execution time when there are no pipeline delay or storage access delays. In our model we do\nnot deal with single machine instructions, but with the set of primitive operations supported by a particular programming language. Each of these primitive operations (parameters) is mapped into several machine instructions. Therefore the mean parameter execution time ( P i ) is equal t o the mean execution time of that sequence of machine instructions. IVe can decompose the mean execution time of each parameter as\n\n(34\n\nPi = E i + D i + S i\n\nwhere the three terms t o the right of the equal sign have the same interpretation, but\nrefer to a sequence of instructions instead of only one.\n.As we noted in the last paragraph the major difference between the two models is\nthat hardware designers are interested in the mean execution time of each machine\ninstruction, while in our model the set of parameters belong t o a higher level of abstraction. The machine implementation (code produced by the compiler) of these parameters\ncould be in its simplest case one machine instruction, but in most cases the compiler generates several machine instructions for each parameter.\n\nThe total execution time of a program is equal t o the nominal total execution time\n(when no pipeline delays or storage delays occurs), plus the total pipeline delay time, and\nthe total storage access delay time.\n\nwhere\ni=l\n\ni-1\n\nwhere Ci is the number of times parameter Pi is executed. We can use these equations to\nobtain the total execution time of the program as\nn\n\nT=\n\nn\n\nC\xe2\x80\x98i(E(\ni=l\n\n+ Di + S i ) = i 2 CiPi\n-1\n\n(3.5)\n\n.A Xlodel for Performance Evaluation\n\n8\n\nSote that by using a system characterizer written in a high-level program it is neither possible t o measure the mean nominal execution time of each parameter, nor the\nmean pipeline delay, nor the mean storage delay. What the system characterizer measures\nis the mean execution time (nominal execution plus pipeline delays and storage delays) of\nthe set of machine instructions that implement each parameter.\n3.4. Limitation of the Linear Model\n\nThe linear model for the execution time of applications proposed in the last section\nhas some limitations. \\Ye assumed that the time it takes for the execution of n operations\nis just the sum of the individual execution times. In highly pipelined machines the execution time when there is a register dependency conflict. may be several times greater than\nthe execution time without this delay. As an example consider the CYBER 205 architecture. The scalar processor is derived from the CDC 7600. I t has five arithmetic subunits\nwithin the Scalar Floating-point unit. All of them are pipelined and can accept a new pair\nof input operands at every clock cycle (20 ns). The Add/Subtract and Multiply Units each\ntakes five clock periods t o produce a result and return it t o the input of another unit\n[IBB82]. Therefore it takes 100 ns from the beginning of the operation to the time the\nresult is available. The execution time of R operations without any data dependency conflicts can take as little as Z O n ns. On the other hand the execution of the same R operations can take loon ns if each operation has a conflict with the next one. Consider the\nfollowing t w o statements\n\nX9 = ((X1\n\n+\n\nX2)\n\n*\n\n(X3\n\n+\n\nX4))\n\n+\n\n((X5\n\n+\n\nX6)\n\n*\n\n(X7\n\n+\n\nX8))\n\nif we compute their timing diagrams w e find that for the first statement the execution of\nthe RHS\' takes approximately 360 ns (the four adds, that are leaves of the syntax tree,\nstart execution in the first four cycles, the two multiplications in cycles 7 and 9, and the\nlast add in cycle 14). For the second statement, the execution of the RHS takes 400 ns\ndue to data dependencies. However the first statement executes seven operations. while\nthe second only four. A simple linear model will not predict that the first statement will\nexecute faster unless the model contains information about the behavior of the processor\nwhen data dependencies are present. Nevertheless we expect t h a t in large programs\ndiscrepancies in different directions will cancel and therefore the total error will be small\ncompared t o the total execution time.\n\nRight Hand Side of the assignment statement.\n\n4\n\nThe System Characterizer\nSystem characterizers ( t o distinguish them from benchmarks) are a set of experiments that detect, isolate and measure hardware and software features. These features\ndescribe the system and determine its performance. The accuracy of the description\ndepends on the number and detail of the experiments. A very coarse model would be one\nin which all floating point operations are represented by only one parameter. A better\napproximation will have as many parameters as there are floating point operations. An\neven better one will distinguish the length of the operand (number of bits) and their\nstorage class. In some systems the time t o access a variable depends on whether the variable is local or global.\nEach parameter must be measured in a controlled way and, if repetition is used, the\ntest must be run for a significant amount of time to reduce the experimental error due to\nclock resolution. If we view system characterization as an incremental process, we can\nbuild different system characterizers with different degrees of resolution. There are at\nleast two possible benefits for doing this: (1) in the early phases of the evaluation process,\nit is appropriate and cost effective to use an approximation with not too many parameters. (2) If our system model does not detect some features for certain kinds of architectures, new experiments are incorporated or some of the existing ones are replaced with a\nminimum number of changes.\n\nIn figure 4.1 we present the process of characterization. On the left of the figure we\nhave a single system characterizer run in several machines executing in uniprocessor scalar\nmode and without optimizing the code generated by the compiler. The output produced\nby the characterizer is the data base that we will use to produce performance estimates of\nthe machines, with the help of the program analyzer and the execution predictor. Only\none characterization per computer system is needed in order to estimates the performance\nof any program written in FORTRAN.\n\nAs an example, let us consider bow characterization will work in the case of vector\noperations. Here we are interested in running some experiments to test the amount of vectorization that systems can do. Not only the vector operations that the hardware supports, but also the kind of language constructs that the compiler can detect as vectorizable. A possible way of characterizing this class of operations is by using two or more\nparameters. In the case of memory-to-memory vector machines only two parameters are\nneeded, the startup time for the vector operation, and the asymptotic execution rate.\nThis last parameter is the maximum rate at which the processor can execute a vector\noperation [HOCSl, HWA84, HOC85, SHI871. For register-to-register machines we need\nalso the overhead time associated with the stripminingl process, and the length of the vector registers [BUC87, MARK\']. Machines with cache, the performance is also affected by\n1 When the number of elements in a vector operation is greater than the number of vector registers, the instruction must be treated as a sequence of vector operations. This technique is called stripmining, and is done a t compile time. Because it takes some time to restart the next operation there is\nan overhead associated with stripmining and this overhead is normally less than the vector startup.\n\n9\n\nT h e System Characterizer\n\n10\n\n4\n\nMachine\n\nI\n\ni\n\n1\nI\n\n4 I\nMa$re\n\nI\n\np(x\nCharacterization\n\nCharacterizer\nI\n\nI\n\nI\n\nI\n\nFigure 4.1: The same system characterizer is used in all systems\n\nthe size of cache: the asymptotic execution rate normally changes for vectors with a length\ngreater than the size of the cache.\n\n4.1. M a c h i n e Implementation of Data T y p e s\nT o be useful the system characterizer must be easy to use and portable. Although\nFORTRAN has been standardized, there exist several differences between FORTRAN\ncompilers and machines that makes complete portability difficult to achieve. One of these\ndifferences is the declaration of d a t a types. Single precision real variables are implemented in CDC and CRAY machines using one word, which is equal t o 64 bits, while double precision variables are assigned two words. The CYBER 205 supports another type\nnamed HALF PRECISION (32 bits). On the other hand, IBM 3090/200, VAX-11, and\nSUN 3 implement single precision with 32 bits and double precision with 64 bits. Also the\nf 77 compiler in UNIX systems implements single precision with 32 bits and double precision with 64 bits [FEL78]. In VAX machines running ULTRIX the fort compiler accepts\nquadruple precision, which is equivalent t o double precision in CDC type machines. One\nway to avoid this problem is to specify explicitly the number of bits in the implementation\n\nThe System Characterizer\n\n11\n\nof the different data types. In some compilers it is possible t o say how many bytes to allocate to the variables by appending at the end of the type an asterisk follow by 2, 4, 8, or\n16. But on the CYBER and CRAI\' the meaning of the first two options is different.\nThese are changed to correspond to single precision on those machines. The problem is\nmore difficult in the case of integer variables. The CDC-type machines support only 64bit integers: on the CRAY, integers have 46 or 64 bits?, while in the VAX and SUN\nmachines integers are implemented either by 16 or 32 bits.\nTo make a fair comparison between different machines we need t o make the evaluation under similar conditions in all sptems. If all the machines are 32-bit microcomputers,\nthe memory cell unit on all the systems is equal, and if the tests are run under the same\nconditions. it is not difficult to make a fair comparison. On the other hand if some\nmachines are 64-bit mainframes, others 32-bit minicomputers and another subset is composed of 32-bit microcomputers, then is not clear how t o make a fair comparison between\nthe machines. Do we have to test the machines with all the data types implemented with\n32 or 61 bits? How do we make a comparison if the machines do not have a common\nrepresentation (same precision) of some data types? If some subset of the tests do not\nneed more than 32-bit real numbers to execute correctly, why do we need to run these\nprograms using (34-bit variables on the microcomputers? If a test gives erroneous results\nwhen run on a 32-bit machine, what is the point of saying that the machine runs at the\nsame speed compared to a (34-bit mainframe?\nThe above discussion gives a hint of the difficulty of making a good comparison\nbetween machines even in the case when these have similar characteristics. The conditions in which comparisons are made should be decided case by case, depending on the\nmachines and the objective of the study. The purpose of this report is not t o make an\nevaluation of some computer systems, but to present a new methodology for performance\nevaluation and system characterization. It is for this reason t h a t we run our system\ncharacterizer using the particular implementation of single and double precision on each\nmachine. Table 4.1 gives for each machine the number of bits used in each of the data\ntypes.\n4.2. Description of the S y s t e m Characterizer\'s P a r a m e t e r s\n\nNormally the characterization of computers at the architecture level is done using\nthe instruction set of the machine. On the other hand, the decomposition and analysis of\nprograms normally reflects the control structures and operators t h a t a particular language\nsupports [WEI84]. Because our representation of both computers and programs uses the\nsame set of parameters, results may be combined.\nT o understand the set of parameters chosen in our system characterizer, we need to\nanalyze the set of constructs that FORTRAN provides, and see how these features affect\nthe execution time of a program. We will then create parameters for the different\nmechanisms that affect the execution and ignore those that are only used as aids either to\nthe programmer or the compilers in the writing of correct and efficient programs. The\nconstructs supported by imperative or statement-oriented languages, like FORTRAN, can\nbe separated in four main categories: data type definition mechanisms, expressions,\nThe default on the CRAY X-MP is 46 bits. There exists a compiler option that extends integers\nto 64 bits.\n\n~~\n\n12\n\nThe System Characterizer\n\nTable 4.1: Characteristici of the machine (I)\nCompiler\nOperating\nName jLocat ion\nversion\nSystem\nCOS 1.16\nC F T 1.14\nNASA Ames\nCRAY X-hIP/48\nNOS\nFTN2OO\nNASA Ames\nCYBER 205\nVM/ChlS r.4\nFORTRAN v2\ncmsa.berkeley .edu\nIBM 3090/200\nF77\nUTS V\narnes-prandt.nasa\nAmdahl 5840\nF C v2.2\nUNIX C-1 ~6\nconvex.riacs.edu\nConvex C-1\nF77 v l . 1\nvangogh.berkeley.edu\nUNIX 4.3 BSD\nVAX 8600\nF77 v l . 1\narpa. berkeley .ed u\nUNIX 4.3 BSD\nVAX-11/785\nF77 v l\nUltris 2.0\narnes-pioneer.nasa\nVAX-11/780\nF77 v l\nUNIX 4.2 r.3.2\norchid.berkeley .edu\nSUN 3/50\nF77 v l\nA M 4.3\njeff.berkeley .edu\nIBM RT-PC/125\nMachine\n\nTable 4.1: Characteristics of the machines (11)\nReal\nMachine\nInteger\nMemory\nsingle double\nsingle\n46\n8 Mwords\n64\n128\nCRAY X-MP/48\n64\n64\n128\nCYBER 205\n8 Mwords\n32\n64\nIBM 3090/200\n32 Mbytes\n32\n32\n64\n32\n32 Mbytes\nAmdahl 5840\n32\n64\n100 hlbytes\n32\nConvex C-1\n32\n64\nVAX 8600\n32\n28 hlbytes\n32\n10 Mbytes\n32\n64\nVAX- 11/785\n32\n64\n32\n2 Mbytes\nVAX-11/780\n32\n32\n64\nSUN 3/50\n4 Mbytes\n32\n64\n32\n4 hlbytes\nIBM R T - F C l l 2 5\n\\\n\nI\n\nTable 4.1: Characteristics of the machines. The size of the data type implementations are in\nnumber of bits.\n\nstatement-level and unit-level control structures, and simple statements.\n4.2.1. Data T y p e Declarations\nThe d a t a type declaration constructs in FORTRAN are used only as direct,ives for\nthe compiler, and the creation of data objects for the global (COMMON) and local variables is normally done before program execution. Therefore we d o not need t o create\nparameters for these statements. IVe will see in section 5.2 t h a t the declarations of a\nFORTRAN program will also help the program analyzer in the decomposition of programs, in the same way it9 these statements help the compiler to generate correct machine\ncode. The exception t o this situation is the DATA statement, given t h a t the initialization\nof the variables declared in the DATA must be done at each activation of a program unit.\nNormally the DATA statement is used for the initialization of global variables, and therefore its effect on the total execution time on scientific programs is small.\n\nThe System Characterizer\n4.2.2.\n\n13\n\nExpressions\n\nFORTRAN is a language for scientific and numeric applications. For this reason the\nrichness of the language lies in the arithmetic operators that it supports. In addition t o\nthe arithmetic operators, FORTRAN also provides six relational and six logical operators.\nOur system characterizer does not distinguished between the relational operators; all of\nthese operators are grouped in the same class (.EQ., .LE., etc). This is because it takes\nthe same time t o compare two values independent of the relation. The same treatment is\napplied t o the logical operators (.OR., AND., and .NOT.). In contrast to the logical\noperators that can only take as arguments logical values, the arithmetic and relational\noperators are polymorphic. This means that, even when the semantic of the operation is\ndifferent for different data types, the same name (symbol) is used. By looking a t the\narguments the compiler identifies the correct use of the operator and produces code\naccordingly. Because the execution time of a multiplication is different using integer arguments compared with real ones, we have to create a set of parameters that represent the\nexecution using integer operands and another using real operands. In similar a way the\nexecution time depends on the precision of the operands; it normally takes less time to\nexecute an operation with single precision compared with double precision operands.\nAnother classification is made with the storage class of the operands. Global variables in\nFORTRAN (variables defined in COMMONS) are sometimes treated differently from local\nvariables. An example of this is the way the CYBER 205 deals with variables stored in\nCOhlllONs, when running without optimization. The compiler treats the COMMON as\nan array and allocates a base-descriptor pointing t o the first element of the COMMON.\nAn operand is loaded by first adding the offset (from the beginning of the COMMON\nblock) t o the base-descriptor and then loading the operand. This way of treating simple\nvariables makes the execution slower when they are allocated as global (variables) as\nopposed to local.\nT h e arithmetic operators defined in the system characterizer are: addition, multiplication, quotient, and exponentiation. The addition operator also includes subtraction. In\nthe c s e of exponentiation with a real base, we distinguish two cases: one when the\nexponent is integer and the other when the exponent is real. This is because in each case\nthe implementation is different. When the exponent is integer the result is computed by\neither executing the same number of multiplications as in the exponent (when this is\nsmall), or by binary decomposition. When the exponent is real the result is computed\nusing logarithms. If the base is an integer, we have two cases, one with the exponent\nequal to two and another with an exponent different from two. Because the number of\nexponentiations executed in most programs is small, these simplifications are enough for\nour purposes.\nIn tables 4.2 and 4.3 we present a description of the arithmetic parameters measured\nby the system characterizer. One table is for local operands and the other for variables\nallocated in COMMON blocks.\nThere are three different subsets of parameters in each table. The first subset is for\nsingle precision real variables; the second for double precision variables; and the last for\nintegers. Two parameters require explanation. One is the set of parameters t h a t measure\nthe overhead of the store operation (SRSL, SRDL, SISL, SRSG, SRDG, and SISG), and\nthe other, what we called memory transfer parameters (TRSL, TRDL, TISL, TRSG,\nTRDG, and TISG). In most high-level programming languages it is not possible t o\n\nT h e System Characterizer\n\nTable 4.2: Arithmetic operators with local oDerands\nData type\nOperation\nLlnemonic\nstore\nreal\nSRSL\nreal\naddition\nARSL\nmultiplication\nreal\nhlRSL\ndivision\nreal\nDRSL\nreal\nERSL\nexp (X ** I )\nreal\nSRSL\nexp (X ** Y )\nmemory transfer\nreal\nTRSL\nstore\nreal\nSRDL\naddition\nreal\nARDL\nmultiplication\ndouble\nlocal\nreal\nh,lRDL\ndivision\ndouble\nlocal\nreal\nDRDL\nexp (X ** I )\nreal\nERDL\nreal\nexp (X ** Y)\nXXDL\nmemory transfer\nreal\nTRDL\ninteger\nstore\nsingle \'\nlocal\nSISL\ninteger\naddition\nsingle\nlocal\nAISL\nmultiplication\ninteger\nsingle\nlocal\nMISL\ninteger\ndivision\nsingle\nlocal\nDISL\nexp (I ** 3)\ninteger\nsingle\nlocal\nEISL\nexp (I ** J )\ninteger\nsingle\nlocal\nXISL\nmemorv transfer\ninteger\nTISL\nsingle\nlocal\n\n,\n\ni,\n\nTable 1.2: Arithmetic parameters with local operands.\n\nexecute a single load operation without executing a t the same time another operation.\nThis is why we do not have a parameter that measures the time it takes t o load a\nsingle operand. These times are included in the execution time of the operators. Another\nreason is that some compilers, even when optimization is turned off, load the variables in\nregisters only once while evaluating an expression3. Even if we measure the time it takes\nt o load an operand, we are left with the problem of deciding when the compiler will reload\nit or use the register that holds a copy of it,s value. On the other hand it is possible to\nr u n experiments that detect and measure the time it takes t o store the result of the\nexpression. In some cases the value of these parameters is negligible (the store operation\noverlaps with the execution of arithmetic operators), while on others the time can be significant. In an assignment where there are no operators on the right hand side of the equal\nsign, the execution time of these statements cannot be explained just by the store operation. This type of statements are characterized by the \'memory transfer\' parameters.\nIn table 4.4 we give the set of parameters associated with compare and logical operations for local and global variables. As in the arithmetic case we distinguish the operands\ndepending on their storage class, the data type and the precision. For the logical operations there is only one data type and one precision.\n3 The compiler does not attempt to eliminate redundant subexpressions; it only keeps a record of\nwhich variables were previously loaded. This information is not used in subsequent statements even\nwhen these are in the same basic block.\n\nThe System Characterizer\n\nF\n\n: 4.3: Arithmetic operators with globa\n\nM n e mon ic\nSRSG\nARSG\nMRSG\nDRSG\nERSG\nXRSG\nTRSG\nSRDG\nARDG\nMRDG\nDRDG\nERDG\nSRDG\nTRDG\nSISG\nAISG\nMISG\nDISG\nEISG\nXISG\nTISG\n\nOperation\nstore\naddition\nm u It i plic a t ion\ndivision\nexp (X ** I )\nexp (S** Y)\nmemory transfer\nstore\nadd it ion\nmultiplication\ndivision\nexp (X ** I)\nexp (X ** Y )\nmemory transfer\nstore\naddition\nmultiplication\ndivision\nexp (I ** 2)\nexp (I ** J )\nmemory transfer\n\n1\n\nData tvDe\nreal\nreal\nreal\nreal\nreal\nreal\nreal\nreal\nreal\nreal\nreal\nreal\nreal\nreal\ninteger\ninteger\ninteger\ninteger\ninteger\ninteger\ninteger\n\n1\n\n~ _ _ _ _ _ _ _\n\nPrecision\nsingle\nsingle\nsingle\nsingle\nsingle\nsingle\nsingle\ndouble\ndouble\ndouble\ndouble\ndouble\ndouble\ndouble\nsingle\nsingle\nsingle\nsingle\nsingle\nsingle\nsingle\n\niperands\nStorage class\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\nglobal\n\nTable 4.81 Arithmetic operations with global operands.\n\nI\n\nTable 4.4: Conditional and 1 tical Darametera\nOperation\nData type Precision\nStorage class\nAND and OR\nlogical\nsingle\nlocal\ncompare\nreal\nsingle\nlocal\ncompare\ndouble\nreal\nlocal\ncompare\ninteger\nsingle\nlocal\nAND and OR\nANDG\nlogical\nsingle\nglobal\nCRSG\ncompare\nsingle\nreal\nglobal\nCRDG\ncompare\nreal\ndouble\nglobal\nCISG\ncompare\ninteger\nsingle\nglobal\n\nMnemonic\n\nTable 4.41 Conditional and logical parameters with local and global operands.\n\n4.2.3. Statement-Level C o n t r o l Structures\n\nFORTRAN has eight different flow control statements that affect the execution of a\nprogram and only a few of them have an effect on the execution time. Here we will\npresent each of the different types of statements and discuss their impact in the execution\ntime of the programs when using these constructs. We will also indicate the parameters\nassociated with these statements.\n- GO TO s t a t e m e n t s : there are three different types of GO TO statements. the\nunconditional GO TO statement, the assigned GO TO statement, and the computed\n\nThe System Characterizer\n\nId\n\nG O T O statement. The unconditional GO T O is the most used of the three and also\nis the fastest t o execute. In most machine this statement is implemented by a single\nmachine instruction, but in pipeline architectures the cost of a pipeline stall can be\nsignificant if its target is not in the CPL\' prefetch buffer. LVe created one parameter\n(GOTO) to measure the mean execution time of an unconditional branch. \\Ve do not\ntake into account the distance from the source of the branch t o its target.\nBranches affect the execution of a program in several ways. In pipelined machines a\npenalty must be paid when a branch is taken and the target instruction has not been\npreviously fetched4. All partially executed instructions in the pipeline must be discarded and the new stream of instructions must be fetched [LEE84]. .A branch to an\ninstruction t h a t is not in the cache involves not only fetching the next instruction,\nbut in addition the miss ratio is affected by changing the spatial locality of the execution [SMI82].\nThe computed GO T O statement is the equivalent of the case and switch statements in PASCAL and C respectively. The control of the transfer is the value of an\ninteger expression. The implementation of this instruction uses a table and executes\nand indirect branch with the integer expression as an offset. This usually requires\nthe execution of several machine instructions, like loading the value of the control\nvariable from memory, selecting the branch displacement from the branch table. and\nbranching to the new instruction. The execution time of this instruction is normally\none order of magnitude greater than the unconditional branch. In fact for some\nmachines the characterizer did not detect the execution time of an unconditional\nbranch. We measure the execution time of this instruction with the parameter\n\nGCOM.\nThe assigned GO T O statement is an old and rarely used feature in FORTRXN; its\npurpose is t o control the transfer to the value of an integer variable that was previously assigned the value of a label. In the system characterizer and the program\nanalyzer this construct is treated in the same way as the computed GO TO.\n\n-\n\nDO loop statement: this mechanism controls the repeated execution of a group of\nstatements. The execution overhead associated with this statement may be significant in scientific applications running in scalar mode. Its implementation has two\nparts, the initialization of the control variable, limit and step, and the repetition control overhead. The first overhead is insignificant and in fact in the first versions of\nthe characterizer there was no parameter associated with it. In programs where\nsmall loops (few iterations) are nested inside other loops, the initialization overhead\nmay affect the total execution time. In the system characterizer we haye four\nparameters t h a t deal with DO statements due to implementation differences in most\nmachines. In FORTRAN there is the possibility of omitting the step value of the\nheader of the loop, and the compiler assigns one t o the step by default5. The code\nproduced by most compilers when the step is one is different than the code when the\nstep is not one. In figures 4.2 and 4.3 we see the code produced by the CYBER 205\nFTN200 FORTRAN compiler for these two cases.\n\n4 Even in machines that have some kind of branch prediction circuitry, a penalty must be paid\nwhen the prediction is incorrect.\n6 D. Knuth reports that 80% of the DO loops have a step value o 1 [KNu71].\nf\n\n~\n\n~\n\n17\n\nThe System Characterizer\n\nD O l I = J . K\n\n...\n1\n\nCOlTIlTUE\n\n;\n;\n;\n;\n\nL1\n\n*..\n...\n\n...\n\nIBXLE,BRB\n\nI , C-4, L 1 , C - 5 , I\n\nL2\n\nload #l i n t o r e g i s t e r C - 4 ( s t e p )\nload K i n t o r e g i s t e r C - 5 ( l i m i t )\nload J i n t o v a r i a b l e I ( c o n t r o l v a r i a b l e )\nis upper l i m i t l e s s than lower l i m i t ?\n\n;\n\nRTOR # l , C-4\nRTOR K, C - 5\nRTOR J , I\nIBXLE,BRF C - 5 , , L 2\n\nbody of l o o p\n\n; i n c r e m e n t I b y C-4, compare\n; w i t h C - 5 , b r a n c h t o l a b e l L1\n; i f l e s s e q u a l , and s t o r e v a l u e\n\nin I\n\nFigure 4.2: Asernbier code of DO loop with step equal one\n\nDO 1 I\n\n= J, K, L\n\n...\n1\n\nCOBTIHUE\n\nSUBX K, J , PR-3\nADDX L , PR-3. PR-4\nD I W P R - 4 , I , PR-5\nTRU P R - 5 , PR-6\nRTOR P R - 6 , C-4\nRTOR #O, C - 5\nRTOR L , C - 6\nRTOR J, I\nIBXLE, BRF C - 4 , , L2\nL1\n\n...\n...\n...\n\nADDX I , C-6, I\nIBXLE,BRB C - 5 , t l , L l , C - 4 , C - 5\n\n, t h e s t a t e m e n t DO 1 I = J , K , L is\n, t r a n s f o r m e d t o t h e e q u i v a l e n t s t a t emen t\n, DO 1 C - 5 = 0, C - 4 - 1, 1\nwhere C - 4 = [(K - J + L) / L]\n0\n\n.\n\nload #O i n t o r e g i s t e r C - 5\nload v a r i a b l e L i n t o r e g i s t e r C-6\nload J i n t o v a r i a b l e I\nis u p p e r l i m i t l e s s t h a n lower l i m i t ?\n\n, body of l o o p\n, i n c r e m e n t I by r e g i s t e r C-6\n; i n c r e m e n t , t e s t , and b r a n c h i n s t\n\nL2\nFigure 4.8: Assembler code of DO loop with step different from one\n\nAs the two figures show, the initialization and iteration overhead are different, and\nin the second case can significantly affect the execution of the program when we have\nvarious DO loops nested. In the system characterizer there exist two parameters for\n\nThe System Characterizer\n\n18\n\neach type of DO loop. For loops with step equal one the initialization and repetition\noverhead are called LOIN, and LOOV. In the other case the parameters are L O I S\nand LOOX. llaking a good measurement of the overheads incurred by the DO loop\nstatement can be difficult.\n\n-\n\nIF statements: there are three different type of IF statements in FORTRAN, the\nblock IF. the logical IF, and the arithmetic IF. The block IF statement is a new\nfeature incorporated in the FORTRAN 7\'7 standard and represents the if-then-else\nmechanisms of .\\LGOL-like languages. In fact the logical IF is a special case of the\nblock IF, when no else clauses are used a d the then part of the if contains only\none executable statement. If we analyze the effect of a block IF statement in the\nexecution time, we will notice that the only overhead incurred by this statement is\nthe same as the one produced by a conditional branch instruction. We can see this\nby looking at the next two examples\n\nL = I .EQ. J . O R I .NE.K\nwhere I, J, and E; are integers and L is a logical variable, and\n\nIF (I EQ J OR\n\nI NE K)\n\nGO\n\nTO\n\n1\n\nThe machine code produce by the compiler for the right hand side of the assignment\nand for the expression inside the parenthesis in the IF statement is the same, even\nfor compilers that short-circuit expressions. The same situation occurs in the case of\nthe else and elseif statements. The arithmetic IF is handled in a similar way. This\nstatement has two parts; one is the evaluation of an arithmetic expression, and the\nother is a jump to one of three possible targets. The arithmetic expression is\nanalyzed as any other expression and the branch is replaced by a computed G O T 0\nstatement.\n\n-\n\nCONTINUE statement: this construct is not an executable statement and its\noccurrence in the source program should affect the execution time of an application.\n\n-\n\nCALL and RETURN s t a t e m e n t s : the first statement transfers control from one\nunit t o another and the second statement returns the control t o the original caller.\nAlso included in the CALL statement are the set of parameters that are passed from\none subprogram to another. The overhead incurred by the execution of the CALL\nstatement is considerable and can be divided in three parts: the overhead incurred in\nthe passing of arguments, the prologue overhead and the epilogue overhead. This\nlast part is the code executed by the RETURN statement. The amount of work that\nhas t o be done depends on the number and type of the arguments. In FORTRAN all\nthe arguments are passed by reference including values computed by expressions.\nThe characterizer has three parameters, these are: ARGS, ARGD, ARGI. These\nmeasure the time to load the corresponding pointer t o single precision, double precision and integer variables either into the static environment of the callee subprogram\nor in the execution stack. Although FORTRAN uses static allocation, many\nmachines use the execution stack t o pass parameters and results between subprogram\nunits. The addition of the prologue and epilogue execution time associated with the\ninvocation of a unit program is characterized by the parameter PROC, even when\neach of them is executed in different subprogram units and at different moments.\n\nThe System Characterizer\n\n-\n\ni9\n\nSTOP and PAUSE statement: these instructions do not generate significant\noverhead, and therefore there are no parameters for these statements in the system\ncharacterizer.\nTable 4.5: Execution control and array access Darameters\n[nemonic\nOperation\nData type Precision\nPROC\nprocedure call\nna\nna\nargument load\nreal\nAGRS\nsingle\nargument load\nAGRD\nreal\ndouble\nargument load\ninteger\nAGIS\nsingle\nARR 1\narray with 1 dimension\nna\nna\nARR2\narray with 2 dimensions\nna\nna\narray with 3 dimensions\nARR3\nna\nn3\narray with 4 dimensions\nna\nARR4\nna\narray with 2 5 dimensions\nna\nna\nARRN\naddition in array index\ninteger\nIADD\nsingle\nsimple goto\nGOT0\nna\nna\ncomputed goto\nna\nGCOM\nna\nLOIN\ndo loop initialization step 1\nna\nn3\nLOOV\ndo loop overhead step 1\nna\nna\nLOIX\ndo loop initialization step n\nna\nna\nLOOX\ndo loop overhead step n\nna\nna\nTable 4.6: Execution control and array access parameters.\n\n4.2.4. Additional P a r a m e t e r s\nIn addition t o the parameters presented in the last subsections, there are also other\nparameters that, although they cannot be associated to any particular statement or operation, have a significant execution time and should therefore be included in our model.\nThe first subset deals with the overhead associated with the access of a value stored in an\narray. If the variable referenced by t h e program is stored in an n-dimensional array and\nthe value of the indices that determine the particular element are not known at compile\ntime, the compiler must generate code t o compute the actual address at execution time.\nThere are three parameters that measure this overhead: ARR1, ARR2, ARR3. Each\nmeasures the additional time it takes to access a variable in an array of one, two and\nthree dimensions. The overhead for variables in four and five dimensions (ARR4, ARRN)\nis computed using a linear combination of the three basic parameters. We do not consider\na more detailed characterization of array references, because, in our benchmarks, they\nwere very few arrays with more than three dimensions and no examples of more than five.\nIntrinsic functions form the last subset of parameters. Although the number of times\nthese instructions are executed in a program is small, their execution time is normally\nvery large compared with t h a t of a single arithmetic operation. These parameters are\nshown in table 4.6 for single and double precision real arguments. The execution time of\nan intrinsic function is not always constant and normally depends on the magnitude of the\narguments. As an example, consider how the IBM 3090/200 computes the sine function\n[IBM87]. In the computation, the execution time of several steps depends not only on how\nlarge is the argument, but also on how small is its difference from the nearest multiple of\n\nThe System Characterizer\n\n20\n\n71. Depending on the magnitude of this difference a polynomial of degree one, three, five,\nor a table and additional arithmetic is needed to compute the result. The CRAY S - l I P\nlibrary reference manual [CRA84] contains a table with execution times for most of the\nintrinsic functions. In most cases the difference between the maximum and the minimum\ntime is less than 20%\'. However for programs with a large number of calls to these functions. a better characterization may be needed to obtain acceptable predictions.\n\nTable 4.6: Parameters for intrinsic functions\nOperation I Data type I Precision 1 Storage class\nhlnem on ic\nsingle\nreal\nlocal\nexponential I\nEXPS\nI\nsingle\nLOGS\nlocal\nlogarithm\nsingle\nloc a1\nsine\nSINS\nsingle\nlocal\ntangent\nTANS\nsingle\nsquare root\nSQRS\ndouble\nlocal\nEXPD\nexponential\nreal\nlogarithm\ndouble\nLOGD\nlocal\nreal\ndouble\nsine\nreal\nlocal\nSIND\ntangent\nj\nreal\ndouble\nlocal\nTAND\nsquare root I\ndouble\nreal\nlocal\nSQRD\n\n1 ;;;\n1\n\n~\n\nTable 4.6: Parameters for intrinsic functions.\n\n4.3. E x p e r i m e n t Design\nTiming a benchmark is very different from making a detailed measurement of the\nparameters in the system characterizer. For some benchmarks the system clock is enough\nfor timing purposes, and repetition of the measurements normally produces an insignificant variance in the results. On the other hand, the measurement of the parameters of a\nsystem characterizer using a high level program is not easy due to a number of factors:\n\n-\n\nThe intrusiveness of the measuring tools\n\n-\n\nVariations in the hit ratio of the memory cache\n\n-\n\nExternal events like interrupts, multiprogramming, and 1/0 activity\n\n-\n\nI\n\nThe short execution time of most operations (20 nsec\n\nThe need t o obtain repeatable results and accuracy\n\nThe resolution of the measuring tools\n\n-\n\n10 psec)\n\n(2 ps)\n1\n\nThe difficulty of isolating the parameters using a program written in FORTRAN\n\nThe parameters in the system characterizer are composed of single or a small\nnumber of machine instructions; for this reason, the events we want to characterize have a\nduration of ten t o thousands of nanoseconds. T o achieve a meaningful measurement of\nthese events using a high-level program and with the resolution of most system clocks\n8\n\nThe maximum difference reported is\n\n100% for the arc\n\ncosine.\n\nThe Systjem Characterizer\n\n2:\n\nrequires clever tests, especially when the characterizer is used in different machines, each\nwith different machine instruction sets and architectures.\n\nTo isolate an operation for measurement normally requires robust tests t o avoid\noptimizations7 from the compiler that would eliminate the operation from the test and\ndistort the results [CLA86]. Different, techniques must be used, in particular avoiding the\nuse of constants inside the test loops: using IF and GO TO instructions instead of the DO\nLOOP statement to control the execution of the test; initializing variables in external procedures to avoid constant folding. Separate compilation of variable initialization procedures, to make sure that the body of the test does not give enough information t o the\ncompiler to eliminate the operation being measured from inside the control test loop.\n4.4. Test S t r u c t u r e and Measurement\n\nThe events that we want to measure and characterize have very small execution\ntimes. For this reason it is not possible to make a direct measurement of a single execution in most systems. The clock resolution in many machines is bigger than the execution\ntime of a single operation. In machines with the UNIX operating system, the clock resolution is almost always 1/60\xe2\x80\x99th of a second. This value is several orders of magnitude\ngreater than the time it takes to execute almost any operation. In addition the overhead\nincurred by executing the clock routine affects our measurements. One way of reducing\nthese factors is t o repeat the test some number of times to obtain a measurement t h a t is\nmuch greater than the errors produced by the clock resolution and the overhead of the\ntiming routine combined. There are problems associated with this technique. In a machine\nwith cache memory the value obtained for the execution time of a single operation using\nrepetition is smaller than the execution time of a single operation when the arguments are\nnot previously in the cache. The results obtained in this way will indicate that the system\nis faster than in the case when the arguments are not in the cache. Nevertheless there are\nat least t w o arguments that support using repetition. The first one is that the very idea of\nusing cache memories in computer systems is because programs tend to satisfy the principle of locality. The second reason is that we expect that the error incurred by using\nrepetition will be small compared with the experimental error, especially if we take into\naccount that the cache hit ratio of typical applications is high.\nFigure 4.4 shows the structure of the tests in our system characterizer. We can identify five parts in each experiment. The initialization, in which the number of iterations\nof the body of the test is computed. For each test we have to make sure that it will execute for a minimum amount of time in fast machines, but not for too long in slow systems. T o control this we have three parameters, the SPEEDUP factor ( > 0), that gives a\ncrude approximation of the relative performance of the system compared t o the CRAY\nX-MP/48, the number of iterations (LIMITO) it takes the CRAY X-MP/48 t o execute the\ntest for one second, and the duration of the test (TMAX). This last parameter will permit\nus to control the execution as a function of the resolution of the clock and the variance of\nthe measurements. The t e s t is the code included in the two lines with ellipsis, and is the\ninstruction or group of instructions t h a t we want t o characterize. The additional code\nEven when we compile without optimization, compilers try to apply some standard optimizing\ntechniques, like constant folding, short-circuiting of logical expressions, and computing the address of\nan element in a n array.\n\n22\n\nThe System Characterizer\n\n1\n2\n\nLIMIT = L I M I T 0 * SPEEDUP * TMAX\nDO 4 K = 1 , REPEAT\nCOUBTER = 1\nTIME0 = SECOlYD 0\nI F (COUBTER . G T . LIMIT) GO TO 3\n\n...\nbody of the test\n\n...\n3\n\n4\n\nCOUNTER = COUBTER + 1\nGO TO 2\nTIME1 = SECOlYD 0\nI F (TIME1 - TIMEO . G T . TMAX) GO TO 4\nL I Y I T = T U X * LIMIT / (TIME1 - TIMEO)\nGO TO 1\nSAMPLE(K) = TIME1 - TIMEO\nCALL STAT (REPEAT, SAMPLE, AVE, VAN\n\nFigure 4.4: The basic struct.ure of an experiment.\n\ndelimited by the two invocations to the SECOND function (timing function) is what we\ncalled an observation. Here we control the number of times the body of the test is executed, so we can obtain a meaningful observation. The additional code delimited by the\nDO loop represents the ezperiment. This consists of a set of observations (controlled by\nthe variable REPEAT). The last part is the computation of the measurement. In this\npart we compute the mean value of our observations and the variance. T o control the\nerror in our measurements w e have t w o possibilities; one is to run each test for a significant amount of time; the other is t o increase the number of observations inside the experiment. In the first case we increase the number of iterations that the body of the test is\nexecuted (increasing the value of LIhlITO and LIMIT in figure 4.4). In the second case we\nexecute the body of the test the same number of times, but increase the size of the sample\nstatistic (increasing the value of variable REPEAT). In section 4.5.1 we discuss the effect\nof each one of these possibilities.\n\n4.4.1. Direct Tests, Composite Tests and Indirect Tests\n\nTo understand the possible sources of experimental error, and how t o compute them,\nwe need the concepts of a \xe2\x80\x98direct test\xe2\x80\x99, \xe2\x80\x98composite test\xe2\x80\x99 and \xe2\x80\x98indirect test\xe2\x80\x99. As we explained\nin the last paragraph, inside of the \xe2\x80\x98if-loop\xe2\x80\x99 construct we have the test. Now in a direct\ntest the body of the test consists of N occurrences of the operation we want t o characterize and nothing more. In a composite test in addition to the N operations there are\nseveral other operations of different type inside the body. This is necessary because in\nmost of the cases it is not possible t o make a direct measurement of the parameters, and\nwe have t o include some additional operations. In an indirect test the execution time for\nthe operation we are measuring ( P i ) is obtained by running two different tests. Some\nparameters of the system characterizer are coupled; it is not possible to execute one\nwithout executing the other, and therefore the way t o isolate one of the parameters is t o\n\nThe System Characterizer\n\n23\n\nrun two tests with different number of operations for each of the parameters. The body\nof the second test is the same as the body of the first test, plus some additional work. The\ndifference in the execution time between the two tests gives us the value of one of the coupled parameters. An example of this is the DO loop initialization and overhead. Every\ntime we have a DO loop in a FORTRAN program, the compiler generates code that\nincludes the initialization of the loop and also the overhead to control the iteration. By\nchanging the number of times the loop is executed, in two tests, we can obtain a pair of\nlinear equations to compute the values of the initialization and the overhead. In the next\nsubsection we will see that the variance of our measurements depends on whether the\nobserntions are done using direct, composite, or indirect tests.\n\n4.5. Experimental Errors a n d Confidence Intervals\nAs we pointed out in the last section, one of the important parts of the characterizer\nis to control the accuracy and exactitude of our measurements. In order to make an\nevaluation of the quality of our measurements, we need to quantify the sources of error in\nour experiments. Currah gives a long list of the causes in the variability in CPU time as\nmeasured by the system clock [CURiS, MER831. Some of these factors are: (a) Timer\nresolution of CPU clock. (b) Improper allocation of CPU time for 1 / 0 interrupt handling.\n(c) Changes in cache hits due t o interference with concurrent tasks. (d) Cycle stealing\nwhile another component is sharing a resource with the CPU. (e) Number of context\nswitches: the time spend by the dispatcher and timer routine before dequeuing or after\nenqueing a process. Some of these events have a length of time far greater than the\nphenomena that we are measuring.\nWe also have t o subtract the execution time of the code t h a t controls our test and\nthe overhead incurred by the timer routine. These measurements have their own variance\nand the subtraction of these overheads increases the variance of our measurements. All\nthe factors combined can be significant compared to the magnitude of our results. IVe\nwill now proceed t o quantify the sources of variability and obtain expressions for the variance for the different types of experiments.\nWe denote the factors affecting our measurements as follows:\nTable 4.7: Definitions of terms used in the time analysis\nTio\n::=\nCPU time before the observation (TIMEO)\n::=\nCPU time after the observation (TIME1)\nTil\n..Cowrhead ..overhead involved in the timing function\n\nIFowrhead.._\n..Ni i\nlmt\n\nNrept\n\nqj\n0\n\n...._\n...._\n....-\n\n.._\n..-\n\nB\nPi\n\n.._\n..-\n\nU2\n\n::=\n\n.._\n..-\n\noverhead involved in the if-loop control\nnumber of times the body is executed (LIMIT)\nnumber of observations in the experiment (REPEAT)\nobservation j\nsample mean of each observation (measurement)\nsample mean execution time of the test\nsample mean of parameter i\nvariance operator\n\nWe know that each observation Oi is equal t o\nOj = Tj, - Ti,\n\n(4.1 1\n\n21\n\nThe S y t e m Charact.erizer\nthen rhe mean value\n\n( 6 )of\n\nthese observations is\n(4.2)\n\nand it5 variance\n(4.3)\n\nSow the mean value of each experiment is equal to the time it takes to execute the\nbody of the test Nlimittimes, plus the overhead of the timing function\n\n6\n\n=\n\nNi i\nlmt\n\n(B + ~ ~ o u r r w ) cotlethead\n+\n\n(4.4)\n\nwhere B is the mean time it takes t o execute once the body of the test. iVe can compute\nthis d u e and the variance with the equations\n\nand\n\nTo obtain the mean value of parameter pi we need t o know if the test is direct, composite or indirect. Let N be the number of times parameter pi is executed inside the body\nof the test, then the mean value and variance of parameter pi in a direct test are\n\n-\n\npi =\n\nB\n-,\n\xe2\x80\x9d\n\nff2B\n$pi = N2\n\n(4.7)\n\nIn a composite and indirect test we have\n\nwhere \\Vetm is the additional work inside the body of the test or in the second test.\nLooking a t the above equations we can see that there are four factors affecting the\nmagnitude of the variance in a direct test and five for composite and indirect tests. These\nfactors are: the resolution of the timing function; the variance of our observations; the\nvariance of the execution time of the timing function; the variance of the IF control statements: and the variance of the additional work executed inside the body of the test or by\nthe second test. If the execution time of each observation is such that we have\n\nthen the only factors that affect our measurements are the dispersion of our observations,\n\nThe System Characterizer\n\n25\n\naffected by concurrent activity on the system, and the variance in the execution time of\nthe extra work present in the composite and indirect tests.\nTable 4.8 gives the experimental values for Creeolution,\nCoverhead,\nZFover,read, the\nand\nminimum duration of one observation (Tmh)such that the magnitude of the right hand\nside of equation 4.9 is less than five percent the magnitude of O j in a direct test.\n\n!\nr\n\nTable 4.8: Sources of ExDerimental Error\nSystem\nCre8olution\n\nIzZT\n\nCRAY S-\\lP/-l8\nCYBER 205\nArndahl 5840\nIBM 3090/200\nConvex C-1\nVAX 8600\nVAX-111785\nVAX-11/780\nSun 3/50\nIBM RT-PC/125\n\n1.0 ps\n1.0 ps\n1.0 ps\n10.0 rns\n10.0 rns\n16.F rns\n16.F rns\n16.6 rns\n20.0 rns\n16.F rns\n\n2.3 ps\n2.6 1 s\n475. ps\n376. ps\n276. ps\n175. ps\n585. ps\n825. ps\n713. ps\n507. ps\n\n0.47 ps\n0.64 ps\n0.95 ps\n0.27 ps\n1.04 ps\n1.31 ps\n4.42 ps\n5.86 ps\n1.49 ps\n2.79 ps\n\n112 ps\n124 p s\n10 rns\n200 rns\n205 rns\n338 rns\n346 rns\n351 rns\n414 rns\n344 rns\n\nT gives the minimum time that a test must be run\nto reduce the error due to the resolution and cd1 overhead of the clock, and overhead of the\n\nTable 4.81 Sources of experimental error.\n\ntest to less t.han 5 percent in a direct test.\n\n4.5.1. R e d u c i n g the Variance\nWe have two ways of reducing the variance of our results and therefore the size of\nthe confidence intervals. The first is by increasing the length of the test by augmenting\nthe value of Nlimit. But the problem is that by doing this the probability of a context\nswitch increases and also the possibility of a cache flush that will be reflected in higher\ncache misses. The second possibility is to increase tlhe number of observations in each\nexperiment ( N r e p t ) . Because each of our observations is an independent and identically\ndistributed random variable we can apply classic statistics and therefore the confidence\nintervals for our measurements will be reduced by the square root of the number of observations made in each experiment. On the other hand by increasing the number of observations, the probability that an event in the system occurs increases (e.g. swapping, the\nupdate of the superblock in UNIX every 30 seconds, etc) and this will increase the variance.\n\nIn some measurements using indirect tests, the variance obtained can be significant\ncompared t o the actual measure. We can see this by considering the following case. T o\nmeasure the overhead and initialization of the DO loop statement we run three experiments. In the first case the test consists of a DO loop with some extra statements inside\nthe loop (to prevent elimination by the compiler) that is executed N times. In the second\ntest the loop is executed 2 N times. For the third test the loop executes N times, but the\nextra work inside the loop is twice as much as in the other tests. We can express the\nabove conditions in terms of the mean execution time of the body of each test (Eli).\n\n26\n\nThe System Characterizer\n\nit is easy to see that we can obtain values for DOinitialiration DOoverhead\nand\nand their variance in terms of the Bis.\n\nand\n(4.12)\n\nIn table 4.9 w e give results obtained on a VtuC-11/785 for a sample statistic of size\nfive and each of length of one second. We can see that even when the sample standard\ndeviation is small ( < 55)for the \xe2\x82\xaclis, in the case of the DO loop parameters the standard\ndeviation is very large.\n\nParameter\nBI\n8 2\nB3\n\nDoinitialization\n\nDO,,TM\n\nMean ( p )\n69.9 p s\n127.3 ps\n107.4 ps\n12.5 P S\n1.99 ps\n\nStd. Dev. (u) u / p (%)\n1.49 ps\n2.13\n4.51\n5.74 ps\n4.22\n4.53 ps\n72.9\n9.11 ps\n36.8\n0.73 ps\n\nTable 4.9, hlean and standard deviation. Relative magnitude of the standard deviation compared to the sample mean for\n\nl)o,nt,d,d,m\n\nand D O d . Each test consists of 5 observations\n\nexecuted for 1 second on a VAY- 11/785.\n\nI t is therefore important to know what are the values for Nlimit and h\',,, that will\ngive a small standard deviation in our measurements. These values are system dependent\nand are affected by the resolution of the clock, the concurrent activity on the system, etc.\nIn figure 4.5 we show the normalized confidence interval of ten parameters for values of\nNIiAt such that the each test is run for at least 0.1, 0.2, 0.5, 1.0, 2.0 and 4.0 seconds on a\nVax-11/780. We also obtain measurements for N ,\nE\nequal to 5, 10 and 20 observations.\nT h e confidence intervals for Pi are obtained using the Student\'s t distribution and the\nstandard error of pi as follows\n(4.13)\nL\n\nand the normalized confidence intervals are\n\n(4.14)\n\nThe System Characterizer\n\n27\n\nWe can see t h a t for a fixed value of N\n,\nthe confidence interval of our measurements decreases as the time of the test increases. but for small values of Nnept, there is a\nlimit to how much we can decrease the confidence interval by increasing the time of the\ntest (,Yliht).The reason for t h i s is that by increasing the length of the test we reduce the\nvariability due to short term variations in the concurrent activity of the system. However\nthe probability of a change in the overall concurrent activity of the system increases with\na larger test. This change may produce a greater variance if the size of the sample statistic is small. \\Ve see that the best results are obtained for 20 observations and 1 to 2\nseconds for duration of the test. In machines w i t h good clock resolution acceptable results\nare obtained for 10 observations and .2 seconds for each test.\n\nIn our system characterizer each test executes for a t least 2 seconds on a CRAY SMP. X potential problem with this is that a test that runs for 2 seconds on a CRAY SMP usually takes much longer on most systems. A system characterizer constructed in\nthis way will have a n excessive execution time, and therefore will be unsuitable for benchmarking. To avoid this problem we calibrated each test t o execute for 2.2 seconds in the\nCRAY S-;LIP/18 and adjusted each particular test according to a \xe2\x80\x98speed-up\xe2\x80\x99 factor that\napproximates the ratio of performance between the CRAY X-MP/18 and the system we\nare characterizing. Even with this approximation the execution time of each test will not\nbe equal to 2.2. 11 the actual running time is greater than 2.2 seconds we keep the measurement, because this value reduces the experimental error even more. In the other case, if\nthe time is less than 2 seconds, t,he system characterizer computes a new approximation\nand runs the test again. The gap between 2 and 2.2 reduces the possibility of unnecessarily repeating the test. At the beginning of the execution of the system characterizer,\nthe system runs four tests that measure the clock resolution, the clock routine overhead,\nthe test control overhead, and the speed-up factor. With these four quantities the system\ncharacterizer computes the execution times needed in each test t o run for 2 seconds.\n4.6.\n\nIs t h e M i n i m u m Better t h a n t h e Average?\n\nIn the previaus section we mentioned that to obtain the expected execution time per\nparameter we have t o compute the average of a number of observations. The .noise\xe2\x80\x99 in\nour measurements is the result of concurrent activity in the system and the resolution of\nour measuring tools. In most systems an increase in the execution load produces an\nincrease in the real and CPU time of programs. The time we measure for the execution of\na basic operation is always greater or equal to the \xe2\x80\x98real\xe2\x80\x99 execution time when there is no\nother activity. Therefore it should be better to take the minimum instead of the average,\ngiven that the minimum is always less or equal than the average. By doing this we reduce\nthe discrepancy between our measurement and the \xe2\x80\x98real\xe2\x80\x99 execution time.\nThe main objective of this research is to characterize the actual performance of systems, and when these systems are used in everyday situations there is always some degree\nof concurrency present. We expect that if we filter the extra time due t o this concurrency\nour predictions will tend to be less than the actual running time of programs. However\nthe only way that we can be sure that this is the case is t o characterize some systems\nusing each of these techniques and see which of them produces better estimates.\nWe ran the system characterizer twice in the Convex C-1 taking first the average\nand then the minimum, and used these results to estimate the execution time of a workload composed of ten program. The difference in the value of the parameters w a s\n\nc\n\nThe System Characterizer\n\nI\n\n.\n\n-\n\n.- - r q F\n\n1\';\n\n28\n\n&Jnssn\n\nGC,\n\n90 percent confidence intends Irwrmdued)\n5 mgsuments UI och test\n\n--kl\n-31\n\n- -\n\nI\n\nI I \'CEa.\n\nI\n\nI\n\n0 1 s ~ 02~\n\n06-\n\nI\n\nAR%\n\n10-\n\nI\n\n\'20-\n\n40-\n\n90 pemm confidence intends (normdid)\n30 mgsuments in mh t e ~ f\n\n,\n\n1\nI\n\n!\n\n0.1 sec\n\n0.2 sa\n\n0.6 sa\n\n1.0sec\n\n2.0 sa\n\na confidenac intend of length t.ro\n\n(om unir och ride)\nthe\nW i t u d e of the n - t w m m e m\n\nI\n\nj\n\n(4\n\nF b r e 4.6: Confidence intervals for ten different parameters. In (a), (b), and (c) we show how\nthe length of the test and the number of observations affect the confidence interval of the\nmeasurements. For a fixed number of observations an increase in the execution time of the\ntest tends to reduce the length of the confidence interval. Figure (d) shows all the confidence\nintervals for three of the ten parameters. All confidence intervals are normalized with\nrespect to parameter Pi.\n\n29\n\nThe System Characterizer\n\nbetween 2-15%. Taking the average of the measurements produced the smallest error in\nthe total execution time of the workload as we can see in table 4.10.\nTable 4.10: Estimates taking the average and the minimum\nMachine 1 Real Time /I Average I Error 11 Minimum I Error\nConvex C:l543 sec 11 551 sec 1 1.47 % 11 499 sec\n8.10 %\n\nI\n\nI\n\nTable 4.101 Estimates taking the average and the minimum. The execution time of a workload\nof ten programs\n\nu\xe2\x80\x99x\n\npredicted with the set, of parameters obtained using the average and\n\nthe minimum of the measurements.\n\n4.7. R e s u l t s Obtained with the S y s t e m Characterizer\nWe executed the system characterizer in the ten machines shown in table 4.1, and in\nfigures 4.6-4.9 we present the experimental values that we obtained. In the appendix (section 9 ) we show the results in tabular form. Table 4.11 explains the meaning of each of\nthe twelve regions in which the set of parameters have been grouped. The number of\neach region is printed at the top of the first graph in each of the three figures. Each point\nin the graphs represents the execution time for a single operation in nanoseconds. Some\nparameters have value zero. as for example, the execution time for the G O T 0 operation\nin the Convex C-1, or the addition of a constant t o an index, if the index is a component\nof an array. This happens when the execution time of a parameter is small and its execution overlaps with other operations; the total execution time does not depend in the\noccurrence of the parameter.\n\nIn figures 4.10-4.11 we show the results for all the machines normalized with respect\nt o the VA4X-11/780. There are several interesting patterns in these figures that give information about, the characteristics of the machines. First, we can see that the execution\ntimes for operations accessing local variables are similar to the times obtained for global\nvariables in all the cases except for the CYBER 205. In this machine the operations using\nglobal variables take longer time to execute, and can take as much as ten times longer as\nin the case of the integer zdd operation and the AND operation. IVe corroborate this\nobservation by looking at the same parameters compared with the ones obtained for the\nCRAY X-hlP. We can also see that to execute floating point operations with single precision arguments, the CRAY X-MP has better times than the CYBER 205, and the IBM\n3090. But if we look at the same operations with double precision operands, we find t h a t\nthe times for the CRAY X-MP are greater than the ones obtained for the CYBER 205,\nIBM 3090, and Amdahl 5840. Moreover, in the case of addition, multiplication and division, the Convex C-1 and the VAX 8600 have smaller times than the CRAY X-MP. We\ncan see from the graphs and tables that the high performance of the CRAY X-MP when\nrunning in scalar mode and without optimizations lies in the fast execution of the arithmetic floating point operations with single precision.\nIt is important to point out that double precision on the Convex, the various VAX\nmachines and the Sun is 64 bits against 128 bits on the other machines. The purpose of\nthis research is t o present a new methodology of performance characterization and not t o\ncompare different machines. A serious evaluation of their performance must address the\nproblem of data type representation carefully in order t o make a fair comparison.\n\n30\n\nTable 4.11: P a r a m e t e r reaions in fiaures 4 . 8 4 . 1 1\nRegion\n1\n\nSet of Parameters\nreal operations (single), local operands\n\n2\n\n01 SRSL\nstore\n02 ARSL\naddition\n03 MRSL\nmultiplication\n04 DRSL\ndivision\nexp ( S ** I)\n05 ERSL\n06 XRSL\nexp (X ** j \xe2\x80\x99 )\n07 TRSL\nmemory transfer\nreal operations (double), local operands\n\n3\n\nstore\naddition\n10 MRDL\nmultiplication\n11 DRDL\ndivision\n12 ERDL\nexp (X ** I)\n13 XRDL\nexp (X ** Y)\n14 TRDL\nmemory transfer\ninteger operations, local operands\n\n8\n\nstore\n15 SlSL\naddition\n18 AlSL\nmultiplication\n17 MlSL\ndivision\nIS D E L\nexp (I ** 2)\n10 E E L\nexp (I ** J )\n20 XISL\nmemory transfer\n21 TISL\nlogical operations with local operands\n43ANDL I A N D k O R\n44 CRSL\ncompare, real, single\n45 CRDL\ncompare. real, double\n46 ClSL\ncompare, integer, single\nfunction call and arguments\n\nRegion\n4\n\n23 ARSG\n24 MRSG\n25 DRSG\n\n10a\n\nI\n\nprocedure call\nargument load, real, single\nargument load, real, double\nargument load, integer, single\nbranching paramet.ers\n\naddition\nmultiplication\ndivision\n\n20 SRDG\n30 ARDG\n31 MRDG\n32 DRDG\n33 ERDG\n34 XRDG\n35 TRDG\n\nstore\naddition\nmultiplication\ndivision\ne.xp (X ** I)\nexp (X ** Y)\nmemory transfer\n\n5\n\n08 SRDL\n00 ARDL\n\n51 PROC\n52 AGRS\n53 AGRD\n54 AGIS\n\nSet of Parameters\nreai operations (single), global operands\n\n6\n\n7b\n\nl3\n\n10b\n\n36 SISG\nstore\n37 AISG\naddition\nmultiplication\n38 MlSG\n30 DlSG\ndivision\n40 EISG\nexp (1 t i 2)\n41 XISG\nexp (1 ** J)\n42 TISG\nmemory transfer\nlogical operations with local operands\n\nReferences to array elements\n55 ARRl\narray 1 dimension\n58 ARR2\narray 2 dimensions\n57 ARR3\narray 3 dimensions\n5 8 IADD\narray index addition\nDO loop parameters\n\ni\n\nI\nintrinsic functions (single precision)\n\nTable 4.11: The\n\nintrinsic functions (double precision)\n\nregions in the g r a p h s represent different aspects of t h e characterization of t h e\n\nmachines. Parameters ARR4 and\n\nARRN are not\n\ndirectly by the system characterizer.\n\nincluded, because these are not measured\n\n~~\n\n~~\n\n31\n\nThe System Characterizer\n\nORlGDNdh li2Gf-i ;:c\nOF POOR QUALITY\nCray X - W / 4 8\nlooamo.\n\n-\n\n!\n\nI\nI\n\nI\n\nI\n\n,\nI\n\nI\n\nI\n\n3\n\n4\n\nI\n\nI\n\n0\n\nI\n\n5\n\n!\n\nI\n\nP\n\n10\n\n6\n\n30\n\n7\n\n1\n\n1\n\n1\n\nI\n\nI\n\n1\n\n1 8 1 9 110\n\nI 1ll\n\nI\n\nl\n\nl\n\nI\n\nl\n\nl\n\nI\n\nI\n\n2\n\xe2\x80\x98\n\nI\n\n-\n\nI\n\nlmooa,\n\n1\n\nl\n\nl\n\nI\n\nm\n\n4\n)\n\n12\n\n1\n\n70\n\ndo\n\n8)\n\npsmrca number\n\nCyber 205 (4 pipes)\nI\n\nI\n\nI\n\nI\n\nl\n\nl\n\nI\n\nl\n\nI\n\nI\n\nI\n\nl\n\nl\n\n1\n\nI\n\nl\n\nl\nl\n\nI\n\n1\n\nlmm0\n\n10WO\nloa,\n\n100\n1\n0\n1\nn.\n\nV. 1\n\n10\n\n20\n\n30\n\n*)\n\nm\n\n70\n\n60\nm\n\ne\n\n8)\n\nr number\n\nIBM 3090/200\n\nlnaa,\n\nn\n\na\nn\n0\n\ne\nC.\n\nlooo0\n1mO\n10\n0\n\n10\n1\n\n0.1\n0\n\n10\n\n20\n\n30\n\n4\n)\n\nm\n\ndo\n\n70\n\n8)\n\npsmrca number\n\nFigure 4.6: Characterization results (Cray, Cyber, and IBM 3090). The graphs show the value of\neach parameter in nanoseconds. The twelve regions represent different aspects of the characterization.\n\nu8:GiNAL PAGE I\nS\n\nThe\n\nOF POOR QUALITY\n\nSj-stem Characterizer\n\nArndahl 5840\n2\n\nI\n\nI\n\n3\n\n,\n\n5\n\n4\n\n1\n\n,\n\n-\n\n1\n\n6\n\nI\n\nI\n\nI\n\n7\n\n1\n\nlCOCC0\nlm00\n\nI\n\nl\n\nI\n\n,\n\nI\n\nI\n\n1\n\n- i\n\n-i\n\n1\n\nI\n\nI\n\n1 -\n\nI\n\nj_\n\nl\n\nI\n\nI\n\n1\n\n1\n\nI\n\n1\n\nl\n\nl\n\nI\n\nI\n\nl\n\nl\n\nI\n\nI\n\n1 8 1 9I\n\n10 I 1 1 1\n\n12\n\nn\na\n\nn\n\nlax,\n100\n\n0\n\nI\n\n10\n\ne\nC.\n\n1\n0.1\n\n0\n\n10\n\n30\n\n3\n)\n\ndo\n\n60\n\n40\n\n70\n\n80\n\nparamaw number\n\nConvex C-1\nI\n\n0\n\n10\n\n20\n\nI\n\n30\n\nI\n\nl\n\nI\n\n!\n\nl\nl\n\nl\nl\n\n\'\n\nI\n\nI\n\n1\n\nI\n\nI\n\nl\n\nI\nI\n\nI\n\nI\n\nm\n\n40\n\n70\n\n60\n\nI\n\n80\n\npanmeter number\n\nVAX 8600\nloam00\n\n!\ni\n\nlmm0\n\nI\nl\n\nI\n/\n\nl\n/\n\nl\nI\n\nI\n/\n\nI\n\nl\n\nl\n\nI\n\nI\nI\n\nI\nI\n\nI\n\nI\n\nn\na\n\nI\nl\nI\nI\n\nloOam\n\nI\n\nlax,\n\nn\n0\n\nI00\nI\n\ne\nC.\n\nI\n\nI\n\nI\n\n10\n\n!\n\nI\n\nS\n\nI\n\nF\n\n1 .\n\n0.1\n\n0\n\n10\n\n20\n\n30\n\n43\n\nI\n\nI\nbo\n\nl\n\nl\n\nI\n\ndo\n\nI\n70\n\nF\n\nm\n\npsnmeca number\n\nF W e 4.7: Characterization results (Amdahl, Convex, and Vax 8600). The graphs show the\nvalue of each parameter in nanoseconds. The twelve regions represent different aspects of\nthe characterization.\n\n33\n\nVax-11/785\nlmaxXa\n\nI -\n\ni\n\n-\n\n1\n\n1\n\n1\n\nI\nI\nI\nI\n\nl\nl\nl\nl\n\ni\n\n-\n\n1\n\nr\n\n1000000\n\nloUm\nn\n\nloo00\n\na\nn\nO\n\n1mO\n\nI\n\n1\nm\n\ns\ne\n\n10\n\nC.\n\nI\n\nI\n\nl\nl\nl\nl\n\nI\n\nI\n\nI\n\nI\n\nI\nI\n\nI\nI\n\n1\n0.1\n\n0\n\n10\n\nzl\n\n60\n\n30\n\n70\n\n60\n\nI\n\nl\n\nl\n\nI\n\nI\n\nI\nI\n\nl\nl\n\nl\nl\n\nI\n\nI\n\nI\n\nI\n\nlooou)\n\nn\n\nloo00\n\na\nn\n0\n\nloo0\n\nloo\n\nI\n\ne\nC.\n\n10\n\n1\n0.1\n0\n\n10\n\n30\n\na0\n\n50\n\nu)\n\n70\n\n60\n\n80\n\npanmeter number\n\nSun 3/50\n\nn\n\na\nn\n\nI\n\n0\n\nI\n\nI\n\nl\n\nl\n\nl\n\nl\n\nI\n\nI\n\nI\n\nl\n\nl\n\nl\n\nl\n\ne\n\nI\n\nl\n\nl\n\nI\n\nI\n\nC.\n\nI\n\nl\n\nl\n\nI\n\nI\n\nI\n\nl\n\nl\n\nI\n\nI\n\nI\n\nI\n\nI\n\nI\n\npanmet- n u m b\n\nFlgure 4.8: Characterization results (Vax 785, Vax 780, and Sun 3/50). T h e graphs show the\nvalue of each parameter in nanoseconds. T h e twelve regions represent different aspects of\nthe characterization.\n\n34\n\nThe System Characterizer\n\nparsnmter number\n\nFigure 4.0: Characterizstion results (IBM RT-PC/125). The graph show the value of each\nparameter in nanoseconds. The twelve regions represent different aspects of the characterization.\n\n35\n\n1\n\n10.0\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1 I I\n\n2\n\n1\n\n-\n\nI\n\nI\n\nI\n\nI\n\nI\n\nI\n\nI\n\nI\n\n3\n\nI\n\n1 1 . 1 I\n\nI\n\nI\n\nI\n\nI\n\nr\n\nt\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n4\n\n1 .o\n\n.10\n\n.01\n\n.oo 1\n\nl\n\n1\n\nl\n\nl\n\n3\n\nl\n\n5\n\nl\n\nl\n\n7\n\n~\n\n9\n\n11 13 15 17 19 21 23 25 27 29 31 33 35 37\nparameter number\n\nFigure 4.10: Characterization results for all machines normalized to the VAX-11/780(I). Re\ngions 1-6 represent different aspects of the characterization (see table 4.11).\n\nORIGINAL PAGE I\ns\nOF POOR QUALITY\n\nThe System Characterizer\n\n10.0\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n1 .o\n\n.10\n\n6\n.01\n\n.001\n\n38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74\nparameter number\n\nFigure 4.11: Characterization results for all machines normalized to the VAX-11/780(II). Regions 6-12 represent different aspects of the characterization (see table 4.11).\n\n5\nThe Program Analyzer\nThe system characterizer allows us t o represent the performance of individual operations of different architectures using a single unified model. On the other hand, the program analyzer decomposes applications in terms of the same group of parameters. The\nprogram analyzer is a tool for measuring static and dynamic properties of programs.\nThese properties determine how the application will be executed by the system me are\nevaluating. The parameters chosen for this decomposition are exactly the set of operations supported by the programming language. It is for this reason that t o implement a\nprogram analyzer we only need to modify the compiler t o obtain the static properties of\nthe application. Also we need to instrument the source code or the object code t o produce\ndynamic Statistics at run time. Using the static and dynamic statistics it is possible t o\nobtain the dynamic behavior of the application.\n\n5.1. Execution Profilers\nMost of the computer systems currently in use have utilities to produce execution\nprofiles using additional information generated by the compiler [POW83]. As an example,\nin UNIX 4.3BSD, the C, Pascal, and FORTRAN compilers have two options t o obtain\nreports about the program\xe2\x80\x99s execution profile. The first option (-p switch), instruments\nthe object code t o record information about the number of times each function is executed\nand the amount of CPU each function consumes. When the program finishes execution, it\nproduces a file called mon.out that contains the values of the counters and timers. A utility program called \xe2\x80\x98prof\xe2\x80\x99 takes this information and using the table of symbols located at\nthe end of the object file, produces a detailed report about how many times each function\nwas executed and the amount of time it spent in each function. A more useful tool is\ngprof [GRA82]. The profile information stored in the monitor file (gmon.out) also contains the call graph of the execution and the report generated Ly gprof gives specific information of who invoked each particular function and how many times.\nThe SUN workstation also provides information about the number of times each line\nin the source code executes. The utility program \xe2\x80\x98tcov\xe2\x80\x99 prints the original source program\nalong with the number of times each lines was executed. As in the case of \xe2\x80\x98prof\xe2\x80\x99 and\n\xe2\x80\x98gprof\xe2\x80\x99 the compiler instruments the object code by including counters for each basic block\nof the source code.\nT h e information that our program analyzer produces is similar t o the one produce by\n\xe2\x80\x98tcov\xe2\x80\x99. However, in addition to the number of times each basic block executes, we need t o\ncount for each line, or more specifically for each statement, how many times each operation appears in the statement. As we saw in the last section, it is not possible t o access at\nthe high level of a programming language the primitive operations like load or store. I t is\nfor this reason that we need t o distinguish for each accessible operation at the high level\nthe type of the operands, their storage class, and the number of bits used in their\nrepresentation. Because this is also the kind of information that the compiler needs in\norder t o produce correct object code, it is the compiler the best place to obtain this information.\n\n37\n\n38\n\nThe Program Analyzer\n\nFigure 5.1 shows how static and dynamic statistics are measured by the program\nanalyzer. As we can see, this process is very similar compared t o how execution analyzers\nwork. In our system we are using the front end of a FORTR-AN compiler t o instrument\nand collect static statistics for each block. and t o instrument the source code to produce\nthe dynamic statistics as well. The statistics of an application produced by the program\nanalyzer depend only on the application itself and not on the computer systems in which\nit runs. The significance of this is that if we have M applications that we are going t o use\nt o evaluate L\'V computer systems, we only need one description for each program and one\nfor each system ( N + M ) . To make a performance evaluation using normal benchmarks, we\nneed t o make 1V.M runs.\n\nCode )\n,\n\nAnalyzer\n\nProgram\n\nExecution\n\nI\n\nI\n\nI\n\n-4.\n\n,A\n\nFigure 6.1: Static and dynamic analysis of programs.\n\n5.2. S t a t i c and Dynamic Statistics\nWe can see how the program analyzer works using as an example a particular statement in one of the kernels of the Livermore Loops [MCM86]. These collection of loops\nrepresent the type of computational kernels found in the codes normally executed a t\nLawrence Livermore National Laboratory. These loops contains mathematical operations,\nsuch as inner product and matrix multiplication, and more complicated algorithms like\nthe Monte Carlo Search Loop and 2-D Particle in a Cell Loop. John Feo (FE0871 has\ninvestigated the computational and parallel complexity of the loops, in an attempt t o use\nthe loops t o evaluate the performance of MIMD machines. In the next few lines we can\n\nThe Program Analyzer\n\n39\n\nsee a single stat,ement taken from the eighth kernel of the Livermore Loops:\nU1 (KX,KY,IiL2) = U1 (KX,KY,IiLl) All * DU1 (KY) +\nA12 * DU2 (KX) + A13 * DU3 (KY) +\nSIC * (U1 (KX+l,KY,BLl) - 2. * u1 (KX,KY,HLl)\nu1 (KX-l,KY,HLl))\n+\n\n1\n2\n3\n\nparam.\nSRSL\nARSL\nMRSL\nARR 1\n.4RR3\nI.4D D\n\noperat ion\n\nstore\naddition\nmultiplication\narray with 1 dim.\narray with 3 dims.\nindex addition\n\nI\n\ntype\n\nI\n\nreal\nreal\nreal\nna\nna\n\ninteger\n\nprecision\nsingle\nsingle\nsingle\nna\nna\nsingle\n\nI\n\nclass\nlocal\nlocal\n\nI\n\nstatic\n\n1\n6\nlocal\n5\nna\n3\n5\nna\n2\nlocal -\n\nI\n\n+\n\ndynamic\n62280\n373680\n31 1400\n186840\n311400\n124560\n\nTable 61 Static and dynamic statistics of kernel (Livermore Loops).\n.:\n\nThe program analyzer decomposes this statement not only in the number and type of\noperations involved, but also makes the distinction that the integer addition ( 2 operations)\nexecutes in the context of an index array. Normally operations between indexes are handled different from other arithmetic operations. The program analyzer keeps separate\ncounters t o distinguish between conventional arithmetic operations in expressions and\narithmetic operations using indexes.\n\n5.2.1. Description of the Test P r o g r a m s\nWe ran the program analyzer for the ten programs in table 5.2. There are three\ngroup of programs: small integer oriented tests like the Baskett puzzle, Shell, and Erathostenes. There are some floating point computational intensive programs like Los Alamos\nbenchmark. The Livermore Loops, the NAS benchmark, Whetstone, the Mandelbrot set,\nand the Linpack benchmark. The last group is represented by the Smith benchmark that\ncontains intensive integer, floating point and logical computations. The execution time for\nthe programs varies from .I of a second to approximately 600 seconds on a CRAY SMP/48.\n0\n\nLos Alamos: this is one of the benchmarks used by LANL Computing and Communication Division [BRI86, BUC85, GRI84, SIM871 t o evaluate the performance of\nsupercomputers. This code is known as BMK8A1 an consists of a series of simple\nvector calculations (run in scalar mode in this study) t o test the rates of vector\noperations as a function of vector length. The vectors are stored in contiguous\nmemory locations. Typically one million floating-point operations are timed.\n\n0\n\nConway-Baskett puzzle: This benchmark is a program developed by Forest\nBaskett [BEE841 and normally used to evaluate the performance of microcomputers\nand RISC-based machines [PAT82]. The program is a depth-first, recursive, backtracking tree search algorithm to find a solution t o a particular puzzle invented by\nJohn Conway. The puzzle consists in placing 18 tri-dimensional pieces to form a\ncube of five units on each side.\n\nThe Program Analyzer\n\nTable *5.2:Characteristics of the test programs\nName\n.A lamos\nBaskett\n\nErathostenes\nLinpac k\n\nLivermore\n11andelbrot\nSAS Kernels\nShell\nSmith\n\n\\Vhetstone\n\nDescription of the program\nLos Alamos benchmark for vector operations execution rates\nA backtrack algorithm to solve the Conway-Baskett puzzle\nThe sieve of Erathostenes on 60000 numbers\nThe standard linear equations software of Argonne N3t. Labs.\nThe twenty four Livermore Loops\nCompute the hlandelbrot set on a grid of 200 by 100 points\nNASA Numerical Aerodynamic Simulation benchmark\nSorts and array of 10000 random number using the Shell sort\nA collection of tests similar to our System Characterizer\n\nThe Whetstone benchmark\n\nTable 6.2: Characteristics of the test, programs.\n\nErathostenes sieve: This program is a simple search for prime numbers using the\ncenturies-old sieve met hod. The computation of arithmetic expressions is minimal\nand most of the time is spent in doing comparisons.\nLinpack benchmark: This is one of the most popular benchmark used in performance evaluation for floating point computations. The program consist of two routines: the first computes the decomposition of a matrix, and the second routine solves\na system of linear equations represented by the above matrix. The program was originally designed t o give the users of the Linpack software package information about\nthe possible execution times for solving linear equations. Nowadays, there are over\ntwo hundred machines reported in the list collected by J. Dongarra at the Argonne\nNational Laboratory IDON85, DON87a, DON87b, DON881. Because Linpack running in single precision fits completely in a 64K cache, the performance reported by\nthis benchmark may be higher than the actual performance obtained by solving\nlinear equations in real problems. In some machines a small memory-cache\nbandwidth can slow down considerable the execution if the matrix does not f i t\nentirely in the cache [MIP87).\nT h e L i v e r m o r e Loops: This benchmark is a set of 24 kernels t h a t measure FORTRAN numerical computation rates [MCM86]. The loops (originally fourteen) were\nwritten by Fred McMahon in the early seventies and represent the kind of computations found in Livermore codes. The benchmarks gives the computational rates for\neach of the loops and for different vector lengths. I t also computes a sensitivity\nanalysis of the harmonic mean for seven work distributions giving a total of forty\nnine possible CPU workloads. The benchmark is a good test of the capabilities of\nthe compiler t o produce efficient (vectorizable) code. The range of performance for\nvector machines can vary up to two order of magnitude in the different loops. The\ntwenty four loops are shown in table 5.3.\nMandelbrot set: This program computes for a window of 200 X 100 points on the\ncomplex plane the mapping Z,CZ,~-~ C,until the norm of 2 is greater than 2. or\n,\nthe number of iterations is equal to one hundred. This program (variations of it) is\nused to benchmark graphic engines. All the computations are scalar and with floating point variables.\n\n+\n\nThe Program Analyzer\n\nVumber\n1\n2\n3\n4\n5\n6\nI\n\n1\n\n8\n\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n21\n22\n23\n24\n\nTable 5.3: Livermore Loops kernels\nKernel Description\nhydro fragment\nincomplete Cholesky - conjugate gradient\ninner product\nbanded linear equations\ntri-diagonal eliminstion\ngeneral linear recurrence equations\nequation of state\nA.D.I. integration\nintegrate predictors\ndifference predictors\nsum of two vector elements\ndifference of two vector elements\nparticle in cell ( 2 dimensions)\nparticle in cell ( 1 dimension)\ncasual FORTRAN (development version)\nAlonte Carlo search loop\nconditional computation\n2 dimensions explicit hydrodynamics fragment\ngeneral linear recurrence equations\ndiscrete ordinates transport\nmatrix product\nPlanckian distribution\n2 dimensions implicit hydrodynamics fragment\nfinds first minimum in an array\n\nTable 6 3 Livermore Loops kernels.\n.:\n0\n\n0\n\nNAS benchmark: The N.4S kernel benchmark was developed by D. Bailey and J.\nBarton to assist in supercomputer performance evaluation [RAI85a]. The program\nconsists of seven kernels that represent calculations typical of NASA Ames supercomputing. The kernels perform the following calculations [BAI85b]: \xe2\x80\x9couter p r o d u c t \xe2\x80\x9d\nmatrix multiplication (MXM), two dimensional complex Fast Fourier Transform\n(CFFT2D), vector Cholesky decomposition (CHOLSKY), vector block tridiagonal\nmatrix solution (BTRIS), sets up an array for a vortex method solution and performs\nGaussian elimination (GMTRY), creates new vortices according to certain boundary\nconditions (EMIT), and inverts three pentdiagonal matrices (VPENTA). The program executes approximately 2 billion floating point operations and has extensive\ncalculations with multidimensional arrays with different loop memory strides. Testing this benchmark can use four different levels of tuning depending on the number\nof lines changed, deleted or inserted. This program is normally run only in supercomputers given that it may take several hours t o run in a machine without vector\noperations.\n\nShell sort: This is a small program t h a t sorts ten thousand random numbers using\nthe Shell sort [KNU73]. The algorithm w a s proposed by Donald L. Shell in 1959 and\nis also called sort by diminishing increments. The number of operations executed by\nthe program is O ( N 3 I 2 ) . The operations executed are comparisons and memory\ntransfers.\n\n~\n\nThe Program Analyzer\n\nPROGRAM STATISTICS\n-> from 1 t o 37 1371\n\nLines processed\nme\nn m operation\nstore (01)\n\n[srsll\n[arsl]\n[mrsll\n[drsll\n\nadd\n\n(02)\n\nmalt\n\n(03)\n\nd i v i d e (04)\n[trsl] trans (07)\n[sisll store (15)\n[ s i s l l add\n(16)\n[ s i s l l trans (21)\n[andll and-or (43)\n[crsll r-sin (44)\n[ c i s l ] i - s i n (46)\n[procl proc\n(51)\n[argr] r-sin (52)\n\noccurrences\noccur :\n8\noccur:\n8\noccur:\n4\noccur:\n2\noccur:\n12\n\noccur :\noccur:\noccur:\noccur :\n\noccur :\noccur:\n\n[gotol goto-s (61)\n\noccur:\noccur:\noccur:\n\n[loin] d o - i n i (63)\ndo-lop (64)\n\noccur:\noccur :\n\n[~OOV]\n\nfraction\n(0.1481)\n(0.1481)\n(0.0741)\n\n(0.0370)\n\n(0.2222)\n1 (0.0185)\n3 (0.0556)\n3 (0.0556)\n1 (0.0185)\n1 (0.0185)\n1 (0.0185)\n2 (0.0370)\n2 (0.0370)\n2 (0.0370)\n2 (0.0370)\n2 (0.0370)\n\nFigure 6.2: Static statistics for the Mandelbrot set. The fraction column gives the static distribution\nof the occurrence of the parameters in the source code.\n\n0\n\n0\n\nS m i t h benchmark: I t is a FORTRAN program which consists of 77 individual\ntimed loops, each of which measures some aspect of machine performance. I t contains tests of branch code, numeric code, procedure calls, and data movement, and\nhas samples of other computations, such as matrix multiplies and bubble sorts\n[SMI88]. This benchmark is designed t o measure various aspects of system performance; the user can then weight the various performance factors as he sees fit. More\nthan seventy machines have been measured, ranging from microcomputers t o multiprocessors and supercomputers. The benchmark has been designed t o prevent most\noptimizations performed by compilers.\nW h e t s t o n e benchmark: this is a synthetic benchmark based on the statistics of\n949 programs written in ALGOL 60 a t the National Physical Laboratory and Oxford\nUniversity during the late sixties [CUR76]. The results using this benchmark are still\nquoted by some manufactures but few accept the validity of this benchmark as a real\nmeasure of floating point intensive calculations. The most important reason is t,hat\nthe distribution of statements in programs has changed as a results of improvements\nin the programming methodologies, programming languages and most important\nmachine architectures. The benchmark produces a single figure of merit for scalar\nprocessing. This number represents the performance for the execution of its ten\nmodules (tests), and combines the performance of floating point operations with the\nperformance of trigonometric and integer operations. It is important t o note that\nmost of the time is spent on module 7 that makes extensive use of trigonometric\nfunctions.\n\n43\n\nThe Program Analyzer\n\n5.3. O u t p u t from the P r o g r a m Analyzer\n\nIn this subsection we present an actual example of how the program analyzer works.\nIn figure 5 . 2 we can see the output for the Mandelbrot program. \\Ve can see that for each\nparameter we report the number of static occurrences in the code and the static distribution of parameters. Although in this research the static distribution does not give us any\nadditional information, these statistics are sometimes by software engineers t o compute\nsoftware complexity metrics.\n\nIt. is possible to insert \xe2\x80\x98compiler\xe2\x80\x99 directives in the source code to instruct the program\nanalyzer to produce partial reports for some number of source lines. This gives the user\nthe opportunity of knowing the static and dynamic statistics of that portion of the code,\nand also to produce, with the aid of the system characterizer and the execution predictor,\nexecution time estimates of subparts of the program.\n5.4. P r o g r a m s Statistics\nTables 5.5 and 5.6 present the dynamic statistics of each program sorted by value.\nand also their cumulat,ive distribution. The table shows the most costly parameters for\neach program in terms of the number of times each parameter is executed. These not\nnecessary represent the most time consuming parameters, which depend on the characteristics of the machines. T h a t type of information will be generated using the program\npredictor and the parameters of each machine. We see in the distributions that there is a\nsmall number of operations that account for almost all the execution time. In all the programs except the IVhetstone benchmark between five and seven operations represent\nalmost ninety percent of the total number of operations executed.\nThe number of times each operation executes depends on the input given t o the program. Executing the program for a different data input will almost always produce a different distribution in the dynamic statistics. The parameters t h a t correspond to the\naccess of 1 dimensional arrays (ARRI), the loop overhead time (LOOV), the arithmetic\noperations -add and multiply- are the most executed operations for all the programs. But\nthe distribution of these parameters varies considerably from program to program. The\nreason for this variation is that some programs (Mandelbrot, Shell, Erathostenes) have a\nsmall number of lines, between 40 and 100. In fact, these program represent only a very\nsmall fraction of the total execution time of our workload. Five of the benchmarks execute in less than 2 seconds on the CRAY X-MP, while the other five programs take\nbetween 10 and 000 seconds. Because these programs use a small number of different\noperations normal errors in our measurements are not balanced out by other errors on\nother operations. An example of this is the hlandelbrot program t h a t executes mainly\nscalar floating point arithmetic without using array elements. In the case of the Los\nAlamos benchmark the code is a repetition of small loops of the form:\n\n20\n\nCALL JOBTIM (T1)\nDO 20 J = 1,LOOPS\nDO 20 I = 1,LEN\nR ( 1 ) = Vl(1)\nCONTINUE\nCALL JOBTIM (T2)\n\n*\n\ns1\n\nThe Program Analyzer\n\n44\n\nthese kind of constructs are not representative of real applications. If we compare the\nstatistics for the Livermore Loops w i t h a program length of 1900 lines, against the results\nreported by Knuth [ K N W l , VVECI841 we find a fairly good agreement between the two distributions: see table 5.4.\nDistribution of Statements\nStatement\nIcnuth\nLivermore\ndyn\nsta\nsta\ndyn\n.5l\n67\'\n55.5\n66.3\nassignment\n3\n3\ncall user\n7.4\n4.9\n1\n1\ncall standard\n9.2\n1.6\n-1\n3\n2.3\n4.9\nreturn\nif\n10\n11\n6.5\n14.1\n9\n3\n11.1\n3.2\ndo loop\n9\n9\n5.7\n2.9\ngot0\n3\nI\n2.3\nother\n2.0\n\n1\n\nI\n\n-\n\n-\n\nTable 6.4: Static and dynamic statistics at the statement level (all quantities in percentages).\n\n~\n\n~~\n\n~\n\nThe Program Analyzer\n\n-15\n\nORIGINAL PAGE I\nS\nOF POOR QUALITY\n\npararn\n\ndy n\n322006015\n934931 12\n9 1003000\n84000000\n49000000\n35000000\n2 1000197\n2314108\n712296\n180000\n179592\n4000\n2000\n1445\n1105\n1000\n1000\n1000\n686\n392\n\nARRl\nLOOV\n\nSRSG\nMRSG\nARSG\n\nIADD\nARSL\nLOIN\nAGRS\nAGIS\nPROC\nAISL\nEISL\nTRSL\nDRSL\nXRSG\nTRSG\nSISG\nCISL\nANDL\n\nfrac\n\ncum\n\n.4607\n,1338\n,1302\n,1202\n.0701\n.0501\n.0300\n.0033\n,0010\n.0003\n.0003\n\n,4607\n,5945\n,7247\n,8449\n,9150\n\n.96.51\n,9951\n,9984\n,9994\n,9997\n\ndYn\n\nfrac\n\ncu rn\n\n.NSL\n\n42082283\n25874691\n23755236\n15721656\n13014396\n10725888\n8765400\n6963630\n6576587\n6576586\n1888908\n1307880\n1121655\n979662\n975927\n878034\n867738\n791125\n710287\n451542\n\n,2447\n.1446\n.I381\n,0914\n.0757\n.0624\n.0510\n.0405\n,0383\n.0382\n,0076\n.0065\n.0057\n,0057\n.0051\n.0050\n.0046\n.0041\n.0026\n\n,2447\n.3893\n,5274\n3188\n,6945\n,7569\n,8079\n,8484\n.8867\n.9249\n.9359\n,9435\n,9500\n.9557\n.9614\n.9665\n.9715\n.9761\n,9802\n.9828\n\nfrac\n\ncum\n\nparam\n\n.3767\n.1341\n.1304\n.1287\n.I230\n.0221\n\n.3767\n.5108\n5412\n,7699\n,8929\n.9 150\n,9300\n.9431\n.9544\n.9637\n.9713\n.9753\n.9792\n.9830\n.9867\n3904\n.994l\n,9962\n,9982\n.9999\n\nARRl\n\nARRl\nSRSL\nSIRSL\nLOOV\nARR2\nARSG\nMRSG\n\nIADD\n\n1.ooo\n\nSRSG\nTRSG\nARR3\nAISL\nAGRS\nPROC\nSISL\nTISL\nAGIS\nCRSL\nAISG\n\ncum\n\npararn\n\n.2273\n.1198\n.099 1\n.0964\n,0791\n.0704\n.0635\n.0550\n.0452\n,0431\n.0322\n.0304\n.O 145\n.0109\n.0033\n.0025\n.0024\n.0021\n\n,2273\n.3471\n,4462\n.5426\n.I3217\n,6921\n,7556\n,8106\n,8558\n,8989\n.93 1 1\n.9615\n.9760\n,9869\n.9902\n.9927\n.9951\n,9972\n,9982\n,9988\n\nARR 1\nLOOV\nSRSL\nARSL\nhlRSL\n\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n\n1 .ooo\n1 .ooo\n1 .ooo\n1 .ooo\n1 .ooo\n1 .ooo\n\n1.ooo\n1.000\n1.ooo\n\nNAS kernela\nparam\n\nARR2\n\nIADD\nARSL\n\nARR3\nMRSL\nLOOV\nSRSL\nMRSG\nARR 1\n\nARR4\nARSG\n\nSRSG\nTRSL\nAISL\nERSL\nAGRS\n\nPROC\nDRSL\nLOGS\nTRSG\n\n~\n\n685238000\n566581256\n551229 173\n452430305\n402504958\n363214880\n3 I4803900\n258770855\n246288901\n183914506\n173863550\n8306759 1\n62408350\n18740872\n14561611\n13931792\n12000684\n5495002\n3413494\n\n.0010\n\n.0006\n\nBaskett\n\npararn\n\nfrac\n\nI\n\nI\n\nLivermore\n\nAlrunO.\n\n.0110\n\nARRZ\nAGRS\nAGIS\nPROC\nTRSL\nCISL\nTISL\n\nL4DD\nDRSL\nMISL\nGOTO\nCRSL\nAISL\nLOIN\nANDL\n\ndYn\n\nfrac\n\nCISG\n\n1365170\n113315.4\n790495\n766720\n685213\n540905\n57795\n43390\n38663\n31913\n30855\n30152\n29909\n2 1468\n19336\n6009\n5996\n445\n4\n\n,2439\n2024\n,1412\n,1370\n,1224\n.096G\n,0103\n.0078\n.0069\n.0057\n.0055\n.0054\n.0053\n.0038\n,0035\n\nIADD\nLOOV\n\nARR2\nARR 1\nANDL\nGOTO\nTISL\nAGIS\nCISL\nTISG\nAISL\nSISL\nLOIN\nPROC\nSISG\nAISG\nMISL\nTRSL\nSRSL\n\nI I\n\nLLnpack\ndYn\n27356020\n9735148\n9469508\n9344931\n8930874\n1604752\n1089573\n952191\n819045\n674414\n548318\n288574\n283722\n275205\n270000\n267488\n265149\n15457 1\n147117\n133822\n\npararn\n\n.0131\n.0113\n.0093\n.0076\n.0040\n.0039\n.0038\n.0037\n.0037\n.0037\n.0021\n,0020\n.0018\n\nCISL\nLOOX\nLOOV\nTISL\nGOTO\nSISL\nAISL\n\nLorn\n\ndYn\n324880\n216994\n150938\nll9999\n113943\n56754\n6057\n6057\n3245\n2\n2\n2\n2\n1\n\n- cu I\nI\n-\n\n.3252\n2172\n.1511\n.1201\n.I141\n.0568\n\n.32.\n.54:\n.69\n.81.\n.92\n.98\n\n-\n\n-\n\n,\n\n.006 1\n\n,991\n\n.006 1\n.0032\n\n.99f\n,991 I\n1.01\n1.01\n\n-\n\n-\n\n-\n\nfrat\n\n-\n\nfraction of the total execution time that each parameter represents and the cumulative dis-\n\n.\n\n1.O(\n1.01\n1.O(\n\n-\n\n-\n\n,\n\n.99!\n\n-\n\nTable 5.51 Dynamic statistics of test programs (I). In the third and fourth columns we report the\ntribution\n\n.oo\n\n-\n\n1\n\n.98i\n.99(\n.99\n.99:\n\n.ooo 1\n.oooo\n.oooo\n\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n-\n\nTRSL\nPROC\nLOIN\nAGRS\nSRSL\nARSL\n\n24:\'\n,441\n.587\n.72\n.841\n.9 4:\n.95:\n,961\n.96t\n\n.0011\n11\n\nErathomtenem\n\nI\n\n.0150\n\n1\n\ncur\n-\n\n-\n\n1.O\'\n\n1.01\n\nI\n\n1.01 1\n\n-\n\n-\n\n-\n\n-\n\n~~~\n\nThe Program Analyzer\n\nShell\n\nMandelbr ot\nARSL\nSRSL\nMRSL\nGOTO\nCRSL\nCISL\n.4\\iL\nNSL\nSISL\nTRSL\nLOOV\nTISL\nLOIN\nPROC\nDRSL\nAGRS\n\nfrac\n\ndy n\n\npsram\n\n2348726\n2328726\n2308524\n597131\n597131\n597131\n597131\n577133\n577131\n80207\n20200\n20002\n20 1\n\nI\n\n2\n?\n\nI\n\n2\n\ncum\n\nparam\n\n,2206\n,2187\n.2168\n,0561\n.Os61\n,0561\n.OS61\n.0542\n,0542\n.0075\n.oo19\n.oo 19\n\n.2206\n,4393\n.6561\n.i122\n.7683\n,8244\n.8805\n,9347\n.9889\n,9964\n.9983\n1.000\n1.ooo\n1.ooo\n1.000\n1.000\n\nARRl\nTISL\nSISL\nAISL\nCISL\nGOTO\nLOOV\nLOIN\nDISL\nTRSL\nPROC\nAGRS\nMISL\n\n.oooo\n.oooo\n.oooo\n.oooo\n\nARRl\nTIS1\nMSL\n\nSISL\nLOOV\n30TO\nXOM\nTRSL\nCISL\nIADD\nANDL\n\nMISL\nAGIS\nLOIN\nARSL\nSRSL\n\nSRDG\nARR2\n\nPROC\nDRSL\n\n1\n\n-\n\nfrnc\n\n.3i30\n,1365\n.1332\n.1332\n,1332\n,0486\n,0424\n\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n.oooo\n-\n\nWhetatone\n~\n\n11\n\n2\n2\n\n-\n\nSmith\nJaram\n\ndyn\n2021588\n739869\n721809\n721794\n721682\n263303\n230018\n16\n14\n2\n\n~~\n\n~~\n\ndyn\n\nfrac\n\ncum\n\nparam\n\n166383224\n58968760\n52178266\n49496467\n43184695\n22826813\n19325095\n12053242\n8077412\n6707232\n5756582\n5585525\n42003 12\n334672 1\n3025742\n2815742\n2457705\n1420727\n900312\n472761\n\n.3535\n.1253\n.1109\n.lo52\n.0918\n.0485\n.04 11\n.0256\n.O 172\n.0143\n,0122\n.0119\n.0089\n.007 1\n.0064\n.0060\n,0052\n.0030\n.0019\n.0010\n\n.3535\n.4788\n.5897\n.6949\n.7867\n.8352\n.8763\n,9019\n.9191\n.9334\n.9456\n.9575\n,9664\n.9735\n.9799\n.9859\n.99 11\n.9941\n.9960\n.9970\n\nARRl\nARSL\nSRSL\nAGRS\n\nMRSG\nLOOV\nTRSG\nAISG\nGOTO\nMISG\n\nTISG\nCISG\nDRSG\nPROC\nSISG\nSRSG\nMISL\nIADD\nSINS\nDRSL\n\ndyn\n301825\n210650\n161900\n135592\n113700\n111650\n92409\n84000\n55250\n.52500\n51i58\n51750\n49150\n45662\n31500\n23400\n21009\n21000\n12800\n7850\n\nfrac\n.1801\n.1257\n,0966\n.0809\n.0678\n\xe2\x80\x980666\n,0551\n.050 1\n,0330\n,0313\n.0309\n.0309\n,0293\n.0272\n.0188\n.O 140\n.O 125\n.0125\n.0076\n.0047\n\ncum\n-\n\n.3730\n,509.5\n,6427\n.7759\n.g090\n\n.9576\n1.ooo\n1.ooo\n1.000\n1.ooo\nI .ooa\nI .ooa\n1.ooa\n\n-\n\ncum\n.1801\n.3058\n.4024\n.4833\n.5511\n,6177\nA728\n.7229\n,7559\n.1872\n.8181\n.8490\n.8783\n.9055\n.9243\n.9383\n.9508\n.9633\n.9709\n.9756\n\n-\n\nTable 61: Dynamic statistics of test programs (II). In the third and fourth columns we report\n.)\nthe fraction of the total execution time that each parameter represents and the cumulative\ndistribution\n\n.\n\nORIGINAL PA=\n\nfS\n\nOF POOR QUALITY\n\n6\n\nThe Execution Predictor\nAs we explained in the last two sections, the system characterizer and the program\nanalyzer are the only tools that w e need t o produce estimates of the execution time of programs running in different architectures. characterizer and the program analyzer to obtain\nan estimate of the expected execution time of the application. I t is clear that these estimates have meaning only for the data used in the dynamic analysis of the programs. In\nthis section we will obtain prediction for our benchmarks and compare these results to the\nactual running times.\n\n6.1. C o m p u t i n g Execution E s t i m a t e s and Experimental Errors\n\nIn section 3.3 we proposed a model of execution in which the total time is a linear\ncombination of the number of times each operation executes in the program and the times\nit takes to execute these operations. We gave expressions to compute for each kind of test\nthe variance involved in the measurements. The variance in the total execution time for\nan application is\nn\n\na2T =\n\nC?.u2Pi\ni=l\n\nwhere Ciis the number of operations of type i executed in the program. If the experimental errors are small compared t o our measurements, the total variance in our predictions will tend to be small. Programs that execute many arithmetic operations tend to\nproduce predictions with small intervals of uncertainty. This also applies for systems\nwhere the clock resolution is fine.\nFigure 6.1 presents a sample output from the execution predictor. Each line contains\nthe number of times the operation was executed, and the fraction of the total that that\nnumber represents. The output also includes the expected execution time, the fraction of\nthe total time and the standard deviation.\nL\n\n6.2. Execution Prediction a n d S y s t e m Characterizers\nAs mentioned in the introduction, one of the problems with benchmarks is relating\ntheir results to the actual characteristics of the machine and the application programs.\nKnowing that a computer system runs the Dhrystone benchmark at a certain rate does\nnot give us sufficient information about what will be the expected execution time of other\nprograms. Obviously if machine A has a Whetstone rate that is several times greater\nthan the rate for machine B, we can expect that scientific applications will run faster on\nmachine . ,but this does not give us precise information about how fast the programs will\n4\nactually run. Also the machines we need to evaluate are normally comparable in their\noverall performance, and knowing that one has better results for a couple of benchmarks\ndoes not imply that it is going t o run our applications faster; especially when these applications execute only a small set of operations that run faster in the machine that did not\nget the best results using a particular set of benchmarks. The situation gets complicated\nwhen the performance evaluation index produced using some group of benchmarks differs\n\n47\n\nThe Execution Predictor\n\nPROGRAM STATISTICS FOR THE VAX-11/785\nLines processed\n-> from 1 to 37 1373\nmnem operation\n[srsl] store (01)\n[arsl] add\n(02)\n[mrsll mult\n(03)\n[drsl] divide (04)\n[trsll trans (07)\n[sisl] store (15)\n[aisll add\n(16)\n[tis11 trans (21)\n[andll and-or (43)\n[crsl] r-sin (44)\n[cisll i-sin (46)\n(51)\n[proc] proc\n[agrsl r-sin (52)\n[goto] goto-s (61)\n[loin] do-ini (63)\n[loov] do-lop (64)\n\ntimes-executed fraction\nexec :\n2328726 (0.2187)\nexec :\n2348726 (0.2206)\n2308524 (0.2168)\nexec :\nexec :\n2 (0.0000)\nexec :\n80207 (0.0075)\n577131 (0.0542)\nexec :\nexec :\n577133 (0.0542)\nexec :\n20002 (0.0019)\nexec :\n597131 (0.0561)\nexec :\n597131 (0.0561)\n597131. (0.0561)\nexec :\n2 (0.0000)\nexec :\nexec :\n2 (0.0000)\nexec :\n597131 (0.0561)\nexec :\n201 (0.0000)\nexec :\n20200 (0.0019)\n\nexecution-time fraction std.dev.\ntime: 0.900751 (0.0703) 0.452488\ntime: 3.453567 (0.2695) 0.128027\ntime: 4.666220 (0.3641) 0.438389\ntime : 0.00000~(0.0000) 0.000000\ntime: 0.212172 (0.0166) 0.022616\ntime : 0.OOOOOO (0.0000) O .000000\ntime: 0.682171 (0.0532) 0.027356\ntime: 0.057306 (0.0045) 0.004174\ntime: 0.536582 (0.0419) 0.027528\ntime: 1.276666 (0.0996) 0.141042\ntime : 0.997388 (0.0778) 0.066700\ntime: 0.000042 (0.0000) 0.000003\ntime : 0.000001 (0.0000) 0.000001\ntime : 0.000000 (0.0000) 0.000000\ntime: 0.003643 (0.0003) 0.000492\ntime: 0.027965 (0.0022) 0.006056\n\nEstimate execution time = 12.814481 sec. Standard Deviation = 0.663125\nFigure 6 1 Execution time estimate lor the Mandelbrot program run on a Vau-11/785.\n.:\n\nfrom the actual evaluation obtained by running the real codes. In these cases it is\nextremely difficult t o find the causes or t o modify the benchmarks t o better represent the\ncharacteristics of our workload. The major flaw in the benchmark approach is that we\nlack a model for the system that we are trying to characterize and this makes very difficult to correlate our benchmark results with application programs.\n\n6.3. Model Validation\nOne of the most important tests for a computer model is the experimental validation\nof the accuracy and sensitivity of the model. This validation is important for several reasons: if the execution estimates agree with the experimental validation, we can have confidence that the model really characterizes the system. This provides evidence that the set\nof parameters used in the program analyzer are adequate t o decompose applications. It\nalso increases our confidence that the estimates produced by the execution predictor are\nacceptable (within a confidence interval) with the real execution time of actual codes. On\nthe other hand, if for some applications the execution estimates do not agree with the\nexperimental validation, w e can conclude that they are some characteristics in the computer system that our model is missing or fails to capture. In this situation it is possible\nt o isolate in the application program the operation or set of operations that cause the\nproblem and t o include them in a new more general model. This is possible because the\nsystem characterizer, the program analyzer, and the execution predictor use a machine\nmodel that is common t o all machines that execute programs written in FORTRAN. \\Ire\n\nThe Execution Predictor\n\n49\n\nmay build a new model by incorporating some additional parameters to our linear equation. T o improve the model ive need to write new tests to detect and measure these\nparameters in the system characterizer, modify the program analyzer to count the\noccurrence of these operations in the source codes, and produce new estimates using the\nresults obtained with the system characterizer and the program analyzer. Isolating the\nportions of the codes that cause the erroneous prediction makes it possible t o redefine\nsome parameters to detect machine features not previously detected with the model. Continuing this process will lead to a more complete model of computer systems and to more\naccurate predictions of execution times. Here the term \'complete\' refers to our ability t o\npredict, using the characterization of a computer system t o obtain the expected execution\ntime of some set of applications. This way of approaching system characterization and\nperformance evaluation agrees with the premises we mentioned earlier about experimentation and incremental model refinement.\nWe can illustrate the point made in the last paragraph with the following example.\nIn the first version of our model arithmetic operations were classified according t o the\ncharacteristics of the operands. independent of where the operation appeared in the text.\nThe first predictions that we made for the Livermore Loops running on a the VAX-11/785\nwere not very far from the actual running times for some loops, but for a couple of them\nthe actual running times were almost three times smaller than the execution estimates.\nWhen we examined the source code w e found that loops 1, 4, 7, 8, and 18 have the characteristic of adding or subtracting a constant t o most of their array indexes. In our model\nan integer arithmetic operation inside of one of the dimensions of the array was considered identical t o the same operation executed between two variables of the same type,\nsize and class storage. In almost all the existing compilers, arithmetic operations between\nindexes inside a loop use registers instead of making reference to memory locations, and in\nother cases, the constant is added to the base-descriptor of the array a t compile time eliminating the unnecessary operation.\nThese are not an optimization but a standard features in most compilers. We\nimproved our model to make a distinction between an integer arithmetic operation executed in the context of making a reference to an array element, and a normal operation\nbetween integer variables in expressions. In addition w e wrote a small set of tests to\nmeasure the execution time of these new operations in the system characterizer. The new\npredictions obtained using this new approximation were as good as the best obtained previously.\n\n8.4. E x e c u t i o n Predictions and Actual R u n n i n g T i m e s\nWe obtained execution estimates for the programs in table 5.2 and for each of the\nmachines in table 4.1. Aside from this, we also executed each of the programs in the same\nsystems, and measured the actual execution times. We tried t o reproduce the same conditions in these tests as when we ran the system characterizers. Only in this way we can\nguarantee that the execution estimates obtained using the results of the system characterizer correspond to the same systems in which the programs were run. In tables 6.1-6.2\nand in figures 6.2-6.4 we present the measurements along with the estimates. All the\nresults are plotted together in figure 6.5. We also show the difference between the real\nmeasurements and the predictions.\n\n~-\n\n~~\n\nORIGINAL PAGE E\nS\nOF POOR QUALW\n\nThe Execution Predictor\n\nTable 6.1: Execution estimates a n d actual running times (I)\n\nII\nreal\n(seci\n\nSystem\n\n63.8\n01.1\n80.5\n345.8\n236.1\n265.3\n701.i\n1581.7\n6273.2\n3881.0\n\nCRAY S-MP/48\nCYBER ?05\n\nIBM 3090/?00\n\nAmdahl 5840\nConvex C-1\nVAX 8600\nVAX- 11/i85\nVkY-11/780\nSun 3/50\nIBM RT-PC/1?5\n\naverage\nroot mean sa.\n\nbskett\nDred\n\n83.0\n73.4\n3n.2\n243.6\n?66.7\n758.3\n1X?.7\n\n-5.41\n+3.18\n+0.53\n+KO7\n+7.65\n\n?.?3\n9.75\n?.8?\n7.38\n14.85\n\n-1.85\n-?.02\n\n6.20\n\n3iQ5.8\n\n3810.0\n\nI\n\n( s 4\n0.66\n1.16\n0.78\n2.67\n?.32\n3.24\n8.27\n16.17\n8.315\n7.40\n\n-5.71\n-10.85\n+18.18\n+18.73\n-15.64\n+14.80\n+12.06\n+8.88\n+17.78\nf10.35\n\nErathostei\nI Dred\n\n1 1\n\nerror 11 real\n\n(sec)\n0.140\n\n-11.88\n-10.77\n-18.64\n-0.40\n-10.80\n\n+33.00\n\n+&OS\n\n-3.80\n12.45\n\n15.80\n\n6.55\n\nivermore\npred\nerror\n(sec)\n16.0\n31.7\n18.5\n\n-\n\n60.0\n88.7\n?55.0\n653.5\n2583.7\n1573.8\n\n(76)\n+10.46\n-1.25\n-5.13\n\n-\n\n+2.06\n+0.57\n+14.60\n+6.06\n\n+5.16\n-2.25\n+3.56\n6.00\n\nerror\n\nisec)\n0.161\n\nn\nreal\n(sec)\n1.002\n0.676\n0.220\n3.344\n3.048\n3.400\n11.36\n33.42\n163.04\n105.43\n\npred\n(set)\n\nerror\n\n1.057\n0.588\n0.2?6\n3.546\n3.380\n3.614\n12.82\n32.13\n165.81\n104.00\n\n+5.48\n-13.02\n+2.73\n+&04\n-14.30\n+3.55\n4-12.85\n-3.86\n+1.14\n-1.27\n-0.07\n8.04\n\n(55)\n\nTable 6 1 Execution estimates and actual running times (I). All real times and predictions in\n.1\nseconds; errors in percentage.\n\nThe result for the Livermore Loops on the Amdahl 5840 is missing because the compiler complained of an error in the program when the tests were run1. The N.4S kernels\nand the linpack program were not available when the test program were r u n on the\nAmdahl 5840 and the IBXl 3090/200. On the Convex C-1, VAX 8600, VAX-11/785, and\nVAx-11/780 the NAS kernels does not run in single precision2.\n\nT h e code generator detected an error in the code produced by the first pass module.\nT h e program divides by zero on these machines if the benchmark is executed using 32-bit f l o a t\ning point numbers. The random number generator needs 6 4 b i t numbers to execute correctly\nPAI871. However on the SUN 3/50 and IBM RT-PC/125 the program executed without errors with\nsingle precision.\n1\n\n2\n\n51\n\nThe Execution Predictor\n\nTable 6.2: Execution estimates a n d actual r u n n i n g times\n\nS kernels\n\n(n)\n\nShell\n\nreal\n\nSystem\n\n(set)\n533.8\n1456.7\n\nCRAY X-MP/48\nCYBER 205\nIBM 3000/200\nAmdahl 5840\nConvex C-1\nVAX 8600\nVkX- 111785\nVAX-11/780\nSun 3/50\nIBM RT-PC/lZ5\naverage\nroot mean sq.\n\nI\n\nt\n\nII\nII\n\n80800.\n50863.\n\n/I\npred\n\n(4\n138.0\n53.2\n108.0\n103.1\n238.7\n683.0\n1087.5\n014.8\n545.1\n\n0.481\n0.305\n1.065\n1.770\n2.140\n6.110\n8.803\n3.522\n4.61\n\n-\n\n14.02\n\nSmith\n\nSystem\n\nCI\'BER 305\nIBM 3000/?00\nAmdahl 5840\nConvex C-1\nVAX 8600\nVAX- 111785\nVAX- 111780\nSun 3/50\nIBM RT-PC/125\n\n0.555\n0.440\n1.803\n1.828\n2.233\n5.800\n0.183\n3.140\n4.68\n\n-\n\n65.77\n02.9\n45.3\n185.4\n107.2\n230.0\n601.6\n1018.8\n877.4\n675.3\n\naverage\nroot mean sq.\n\nerror\n\n(96)\n-1.30\n-32.68\n-14.85\n-6.36\n+2.12\n-3.64\n+1.13\n-6.32\n-4.00\n+23.80\n-4.22\n14.06\n\nWhetstone\nreal\npred\nerror\n(sec)\n(sec)\n(%)\n0.302 0.206\n-1.00\n1.128 0.034 -17.27\n0.350 0.335\n-4.20\n1.607 1.042 +14.44\n1.111 1.170\n+5.31\n2.870 2.631\n-8.33\n7.05 7.385\n-7.11\n21.57 21.74\n+0.70\n34.24\n30.5 +15.36\n12.05 11.05\n-0.82\n-0.30\n4.45\n\n-13.33\n-10.23\n+3.80\n-3.17\n-4.16\n+5.34\n-4.14\n+12.17\n-1.50\nI\n-2.84\n8.33\n\naverage\nerror\n\n(57)\n+1.01\n-0.04\n-4.34\n+2.01\n-4.61\n-3.45\n+5.80\n+0.26\n+2.61\n+3.63\n\nrm\nerror\n\n(96)\n8.15\n10.85\n0.47\n0.43\n0.08\n0.88\n0.17\n0.52\n13.01\n11.82\n\nTable 62 Execution estimates a n d actual running times (11). All real times a n d predictions in\n.:\nseconds: errors in percentsge.\n\n52\n\nThe Execution Predictor\n\nCyber "05 ( 4 pipes)\n\nCRAY X-MP/48\nk\n\nT-loWRh\n\ni\n\nn\n\nNAS kernels\n\nd\n\n1OOO\'\n\n1000\n\ne\na\nI\n\ne\na\nI\n\n/\n/\n\n/\n\nT loo\n1\n\n1\n\n*-Smith\n\n1\n\n4\nLivermore -\n\n/\n\n/\n\nLinpack p\n\nLivermore 7\n\n/\n\nLinpack -4\n/\n\n/\n/\n\n/\n\n/\n\n1\n\nShell++\'-\n\n4\n-\n\nd\n\nWhetstone\n\n4-\n\n+\n\nWhetstone\nMandelbrot\nT\nh\nShell\n\nMandelbrot\nBaskett\n\n/\n\n/\n\nd\n-\n\nEras\n\n0.1\n0.1\n\n/\n\nBaskett-\n\n+/e-\n\n9\n-\n\n10\n\n1\n\n100\n\nEras\n\n0.1\n0.1\n\nlo00\n\n10\n\n1\n\n100\n\nIBM 3000/200\n\nR\n\nk\n\ni\n\nn\n\nAmdahl5840\n\n-\n\nSmith\n\na\n1\n\ni\n\nn\n\nd\n\ne\n\nAlamos\nSmith\n\na\nI\n\n/\n\nLivermore -#\n\nm\n\n/\n\n/\n\ne\n\n/\n/\n\n/\n/\n\n/\n\n10\n\n/\n\n1\n\n+ c\n,\n- Mandelbrot\n\n/\n\n$\nShell - :\n\n/\n\n/+ c - Baskett\nShell\n*\nWhetstone\nd t Mandelbrot\n-\n\n/\n\n1\n\nd\n\n1\n\nEras\n\n/\n/\n\nEras\n\n0.1\n\nBaskett\nWhetstone\n\n/\n\n/\n\np\n-\n\n/\n\ni\n\n/\n\n\'\n\nt/\n\nj\n\n/\n\nI\n\n/\n\n/\n\n/\n\nT loo\n\n/\n\nm ia\ne\n\nk\n\nR\n\n+/\n\n/\n\nT\n\n-\n\nlo00\n\nAlamos - y\n\nd\n\ne\n\nlo00\n\nPredicted Time\n\nPredicted Time\n\n100\n\n/\n/\n\n/\n\n/\n\n10\n\n/\n\n-\n\n*l-s\n\nT \'0\xc2\xb0\'\n\nAamos -++Smith\n\nm\ne\n\n/\n\nR\n\nNAS kernels+\n,\'\n\nR\n\n7\n\n/\n\n0.1\n10\n\nPredicted Time\n\n100\n\n0.1\n0.1\n\n1\n\n10\n\n100\n\nlo00\n\nPredicted Time\n\nFigure 6.2: Predicted times versus real execution times (I). Results for the CRAY X-MP/48, the\nW E R 205 (4 pipes), the IBM 3090/200, and the Amdahl 5840. Scales a r e logarithmic and\nvalues are reported in seconds.\n\n53\n\nThe Execution Predictor\n\nConvex C 1\n\nVAX\n\n1000\nnmm-\n\nR\ne\n\nAlams\n\n-\n\nSmith\n\nT loo\n\n-J\n\'\n-7\n\ni\nm\n\nLivemre\n\ne\n\nLiipack A +\n\n/\n\n8600\n\n..\n..\n\n7\n\nR\n\ne\n\n/\n\na\n1\n\n=\n\n/\n\ni\nm\n\nY\n\n/\n\n100\n\ne\n\n/\n/\n\n/\n\n10\n\n10\n\n/\n\nMandelbmt\n\n-+/\n\n+/-\n\ny\n\n+\n,\n\n1\nJ\n-\n\nBaskdt\n\nShell\nWhetstone\n\nC -\n\n1\n\nBaa\n\n/\n\n/\n\n/\n\n=\n.\n\n01\n.\n\n1\n0\n\n1\n\n01\n.\n\n(\n\n100\npredied\n\nlo00\n\nT\nm\n\nVU-11/780\n\n10000\n\n1Oooo\n\nR\ne\n\nR\ne\n\na\n\na\n\n1\n\n1\n\nloo0\n\nT\n\nI\nm\n\n1\n\nm\ne\n\n1000\n\nT\ne\n100\n\n100\n\n/\n/\n\nMandelbrot -+\n\nd-\n\n\\Vhetstone\n\nEbskett - p\n10\n\n10\n\n1\n\n1\n\nb\n\nShell\n\n/\n\n/\n\n/\n\n01\n.\n\n01\n.\n01\n.\n\n1\n\n1\n0\n\n100\n\nlo00\n\nFrediied Tm\nie\n\n10000\n\n01\n.\n\n1\n\n1\n0\n\n100\n\nlo00\n\nloo00\n\nRedicted Time\n\nFigure 6 3 Predict.ed times versus real execution times (II). Results for the Convex C-1, the\n.:\n\nVAX 8600, the VAX-11/785, and the VAXl1/780. Scales are logarithmic and values are\nreported in seconds.\n\n54\n\nThe Execution Predictor\n\n1BM RT-FC/I?S\n\nSUN 3/50\n100000\n\nR\n\nIUS kernels\n\nh,"d\n\ne\n10m\n\n10000\n\nAlanws -+\'\n\nT\n\nL i e m r e -+\n\ni\nm\ne\n\na\n\n/\n\nT\n\nSmith\n\n1000\n\nLmpack\n\nR\n\ne\n\n/\n\na\n\n\'\n\n-\'\n;\n\n100000\n\ni\n\nm\n\n- ,\n+\n\ne\n\n+\n,\n\n1000\n\nhhndelbrot A+\'\n/\n\n1M:\n\n100\n\n/\n\np\n-\n\nWhetstone\n\n/\n\n1\nC\n\nio\n\n/\n\n+ e Bkskett\n, SheU +\n,\na\n1\n\n1\n1\n\n10\n\n100\n\n1000\n\nloo00\n\npredicted\n\nT\ni\nm\n\n1ooooo\n\n1\n\n10\n\n100\n\n1000\n\nlorn\n\n1c\n\n100\n\nplpdicted Tim,\n\nFigure 8.4: Predicted times versus real execution times (111). Results for the SLW 3/50, and the\nIBM RT-PC/125. Scales are logarithmic and values are reported in seconds.\n\nThe Execution Predictor\n\n*\n\nfJ\'\n:\n\'\n\nN\n\n0\n\nIdz\n\na\nV\nX\n\n0\n\n+\n0\n\n0\n\n0.1\n0.1\n\n0.1\n\nR\ne\na\n\n0.1\n\n1 0.1\n\nT\n\n0.1\n\nI\n\nm\ne 0.1\n0.1\n\n0.1\n0.1\n\n01\n.\n\n1\n\n10\n\n100\n\n1000\n\n10000\n\n100000\n\nPredicted Time (sec)\nFigure 6.6: Predicted times versus real execution times ( V .Each diagonal line represents one\nI)\ngraph from figures 6.2-6.4.\n\n7\nAnalysis of Results and Summary\nIn this section we make an analysis of the data obtained with the system characterizer. the program analyzer and the execution predictor and show how these results can be\ncombined to identify the strong and weak features of the systems with respect to the\nworkload used. In section 7.2 we discuss some of the factors that must be addressed in\norder to improve the accuracy of our execution estimates. LVe finish this report by giving\na summary in section 7.3.\n\n7 1 Analysis of Results\n..\n,A comparison of the execution times between our predictions and real measurements\nshow 5everal interesting patterns (figures 6.2-6.4 and tables 6.1-6.2). First we can see that\nthe relative performance of the systems is not the same in all programs. For example if\nwe consider the behavior of the three fastest machines used in these study we find the following. The CYBER was the fastest t o run the hlandelbrot program; The CRAY X-MP\nhas the shortest t,imes for Los Alamos, The Livermore Loops, the Linpack, the NAS kernels and the Lbletstone benchmarks1; while the IBM was the fastest on the Smith benchmark. Shell sort, and the Baskett puzzle. If we look a t the codes of these programs we\nfind that in the Mandelbrot. program almost 80 percent of the dynamic statistics\ncorrespond t o scalar arithmetic and logic operations. On the other hand the programs\nthat the CRAY runs faster have intensive floating point arithmetic operations with\narrays. LVhile for the Baskett puzzle, the Shell sort and the Smith benchmark the\npredominant char,acteristic is the execution of integer operations with arrays. Except in\nthe case of the Erathostenes sieve, our execution eqtimates correspond closely to the\nresults obtained in the real executions. W t h the Erathostenes program the predicted\ntimes and the real times are almost identical for the three machines (the value between\nthe minimum and the maximum execution time is six percent). This difference is less than\nthe experimental error due to clock resolution for the IBM 3090/200.\nRelative differences in performance is clearer in the case of the VAX 785, the V.4X\n780, the IBM RT-PC and the Sun 3/50. For the Livermore Loops, the Mandelbrot program. the Linpack benchmark, and Los Alamos, the real measurements and the predictions indicate a relative performance that varies from 8:5:2:1 t o 14:9:3:12. On the other\nhand, the result of the Shell sort and the Erathostenes sieve indicat,e that the Sun 3/50\nand the IBM RT-PC are faster than both the VAX 785, and the VAX 780; this agrees\nwith the real measurements and our estimates. In this case, their relative performance is\naround .5:.3:.75:1.\nIn table 7.1 we present the real and estimated relative performance between the SUN\n3/50 and the IBM RT-PC/125. We see that the estimates agree with the real times in\npredicting which machine will execute faster each of the programs. Except for the Smith\n1\n\n2\n\nThe Linpack and the NAS kernels were not run on the IBM 3090/200\nThe order is VAX $85,VAX 780, IBM RT-PC, and Sun 3/50.\n\n56\n\nAnalysis of Results and Summary\n\n57\n\nTable 7.1: Relative performance\n11 SUN 3/50 : IBhl RT-PC/125\nprediction\nprogram\nerror (%)\n1.679\nLos Alamos\n+3.90\nBaskett\n1.139\n1.124\n-1.32\n0.818\n0.67\nErathostenes\n-7.24\n1.611\n1.679\nLinpack\n+4.20\n1.642\nLivermore\n1.526\n+7.60\n1.593\nMandelhrot\n1.555\n+2.44\n1.743\nNAS kernels\n1.765\n-0.25\n0.67 1\n0.764\nShell\n+12.17\n1.299\nSmith\n1.678\n-22.59\nWhetstone\n2.841\n3.305\n+16.33\n1.551\naverage\nLS;~\n+0.42\ngeometric\n1.411\nroot mean sq.\n10.37\n\n1\n\nTable 7.11 Relative performance between the SUN 3/50 and the Il3M RT-PC/lZS. A value\ngreater than one indicates that the IBM RT-PC executes faster than the SUN. The first two\ncolumns are dimensionless and quantities on the third column are in percentages.\n\nbenchmark, the absolute differences between the real and predicted relative performance\nwere less than 20 percent. For this program the predicted time on the IBM RT-PC was\nalmost 24 percent greater than the real time.\nThe results also indicate that our model works better for programs with long execution times and arithmetic operations. In table 7.2 we see how the predictions agree with\nthe real execution times for each machine and for different intervals of error. We observe\nthat approximately 00 percent of all predictions are within a distance of 10 percent from\nthe real execut ion times.\n\nSvstem\nCRAY X-MP/48\nCYBER 205\nIBM 3090/200\nAmdahl 5840\nConvex C-1\nVAX 8600\nVAX-111785\nVAX-11/780\nSun 3/50\nIBM RT-PC:/125\nTotal\n\nII c - 5 %\n"\n\n11\n\n3 (30.0)\n1 iio.oj\n2 (25.0)\n1 (14.3)\n4 (44.4)\n5 (55.5)\n2 (22.2)\n4 (44.4)\n4 (40.0)\n\ns (SO.0)\n\nn 31 134.11\n\n<lo%\n7 (70.0)\n4 (40.0j\n4 (50.0)\n4 (57.1)\n5 (55.5)\n6 (66.6)\n6 (66.6)\n8 (88.8)\n6 (60.0)\n6 (60.0)\n56 (61.5)\n\n< 15%\n10 (100.)\n7 (70.0)\n7 (87.5)\n6 (85.7)\n8 (88.8)\n8 (88.8)\n9 (100.)\n9 (100.)\n8 (80.0)\n6 (60.0)\n78 (85.7)\n\n[\n\n< \'20%\n10 (100.)\n9 (90.0)\n8 (100.)\n7 (100.)\n9 (100.)\n9 (100.)\n9 (100.)\n9 (100.)\n10 (100.)\n8 (80.0)\n88 (96.7)\n\nTable 7.21 Accuracy of the model for different intervals. The numbers inside the parenthesis\nshow the proportion of the programs that are inside the error interval.\n\n~~\n\n58\n\nAnalysis of Results and Summary\n\nTable 7.3: Predicted Distribution of Execution Time by Operation (Loa Alamoa)\nparameter\narray reference ( 1 dim)\nloop overhead (step 1)\nstore-real-single-global\nrnultiply-real-single-global\nadd-real-single-global\naddition in index array\n\nadd-real-single-local\nloop initialization\nargumen t-real-single\nargument-integer-single\n\nDarameter\narray reference ( 1 dim)\nloop overhead (step 1 )\nstore-real-single-global\nrnultiply-real-single-global\n\nadd-real-single-global\naddition in index array\n\nadd-real-single-local\nloop initialization\nargument-real-single\nargurnent-integer-single\n\n7\n\ndyn\n.4607\n.1338\n,1302\n.1202\n.0701\n.os01\n.0300\n,0033\n.0010\n.0003\n\nCRAY 1-LIP\n.2658\n.3390\n.1142\n.1764\n,0533\n,0231\n.0242\n.0018\n.0005\n\n.1138\n.loo9\n.2731\n.1198\n.0015\n,0107\n.0134\n.0026\n.0010\n\ndyn\n\nVAX 8600\n.4187\n.1796\n.0279\n.2045\n,0946\n\nVAX 785\n,3578\n.1693\n.0578\n.2201\n.0926\n\nVXX 780\n.%lo\n,1594\n.0263\n\nSUN 3/50\n.1113\n.0146\n.0032\n\nIBM RT\n.0622\n.0517\n.0652\n\n.3650\n\n.5529\n\n.4907\n\n.lo42\n\n.oooo\n\n.oooo\n\n.oooo\n\n,0309\n.0377\n.0013\n.0004\n\n.0404\n.0549\n.0006\n\n.0443\n.0155\n.0013\n.0003\n\n.2155\n.0054\n.0918\n.0024\n.0018\n.0002\n\n,2282\n.0041\nA924\n.0045\n.0002\n\n.4607\n.1338\n.1302\n.1202\n.0701\n.0501\n.0300\n.0033\n.0010\n.0003\n\n.oooo\n\n.0004\n\n.3915\n,2842\n,0732\n.1531\n.0519\n,0039\n.0232\n,0132\n.0015\n.0005\n\nAmdahl\n.4871\n2015\n.0848\n.1303\n.0508\n\n.oooo\n\n,0309\n.0090\n.0003\n.0002\n\n.2771\n\n.oooo\n.0330\n\n.oooo\n\n.oooo\n\nTable 7.3: Distribution of time for the ten most common operations in Los Alams benchmark. The\nnumbers in bold have a magnitude that i 50% higher than the geometric mean taking the distribus\ns\ntions of the ten machines a sample.\n\nTable 7.3 shows the distribution of the execution times per operations using the estimates for the Los Alamos benchmark. As w e expect on different systems the distribution\nof the operations is different, and some operations affect the total execution time more\nstrongly than others. Although the add and multiply operations represent only 20 percent\nof the total, for the IBhl and the Amdahl they amount to 18 percent of the execution\ntime, but in the case of the Sun 3/50 and the IBM RT-PC/125, this quantity is more than\n70 percent. For the Amdahl 5840 its distribution is quite similar to the dynamic distribution. We cannot conclude from this table t h a t the Amdahl is a more balanced system,\nbecause the execution time of the parameters must be proportional t o the complexity of\neach operation in addition to how many times the operation is executed. We can see this\nmore clearly if we compare the Convex C-1 against the Amdahl 5840. In three of the\nseven programs the Amdahl has better execution times than the Convex (in the predictions the Amdahl has better times in only two). For the Los Alamos benchmark, the Convex has a completely different distribution compared to the Amdahl. but because the\naccess time of an array element. is 3 times faster in the Convex, the total execution time is\napproximately 30 percent less for the Convex. The numbers in bold type in the table are\n50% above the value of the geometric mean of the same parameter when we take as sample all the distributions.\n\nAnalysis of Results and Summary\n\n59\n\nFigures 7.1-7.3 show the systems characterization from a different perspective to help\nus explain the relative performance of the systems. In these figures the value of each\nparameter is normalized with respect to the execution of the VAY-11/780. Instead of\nshowing all the parameters we chose a representative subset of the most executed operations. In particular, arithmetic operations with global variables were omitted given that\non most systems, except the CYBER 205, the execution times are almost the same with\nlocal and global operands.\nThe CRAI\xe2\x80\x99 X-MP executes faster for almost every parameter, especially floating\npoint nrithmet ic operations with single precision, references t o array elements, procedure\ncalls, and intrinsic functions. The first two groups represent the most frequently executed\noperations in scientific programs and for this reason the CRAY executes faster the floating\npoint intensive benchmarks. On the other hand, the scalar floating point arithmetic\noperations with double precision operands are executed faster on the IBM 3090, and even\nthe VAX 8600 has better results than the CRAY X-MP. However as we pointed out in\nsection 4.1, our benchmarks executed using 64-bit floating point numbers on the CRAY\nand CYBER 205.\n\nIn figure 7.2 we see that on the Convex C-1 the execution time of almost all arithmetic parameters is smaller than the VAX 8600, with the exception of the divide operation. This parameter executes slower for single precision floating point and integer data\ntypes. The normalized results for the VAX 8600 show that for almost every parameter\nthe execution on the VAX 8600 is between 4.5 and 7 times faster than the VAX-11/780.\nIn the case of the VAX-11/785, arithmetic operations and intrinsic functions are between\n3 and 4 times faster with respect t o the VAX-11/780, but the difference is less for other\nparameters. TheSSUN 3/50 executes faster integer operations, access t o array elements,\nbranching and loops, but arithmetic operations take more time t o execute. The reason for\nthis is that on the SUN the benchmarks were executed using software emulation of floating point operations.\nThe most interesting aspect of these figures is that the relative performance is not\nuniform: some architectures execute faster for some operations but are slower in others.\nAgain, this tells us that a single figure of merit cannot show all the dimensions of the\nsystem\xe2\x80\x98s performance.\n\n7.2. Future Improvements to Our System\nIn last section we showed that most of our predictions are within 15% of the real\nexecution times and all but three within 20%. The discrepancy between real and\npredicted times is greater on small programs that use a small number of operations, like\nthe Erathostenes sieve, and better on computationally intensive programs. There are still\nsome factors that affect our predictions and they must be taken into account in a new\nversion of the system if we desire to produce better estimates. The following paragraphs\npresent a discussion of these factors.\n\ni)\n\nLocality and Cache Memory. The code we use to measure individual parameters has\na degree of locality in the reference of variables. For this reason, estimates for a program that exhibit less locality than our tests will tend t o produce a larger\ndiscrepancy with respect to its actual execution time. Although scientific programs\nnormally spend most of their time in a small number of DO loops, the amount of\nmemory \xe2\x80\x98touched\xe2\x80\x99 by these loops tends to be very large. Therefore the hit ratio for\n\n~\n\nORIGINAL PAGE IS\nOF POOR QUALITY\n\nAnalysis of Results and Summary\n\n1.000 ?\n\nd\nI\n\nm\nc\n\n0.100\n\n?\n\n0.010\n\n?\n\nn\n\ns\nI\n0\n\nn\n\nI\n\n.\nc\n\nS\n\n+\n\nc\n\n0\n\nR\nS\nL\n\nR\nS\nL\n\nR\nS\nL\n\nR\nS\nL\n\nR\nS\nL\n\nR\nS\nL\n\nCray X-MP/48\n\nx Cyber 205\n\nR\nS\nL\n\nR\nD\nL\n\nR\nD\nL\n\nR\nD\nL\n\nR\nD\nL\n\nR\nD\nL\n\nR\nD\nL\n\ne\n\n.ow\n\n1\n\nR\nD\nL\n\nI\nS\nL\n\ne::\n\nI\n\nI\n\nZ\n\nZ\n\nIBM 3000/200\nVAX- 111780\nI\n\n-\n\ns\n\nL\n\nI\n\ns\n\nL\n\nI\n\ns\n\nL\n\nI\n\ns\n\nL\n\n3\n\nd\nI\n+\n\nm\ne\n\nCray X-MP/48\n\nx Cyber 205\n\n0.100\n\na IBM 3000/200\nVAX-11/780\n\nII\n\ns\n\n0\n\nI\n0\nI\n)\n\nI\ne\n\n*\ns\n\n0.010\n\nE\nN\nD\nL\n\nR\nS\nL\n\nR\nD\nL\n\nI\nS\nL\n\nR\nO\nC\n\nG\nR\nS\n\nG\nR\nD\n\nG\nI\nS\n\nR\nR\nl\n\nR\nR\nZ\n\nR\nR\na\n\nA\nD\nD\n\nO\nT\nO\n\nC\nO\nM\n\nO\nI\nN\n\nO\nO\nV\n\nL\n\nS\n\nT\n\nX\n\nO\n\nl\n\nA\n\nQ\n\nP\n\nC\n\nN\n\nN\n\nR\n\ns\n\ns\n\ns\n\ns\n\nS\n\ns\n\nFIgure 7.1: Parameters normalized against the VkX-11/780 (I). The GRAY executes faster\nfloating point arithmetic operations (single precision) and has the shortest access time for\narray elements. These are the most frequently executed operations in the ten benchmarks.\n\n~~\n\n~~\n\n~~\n\nAnalysis of Results and Summary\n\nreal (double precision)\n\nreal (single precision)\n\ninteger\n\n1.00\n\n-\n\n0.10\n\n-\n\nd\nI\n\nm\nC\n\nn\n\n*\nI\n\n0\n\n"\nI\ne\n\n.\ns\n\n0.01\nO\n\nS\nR\n\nA\nR\n\nM\nR\n\nL\n\nL\n\nL\n\ns\n\ns\n\ns\n\nD\nR\n\ns\n\nL\n\nE\nR\nS\nL\n\nX\nR\nS\nL\n\nT\nR\nS\nL\n\nS\nR\nD\nL\n\nA\nR\nD\nL\n\nM\nR\nD\nL\n\nD\nR\nD\nL\n\nE\nR\nD\nL\n\nX\n\n\\\n\nT\n\nd\nm\ne\n\nn\ns\nI\n\n0.10\n\ne\n\n*\n8\n\n0.01\n\nA\n\nC\n\nA\n\nA\n\nN\n\nR\n\nR\n\nI\n\nR\n\nC\n\nC\n\nC\n\nR\n\nR\n\nR\n\nA\n\nD\nL\n\nS\nL\n\nD\nL\n\nC\n\nS\nL\n\nC\n\nO\nC\n\nP\n\nA\nR\nS\n\nR\nD\n\nI\nS\n\nR\nl\n\nA\n\nR\na\n\nA\n\nR\nS\n\nA\n\nA\n\nM\n\nD\n\nE\n\nX\n\nT\n\n. . . . . . . .\n\n1.00\n\nI\n\nS\n\nVAY 8600\n\\.\'kY-11/780\n\nD\nD\n\nI\n\nFigure 7.2: Parameters normalized against the VAX-11/780(11).\n\n,\n\n,\n\n-\n\n62\n\nreal (single precisionj\n\nreal (double precision)\n\ninteger\n+ VU-11/785\n\n10.0\n\n0\n\nx\n\nVkY-11/780\nRT-PC/125\n\nd\nI\n\nm\ne\nD\n\ns\n1\n0\n\nn\nI\n\n.\n.\ne\n\n1.0\n\n0.1\n\nR\n\nM\nR\n\nR\n\nR\n\nX\nR\n\nT\n\nR\n\nR\n\nR\n\nR\n\nS\nL\n\nS\nL\n\nS\nL\n\nS\nL\n\nS\nL\n\nS\nL\n\nS\nL\n\nD\nL\n\nD\nL\n\nS\n\nA\n\nD\n\nE\n\nS\n\nR\n\nX\nR\n\nT\nR\n\nS\n\nR\n\nI\n\nI\n\nI\n\nD\nL\n\nD\nL\n\nD\nL\n\nD\nL\n\nS\nL\n\nS\nL\n\nS\nL\n\nE\n\nD\n\nhi\n\nA\n\nR\nD\nL\n\nD\n\nM\n\nA\n\nE\n\nI\n\nX\n\nI\n\nS\nL\n\nS\nL\n\nT\n\nI\nS\nL\n\nI\nS\nL\n\n+ VAX-11/785\nVAX-11/780\nx RT-PC/l?L\nA Sun 3/50\n0\n\n10.0\n\nd\nI\n\nm\ne\nI\n)\n\ns\n\n1.0\n\nO\n\nI\n\n.\n.\ne\n\n0.1\nI\n\na\n\na\n\ni\n\ni\n\ni\n\nn\n\n1\n\na\n\n1\n\nA\n\nC\n\nC\n\nC\n\nP\n\nA\n\nA\n\nA\n\nA\n\nR\n\nR\n\nI\n\nR\n\nG\n\nC\n\nG\n\nR\n\nR\n\nD\nL\n\nS\nL\n\nL\n\nS\n\nR\nD\n\nR\nS\n\n1\n\nD\n\nS\nL\n\nO\nC\n\nI\n\n*\n\nA\n\nR\nR\n\n2\n\n*\n\nn\n\nm\n\n1\n\nI\n\nC\n\nC\n\nL\n\nL\n\nE\n\nL\n\nS\n\nT\n\nO\n\nC\n\nO\n\nO\n\nX\n\nO\n\nR\nO\n\nD\nM\n\nT\nN\n\nV\n\nS\n\nP\nS\n\nC\nS\n\nR\n3\n\nD\n\n.\n\n~\n\n9\n\nFigure 7.8: Parameters normalized against the \\rAXl 1/780 (111).\n\n.\n\na\n\n*\n\nO\n\nI\nS\n\nO\nS\n\ns\n\nt\n\nS\n\nA\n\n9\n\nA\n\nN\n\n~\n\n~\n\n~\n\nAnalysis of Results and Summary\n\n63\n\nthe code is high, but for the data is low. 1Ve ran some tests increasing the number of\ndifferent variables inside the body of the test and also increasing the time between\nsuccessive reference t o the same variable. We found that the measurements obtained\nin this way were larger by four to ten percent.\nii)\n\nChange of Environment in Branches. When a branch is taken or a subroutine call is\nexecuted, there is normally a change in the set of variables t h a t are referenced. This\nincreases the number of cache misses and also the total execution time of the program. If the branch jumps to a new page this may cause a page fault along with a\ncontext switch. A context switch normally involves flushing the cache and this has\nthe effect of increasing the execution time of the program. Several parameters that\ncharacterize the \xe2\x80\x98size\xe2\x80\x99 of the branch will help to measure the penalty that we pay as\na function of the distance between the branch and its target.\n\niii)\n\nHardware and/or Software Interlocks. In pipelined machines the time it takes to produce the next result for a particular operation depends on the context in which this\noperation executes. This time normally depends on the functional and data dependencies with respect to the previously scheduled instructions. The data dependencies\nare a function of the source code, the code produced by the compiler, and the\nhardware. We discussed this problem on section 3.4.\n\niv)\n\nhlissing Parameters. In our model there are some simplifications that may increase\nthe discrepancy between our predictions and the real execution times for some programs. An example of this is the access of array elements. We assumed that the\noverhead in accessing an element is constant for different data types and also that\nthis overhead is independent of the context in which the access occur. The traversing of a multidimensional array inside a loop is normally done in a regular way (fixed\nstride). and the compiler may detect that some dimensions remain constant during\nthe whole execution of the loop. With this information the compiler may compute\nthe address for the next element using less operations than it will require if we reference the element outside the loop. Several new parameters are needed t o represent\nall the different variation in the reference of an array element.\n\nv1\n\nLimitations of the Linear Model. The assumption that the cost of executing an operation is independent of the adjacent operations, data dependencies, etc, does not\nremain valid if w e want t o reduce the error in our predictions. Although it is possible to create new parameters that characterize pair of instructions and with this\nkeep the linear model hypothesis, this will create an explosion in the number of\nparameters. An additional disadvantage is that these \xe2\x80\x98compound\xe2\x80\x99 parameters lose\ntheir natural interpretation and it is more difficult to identify weak features in the\nsystems.\n\nvi)\n\nMachine Idioms. Some architectures implement special cases of some instructions\nvery efficiently. On the VAX architecture it is possible t o multiply an integer by two,\nfour or sixteen, then add another integer and use this result as an address during the\nexecution of the same instruction. Unless we know the architecture and how the\ncompiler works it is not possible for us to detect which are the idioms of a given\narchitecture.\n\nvii) Random Soise Produced by Concurrent Activity. Although we discused this in section 4.1, there is still some potential problem when we run in a loaded system.\n\n.4nalysis of Results and Summary\n\n61\n\nIf there is a peak of activity during the execution of an experiment, our measurements will be slightly affected by this \xe2\x80\x98unusual\xe2\x80\x99 high activity. In programs where\nthese paramet,ers are the most executed the \xe2\x80\x98noise\xe2\x80\x99 will increase our figures in a significant way.\n7 3 Summary\n..\nIn this report, we have presented a new paradigm for system characterization and\nperformance evaluation. The principal attribute of this model is t h a t the set of parameters wed in the characterization of systems are the same set of parameters used to estimate t h e expected esecution time of programs. The characterization is achieved by running a set of software experiments that identify, isolate and measure hardware and\nsoftware features. LVe exposed the disadvantages and limitations of using current benchmarks to characterize systems and infer their performance on workloads different from\nthemselves. l y e think that our approach will enrich the area of performance evaluation in\nseveral ways.\n(1) .A uniform \xe2\x80\x98high level\xe2\x80\x99 model of the performance of computer systems allow us to\nmake a better comparison between different architectures and identify their differences and similarities when the systems execute a common workload.\n(2)\n\nCsing the characterization t o predict performance provides us with a mechanism to\nvalidate our assumptions on how the execution time depends on individual components of the system.\n\n(3) Lye can study the sensitivity of the system to changes in the workload, and in this\nway detect imbalances in the architectures.\n\n(4)\n\n,Application programmers and users can identify the most time consuming parts of\ntheir programs and measure the impact of new \xe2\x80\x98improvements\xe2\x80\x99on different systems.\n\n( 5 ) For procurement purposes this is a less expensive and more flexible way of evaluating\ncomputer systems and new architectural features. Although the best way to evaluate\na system is to run a real workload, a more extensive and intensive evaluation can be\nmade using system characterizers to select a small number of computers for subsequent on-site evaluation.\n\nIn the last thirty years we have seen an explosion of new ideas in many field of computer science, but one problem that hasn\xe2\x80\x99t received much attention is how t o make a fair\ncomparison between two different architectures. Given the impact that computers have\nin all aspects of society we cannot afford to continue characterizing the performance of\nsuch complex systems using hlIPS, MFLOPS or DHRYSTONES as our units of measure.\n7.4. Acknowledgements\n\nI want t o give special thanks to my research adviser Prof. Alan J. Smith for suggesting this problem and for his patience and continuous support; his valuable comments\nimproved the quality of this report enormously. Thanks are also due to Eugene Miya and\nKen G. Stevens Jr. for their comments on earlier drafts of this document and for the provision of resources a t NASA Ames, as well as to JosC A. Ambros-Ingerson and Prof.\nDomenico Ferrari who made many useful suggestions during discussions.\n\n8\n\nBibliography\n[BX I85 a]\n\nBailey, D.1-I.. Rarton, J.T.. \xe2\x80\x9cThe NAS Iiernel Benchmark Program\xe2\x80\x9d, NASA\nTechnical Memorandum 86711, August 1885.\n\n[BAI8.5 b]\n\nBailey, D.H., \xe2\x80\x98\xe2\x80\x9cAS Iiernel Benchmark Results\xe2\x80\x9d, Proc. First Int. Conf. on\nSupercomputing, St. Petersburg, Florida. December 16-20, 1985. pp. 341-345.\n\n[BAI87]\n\nBailey, D.H., personal (electronic mail) communication.\n\n[BR,486]\n\nBratten, C., Clark. R., Dorn. P., and Grant, R., \xe2\x80\x9cIBM 3090:\nEngineering/Scientific Performance\xe2\x80\x9d, IBhl\xe2\x80\x99s Technical Report No. GG660245, June, 1986.\n\n[BEE811\n\nBeeler, M., \xe2\x80\x9cBeyond the Baskett Benchmark\xe2\x80\x9d, Computer Architecture News,\nVol. 1, No. 1. )larch 1986.\n\n[BRI86]\n\nBrickner, R.G., IYasserman. II.J., Hayes, A.H., and Moore, J.W., \xe2\x80\x9cBenchmarking the IBXI 3090 with Vector Facility\xe2\x80\x9d, Los Alamos Technical Report\nNO. L.4-UR-86-3300, 1986.\n\n[BLTC85]\n\nBucher, I.Y., Simmons, \\l.L., \xe2\x80\x9cPerformance Assestment of Supercomputers\xe2\x80\x9d,\nVector and Parallel Processors: Architecture, applications, and Performance Evaluation, Editor: M. Ginsberg, t o be published by North Holland.\n\n[BUC87]\n\nBucher, I.Y., and Simmons, L.M. \xe2\x80\x9cA Close Look a t Vector Performance of\nRegister-to-Register Vector Computers and a New Model\xe2\x80\x9d. ACM Sigmetrics\nConference o n ,ifodeling and Measurement of Computer Systems, Banff,\nCanada, May 1887.\n\n[CLA85]\n\nClark, D.W., and Emer. J.S. \xe2\x80\x9cPerformance of the VAX-1 l / X O Translation\nBuffer: Simulation and I\\leasurement\xe2\x80\x9d. Transactions on Computer Systems,\nVol. 3, No. 1, February 1985, pp. 31-62.\n\n[CLA86]\n\nClapp, R.M., Duchesneau, L., Volz, R.A., Mudge, T.N., and Schultze T., \xe2\x80\x9c\nToward Real-Time Performance Benchmarks for ADA\xe2\x80\x9d, Communications o f\nthe ACM, Vol. 29, No. 8, August 1986. pp. 760-778.\n\n[CRA84]\n\nCRAY X - M P and CRAl=l Library Reference Manual, SR-0014, December\n1984.\n\n[CUR751\n\nCurrah B., \xe2\x80\x9cSome Causes of Variability in CPU Time\xe2\x80\x9d, Computer Measurement and Evaluation, SHARE project, Vol. 3, 1975, pp. 389-392.\n\n[CUR761\n\nCurnow, H.J., Mchmann, B.A., \xe2\x80\x9cA Synthetic Benchmark\xe2\x80\x9d, The Computer\nJournal, Vol. 19, No.1, February 1976, pp. 43-49.\n\n[DEN801\n\nDenning, P.J., \xe2\x80\x9cWhat is Experimental Computer Science\xe2\x80\x9d, Communications\no f the ACM, Vol. 23, No. 10, October 1980, pp. 543-544.\n\n[DEN811\n\nDenning, P.J., \xe2\x80\x9cPerformance Analysis: Experimental Computer Science at Its\nBest\xe2\x80\x9d, Communications of the ACM, Vol. 24, No. 11, November 1981, pp.\n725-727.\n\n65\n\n66\n\nBibliography\n\n[D 0 X85]\n\nDongarra, J.J.. \xe2\x80\x9cPerformance of i\xe2\x80\x99arious Computers Using Standard Linear\nEquations Software in a Fortran Environment\xe2\x80\x9d, Computer Architecture\n-Vews, Vol. 13, No. 1. .\\larch 1985, pp. 3-11.\n\n[D 0 N87 a]\n\nDongarra, J.J.. \xe2\x80\x9cThe Linpack Benchmark: An Explanation\xe2\x80\x9d. Supercomputing\nFirst International Conference Proceedings, ,-ithens 1987. Lecture Notes i n\nComputer S c i e n c e 297, pp. 456-473.\n\n[DON87b]\n\nDongarra, J.J.. llartin. J., and Worlton J., \xe2\x80\x9cComputer Benchmarking: paths\nand pitfalls\xe2\x80\x9d, Computer. Vol. 24, No. i, July 1987, pp. 38-43.\n\n[D0N 881\n\nDongarra, J.J.. \xe2\x80\x9cPerformance of Various Computers Using Standard Linear\nEquations Software in a Fortran Environment\xe2\x80\x9d. Computer Architecture\n,Vews, Vol. 16, So. 1. .\\larch 1988, pp. 47-69.\n\n[EME84]\n\nEmer, J.S. and Clark, D.W., \xe2\x80\x9cA Charact,erization of Processor Performance\nin the VAX-11/780\xe2\x80\x9d. Proceedings of the 11th Annual Symposium on Computer ,4rchitecture, .Ann Arbor, Michigan, June 1984.\n\n[F E Li8]\n\nFeldman, S.J., and li\xe2\x80\x99ienberger, P.J., \xe2\x80\x9cA Portable Fortran 77 Compiler\xe2\x80\x9d,\nUNIX 2.2.10 (1981).\n\n[FELT91\n\nFeldman, J.A., and Sutherland, W.R., \xe2\x80\x9cRejuvenating Experimental Computer\nScience\xe2\x80\x9d, C\xe2\x80\x98ommunirations of t h e -4CA4, Vol. 24, No. 11, November 1981,\npp. 497-502.\n\n[FE087]\n\nFeo, J.T., \xe2\x80\x9c,b\nAnalysis of the Computational and Parallel Complexity of the\nLivermore Loops\xe2\x80\x9d, to appear Parallel Computing, 1987.\n\n[FLY721\n\nFlynn, M.J., \xe2\x80\x9cSome Computer Organizations and their Effectiveness\xe2\x80\x9d, IEEE\nTransactions on Computers, C-21 pp. 948-960 (1972).\n\n[GRA82]\n\nGraham, S.L., Kessler, P.B., McKusick, M.K., \xe2\x80\x9cgprof: A Call Graph Execution Profiler\xe2\x80\x9d, Proceedings of the S I G P L A N \xe2\x80\x9982 Symposium on Compiler\nConstruction. SIGPL.L\\N Notices, 1-01. 17, No. 6, pp 120-126, June 1982.\n\n[GRI84]\n\nGriffin, J.H., Simmons, hl.L., \xe2\x80\x9cLos Xlamos National Laboratory Computer\nBenchmarking 1983\xe2\x80\x9c. Los Alamos Technical Report No. LX-10151-MS, June\n1984.\n\n[HOC811\n\nHockney, R.\\V. and Jesshope, C.R.. Parallel Computers (Adam Hilger, Bristol, 1981).\n\n[HOC851\n\nmeasurements on the 2-CPU CRAY X-MP\xe2\x80\x9d,\nHockney, R.W., \xe2\x80\x98\xe2\x80\x98(roo,n1/2,s1/2)\nParallel Computing, 1\xe2\x80\x9801.2, pp. 1-14 (1985).\n\n[HWA84]\n\nHwang, K. and Briggs, F.A., Computer architecture a n d Parallel Processing,\nMcGraw Hill, New York, 1984.\n\n(IBB82]\n\nIbbett, R.N., The Architecture of High Performance Computers (SpringerVerlag, New York, 1982).\n\n[IBM87]\n\nIBM 9090 ITSFORTRAN v.2 Language a n d Library Reference, SC26-422102, 1987.\nKnuth, D.E., \xe2\x80\x9cAn Empirical Study of FORTRAN Programs\xe2\x80\x9d, SoftwarePractice and Experience, Vol. 1, pp. 105-133 (1971).\nKnuth, D.E., The Art of Computer Programming: Vol. 3, Sorting a n d\n\n[KNU71]\n\n[K NU731\n\nSearching, Addison-Wesley, Reading, Mass. 1973.\n\nBibliography\n\n\xe2\x80\x99\n\n67\n\n[LEE841\n\nLee, J.K.F., Smith, A.J., \xe2\x80\x9dBranch Prediction Strategies and Branch Target\nBuffer Design\xe2\x80\x9d, Computer, Vol. 17, No. 1, January 1984, pp. 6-22.\n\n[L I N 86a]\n\nLindsay, D.S., \xe2\x80\x9cMethodology for Determining the Effects of Optimizing Compilers\xe2\x80\x9d, CMG 1986 Conference Proceedings, Las Vegas, Nevada, December\n9-12. 1986, pp. 366-373.\n\n[LI N 86b]\n\nLindsay, D.S., \xe2\x80\x9cDO F O R T R A N Compilers Really Optimize\xe2\x80\x9d, CMG Transactions. Spring 1986, pp. 23-27.\n\n[LUB85]\n\nLubeck, O., Moore, J., and Mendez, R.,\xe2\x80\x9c.A Benchmark Comparison of Three\nSupercomputers: Fujitsu VP-200, Hitachi S810.20, and CRAY XY-MP/12\xe2\x80\x9d,\nProceedings o f the First Internatioriul Conference on Supercomputing Systems, St. Petersburg, Florida, December 16-20, 1985, pp. 320-329.\n\n[MAC841\n\nMacDougall, M.H., \xe2\x80\x9cInstruction-Level Program and Processor hlodeling\xe2\x80\x9d,\nComputer, Vol. 7 No. 14, July 1982, pp. 14-24.\n\n[MARS]\n\nMartin, J.L., \xe2\x80\x9cPerformance Evaluation: Applications and Architectures\xe2\x80\x9d,\nProc. Second Int. Conf. o n Supercomputing, Vol. 111, pp. 369-373.\n\n[M C C X ]\n\nMcCraken, D.D., Denning, P.J., Grandin, D.H., \xe2\x80\x9c,An ACXI Executive Cornmittee Position on the Crisis in Experimental Computer Science\xe2\x80\x9d, Communications o f t h e ,4C\xe2\x80\x98Ad, Vol. 24, No. 11, November 1981, pp. 503-504.\n\n[MCM86]\n\nMcMahon, F.H., \xe2\x80\x9cThe Livermore Fortran Kernels: A Computer Test of the\nFloating-Point Performance Range\xe2\x80\x9d, Lawrence Livermore National Laboratory, UCRL-53745, December 1986.\n\n[MER831\n\nMerrill, H.W., \xe2\x80\x9cRepeatability and Variability of CPU timing in Large IBM\nSystems\xe2\x80\x9d, CMG Transactions, Vol. 39, March 1983.\n\n[MIP8i]\n\nMIPS Computer Systems, \xe2\x80\x9cA Sun-4 Benchmark Analysis\xe2\x80\x9d, July 1987.\n\n[PAT821\n\nPatterson D., \xe2\x80\x9cA Performance Evaluation of the Intel 80286\xe2\x80\x9d, Computer\nArchitecture News, Vol. 10, No. 5, September 1982, pp. 16-18.\n\n[PEU77]\n\nPeuto, B.L. and Shustek, L.J., L\xe2\x80\x99AnInstruction Timing Model of CPU Performance\xe2\x80\x9d, The fotrrth Annual Symposium on Computer Architecture, Vol. 5 ,\nNo. 7, March 1977, pp. 165-178.\n\n[POW831\n\nPower, L.R., \xe2\x80\x9cDesign and Use of a Program Execution Analyzer\xe2\x80\x9d, IBhd Systems Journal, Vol. 22, No.3, pp. 271-292, 1983.\n\n[SHI87]\n\nShimasaki, M., \xe2\x80\x9cPerformance Analysis of Vector Supercomputers by\nHockney\xe2\x80\x99s Model\xe2\x80\x9d, Proe. Second Int. Conf. on Supercomputing, Vol. 111, pp.\n359-368.\n\n[SIM87]\n\nSimmons, M.L. and Wasserman H.J., \xe2\x80\x9cLos Alamos National Laboratory Computer Benchmarking 1986\xe2\x80\x9d, Los Alamos National Laboratory, LA-10898-MS,\nJanuary 1987.\n\n[SMI82]\n\nSmith, A.J., \xe2\x80\x9cCPU Cache Memories\xe2\x80\x9d, ACM Computing Surveys, Vol. 14,\nNo. 3, September 1982, pp. 473-530.\n\n(SMI881\n\nSmith, A.J., paper in preparation.\n\n[WE1841\n\nWeicker, R.,P., \xe2\x80\x9cDhrystone: A Synthetic Systems Programming Benchmark\xe2\x80\x9d,\nCommunications of the ACM, Vol. 27, No. 10, October 1984.\n\n68\n\nBibliography\n\n\xe2\x80\x9c,.\n\n[WE821\n\nWiece.., ,4.C.,\ncase St.udy o r\nInstruction Set Usage for Compiler\nExecution\xe2\x80\x9d. Symposium on Architectural Support for Programming\nLanguages a n d Operating Systems, Palo Alto, California, March 1-3, 1982,\npp. 177-184.\n\n[LVOR84]\n\nIVorlton, J., \xe2\x80\x9cUnderstanding Supercomputer Benchmarks\xe2\x80\x9d, Datamation,\nSeptember 1, 1984. pp. 121-130.\n\n9\n\nAppendix\n\nRegion 1: Floi ng Point Arithmetic Operat ns (sing11\nERSL\nmachine\n5\nCRAY X-MP/48\n73\n145\n352\n86\nCYBER 205\n73\n83\n847\n168\n1<\nIBM 3080/200\n34\n74\n128\n564\n238\nArndahl 5840\n118\n341\n407\n1028\n812\nConvex C1\n385\n514\n?em\n104\n172\nVAX 8600\n55\n421\n575\n1583\n1101\nVAX-111785\n387\n1470\n2021\n4200\n3876\nVAX- 111780\n547\n3601\n7310\n10135\n11422\nIBM RT-PCl125\n3821\n16480\n?0525\n??874\n31633\nSUN 3/50\n767\n26364\n41204\n46172\n58030\n\n-\n\nlocal)\nXRSL\n6\n\n5155\n8844\n6358\n42516\n28621\n221337\n703410\n2104654\n1265778\n4745064\n\nRemon 2: Floatine Point Arithmetic ODerations (double, local)\nSRDL ARDL MRDL\nERDL I XRDL\nDRDL\nmachine\n0\nI\n\n11\n\nd\nCYBER 205\nIBM 3080/200\nArndahl5840\nConvex C1\nVAX 8800\nVkY-lll785\nVAX- 111780\nIBM RT-PCl125\nSUN 3/50\n\n1<\n\n43\n60\n180\n183\n1038\n831\n833\n1<\n\nI\n\n1128\n\n847\n74\n240\n481\n805\n3374\n12188\n15102\n56163\n\n3170\n014\n1541\n4263\n5885\n8312\n51281\n18487\n136108\n\n707\n137\n432\n820\n1681\n5018\n27778\n16324\n101565\n\n3620\n383\n782\n130\n2023\n10177\n35004\n27030\n114533\n\n55350\n23300\n42170\n48303\n210304\n651684\n2007e80\n1217774\n5568884\n\nRerrion 3:lnteeer Arithmetic Ooerations single, local)\nSISL\nAlSL\nMlSL\nDlSL\nmachine\n15\n16\nCRAY X-MP/48\n86\n1<\nCYBER 205\n39\n1130\n101\n221\n1<\nIBM 3000/200\n75\n148\n430\n167\n354\n1<\nAmdahl5840\n68\n684\n347\nme\n1<\nConvex C1\n303\n407\n766\n1<\nVAX 8600\n345\n865\n1821\n1<\nVAX-111785\n1182\n2587\n5058\n1<\nVAX-111780\n1e24\n7860\n16151\n1<\nIBM RT-PC1125\n11\n18\n4640\n0804\n1<\nSUN 3/50\n202\n27eoo\n33418\n1<\n\nTRSL\n7\n276\n74\n62\n420\n676\n450\n2645\n3307\n6034\n2843\n\nTRDL\n14\n103\n60\n68\n\n170\n1110\n1035\n5821\n6103\n1010\n5024\n\nY\n\nI\n\nTable 91\n.:\n\nCharacterization results for regions 1-3. A value 1\n\nnot detected by t h e experiment.\n\n69\n\nTISL\n21\n307\n20\n80\n178\n\n627\n422\n2865\n2418\n1039\n1<\n\n< indicates that the parameter was\n\n70\n\nAppendix\n\nCYBER 205\nIBM 3080/?00\nAmdahl5840\nConvex C1\nVAX 8600\nVAX- 11/785\nVAX- 11/780\nIBM RT-PC/125\nSUN 3/50\n\nARSG\n23\n\nDRSG\n25\n348\n1382\n568\n1033\n2658\n1629\n4205\n10148\n23784\n47488\n\nMRSG\n24\n\n72\n\n138\n\n354\n\n470\n\n76\n\n138\n\n338\n396\n553\n1445\n3628\n16776\n26518\n\n4Og\n\n1\n\n523\n688\n2004\n7415\n21058\n38684\n\n238\n\n11312\n\n52740\n\n1714\n4708\n25783\n17084\n96362\n\nERSG\n\n818\n186\n1060\n3871\n11443\n31328\n61871\n\nSRSG\n27\n5168\n8538\n5370\n43108\n27885\n220843\n675706\n2091213\n1260805\n4778238\n\n32\n\nSRSG\n22\n83\n-CRAY s - M P / ~ ~\n160\nW E R 205\n37\nlBhl 3090/200\n118\nAmdahl 5840\n74\nConvex C1\nVAX 8600\n88\n486\nVAX- 11/785\nVAX-11/780\n483\n3820\nIBM RT-PC/1?5\n215\nSUN 3/50\nmachine\n\n33\n\n34\n\n3785\n914\n1552\n4318\n5887\n8308\n48574\n18113\n134008\n\n3788\n288\n795\n84\n2808\n8575\n34300\n27205\n112031\n\n55516\n22404\n42703\ni6083\n220448\n648898\n2067708\n1216348\n5625248\n\nR e d o n 6 lnteeer Arithmetic 0 rations\n:\n,\n.\nDISC\nMISG\nAlSG\n38\n38\n37\n\nmachine\nCRAY X-MP/48\nCYBER 205\nIBM 3080/?00\nAmdahl 5840\nConvex C1\nVAX 8600\nVkY-11/785\nVkY-11/780\nIBM RT-PC/125\nSUN 3/50\n\nmach ine\nCRAY X-MP/48\nCYBER 205\nIBM 3080/200\nhdahl5840\nConvex C1\nVAX 8600\nVAX-111785\nVAX-11/780\nIBM RT-PC/l25\nSLW 3/50\n\n86\n\n1<\n1<\n\n387\n\n1<\n\n74\n288\n535\n1185\n16??\n987\n420\n\n82\n\n1<\n1<\n1<\n1<\n\n1<\n1<\n\n432\n188\n145\n221\n276\n615\n1871\n7343\n4884\n32?0\n\nRegion 7: Conditional\nCRSL CRDL\n45\n44\n\nANDL\n43\n52\n46\n122\n110\n302\n300\n888\n1170\n583\n568\n\n207\n167\n124\n477\n515\n644\n4138\n3161\n11820\n13372\n\n1247\n447\n130\n284\n800\n860\n\n2641\n4068\n11850\n2m4\n\n708\n1482\n442\n687\n\n2651\n1585\n7653\n11063\n7801\n6642\n\n26\n\n85\n312\n246\n\nTRSG\n28\n273\n160\n97\n431\n648\n445\n2311\n2322\n\n54?0\n3401\n\nI\n\nngle. global)\n- .XISG\nEISG\n41\n40\n785\n407\n346\n467\n545\n200\n350\n632\n387\n768\n960\n1851\n2780\n5884\n7812\n16205\n10063\n4767\n33728\n28881\n\n35\n235\n88\n164\n1081\n874\n4842\n5422\n2808\n1<\n\nTISG\n42\n307\n201\n103\n173\n624\n\n558\n2882\n2415\n1145\n854\n\nd Logical Parameters\nCISL\n\nI\n\nANDG\n\n151\n488\n\n116\n\n407\n1670\n\n318\n\n1281\n\n674\n1158\n\nCRSG\n\n628\n141\n476\n502\n821\n2248\n3003\n12282\n14083\n\nI CRDG\n1074\n258\n238\n806\n865\n\n3025\n5023\n11820\n25556\n\nClSG\n50\n202\n548\n155\n150\n507\n778\n1670\n2305\n1643\n1615\n\nTable 9.2: Characterization results for regions 4 7 . A value 1< indicates t h a t the parameter was\nn o t detected by the experiment.\n\n71\n\nhppendix\n\nRegions 8 , Q: Function Call, Arguments and References to Array Elements\nARR2 ARR3\nPROC\nAGRS AGRD I AGIS I ARRl\n53\n56\n51\n52\n57\nmachine\n165\n214\n00\n500\n142\nCRAI\xe2\x80\x99 S-.\\1P/48\n521\n560\n5322\n330\n801\nCYBER 205\n103\n201\n1242\n178\n322\n565\n1BM 3000/200\n1088\n100\n80\n176\n715\nAmdahl 5840\n47 1\n74\n348\n570\nOf38\n5270\nConvex C1\n674\n5848\n512\n1322\n2728\nVAX 8800\n1078\n1708\n20808\n602\n3613\n6046\nVAX-ll/i85\n3070\n21716\n3165\n11820\n20263\nVAX-l1/780\n428\n1078\n3566\n7014\n2612\nIBM RT-PC/125\n8588\n15164\n15326\n2452\n6328\nSUN 3/50\n\nIADD\n58\n1<\n6\n\n7\n5\n1<\n\n1<\n1<\n1<\n06\n037\n\nR k o n 10: Branching and DO loop Parameters\nmachine\nCRAY X-MP/48\nCYBER 205\nIBM 3000/200\n.amdahl 5840\nConvex C1\nVAX 8600\nVAX-111785\nVAX- 111780\nIBM RT-PC1125\nSUN 3/50\n\nGCOM , LOIN LOOV\n60\n\xe2\x80\x99 61\n62\n=\n451\n003\n240\n1000\n838\n176\n234\n654\n107\n\nGOT0\n50\n23\n20\n41\n\nLOIX\n63\n473\n1542\n754\n\nLOOX\n64\n265\n200\n243\n\n1402\n1386\n3884\n4825\n3154\n?428\n\n-\n\n1<\n\n163\n1<\n468\n45\n385\n\n-\n\n686\n\n-\n\n1531\n2523\n3302\n6874\n6230\n2857\n\n20\n\n3470\n4660\n18123\n7085\n4444\n6237\n\n722\n550\n1384\n5138\n2038\n040\n\n2078\n3682\n1<\n0783\n8641\n7508\n\n-\n\nRegion 11: Intrinsic Functions (single precision)\n\nI\n\n]I\n\nEXPS\n65\n1864\n6116\n5434\n\nb\nmachine\n\nCYBER 205\nIBM 3000/200\nAmdahl 5840\nConvex C1\nVAX 8600\nVAX- 11/785\nVAX-11/780\nIBM RT-PC/125\nSUN 3/50\n\n-\n\n11683\n67253\n205647\n728008\n65016\n\n760844\n\nEXPD\n7\n\nb\nmachine\n\nCYBER 205\nIBM 3000/200\nAmdrhl5840\nConvex C1\nVAX 8600\nVAX-111785\nVAX-11/780\nIBM RT-PC/1?5\nSUN 3/50\n\n70\n62027\n28372\n25433\n\n-\n\n20007\n66552\n200483\n738768\n46847\n2088322\n\nI\n\nI\nI\n\nLOGS\n66\n\n1647\n5248\n5287\n\nI\n\nI\nI\n\n-\n\n67\n1848\n5212\n4865\n\nI\n\nI\n1\n\nTANS\n68\n2046\n6651\n5887\n\n11106\n68678\n181380\n556320\n\n00181\n\n1525150\n\nSIND\n72\n\n25686\n23888\n\n1\n\nI\n\nTAND\n73\n\nI\n\nSQRD\n74\n\n1\n\n32122\n22542\n\n15684\n76417\n240538\n816048\n41634\n2355612\n\nSQRS\n\n11166\n24108\n50720\n186008\n43200\n87532\n\n04385\n\n1278520\n\nI\n\nI\n\n-\n\n-\n\n7455\n42078\n111806\n487725\n\n0068\n76578\n238165\n800876\n50481\n055845\n\nLOGD\n71\n\nSINS\n\n12801\n41673\n1118%\n482781\n38524\n2225082\n\n-\n\n-\n\nTable 9.8: Characterization results for regions 8-12. A value I < indicates that the parameter\nwas not detected by the experiment. The results for the Amdahl 5840 were obtained using\na simpler model.\n\nReport Documentation Page\n2. Government Accession No.\n\n1. Report No.\n\n3. Recipient\'s Catalog No.\n\nNASA CR 1775 1 1\n5. Report Date\n\nDecember 1988\n\nMachine Characterization and Benchmark Performance\nPrediction\n\n6. Performing Organization Code\n\n~~\n\n8. Performing Organization Report No.\n\n7. AuthorM\n\nRafael H. Saavedra-Barrera (University of California,\nBerkeley, CA)\n\n10. Work Unit No.\n\n505-65-01\n\n9. Performing Organization Name and Address\n\n11. Contract or Grant No.\n\nAmes Research Center\nMoffett Field, CA 94035\n\nNCA2- 128\n13. Type of Report and Period Covered\n\n12. Sponsoring Agency Name and Address\n\nContractor Report\nNational Aeronautics and Space Administration\nWashington, DC 20546-000 1\n\n14. Sponsoring Agency Code\n\n15. Supplementary Notes\n.\n-\n\n--\n\nPoint of Contact: K. G. Stevens, Jr., Ames Research Center, MS 258-5, Moffett Field, CA 94035\n(415) 694-5949 or FTS 464-5949\n16. Abstract\n\n.\n\nFrom runs of standard benchmarks or benchmark suites, it is not possible to characterize the\nmachine nor to predict the running time of other benchmarks which have not been run.In this paper, we\nreport on a new approach to benchmarking and machine characterization. We describe the creation and\nuse of a machine analyzer, which measures the performance of a given machine on Fortran source language constructs. The machine analyzer yields a set of parameters which characterize the machine and\nspotlight its strong and weak points. We also describe a program analyzer, which analyzes Fortran programs and determines the frequency of execution of each of the same set of source language operations.\nWe then show that by combining a machine characterization and a program characterization, we are able\nto predict with good accuracy the running time of a given benchmark on a given machine.\nCharacterizations are provided for the Cray X-MP/48, Cyber 205, IBM 3090/200, Amdahl 5840,\nConvex C-1, VAX 8600,VAX 11l785, VAX 11D80, SUN 3/50 and IBM RT-PC/125, and for the\nfollowing benchmark programs or suites: Los Alamos (BMK8A1), Baskett, Linpack, Livermore Loops,\nMandelbrot Set, NAS Kernels, Shell Sort,Smith, Whetstone and Sieve of Erathostenes.\n\n18. Distribution Statement\n\n17. Key Words (Suggested by Author(s))\n\nExecution profilers\nPerformance prediction\nMachine characterization\n19. Security Classif. (of this report)\n\nUnclassified\n\nUnlimited-Unclassified\nSubject category: 62\n20. Security Classif. (of this page)\n\nUnclassified\n\n21. No. of pages\n\n83\n\n22. Price\n\nA04\n\n'