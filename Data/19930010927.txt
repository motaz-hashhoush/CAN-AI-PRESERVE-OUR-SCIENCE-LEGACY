b"IIIIII 111111ll111111111111111111ll111111111111111ll111111111111111111\nUnited States Patent\n\nUS005 195170A\n\n[191\n\n[ill\n\nEberhardt\n\n1451\n\nNEuRAcNETWOFtK DEDICATED\nPROCESSOR FOR SOLVING ASSIGNMENT\nPROBLEMS\nInventor: Silvio P. Eberbardt, Pasadena, Calif.\nAssignee: The United States of America as\nrepresented by the Administrator of\nthe National Aeronautics and Space\nAdmiitration, Washington, D.C.\nAppl. No.: 744,042\nFiled:\nAug. 12,1991\nInt. Cl.5 ..............................................\nco6F 15/18\nU.S. C . ........................................\nI\n395/24; 395/1 I;\n395/23\nField of Search ............\n395/24, 11, 27, 23;\n364/402\nReferences Cited\nU.S. PATENT DOCUMENTS\n3,134,017 5/1964 Burhans et al. .....................\n364/402\n\n4,660,166 4/1987 Hopfield .............................\n4,866,645 9/1989 Lish .....................................\n5,016,188 5/1991 Lan .......................................\n5,047,65S 9/1991 Chambost et al. ....................\n\n364/807\n364/602\n395/24\n395/24\n\nOTHER PUBLICATIONS\nMajani, et al., \xe2\x80\x9cOn the K-Winners-Take-A11 Network,\xe2\x80\x9d Advances in Neural Network Information Systems 1, D. Touretzky, ed., pp. 634-642, Palo Alto,\nCalif., Morgan Kaufman Publishers, Inc. 1989).\nBrown, et al., \xe2\x80\x9cNeural Network Design of a Banyan\nNetwork Controller,\xe2\x80\x9d IEEE Journal of Selected Areas\nin Communication, vol. 8, No. 8, pp. 1428-1438, (1991).\nMoopenn, et al., \xe2\x80\x9cA Neural Network for Euclidian\nDistance Minimization,\xe2\x80\x9d Proc. IEEE, Intl. Conf. on\nNeural Networks, v61: 11, pp. 349-356, San Diego,\nCalif., Jul. 24-27, (1988).\nJ. Munkres, \xe2\x80\x9cAlgorithms for the Assignment and Transportation Problems,\xe2\x80\x9d Journal of the Society for Industrial Applications of Mathematics, vol. 5, No. 1, pp.\n32-38, (1957).\nS. Blackman, \xe2\x80\x9cMultiple-Target Tracking with Radar\nApplications,\xe2\x80\x9d Artech House Inc., Norwood, Mass., pp.\n397-403, (1986).\nHopfield, et al., \xe2\x80\x9cNeural Computation of Decisions in\nOptimization Problems,\xe2\x80\x9d Biol. Cybern., 52:147-152,\n(1985).\nTagliarini, et al., \xe2\x80\x9cA Neural-Network Solution to the\n\nPatent Number:\nDate of Patent:\n\n5,195,170\nMar. 16. 1993\n\nConcentrator Assignment Problem,\xe2\x80\x9d Proc. IEEE Conf.\non Neural Information Processing Systems-Natural and\nSynthetic (1988).\nKirkpatrick, et al., \xe2\x80\x9cOptimization by Simulated Annealing,\xe2\x80\x99\xe2\x80\x99 Science, vol. 220, pp. 670-680, (1983).\nBilbro, et al., \xe2\x80\x9cOptimization by Mean Field Annealing,\xe2\x80\x9d\nAdvances in Neural Information Processing Systems I,\nTouretzky, ed., Morgan, Kaufman Publishers, Inc., pp.\n91-98, (1989).\nPeng, et al., \xe2\x80\x9cA Connectionist Model for Diagnostic\nProblem Solving,\xe2\x80\x9d IEEE Trans. on Systems, Man and\nCybernetics, vol. 19, No. 2, pp. 285-289, (1989).\nSaeckinger, et al., \xe2\x80\x9cA Versatile Building Block: The\nCMOS Differential Difference Amplifier,\xe2\x80\x9d IEEE J.\nSolid-state Circuits, SC-22, No. 2, pp. 287-294, Apr.\n1987.\nLazzaro, et al., \xe2\x80\x9cWinner-Take-All Networks of 0 0\nComplexity,\xe2\x80\x9d Technical Report Caltech-CS-TR-21-88, California Institute of Technology 1988.\nAnalog Devices, \xe2\x80\x9c1990/9 1 Linear Products Databook,\xe2\x80\x9d\nAnalog Devices, Inc., 1990, p. 4-61.\nHillier et al., Introduction to Operations Research,\nFourth Edition, Holden-Day, Inc., 1986, p. 217.\nPrimaiy Examiner-Allen R. MacDonald\nAttorney, Agent, or Firm-John H. Kusmiss; Thomas H.\nJones; Guy M. Miller\n1571\nABSTRACT\nA neural network processor for solving first-order competitive assignment problems consists of a matrix of\nN X M processing units, each of which corresponds to\nthe pairing of a first number of elements of {Rj} with a\nsecond number of elements {Cj}, wherein limits of the\nfirst number are programmed in row control superneurons, and limits of the second number are programmed\nin column superneurons as MIN and MAX values. The\ncost (weight) Wgi of the pairings is programmed separately into each PU. For each row and column of PUS,\na dedicated constraint superneuron insures that the\nnumber of active neurons within the associated row or\ncolumn fall within a specified range. Annealing is provided by gradually increasing the PU gain for each row\nand column or increasing positive feedback to each PU,\nthe latter being effective to increase hysteresis of each\nPU or by combining both of these techniques.\n10 Claims, 6 Drawing Sheets\n\nUaSa Patent\n\nMar. 16, 1993\n\nI\n\nQ\n\n1 NxN\n\na\ne\n\n0\n\nI\nON&--\n\n*\n\nm\n\nINPUT ;\n\n..\n.\n.\ne\n.\n\n-\n\nN-ELEMENT\n\nGEURALARRAY\n\n.\ne\n\nJ\nR THRESHOLDER\n?E\n?R\n\nFIG. I\n(Prior Art)\n\n5,195,170\n\nSheet 1 of 6\n\nU.S. Patent\n\nMar. 16,1993\n\n5,195,170\n\nSheet 2 of 6\n\no n\n\nU,S. Patent\n\nSheet 3 of 6\n\nMar. 16,1993\n\n5,195,170\n\nCOL. EXCITE\n\nFIG.3\n\n\\\n\nPROCESSING UNIT\n(PU)\n10\n\nSense Line 13a (or 14a)\n\nMIN7\nROW ADD.\n\n34-135\nI\n\ncollADD.\nD\n\n- 3 6\n\nMAXJ\n\nFIG. 4\n\nTIME CONSTANT\n\nII\n-or12\n\nU.S. Patent\n\nMar. 16, 1993\n\nSheet 4 of 6\n\n--t----i\nI\n\nI\n\nI\nI\nI\n\nI\nI\nI\nI\nI\n\nI\n\nI\nI\n\nI\n\nI\nI\n\nI\n\nI\n\nI\n\nII\nI\n\nI\n\nN\n\n5,195,170\n\nU.S. Patent\n\nMar. 16, 1993\n\nSheet 5 of 6\n\n5,195,170\n\nI\n\nI\n\nI\n\nI\n\nw\n\nI\n\nI-I\n\nI\nI\n\nI\nI\n\nJ\n\nIL/:\n\nI\n\nI\n\nI\nI\nI\nI;\nI\n\ng\n\nt-\n\n-J\nW\n\n2\nJ\nIv)\n\n2\n\n%!\nLL\nLL\n\n0\n\n-7-\n\nI\nI\n-J\n\nU.S. Patent\n\nSheet 6 of 6\n\nMar. 16, 1993\n\nV\n\nVARIABLE GAIN\nANNEALING\n\nFIG.7\n\nOFF\n\nOFF^-\n\n#\nv\n\nFIG.8\n\n5,195,170\n\nVARIABLE\nFEEDBACK\nANNEALING\n\n5,195,170\n\n1\n\nNEURAL-NETWORK DEDICATED PROCESSOR\nFOR SOLVING ASSIGKMEhT PROBLEMS\nORIGIN OF T H E INVENTION\n\n5\n\nThe invention described herein was made in the performance of work under a NASA contract, and is subject to the provisions of Public L~~ 96-517 (35 usc\n202) in which the contractor has elected not to retain\n10\ntitle.\nTECHNICAL FIELD\nThe invention relates to a neural-network processor\nfor solving\noptimization problems, and\nparticularly to solving first-order assignment problems l5\nsuch as\nallocation\nscheduling of\nsegmented data cells queued at input terminals of an\nasynchronous transfer mode (ATM) telecommunication switching, and other first-order assignment prob- 2o\nlems.\nBACKGROUNDART\nA neural-network architecture for competitive assignment requires that elements of two Sets (e.g., reSources and targets) be associated with each other so as 25\nto minimize the total Cost of the associations. It is assumed that all pairwise Costs of associating an element\nofone set with an element ofthe others are given. such\nan assignment is a class of global optimization problems 3o\nthat pose a computationa~ bottleneck in a number of\ncomputation-intensive real-time systems. In the assignment paradigm, members of one set {R;1 Z i s M } (e.g,,\nresources) are\xe2\x80\x98to be assigned to members of another set\nassign- 35\nor targets).\n(c:] Z j s N } (e.g.,\nmen1 is denoted by a binary matrix x={Xij), where\n1 if & is associated with cj and\notherwise.\nThe total cost of a selected assignment is given by\n\n+ i=lj=l\n.? ?\n\nCOSTTOTAL(X)\n\nWeij\n\n(\xe2\x80\x99)\n\nwhere Wo is the pairwise assignment cost matrix. Assignments are not arbitrary; \xe2\x80\x9cblocking\xe2\x80\x9d constraints\nspecifying the number of elements of {Cj} that may be\nassociated with each element Ri, and the number of\nelements of {R;} that may be associated with each Cjare\ngiven as well. While one-to-one blocking constraints are\nmost commonly encountered, some applications require\none-to-several or several-to-several associations. Thus,\nthe assignment problem is to choose an assignment x\nthat minimizes C O S T ~ ~ ~ Land satisfies the block(X)\ning constraints.\nFrom a circuit implementation standpoint, the traditional additive neural network for assignment requires a\ntwo-dimensional matrix of neurons referred to hereinafter as processing units, PUS, and therefore the connection matrix requires a third dimension. As current VLSI\ntechnology is essentially two-dimensional, only small\nadditive networks may be implemented on standard die\nsizes, and even nonstandard technologies such as waferscale implementations and chip stacking may limit N\nand M to a hundred or so. Larger problem sizes require\na reformulation of the network model to reduce connectivity. The new architecture presented below has only a\nfew times more connections than PUS and is compatible\nwith VLSI implementation. Means have been developed for applying association costs, satisfying blocking\n\n40\n\n45\n\n2\n\nconstraints, forcing PUS to hypercube corners and implementing annealing.\nA constraint on the number of elements of {Ri} that\nmay be associated with Cj is equivalent to a constraint\non the number of activated PUS in column j of the PU\nmatrix. Similarly, constraints on the number of elements\nof {ci>\nassociated with Rispecify how many pus may\nbe on in row i. Focusing on a single row, a circuit that\nforces Only One neuron to turn On* thus enforcing a\none-to-one assignment, is the winner-take-all (WTA).\nThe WTA, via mutual inhibition, allows only the PU\nwith the maximum input excitation to become actiOther pus are turned Off. The wTA\nvated;\nhas been extended to allow multiple winners in the\nk-WTA formulation of Majani, et a1.y \xe2\x80\x9cOn the k-winners-take-all network,\xe2\x80\x9d Advances in Neural Information Systems 1, 0.\nTouretzky, ed., pp. 634-642, Palo\nAlto, Calif., Morgan Kaufrnan Publishers, Inc. (1989).\nthe k-WTA\neach Riand Cjtobe\nassociated with a programmable number k,A or kf of\nelements of the other set, the new architecture presented below has generalized the method one step further to allow elements to be independently associated\nwith a range or window of eh\xe2\x80\x99lents of the other set.\nThe resulting function, the k,m-WTA, uses the variable\nPair (km) to specify the minimum and maximum number of associated elements. By setting k=m, the k-WT.4\none-to-several association function is obtained, and by\nsetting k=m= 1, one-to-one association may be specified. obtained, and by setting k = m = 1, one-to-one association may be specified.\nEach row and each column is connected using the\nk,m-WTA. The question whether these multiply overlapping winner-take-all circuits can still function correctly was addressed by Brown and Liu, \xe2\x80\x9cNeural network design of a Banyan network controller,\xe2\x80\x9d IEEE\nJournal of Selected Areas in Communication, Vol. 8,\nNo. 8, pp. 1428-1438, (1991), who found that WTA and\nk-WTA circuits still function correctly when overlapped arbitrarily. Note, however, that it is possible to\nspecify inconsistent blocking constraints which cannot\nall be satisfied simultaneously. Letting (k,x,m,a) and\n(k,c,m,c) be the row and column constraints (MIN,MAX), respectively, a necessary condition for consistency is\nM\nN\nM\nI kiR S I mfand ,I rn?\ni=l\nj+l\nI+ 1\n\n,\xe2\x80\x98\nI\n\n2 I kr\n\nj=1\n\n(2)\n\nJ \xe2\x80\x99\n\n50\n\nThe association Costs Wij are distributed throughout\nthe interconnection matrix. A key to the present assignment architecture is that association costs are subtracted\nlocally from the input PUS representing the assign55 ments, Moopenn, et al., \xe2\x80\x9cA neural network for euclidian\ndistance minimization,\xe2\x80\x9d Proc. IEEE, Intl. Conf. on\nNeural Networks, Vol. 11, pp. 349-356, San Diego,\nCalif., Jul. 24-27, (1988). This tends to inhibit PUS with\na high association cost while pushing PUS with low\n60 energy costs towards activation. Consequently, lowcost assignment configurations are favored by the k,mWTA circuits. processor can be designed with connections only along rows and columns of a matrix of PUS\n(to implement the k,m-WTA) and a few global connec65 tions to all PUS.\nThe complexity of the assignment problem may be\nunderstood by considering that for sets of N and M\nelements, the \xe2\x80\x98number of possible solutions satisfying\n\n3\n\n5,195,170\n\n4\n\nTraditionally, global optimization problems of this\none-to-one (and in some cases one-to-zero) blocking\ncompetitive assignment type have been solved by comconstraints is N!/(N-M)!, assuming NaM. Because the\nproblem requires a global solution, it cannot be decomputationally exhaustive searches where the number of\nposed without risk of losing the optimal solution. Nevcomputations increases as the cube of the number of\nertheless, when fast solutions are required, the problem 5 resources. The computational load may be reduced by\nheuristic means, but in that case the solution is not guaris generally solved either by decomposition or limited\nanteed to be optimal or even good.\nbest-fit search. Search algorithms that find optimal soluA means for overcoming the limitations in computations include Munkres\xe2\x80\x99 method which has a computation speed imposed by serial digital computers is paraltional complexity of O(MN2). See J. Munkres, \xe2\x80\x9cAlgorithms for the assignment and transportation problems,\xe2\x80\x9d 10 lel processing, whether digital or analog. One possible\nmethod for solving assignment problems is based on\nJournal of the Society for Industrial ADDliCatiOnS, of\nmassively parallel analog computation using neural\nMathematics, Vol. 5,No. 1, pp. 32-38,\xe2\x80\x99(1957) and S.\nBlackman, Multiple-Target Tracking with Radar Apnetworks.\nThe neural network architecture that has received\nplications, Artech House Inc., Norwood, Mass., pp.\n397-403, (1986). While more efficient than exhaustive 15 the most attention for solving optimization problems is\nthe Hopfield network described by Hopfield and Tank,\nenumeration, such algorithms remain computation\xe2\x80\x9cNeural computation of decisions in optimization probintensive, requiring seconds or minutes of processing on\nlems,\xe2\x80\x9d Biol. Cybern, 52:147-152, (1985). This architecpowerful computers. These methods are therefore not\nture, shown in FIG.1, consists of an array of processing\nsuitable for many real-time applications. Furthermore,\nfor problems with one-to-several and several-to-several 20 operational amplifiers (called neurons but generally\nreferred to herein as processing units) interconnected\nblocking constraints, the number of possible solutions\ncan be considerably larger and no known efficient\nfully via a matrix of connecting elements of programmable strength (called synapses). The connection\nsearch algorithms exist. Practical applications that fit\nstrengths of the synapses are variable and it is these\ninto this assignment paradigm are detailed below.\nIn the military domain, an obvious and time-honored 2s weight variables that determine the behavior of the\napplication is weapon-target association. Of particular\nneural network.\nimportance is .neutralization of intelligent missiles,\nWith few exceptions, neural network applications\nhave been mapped onto the additive networks by first\nwhich have attained a position of primary importance\namong munitions in today\xe2\x80\x99s military defense tactics. In\nformulating a Lyapunov function that has energy minmany cases, the time between threat detection and 3 0 ima which correspond to the desired solutions. Typicountermeasure deployment is on the order of seconds,\ncally, this \xe2\x80\x9cenergy equation\xe2\x80\x9d is composed of simple\nquadratic terms each of which is minimized when a\nso the automated detection and decision-making comparticular application constraint is satisfied. From this\nponents of a defense system must be highly optimized\nequation, the interconnection weight matrix is then\nfor speed, especially if humans are in the loop as well. In\nan idealized situation, the various threats are identified 35 derived by a simple transformation. An alternative apand the most appropriate countermeasures applied.\nproach is to use heuristics to implement required conIndeed, it may be necessary to allocate several disparate\nstraints directly.\nThe system must be configured such that only as\ncountermeasures to the more serious threats, or perhaps\nmany PUS can be active in each row or column as alin some cases several threats may be dealt with by one\ndefensive system. Consequently, the traditional one-to- 40 lowed by the blocking constraints, and association costs\nmust be incorporated such that as the system settles, or\none assignment paradigm is no longer adequate.\nAnother application is radar tracking of aircraft (e.g.,\nrelaxes, the better solutions have a competitive advanat large airports) or missiles (e.g., ICBM tracking as a\ntage over the other. The PUS apply a nonlinear saturatcomponent of Strategic Defense Initiative systems). In\ning function to the sum of input signals. When they are\norder to track individual objects from a swarm of tens 4s allowed to take on an analog range of values, as would\no r hundreds, \xe2\x80\x9cblips\xe2\x80\x9d from successive radar sweeps must\ngenerally be the case in hardware implementations, the\nbe matched. An alternative approach is to use Kalman\nsolution space takes the form of an MN-dimensional\nfilters to estimate the bearing and velocity of the indihypercube, the corners of which correspond to the\nvidual objects and to assign the instantaneous estimated\nassignment decisions. T o form a complete, unambigupositions to the actual positions determined by radar. 5 0 ous solution X, all PUS must relax to saturated \xe2\x80\x9con\xe2\x80\x9d.and\nFinally, an application that is becoming increasingly\ncompletely \xe2\x80\x9coff\xe2\x80\x99 states.\n-This additive network has been used for solving, by\nimportant as computer and national telecommunication\nnetworks convert to the transmission of information\ncomputer simulation, the concentrator assignment\nproblem, a one-to-several assignment problem in which\n(e.g., audio, fax, video) by discrete units of digital data,\nis high-speed network packet switching. Within a com- 55 multiple elements of the one set (named \xe2\x80\x9csites\xe2\x80\x9d) are\nmatched with each element of the other set (named\nmunications network the function of a switching node\n\xe2\x80\x9cconcentrators\xe2\x80\x9d), as described by Tagliarini and Page,\ncan be viewed as an assignment of node inputs to node\n\xe2\x80\x9c Aneural-network solution to the concentrator assignoutputs. The blocking constraints require that at any\nment problem,\xe2\x80\x9d Proc. IEEE Conf. on Neural Informaone time each input is assigned to one output vice versa.\nThe switch may have additional constraints if there is 60 tion Processing Systems-Natural and Synthetic (1988).\nOne application of this problem is selecting how goods\nno direct path from each input to each output. The cost\nshould be distributed from a number of warehouses to a\nof assigning an input to an output depends on whether\nor not the input has data waiting for that output. Cumuchain of retail outlets scattered in a geographic area\nencompassing the warehouses, such that total transport\nlative latency time and packet priority may need to be\ntaken into account as well. The assignment paradigm 65 distance is minimized. Capacities of the individual concentrators were programmed by allocating kiadditional\ndeveloped in this application is compatible with a\n\xe2\x80\x9cslack\xe2\x80\x9d neuron rows to the basic assignment matrix.\nswitching network having maximal throughput and\nnear-optimal delay.\nThe energy function used was\n\n.\n\n5\n\n5,195,170\n\n6\n\ncorresponds to one row\xe2\x80\x99 in the matrix and each set C.u\nelement Ci to one column. For a one-to-one assignment\nproblem, every PU of the matrix thus represents a\nunique pairing of one element of the set {R,} with one\nelement of the set {Cj}, or equivalently, one entry of the\nassignment matrix X. For a one-to-several or severalto-one assignment problem, every PU of the matrix\nrepresents a pairing of one element of the set {R,} with\nor\nseveral elements of the set {C,}, several elements of\n{R;} with one element of the set {Cj}. For a several-toseveral assignment problem, which is also possible,\nevery PU represents a pairing of one element of the set\n{R;} with several elements of the set {Cj}, and viceversa, in which case the matrix of PUS may need to be\nrectangular rather than square.\nThus, in accordance with the present invention, a\nneural network processor for solving first-order competitive assignment problems consists of a matrix of\nNxM processing units, each of which corresponds to\nthe pairing of a first number of elements of {R;} with a\nsecond number of elements {C,}, wherein limits of said\nfirst number are programmed in row control superneurons as MIN and MAX values, and limits of said second\nnumber are programmed in column superneurons as\nMIN and MAX values. The weight of the pairings is\nprogrammed separately into each of the PUS. For each\nrow and each column of PUS, a dedicated constraint\nsuperneuron implementing the k,m-WTA function insures that the number of active neurons within the associated row or column fall within a specified range.\nThese constraint superneurons are individually programmed with the minimum and maximum number of\nrow or column neurons that may be active. If the actual\nnumber of active PUS is outside of this range, the row or\ncolumn superneuron alters an excitatory signal that is\npresented in parallel to the PUS of that row/column,\nwith the effect that additional PUS are turned on or off\nto comply with the programmed range.\nAnnealing is provided by gradually increasing the\nPU gain for each row and column or increasing positive\nfeedback within each PU, the latter being effective to\nincrease hysteresis of each PU or some combination of\none or both of these techniques with other known techniques for annealing. The result is a control of the settling of the PUS to a final stable solution in which some\nPUS are turned on, specifying that the corresponding\nassociation should be made, and some are turned off,\nspecifying that it should not be made. It is the stable\npattern of on and off PUS that gives the solution to the\nprogrammed problem.\n\n-\n\n(3)\n\n5\n\nwhere kj gives the maximum number of elements sup- 10\nported by concentrator j. From this Lyapunov function,\nthe required weights for satisfying the blocking and\nneuron on/off constraints were derived. The association costs were heuristically added to the resulting interconnection matrix. In a series of 100 trials of a small l5\n5 X 12 problem, the network never found the optimal\nsolution, but all solutions were within 0.3% of optimal,\nand most within 0.01%, so it was concluded that the\nnetwork effectively rejected poor solutions. While solutions of this quality are quite satisfactory for some appli- 2o\ncations, the problem size was very small. Of concern is\nwhether more realistic problem sizes would result in\nsolutions as good. Also, consider the connectivity complexity for this architecture. If k\n,\nis the largest ex- 25\npected kithen the number of connections is O(M2(M + k,&.\nSuch extensive connectivity would pose serious\ndifficulties for hardware implementations.\n\xe2\x80\x9cSimulated annealing\xe2\x80\x9d is one means to improve the\nquality of solutions found by such competitive optimi- 3o\nzation architectures, as described by Kirkpatrick, Gelatt\nand Vecchi, \xe2\x80\x9cOptimization by simulated annealing,\xe2\x80\x9d\nScience, Vol. 220, pp. 670-680, (1983). Traditionally,\nannealing involves injecting uncorrelated noise into\neach neuron and gradually reducing the level of the 35\nnoise according to an annealing schedule. This has the\neffect of exciting the system out of local minima in the\nenergy surface and into deeper minima corresponding\nto better solutions.\nAn alternative method of annealing, known as\n\xe2\x80\x9cmean-field annealing\xe2\x80\x9d is described by Bilbro, et al., 40\n\xe2\x80\x9cOPTIMIZATION BY MEAN F I E L D ANNEALING\xe2\x80\x9d Advances in Neural Information Processing Systems 1, Touretsky, ed., Morgan, Kaufman Publishers,\nInc., pp. 91-98 (1989) and is especially appealing for\nhardware implementations. It can be implemented by 45\ngradually increasing the slope of the saturating neuron\nfunction in the presence of a small level of fixed noise.\nApplication of that annealing to significantly improve\nthe solutions found by competitive optimization architectures is reported by Peng and Reggia, \xe2\x80\x9cA connec- 50\ntionist model for diagnostic problem solving,\xe2\x80\x9d IEEE\nTrans. on Systems, Man and Cybernetics, Vol. 19, No.\n2, pp. 285-289, (1989).\nInitially, a method for \xe2\x80\x9cresettling\xe2\x80\x9d the network was\nused in which multiple production operations were 55\nperformed in order to reduce the probability of obtaining a poor solution. With the incorporation of simulated\nannealing, the network produced quite good solutions\nto small-scale problems without resettling.\n60\nSTATEMENT OF T H E INVENTION\nThe classical one-to-one and one-to-several (or several-to-one) assignment uroblems mav be mauued onto a\nsumming architecture\xe2\x80\x99by allocating an NxM matrix of\nneurons,- referred to hereinafter as \xe2\x80\x9cprocessing units\xe2\x80\x9d 65\n(PUS) or more simply as \xe2\x80\x9cunits,\xe2\x80\x9d to a first set of resource elements R.v and a second set of consumer or\ntarget elements CM, such that each set R.y element R;\n\nBRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 illustrates schematically a.prior-art Hopfield\nfeedback neural network.\nFIG. 2 illustrates schematically a neural network\nprocessor for solving linear global optimization problems in accordance with the present invention.\nFIG. 3 illustrates a logic diagram for each processing\nunit Le., (PU), i.e., neuron cell of FIG. 2.\nFIG. 4 illustrates schematically a logic diagram for a\nconstraint superneuron for column and row constraint\nsuperneurons CCSi and RCS;.\nFIGS. 5 and 6 illustrate CMOS circuits for implementing the logic diagrams shown in FIGS. 3 and 4.\nFIG. 7 illustrates sigmoid characteristic gain curves\nA, B and C programmed by a host computer for annealing the PUS.\n\n7\n\n5,195,170\n\n8\n\nrange. Optionally, a global constraint superneuron (not\nshown) may be provided to constrain the total number\nof active PUS to a desired range, although in practice\nthat may not be necessary for most applications. Also\n5 required is some means for annealing (controlling the\nDETAILED DESCRIPTION O F THE\nsettling) of the network which will be described with\nINVENTION\nreference to FIGS. 3 and 4.\nIt should be noted that the implementation described\nThe organization of each PU is shown schematically\nbelow differs in several respects from the Hopfield\nin FIG. 3.A summing circuit comprised of four variable\nnetwork. While it cannot be used to solve higher-order 10 gain amplifiers 16,17,18and 19 connected to a current\nsumming junction Js serves to sum the individual input\noptimization problems, such as the traveling salesman\nsignals from a cost memory 20,row superneuron 11 via\nproblem, it can be used to perform resource allocation\nrow excite line 136 and column superneuron 12 via\nin an efficient manner with respect to circuit complexcolumn excite line 146.The fourth amplifier 19 applies\nity. The architecture differs from the Hopfield network\nin that the interconnections are sparse, all have unit 15 positive feedback to the summing junction to provide\nsome hysteresis in order to force the network to settle in\nweight, and the programmable variables are stored in\nan orderly manner.\nexplicit memory cells rather than being distributed\nThis positive feedback can be used for annealing the\nthroughout a synaptic matrix. In addition, means for\nnetwork by gradually increasing the positive feedback\nannealing may be readily provided. Finally, constraints\nare programmed via separate column and row super- 20 to the amplifier 19,thus gradually increasing the hysteresis of the network. This is readily accomplished by\nneurons that apply excitation and inhibition signals to\ngradually increasing the gain of the amplifier 19 by a\nthe neuron cells. Such structures are not implicit in the\nHopfield network.\nprogrammed parameter H issued by the host computer\nReferring to FIG. 2,a neural network processor for\n24 to all superneurons in parallel.\nsolving linear global optimization problems is com- 25 The currents summed at the junction Jsflow through\na transimpedance circuit 21 shown in FIG. 3 schematiprised of a matrix of MxN processing units (PUS) 10\neach of which corresponds to the pairings of M recally as a potentiometer, but in practice implemented\nwith active electronic elements in a circuit. The circuit\nsources with N consumers or targets, either on a one-to21 converts the summed current to a voltage, the excurone, one-to-several, several-to-one, or several-toseveral basis. The cost of the pairings represented by an 30 sion range of which is limited by the forward voltage\ninput Wgat a terminal of each PU is programmed and\ndrop of diodes D1 and D2. The level of the resulting\nvoltage signal is converted to a unidirectional current in\nstored in the PUS, as will be described below with refertransconductance amplifier 23. The effective resistance\nence to FIG. 3.For each row, a constraint superneuron\n(RCS) 11 is provided to insure that the number of active\nof the circuit 21 is controlled by a gain control signal\nneurons within the row fall within a desired range of 35 from the host computer 24 for all RCS and CCS superneurons only one of which is shown in FIG. 3. The\nMIN to MAX.\nFor an aircraft assignment problem of scheduling\nvariable gain circuit 21 connected to the summing juncairplanes (resources) to flight routes or destinations\ntion J,may be used to achieve a variable slope sigmoidal\n(targets), MIN= 1 and MAX= 1, but for other problem\ncharacteristic for annealing the PUS. Thus, by increasMIN and MAX may have different values, such as in 40 ing the variable gain as solution of the problem progresses, annealing can be provided.\nthe problem of assigning multiple weapons for each\nThe output current of the transconductance amplifier\ntarget, individually assigning each weapon equipped\n23 is duplicated by current mirrors 25, 26 and 27.The\nwith multiple projectiles to multiple targets, or assigncurrent mirror 25 drives its current onto a row sense\ning several weapons equipped with multiple projectiles\nto multiple several targets. Similarly, for each column, a 45 line 13a and the current mirror 26 drives its current onto\nconstraint superneuron (CCS) 12 is provided to insure\na column sense line 14u of the matrix shown in FIG. 2.\nthat the number of active PUS within the column fall\nThe output of the third current mirror 27 is coupled to\nwithin a desired range. The row and column constraint\na PU sense output line 28 by a MOS transistor gate 29.\nsuperneurons are individually programmed with the\nThat gate and output line is used by the host computer\nminimum (MIN) and maximum (MAX) number of row SO 24 to poll each of the PUS after the solution to the proband column PUS that may be active. As in the case of\nlem has been reached. That is done by addressing the\ngate 29 for connecting the output line 28 of the PU to\nprogramming cost into the PUS, the MIN and MAX\nvalues programmed are individually determined and\nthe host computer 24 using an addressing gate 30 so that\nexplicitly stored in the superneurons.\nwhen both row and column select .terminals are actiA computer 24 is programmed to control all parame- 55 vated the MOS gate transistor 29 is turned on. If the\nvoltage signal coupled to the line 28 by the \xe2\x80\x9con\xe2\x80\x9d transisters of each PU 10,as will be described more fully with\nreference to FIG. 3. If the actual number of active PUS\ntor is zero, the PU is in an \xe2\x80\x9coff\xe2\x80\x99 state; otherwise the PU\ncontributing current in a sense line 13a (or 14u) conis in an \xe2\x80\x9con\xe2\x80\x9d state. It should be noted that the addressing\nnected t an input of a superneuron 11 (or 12)is outside\no\ngate 30 is also used by the host computer during initialof this range, the superneuron determines that the 60 ization to program an explicit cost factor to be stored in\nsummed current on the sense line is too high or too low\na memory 20 of the individual PU via a MOS transistor\ngate 22. The cost term Wg is provided as an analog\nby comparison with the MIN and MAX value stored in\nthe superneuron and increases or decreases an excitsignal stored in the PU using, for example, a capacitor\natory signal at an output terminal that is presented in\nC1 as shown.\nparallel to the PUS of that row or column over an excite 65 The variable gain amplifiers 16, 17, 18 and 19 are\nline 136 (or 1 6 . This will tend to activate or deactivate\n4)\npreferably implemented with MOS transistors in order\nadditional PUS in that row or column bringing the numto provide high input impedance. The gain control\nber of active PUS into compliance with the MIN, MAX\nvoltages X, Y , Z and H applied to the respective ampliFIG. 8 illustrates a single sigmoid gain curve and the\neffect of increasing positive feedback in each PU to\ngradually increase hysteresis and thus provide a new\nmethod of annealing in the PU network.\n\n9\n\n5,195,170\n\nfiers 16,17 18 and 19 set the gain factors for the activation signals on the row and column excitation lines 13b\nand 14b driven by row and column superneurons 11 and\n12,and for the association cost Wgspecific to each PU\n10 as shown in FIG. 2. The bias voltage applied to the 5\namplifier 19 sets the level of positive feedback to the\nsumming junction Js in order to provide the desired\nhysteresis in the PU.\nThe host computer 24 is programmed to set those\nscaling bias voltages X, Y, 2 and H which are applied in 10\nparallel to all the neurons. All except H are parameters\nthat are set during an initialization period, as are the\nassociating costs Wg for the separate PUSset individually. The host computer similarly sets the bias for the\ntransconductance amplifier 23 and the gain for the tran- 15\nsimpedance circuit 21 (represented in FIG. 3 as a\ndashed line) for variable gain control of the PU. In\npractice, that gain would be a voltage signal applied to\nthe transimpedance circuit 21 during the initialization\nphase, but the gain for the transimpedance circuit 21 20\nschematically represented in FIG. 3 as a potentiometer\ncould be programmed by the host computer to increase\nas a solution to the problem progresses for annealing, as\ncould the parameter H for increase of the positive feedback to the summing junction J1. Thus, either increased 25\ngain or increased feedback, or both, may be used for\nannealing the network during operation for solution of\na problem.\nReferring to FIG. 2,it is evident that row and column\nexcitation lines 130 and 14u serve to provide feedback 30\nfrom each row superneuron 11 to the Pus 10 of a related\nrow, and from each column superneuron 12 to the PUS\n10 of a related column. The row and column superneurons 11 and 12 are identical in organization and operation. Consequently, the organization of one superneu- 35\nron shown in FIG. 4 applies to bcth. What is unique to\neach is the constraint window bounds, the programmed\nMIN and MAX number of PUSthat may be on which\nare set during initialization of the network for a problem.\n4\n0\nReferring to FIG. 4, an A N D gate 34 is used to address a row superneuron 11 (or column superneuron 12)\nthat is in the same row (or column) with which it is\nassociated to turn on MOS transistors 35 and 36, thus\nenabling MIN and MAX voltages programmed by the 45\nhost computer 24 (FIG. 3) to be stored on respective\ncapacitors C2 and C3 during the initialization phase.\nThose voltages remain stored throughout the duration\nof the problem due to the high input impedance of MOS\ncomparators 37 and 38.However, as in the case of stor- 50\ning a cost factor Wgin each PU,the MIN and MAX\nvalues may be entered and stored in digital form and\nthen converted to analog form for comparison with the\nvalue of the row (or column) sense line in a superneuron\n55\n11 (or 1 )\n2.\nThe comparators 37 and 38 compare the summation\ncurrent of PUS connected to the associated row (or\ncolumn) sense line 13a (or 140)with the MIN and MAX\nvoltage levels stored in capacitors C2 and C3. The comparators each have high gain and drive the charge and 60\ndischarge circuits 39 and 40. These control the voltage\non an output capacitor C4 which provides the excitation\nvoltage signal on a row (or column) excitation line 13b\n(or 146) associated with the superneuron.\nIn response to the excitation voltage signal fed back 65\nthe\nfrom the row (or column) superneuron to the PUS,\nassociated row (or column) of PUSadjusts to maintain\nthe number of PUSin the row (or column) that may be\n\n10\n\non within the specified MIN-MAX window. If the row\n(or column) MIN stored in capacitor C2 exceeds the\nassociated row or column current signals summed on an\nassociated row (or column) sense line 130 (or l )as\nk,\ndetermined by the comparator 37,the charge circuit 39\nwill add charge to the capacitor C4, thus increasing the\nvoltage signal on the row (or column) excitation line\n130 (or 140) to cause more neurons in the associated\nrow (or column) to turn on. If, on the other hand, the\nrow (or column) current signals summed on the sense\nline 13a (or 140)exceeds the MAX stored in the capacitor C3, as determined by the comparator 38, the discharge circuit 40 will drain charge from the capacitor\nC4, thus decreasing the voltage signal on the row (or\ncolumn) excitation line 136 (or 14b)to cause fewer PUS\nin the associated row (or column) to remain on.\nThe row sense line 13a and the column sense line 140\nwill sum the currents from respective rows and columns\nof PUS.\nConsequently, it should be noted that either the\ncurrent signal on the sense line must be converted to a\nvoltage signal for comparison with the MIN and MAX\nsignals stored in capacitors C2 and C3, or the MIN and\nMAX signals must be converted to proportional current\nsignals for comparison with the current signal on the\nsense line. The CMOS circuit diagram of the superneuron shown in FIG. 6 effectively converts the MIN and\nMAX voltage to current and compares currents.\nIt is desirable to control the charge-discharge time\nconstant of the capacitor (i.e., the rate at which current\nwill flow through the circuits 39 and 40 to or from the\ncapacitor C4). That is set by the host computer 24 (FIG.\n2) via a line 41 for the optimum speed of operation in\nsolving global optimization problems of resource allocation. While speed may always be desired, a longer\ntime constant in the feedback to the PUS may be required in order to avoid having the PU network oscillate about the final solution before it settles, or settles to\na poor solution. This is distinct from the need for annealing which requires increasing the annealing factor\nas solution of the problem progresses. Consequently,\nthe time constant is set during initialization of the network and stored in the host computer 24.\nAn analog VLSI implementation of the neuron network processor of FIG. 2,organized as shown in FIGS.\n3 and 4,will now be described, first with reference to\nFIG. 5 for a CMOS circuit that implements a single PU\n10 shown in FIG. 3 for the neuron network processor,\nand then with reference to FIG. 6 for a CMOS circuit\nthat implements a single superneuron 11 (or 12) shown\nin FIG. 4 Each superneuron provides and enforces\n.\nconstraints in matching resources to expenditures (consumers) in each row and column. Both the PU circuit of\nFIG. 5 and the superneuron circuit of FIG. 6 have been\ndesigned and fabricated using a 2 pm n-well CMOS\ntechnology. For the convenience of associating the\nCMOS circuit diagrams of FIGS. 5 and 6 with the\norganization diagrams of FIGS. 3 and 4,the same reference numerals employed in FIGS. 3 and 4 are used in\nFIGS. 5 and 6 to refer to sections of active MOS transistor elements that are interconnected to provide the\nrequired functions described above.\nReferring to FIG. 5, the a d d e r h a l e r network described with reference to elements 16 through 19 and 21\nhas been implemented as an extension of the differential\ndifference amplifier described by E. Saeckinger and W.\nGuggenbuehl, \xe2\x80\x9cA versatile building block: The CMOS\ndifferential difference amplifier,\xe2\x80\x9d IEEE J. Solid-State\nCircuits, SC-22, No. 2, 287-294, April 1987, to include\n\n11\n\n5,195,170\n\nfour sections 16,17,18 and 19 connected to junction Js\nfor summing inputs from the row and column excitation\nJines 136 and 146 with the cost Wgfrom capacitor CI.\nAn addressing circuit 30 responds to row and column\naddress signals to enable transistor 22 to conduct a cost 5\nsignal Wg into the capacitor C1. That transistor and\ncapacitor together function as the cost memory (Sample-and-hold section) 20. Diode-connected source transistors Q1 through 4 8 are used to reduce the gain of the\n0\ndifferential difference amplifiers 16 through 19.Transis- 1\ntors 4 9 and Q l o are similarly used in the variable gain\ncontrol transimpedance circuit 21. The summing junction Js is clamped by source transistors Qll and 4 1 2\ndiode-connected to function as clamping diodes DI and\nD2 at the input to the variable gain transimpedance 15\ncircuit 21 which electronically varies gain in response\nto a gain control input signal.\nThe level of the output signal at junction J, is shifted\nin the transconductance amplifier 23 to a desired level in\nresponse to a bias signal applied to the amplifier 23 20\ncomprising MOS transistors Q19, 420,4 2 1 and 422. The\ncurrent output of MOS transistor 4 2 2 is reflected by\nMOS current mirrors 25, 26 and 27 onto output lines.\nThe output lines of the current mirrors 25 and 26 are\nconnected directly to row and column sense lines 13a 25\nand 14u. The output of the current mirror 27 is connected to a PU sense output line 28 by the MOS transistor 29 addressed through AND gate 30 comprised of\nMOS transistors 4 2 3 through 4 2 6 arranged in a NOR\nlogic configuration which requires both the row and 30\ncolumn select signals to be at a zero Boolean logic level\nto enable the transistor 29 to conduct, thus providing a\nlogic AND function in addressing the transistor 29. As\nnoted with reference to FIG. 3, the addressing gate 30\nalso enables MOS transistor 22 in the cost memory 20 in 35\norder that the capacitor Cl may receive and store a cost\ninput voltage signal WG.\nIt should be noted that all MOS transistors in FIG. 5\nare of the NMOS type except those shown with a small\ncircle at the gate which is a conventional way in CMOS 40\ntechnology of indicating that those MOS transistors are\nof the PMOS type. That convention is also used in FIG.\n6 which illustrates the CMOS circuit used for the row\nand column constraint superneurons (RCS and CCS).\nRow and column address gate 34 is implemented as a 45\nNOR gate with MOS transistors as is the gate 30 in the\nPU circuit of FIG. 5 It enables MOS transistors 35 and\n36 to store MAX and MIN voltages (the constraint\nwindow bounds) in capacitors C2 and C3. A row or\ncolumn PU sense signal on line 13a (or 14u)is compared 50\nwith the window bounds in comparators 37 and 38.The\nactivation signal on the sense line 130 (or 14u) is first\napplied to MOS transistor 4 3 0 connected as a diode in\norder to sink current. The voltage on the gate of the\ntransistor 4 3 0 is also applied to the gate of transistors 55\n4 3 1 and 4 3 2 which duplicate the row or column activation current signal. As a consequence, comparators 37\nand 38 compare the summation current on the sense line\n130 (or 14a) from row (or column) PUS mirrored by\ntransistors 4 3 1 and 4 3 2 with currents generated by dif- 60\nferential amplifiers of the comparators 37 and 38 comprised of MOS transistors 441, 442, 4 4 3 and transistors\n4 4 445, 4 4 6 that convert the voltages stored in the\n4,\ncapacitors C2 and C3 into 1~1.v IMAX\nand\ncurrents.\nThe range of MIN and MAX currents may be set by 65\nadjusting the gate voltage of transistors 4 4 2 and 4 4 3 of\ncomparator 37 and transistors 4 4 5 and 4 4 6 of comparator 38.The sense line current mirrored by MOS transis-\n\n12\n\ntors 4 3 1 and 4 3 2 are subtracted from I.wxand I,w.v,and\nthe resulting currents are converted to voltages applied\nto the gates of transistors 4 3 4 and 4 3 3 of charge and\ndischarge circuits 39 and 40. The overall result is that\nthe activation level of the PUS on sense line 13a (or 14u)\nis compared with I ~ ~ x a n ~ 1 . If .the PU activation\n1d ~\ntransistor 4 3 3 is turned on and\nlevel is higher than IMAX,\ncapacitor C4 is discharged until the PU activation decreases to a level equal to IMAX.\nSimilarly, if the PU\nactivation level is less than IMIN, transistor Q34is turned\non to charge the capacitor C4 to increase neuron activation to a level equal to 1 ~ 1If the neuron activation\n~ .\nlevel is within the window between and IMI.vand IMAX,\nneither of the transistors 4 3 4 and 4 3 3 in the charge and\ndischarge circuits 39 and 40 is turned on, and the charge\nstored in the capacitor C4 remains unchanged.\nInitially, the capacitor C4 is uncharged at the commencement of problem solution and so transistor 4 3 4\nwill be turned on to charge the capacitor C4, thus allowing additional PUS to be turned on. As PUS turn on and\noff while searching for a solution, the number of PUS\nturned on will increase until Iicrm; is exceeded. The\ncharge on the capacitor Cqwill then remain constant. If\nso many PUS are turned on that IMAXis exceeded, transistor 4 3 3 is turned on to discharge the capacitor C4.\nThis \xe2\x80\x9chunting\xe2\x80\x9d will continue until the PU network\nreaches a stable condition which is a solution to the\nproblem. In order to avoid oscillations, the superneuron\ntime constant is made larger than the PU time constant\nby a superneuron time-constant control signal applied\nto a MOS transistor connected as a diode D3 to sink\ncurrent. This provides a time constant control voltage\nsignal to the gates of transistors 4 3 5 and 4 3 6 to control\nthe discharge rate of the capacitor C4. The time-constant signal applied to transistor 4 3 6 is mirrored to transistor 4 3 7 to set the charge rate by setting the conduction level of transistor 4 3 8 . In that manner, the conduction level of transistors 4 3 5 and 4 3 8 determine the rate\nof change in the charge level of capacitor C4.\nAs noted hereinbefore with reference to FIGS. 3 and\n5,the host computer 2 shown in FIG. 3 controls gain\n4\nof the PUS at the variable gain control element 21. This\nmay be used for annealing by gradually increasing PU\ngain until PUS turn on or off, a condition of the PU\nnetwork that is stable. This variable gain annealing\ntechnique is illustrated by the sigmoid characteristic\ncurves A, B and C shown in FIG. 7 As the gain is\n.\nincreased, the sigmoid shape of the curve A becomes\nmore pronounced as shown by the curves B and C, and\nthe slope of the curves in the central portion increases.\nAs a result, the distribution of PUS at levels between\nON and O F F begins to clearly separate between those\nthat are more conductive from those that are less conductive by driving those more conductive toward saturation and driving those less conductive toward cutoff\nuntil finally they become fully segregated with some on\nat saturation and others completely o f This is the clasf.\nsic annealing method for neural networks of the prior\nart.\nAn alternative annealing method is to provide positive feedback in each PU and gradually increase the\nfeedback, thereby gradually increasing the hysteresis of\nthe PU. This method is illustrated in FIG. 8 by a single\nsigmoid curve with levels of PU conduction distributed\nbetween the extremes of ON and OFF. As positive\nfeedback is increased, those at a level of conduction\nabove a midpoint will begin to move closer to saturation, and those below will begin to move closer to shut-\n\n13\n\n5,195,170\n\noff, until finally those above are on fully and those\nbelow are completely off. As a result, the segregation of\nPUS between O N and OFF is more quickly and positively achieved. This new hysteresis annealing method\nis preferred for the neural-network dedicated processor 5\ndisclosed over the prior-art gain method referred to\nabove, or in combination with the prior-art gain\nmethod, in which case the slope of the sigmoid curve of\nFIG. 8 is increased as feedback (hysteresis) of the PUS\nis increased.\n10\nT O ihstrate the manner in which the invention is to\nbe used, a situation is shown in which 70 scattered retail\nstores are to be supplied by 20 warehouses among them\nand each warehouse can S~PPlY 3 or 4 stores. To map\nto\nthis problem onto the neural-network dedicated proces- 15\nsor* the warehouses and Stores are numbered, each\nwarehouse is mapped Onto One pu row, and each store\nis mapped Onto a pu\nThe weightsare givenby\nthe distances between the stores and warehouses. If\neach\nprogrammed into the memory corresponded 2o\nto 10 miles of distance, and store 1 was 4.6 miles from\n'' then the weight programmed into pu\n(lY1)\nbe 0'46\nThe weight\nbe downloaded by\nthe appropriate digita1\nsuch that PU (13) is selected, and a digital-to-analog 25\nconverter in the host computer's backplane would be\nused to generate the 0.46 volt level that would be applied to the COST terminal.\nIn this simple case, 2o rows and 7o columns would be\nused\nof possibly\nrows and\ncolumns. The 3o\nunused rows and columns are deactivated by programming M I N = M ~ X = Olevels into the superneurons of\nthose unused rows and columns. The superneurons for\nthe rows that are used are programmed with M I N = ~\nand MAX=4 since each warehouse can supply from 3 35\n4 stores. superneurons columns are profor used\ngrammed with MIN = 1 and MAX= 1 since each\nis to be supplied by only one warehouse. N~~~ that\nconversion factors must be experimentally measured\nthat give the voltage levels corresponding to MIN or 40\nMAX thresholds of 1, 3 and 4.\naddition, the gain control is set initially so as to\ngive a shallow sigmoid slope. After these values have\nbeen programmed, the network will settle into a state\nsuch that all PUS are partially on, with activation corre- 45\nsponding generally to the inverse of the cost, but also\ninfluences by the row and column superneurons. At this\npoint, annealing is applied by slowly increasing the\nsigmoid gain and/or the PU positive feedback (hysteresis). This forces all PUS to approach over time a fully-on 50\nor fully-off state. While annealing is proceeding with a\nslow time constant, the superneurons, with a faster time\nconstant, are changing their activations to insure that\nblocking constraints remain satisfied (i.e., 3 or 4 stores\nare serviced by each warehouse, and each store is ser- 55\nvices by only one warehouse). After annealing has proceeded to the point where only some of the PUS are\nfully on, the rest are off, and the pattern of on and off\nPUS is stable, a solution has been generated. The host\ncomputer then accesses each PU in turn by activating 60\nits output gate transistor 29 and noting the PU state.\nThis gives the assignment matrix X directly.\nFrom this illustration of an assignment, it is evident\nhow virtually any assignment problem may be mapped,\ngiven:\n65\n1) Two sets {R;l<i<M)\nand {C;l<j<N}\n(e.g.,\nwarehouses and retailers) whose elements are to be\nassigned to each other.\n\n14\n\n2) An NxM assignment cost matrix (W,,) giving the\ncost of pairing any R, to any C,.\n3) Blocking constraints that specify the number of C\nelements that may be paired with any R,, and the\nnumber of R elements that may be paired with any\nCj.\n4) A binary assignment matrix X=(xlj), where x,/= 1\nif R; is associated with C and x,j=O otherwise.\n,\nThe present invention gives an assignment matrix X that\nminimizes Equation (1).\nA neural-network architecture has been described\nthat is capable of solving first-order assignment problems larger than 64x64 in size. By reducing connectivity to the Same order as number of processing units\n(neurons), the architecture is amendable to hardware\nimplementation. Blocking constraints may be independently set for each element to be one-to-one, one-toseveral, OF even a range of several-to-several acceptable\npairings. This degree of flexibility is unmatched by any\nother algorithm for assignment. The novel method of\n,chysterebic annealing,,, implemented by increasing Over\ntime the levels of positive feedback within processing\nunits, guarantees system convergence and a maximum\nconvergence time. The tradeoff between settling tirne\nand goodness of solution may be explicitly prohave shown that the system can\ngrammed'\nbe expected to settle in time scales of tens or hundreds\nof microseconds-orders of magnitude faster than other\nmethods that can guarantee a good or optimal solution.\na particular embodiment Of the invention\nhas been illustrated and described with some modifications and\nfor\nw ~ , and MAX\nvalues in digital form in the network that is otherwise\nimplemented as an analog neural-network processor, it\nis recognized that other modifications and equivalents\nmay readily occur to those skilled in the art. Consequently, it is intended that the claims be interpreted to\ncover such modifications and\n\n*1.\n\nA neural-network dedicated processor for solving\nfirst-order assignment problems by allocating a matrix\nof NxM processing units, wherein assignment is of one\nor several elements of a first set (R;} of elements which\ncorrespond to one of N rows in Said matrix, to one Or\nSeveral elements of a Second set {c,}which corresponds to one OfM COhmns in Said matrix,\nf\neach processing unit Pub at an intersection ij O a set\n{R,} in a row and of a set {Cj} in a column having\nan output c ~ r e n conducting line connected to a\nt\nrow sense line RSLjand a column sense Iine CSL,,\na first input terminal connected to a row excitation\nline RELjand a column excitation line CELjand a\nunit output terminal,\neach of said processing units corresponding to the\npairing of one of said first set of elements {R;} with\none of said second set of elements {Cj},\nmeans for initially programming a separate weight of\nsaid pairing into each of said processing units as a\ncost factor Wij,\na set {RCS;} of N row control superneurons associated with said N rows of processing units, each row\ncontrol superneuron having an input terminal and\nan output terminal,\na set {CCSi) of M column control superneurons associated with said M columns of processing units,\neach column control superneuron having an input\nterminal and an output terminal, and\n\n15\n\n5,195,170\n\n16\n\nmeans for initially programming a separate minimum\nspecified cost factor in each processing unit, and a samand maximum number of processing units of the\npling gate responsive to said logic gate for coupling said\nfirst set {R;} that may be conducting a current onto\nweight factor Wg into said storage means.\nsaid row sense line RSLi, and initially program5. A neural-network dedicated processor as defined\nming a separate minimum and maximum number of 5 in claim 4 wherein said means for probing said output\nprocessing units of the second set {Ci> that may be\nterminal of each processing unit is comprised of a samconducting a current onto said column sense line\npling gate responsive to said separate logic gate in each\nprocessing unit responsive to said address line i and said\nCSLj,\neach row sense line RSLiconnected to sum output\naddress line j.\ncurrents of each processing unit in row i being 10 6. A neural-network dedicated processor as defined\nconnected to said input terminal of a row control\nin claim 1 including\nmeans for initially specifying said number of said first\nsuperneuron associated with row i, and\nset of elements {R;} paired with said number of\neach column sense line CSLj connected to sum\nsaid second set of elements {Ci} by programming\noutput currents of each processing unit in a colinto each row control superneuron a minimum and\numn j connected to said input terminal of a col- 15\numn control superneuron associated with cola maximum number of processing units of row i\nthat may be in a condition to conduct current onto\numn j,\na sense line RSLi and programming into each coleach row control superneuron having said output\nterminal connected to each processing unit of an\numn control superneuron a minimum and a maxiassociated row of units through a row excitation 20\nmum number of processing units in column j that\nmay be in a condition to conduct current onto a\nline for voltage feedback, and\nsense line CSLj, said specifying means comprising\neach column control superneuron having said outa minimum-number storage means and a minimumput terminal connected to each processing unit\nof an associated column of units through a colcoupling gate for coupling a voltage signal rep25\numn excitation line for voltage feedback,\nresenting said minimum number into said storage\nmeans within each processing unit PUij for summing\nmeans,\nexcitation s.ignals on said row and column excitaa maximum-number storage means and a maximum-coupling gate for coupling a voltage signal\ntion lines which are connected thereto with a signal\nrepresenting said programmed cost factor Wy,\nrepresenting said maximum number into said\nstorage means, and\nmeans for coupling said summing means within each 30\na logic gate responsive to an address line i and an\nprocessing unit P u g onto a row sense line RSL;, a\naddress line j, where lines i and j are associated\ncolumn sense line CSLj, and said output terminal of\nwith a row and column of processing units for\nsaid processing unit PUlj, and\nselecting said minimum-number and maximummeans for probing said unit output terminal of each\nnumber storage means, said logic gate being\nprocessing unit PUijto determine which processing 35\nconnected to enzble said minimum-coupling and\nunits are conducting current onto associated row\nmaximum-coupling gates to couple minimumand column sense lines, thereby to determine the\nand maximum-number voltage signals to said\nsolution of said problem after all processing units\nrespective minimum-number and maximumhave been driven to a stable pattern of conducting\nnumber storage means,\nand not conducting a current onto said row and 40\nwherein each row superneuron i includes means for\ncolumn sense lines.\ncomparing a signal on said row sense line RSL;\n2. A neural-network dedicated processor for solving\nwith said minimum-number and said maximumfirst-order assignment problems as defined in claim 1\nnumber voltage signals, and means for adjusting\nwherein said means for coupling said summing means\na signal on said row excitation line RELi by\nonto a row sense line, a column sense line, and said unit 45\nincreasing said signal if below said stored minioutput terminal includes positive feedback control\nmum-number voltage and decreasing it if above\nmeans and means for programming said positive feedsaid stored maximum-number voltage, and\nback control means for an increase of said positive feedwherein each column superneuron j includes\nback while solving a first-order assignment problem,\nmeans for comparing a signal on said column\nthereby annealing said neural-network dedicated pro- 50\nsense line CSLj with said minimum and maxicessor to force it toward said solution.\nmum, and means for adjusting a signal on said\n3. A neural-network dedicated processor for solving\ncolumn excitation line CELj by increasing said\nfirst-order assignment problems as defined in claim 2\nsignal if below said stored .minimum-number\nwherein said means for coupling said summing means\nvoltage and decreasing said signal if above said\nonto a row sense line, a column sense line, and said unit 55\nstored maximum-number voltage.\noutput terminal includes gain control means and means\n7. A neural-network dedicated processor as defined\nfor programming said gain control means for an inin claim 6,including means for comparing a sense line\ncrease of said gain while said positive feedback is insignal with a minimum-number and a maximum-number\ncreased for more rapid annealing of said neural-network\n60 signal, wherein said means for adjusting a signal on said\ndedicated processor.\n4. A neural-network dedicated processor as defined\nrow and column excitation lines in respective row and\ncolumn superneurons is comprised of means for storing\nin claim 1 wherein said means for programming a sepaan output excitation signal level and separate increase\nrately specified cost factor Wijfor each processing unit\nis comprised of a separate logic gate in each processing\nand decrease means responsive to said means for comunit responsive to an address line i and an address line j, 65 paring a sense line signal with a minimum-number and a\nmaximum-number voltage signals, respectively, for\nwhere lines i and j are associated with a row and a\nincreasing and decreasing said output signal level in said\ncolumn of processing units that intersect at each prostorage means, thereby to adjust a row or column excicessing unit addressed, means for storing said separately\n\n17\n\n5,195,170\n\n18\n\ntation signal to be between said minimum-number and\nming excitation signals, a programmable positive feedsaid maximum-number.\nback circuit connected to said summing means within\n8. A neural-network dedicated processor as defined\neach processing unit, and means for programming an\nin claim 7 wherein said means for Storing an output\nincrease in positive feedback in all said processing units\nexcitation signal level is comprised of a storage capaci- 5 in unison, whereby annealing said neural network is\nare\ntor, and said separate jncrease and decrease\nachieved as operation progresses toward a stable state in\ncomprised of a charge circuit and discharge circuit,\nwhich each processing unit is either driven to conduct\nrespectively, and said charge circuit and said discharge\nat saturation or driven to be nonconducting.\ncircuit of each superneuron includes means responsive\n10. In a neural network as defined in claim 9,includto a programmed signal specifying a rate of change of\nsaid charge and discharge circuits, thereby to specify\ning a programmable gain Control means connected to\nthe rate at which said storage capacitor charges or dissaid summing means for programming an increase in\ncharges to adjust said excitation line to be between said\ngain in all said processing units in unison, whereby more\nminimum -number and said maximum -number voltages.\nrapid annealing of said neural network may be\n9.In a neural network having a matrix of processing I5 achieved.\n* * * * *\nunits and means within each processing unit for sum-\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n50\n\n55\n\n60\n\n65\n\n"