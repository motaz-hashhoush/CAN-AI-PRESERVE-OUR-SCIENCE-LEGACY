b'Source of Acq uisition\nNASA Johnson Space Cen ter\n\nAIAA 2001-5110\n\nSPACE STATION TELEROBOTICS: DESIGNING A HUMAN-ROBOT\nINTERFACE\nJennifer L. Rochlis\\ John-Paul Clarke2 , S. Michael Goza3\nINASA Johnson Space Center, 2101 NASA Rd 1 Mail Code ER4 Houston, TX 77058\n2\nMassachusetts Institute oj Technology, 77 Massachusetts Ave, Cambridge, MA 02139\n3NASA Johnson Space Center, 2101 NASA Rd 1 Mail Code ER4 Houston, TX 77058\n\nThe experiments described in this paper are part of a larger joint MIT/NASA research effort and \' focus on the\ndevelopment of a methodology for designing and evaluating integrated interfaces for highly dexterous and multifun ctional telerobot . Specifically, a telerobotic workstation is being designed for an Extravehicular Activity (EVA)\nanthropomorphic space station telerobot called Robonaut. Previou researchers have designed telerobotic\nworkstations based upon performance of discrete subsets of tasks (for example, peg-in-hole, tracking, etc.) without\nregard for transitions that operators go through between ta k performed equentially in th e context of larger\nintegrated tasks. The experiments presented here took an integrated approach to describing teleoperator\nperformance and assessed how subjects operating a full-immersion telerobot perform during fin e position and gross\nposition tasks. In addition, A Robonaut simulation was also developed as part of this research effort, and\nexperimentally tested against Robonaut itself to determine its utility. Results show that subject performance of\nteleoperated tasks using both Robonaut and the simulation are virtually identical, with no significant clifference\nbetween the two. These results indicate that the simulation can be utilized as both a Robonaut training tool, and as a\npowerful design platform for telepresence displays and aids.\n\nINTRODUCTION\nResearch is currently being conducted to design and test an intuitive and ynthesized telerobotic workstation\nconfiguration for controlling a high degree of freedom dexterous manipulator for use on the Intem ational Space\nStation. The construction and maintenance of the International Space Station is expected to increase the number of\nExtravehicular Activity (EVA) hours by a factor of four over current Space Shuttle missions, resulting in higher\ndemands on the BVA crewmembers and BVA crew systems. One approach to utilizing BVA resources more\neffectively while increasing crew safety and efficiency is to perform routine and high-risk EVA tasks telerobotically.\nIn respon e, NASA\'s John son Space Center (JSC) is developing an an thropomorphic telerobot called Robonaut (see\nFigures 1,2) that is capable of performing all of the tasks required of an BV A uited crewmember.\n\n, Copyright \xc2\xa9 2001 by Jennifer Rochlis. Published by the American Institute of Aeronautics and Astronautics, Inc.\nwith permission\n\n1\nAmerican In stitute of Aeronautics and Astronautics\n\ni-~\n\nFigure 1. Robonaut (i n development).\n\nRobonaut is comparable in ize to a suited crewmember and requires the operator to command over 46 degrees of\nfreedom while performing full immersion telerobotic tasks. The desire to develop a methodology for designing\nintegrated workstations is motivated by next-generation robot ucb a Robonaut. Tbe current robotic work tation\nfor the Space Station robots consists of flat panel display and 6 DOF hand controner . Tbi is insufficient for\ncontrolling highly dexterous anthropomorphic manipulators such as Robonaut. The workstation must be designed to\nallow an operator to intuitively control numerous degrees of freedom imultaneou ly, in varying levels of\nsupervisory control, for all types of BVA tasks.\n\nFigure 2. Graphic of Robonaut performing an ISS BVA task\nGreat amounts of research have been conducted in buman factors areas such as telerobotic interfaces, humanmachine interactions, and sensory substitution. However, many of the tasks performed in the experiments described\nin the literature do not capture the variety and complexity of the tasks required of an BVA crewmember. In most\nstudies, optimal workstation components are determined based on performance of discretized subta ks (sucb as pegin-hole, tracking, target acquisition, etc.) without regard to the tran sitions that the operator must go through between\ntasks performed sequentially (Burdea, 1996; Cannon and Thomas, 1997; Massimino, 1992; Sheridan , 1994). In\naddition, mucb of the research focuses on a particular bardware or software a peet of the workstation without\naddressing the synthesis of components required to tackle the buman factors and controls issues of the system as a\nwhole (Kazerooni and Snyder, 1995; Liu and Tharp, 1993; Massimino, 1988; Patrick, 1990; Shattuck, 1994;\nSheridan, 1993; Vidov, 1993). Finally, the few groups that have looked at workstation s as a whole either have not\nhad to control as many degrees-of-freedom as Robonaut demands, or have controlled high degree-of-freedom robots\nthat lack the dexterity of Robonaut, and therefore employ hand controllers (Akin, 1986; Homan and Gott, 1996; Li\nand Cox, 1995; Sheridan, 1992; Tachi, 1991).\nA concern when designing work tations for robots sucb as Robonaut is what method should be used for\ndevelopment of new interfaces, displays and aids. It i afer and more effective to refine such ituation awareness\ndisplays and aids before applying them to the Robonaut bardware, however one must en ure th at the methods used\nto develop them are tran sferable to the actual robot hardware. Finally, the question arises of how to train operator\n\n2\nAmerican In stitute of Aeronautics and Astronautics\n\n1to use the robot. It is desirable to create a library of knowledge and experience for any new operator before allowing\nthem to command the robot hardware directly.\nA series of experiment has been devised and conducted at JSC to characterize the effects of telepresence hardware,\nsensory feedback degradation and task integration on full-immersion telerobotic task performance and workstation\ndesign. In addition, a Robonaut simulation has been developed and evaluated for use a a potentially powerful\nsituation awareness development and operator training tool. One hypothesis tested is that subject teleoperation task\nperformance using the simulation is comparable to Robonaut teleoperation performance, and therefore the\nimulation can be utilized as a telepresence interface development and operator training platform.\n\nROBONAUT TELEOPERA TION\nA Robonaut teleoperator wears a variety of virtual reality display and control technology to immerse them in the\nrobot\'s workspace, thereby creating a sense of \'presence\' at the robot worksite. The user\' body position, tracked by\nan array of sensors, is sent as a command to the robot oftware that in turn generate the robot motions. For the\nRobonaut system, the teleoperator is seated in a remote location wearing instrumented Virtual Technologies, Inc.\n(Palo Alto, CA) Cyber Gloves that measure the displacement and bending of the fingers. A Polhemus FASTRAK\xc2\xae\n(Colchester, VT) system measures the position of the ubject\'s hands, head, arms and head relative to a fixed\ntransmitter. For these experiments, only the right hand/arm, chest and head sensor is utilized.\nRobonaut has two camera for eyes and the live video feed received from them is ent to a Kaiser Electro Optics,\nInc. (Carlsbad, CA) ProView 60 helmet-mounted display (HMD) such that the human sees through the HMD what\nthe robot sees. A transmitter is al 0 mounted on the helmet so that the motions of the user\' s head are tracked. As the\noperator moves his/her head to the right or left, the robot likewise turns its head. In this way, the human is meant to\nfeel that they are immersed and present at the robot ite doing the ta ks themselves. Figure 3 shows a subject seated\nwearing the telepresence hardware.\n\nFigure 3 Subject wearing telepresence hardware including HMD, CyberGlove, and Polhemus trackers\n\nROBOSIM\n\n3\nAmerican Institute of Aeronautics and Astronautics\n\nRobosim (See Figure 4) is under development at the JSC Dexterous Robotics Laboratory. It uses the Interactive\nGrapbics, Operations and Analysis laboratory (IGOAL) Enigma modeling software (Houston, TX) to create the\nrobot models, environment conditions and camera view, and Real Time Innovations, Inc. (RT!) Network Data\nDelivery Service (NDDS) software for developing the necessary communication networks and protocols.\n\nFigure 4 Robonaut and Robosim grapbic\nRobosim employs the identical forward and inverse kinematics as the Robonaut brainstem, therefore given the same\ncommand signal, the resultant motion of the simulated robot will matcb that of the Robonaut. Currently, the\nsimulation is limited in that it does not model contact forces, therefore is not possible to study grasping and tool\nbandling tasks.\nTwo Dell Latitude C600 laptops generate the 3-D Robosim views of the robot arms and task panels. Recall that .\nRobonaut bas two cameras, one for eacb eye, whicb together provide stereo vision to the operator. To generate\nstereo vision with an HMD using Robosim, it is necessary to generate two different graphical views of the same\nscene, separated by the same interoccular spacing as the Robonaut cameras. Note that the HMD view generated\nthrougb the simulation bas the identical 60 degree diagonal field of view as the Robonaut cameras. Figure 5 show\nthe view from one eye that the subject sees in the HMD.\n\n4\nAmerican Institute of Aeronautics and Astronautics\n\nI~\n\nFigure 5 View of rigbt eye througb HMD of Robo im\n\nEXPERIMENTAL METHODS\nA protocol bas been developed wbich tests task time, workload (both objective and subjective) and accuracy for\ngross position, and fine position tasks perfonned individually. The tasks span the workspace of the robot arm in all\ndirections. Examples include winging the ann througb a large range of motion fann an initial to final position, and\nfine positioning of the ann utilizing single joints or multiple joints in different orientation. Tbis work was based in\npart, on teleoperation experiment previously conducted by the author. It was shown that up to thirty percent of the\ntotal task time was spent gaining better situation awareness (SA) (scanning the work ite between movements or\nta ks for a greater ense of the workspace layout and their position within it, and to decide bow best to perfonn the\nnext task) and the average across all operators and days was 10%. Tbis work revealed two important observations,\n1) as operator experience with teleoperation increases the time spent gaining SA infonnation does not decrease\nwithout bound, there exists a baseline amount of SA time required for a given task and workstation configuration; 2)\nthi baseline amount of SA time may be reduced by designing appropriate workstation interface aids. Tbe current\nexperimental methodology bas been designed to both a) isolate the effects of interest and b) minimize the effect of\nconfounding variables. Tbi tudy is likewise de igned to not only describe teleoperator perfonnance using\nRobonaut, but to evaluate the newly developed imulation.\nBasis Task Testing\nTbe basis tasks were devised in order to describe teleoperator behavior during fine position and gross position ta ks.\nTbey were so named after an initial study revealed that all astronaut motion during an BVA operation could be\ncategorized as movements of gro s position, fine position, gra ping or combination of the three. In addition , the\nba i task are designed for simplicity and do not require force ensing or force feedback (althougb could be\naugmented with sucb). Tbe basis tasks can be completed and compared acros a variety of modalities, including\nzero-G, and performed u ing almo t any teleoperated robots or robotic manipulator.\nTbe basis tasks are comprised of two task panels (see Figure 6), one similar to a FITT tapping task, and the other\ncontaining a tracing pattern. Wbile each panel combines elements of both fine and gross position movements, the\ntapping task is primarily a fine position task while the tracing task examines gros position movements.\n\n5\nAmerican Institute of Aeronautics and Astronautic\n\n1-\n\nOn the tapping panel, subjects will be in tructed to tap between like colors with their index finger. The size of each\ntarget is one-half inch quare (the approximate width of the Robonaut index fmgertip) and they are arranged in both\nthe horizontal and vertical directions. The white target in the center is the starting point for each trial. The red and\ngreen targets are one inch from center, orange and blue are three inches from center, and yellow and purple are [lve\ninches from center.\nThe tracing panel involves following a path around the square and through the diagonal with the index finger. The\nclockwise path traces the red-orange-yellow-green-blue (top right to bottom left)-purple (top left to bottom right)\npath, and the counterclockwi e path begin with purple (bottom right to top left) and goes in the reverse order. The\nblue and purple lines were oriented such that ubject mu t reposition their hand and arm before tracing tho e lines.\nEach line is ten inches long and one-half inch wide. The area of the tracmg square and the maxIDlUffi di tance from\nthe center to the yellow and purple targets was cho en to comply with the reach envelope of the right arm of the\nRobonau t.\nA is mentioned in the previous chapter, the Robonaut imulation was created with future Robonaut training in\nmind. To this end, it was desired to test if the simulation performance could match th e robot performance as it was\ntasked to do. As th ese are the fir t set of experiments conducted using the simulation, its ability to match\nRobonaut\'s performance had not been quantified.\nThe basis tasks were tested in three modes. First, in order to describe the baseline performance for each ubject and\nidentify their particular ubject effects, the basis ta k will be performed manUally. In addition, the basi task will\nalso be performed u ing a Robonaut simulation and telerobotical1y, where the subj ects command the robot to do the\ngrasping, fme po ition and gro po ition tasks.\n\xe2\x80\xa2 Manually with HMD\nTo isolate the effect on basis ta k performance of the vision system hardware, the protocol will be performed\nmanually with the addition of the HMD. This will reveal the effect of degraded field of view and depth perception.\n\xe2\x80\xa2 Telerobotically with Simulation\nSubjects will perform the basis tasks by teleoperating a simulation of Robonaut. The computer generated Robonaut\nwill be commanded by the subject and the view from the imulated Robonaut eyes will be displayed to the subject\nthrough a helmet-mounted display. These tests will act a a training buffer between the manual and the telerobotic\ntask , as the simulation possesses the arne kinematics as the robot and therefore subject will gain practice with the\nkinematics before controlling the robot directly.\n\n6\nAmerican Institute of Aeronautics and Astronautics\n\nI\n\n\xe2\x80\xa2 Fully Immersed\nTo quantify any coupled effects between the vision system and the proprioceptive system, the basis ta ks will be\ntested while the subject is fully immersed. They will again wear an HMD displaying the view from the Robonaut\ncamera eyes.\nNote that ensuring that the subj ect wears an HMD for each modality removes the vision system as a parameter in the\nteleoperation description.\nA total of eight subjects (four male and four female) participated in the experiment. None of the subjects had prior\nexperience teleoperating Robonaut. For this reason, one hand-one arm tasks were cho en to minimize the number of\ndegrees of freedom and therefore complexity of the teleoperation task. This also allowed for greater conlIol over\nRobonaut safety during the lIials. All tasks were conducted using the right hand and only right-handed subjects are\nused.\nFor each modality (manual, simulated and robotic), a session consi ted of 32 lIials. There are six colored pair of\ntapping targets and two lIacing directions (clockwise and counterclockwise). Each is performed four times in a\nbalanced order. Each lIial is 25 seconds in duration . Training sessions were conducted for each modality. Subjects\nwere inlIoduced to the specific modality and given the task inSlIuctions. For the manual tasks, subjects are\ninslIucted to tap between like color pairs, or lIace the pattern continually until time is called. For the simulated\ntasks, subjects are inslIucted to do the same however are told additionally not to penelIate the virtual task board with\neither their index finger or their hand (as contact forces are not modeled, subjects may drive the virtual robot hand\nthrough the plane of the virtual task board). Likewise for the robot lIials, subjects are inSlIucted not to "punch" the\nboard or drag the robot finger along the board. For the latter two experiments, there wa no force feedback to the\noperator as to whether contact was made, however subjects could visually observe if any part of the hand went\nthough the virtual task board, and the deflection of the task panel if the robot was in contact with it. Following the\nlIials, a subjective questionnaire is administered to the subjects.\n\nRESUL TS AND DISCUSSION\nTable 1 summarizes the repeated measures analysis effects of modality, color (distance from center), location\n(vertical or Horizontal), and gender on number of taps, number of errors. As expected, the number of taps and\nlIaces completed during the manuallIials was greater than with the simulation or robot, however across virtually\neffect, there is no significant difference between telerobotic and simulated telerobotic task performance. All color\ntapping and lIacing were similar with the exception of red taps where subjects averaged three more taps than\ntelerobotically, enough to make a significant difference (p=O.009) Other areas that did show significance were\ngender effects. Men had signiflcantly more taps than women for the robotic tasks in all tasks except the clockwise\nlIace, however they also had significantly more errors than the female subjects overall (p=O\n.002). There was no\nstatistical difference between horizontal and vertical directions in either number of taps or number of errors.\nHowever there were significantly more lIaces in the clockwise direction than in the counter-clockwise direction.\nIn conclusion, these experiments have demonSlIated that Robonaut telerobotic performance can be equally achieved\nusing Robo im and therefore Robosim can be used in the future to develop Robonaut workstation situation\nawareness aids, as well as to develop operator lIaining skills.\n\nBffiLIOGRAPHY\n1) Akin, D. L., "Quantifying human performance in space operations," MIT Report, SSL #23-86, (1986).\n2) Burdea, G. c., Force and Touch Feedback/or Virtual Reality, New York, John Wiley & Sons, Inc, (1996).\n3) Cannon, D. 1. and G. Thomas, "Virtual Tools for Supervisory and Collaborative Control of Robots," Presence 6(1) :\n1-28, (1997).\n4) Homan, D. J. and C. 1. Gott, "An Integrated BVAfRMS Virtual Reality Simulation Including Force Feedback, for\nAstronaut Training", Technical R epo/1 AlAA-96-3498-CP, (1996).\n5) Kazerooni, H. and T. J. Snyder, "Ca e Study on Haptic Devices: Human-Induced Instability in Powered Hand\nControllers," Journal of Guidance. Control and Dvnamics 18(1): 108-113 , (1995).\n6) Li, L. and B . Cox, "Telepresence Control of an Ambidextrous Robot for Space Applications", Houston, TX, NASA,\n\n7\nAmerican Institute of Aeronautics and AslIonautics\n\ni\n\nI\n\n\\\n\nJohnson Space Center Report #JSC-33306, (1995).\n7) Liu, A. and G. Tharp, "Some of What One Needs to Know About Using Head-Mounted Displays to Improve\nTeleoperator Performance," IEEE Transaction on Robotic and Automation 9(5): 638-647, (1993).\n8) Massimino, M. J. , Sensory Substitution for Force Feedback in Space Teleoperation. Doctoral thesis in Mechanical\nEngineering, Massachusetts Institute of Technology, Cambridge MA, (1992).\n9) Massimino, M. J., Effects offorce and visualfeedback on space teleoperation; with policy implications. Masters\nthesis in Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, (1988).\n10) Patrick, N . J. M. , Design, Construct.ion and Testing of a Fingert.ip Tactile Display for Interaction with Virtual and\nRemote Environments, Mechanical Engineering. Cambridge, MA, Massachusetts Institute of Technology:\n109, (1990).\n11 ) Shattuck, P. L. , "Control of Remotely Operated Manipulation System", Proceedings of the Advanced Guidance and\nControl Aspects in Robotics Conference, Li bon, (1994).\n12) Sheridan, T. B., "Human Factors Considerations for Remote Manipulation", Proceedings of the Advanced Guidance\nand Control Aspects in Robotics conference, Lisbon, (1994).\n13) Sheridan, T. , "Space Teleoperation Through Time Delay: Review and Prognosis." IEEE Transactions on Robotic\nand Automation 9(5): 592-606, (1993).\n14) Sheridan, T. B., Telerobotics, Automation, and Human Supervisory COnlrot Cambridge, MA, MIT Press, (1992) .\n15) Tachi, S. , Tele-Existence ancI/or Cvbernetic Interface Studies in Japan. Human Machine Interfaces for\nTeleoperators and Virtual Environments, NASA Ames Research Center, (1991).\n16) Vidov, M. E., Visual Interface Issues in a Virtual Environmentfor Space Teleoperation, Mas ters thesis in\nAeronautics and Astronautics . Massachusetts Institute of Technology, Cambridge, MA, (1993).\n\n8\nAmerican Institute of Aeronautics and Astronautics\n\n'