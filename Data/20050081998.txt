b'I\n\nI\n\n,\n\n*\n\nWhat Information Theory says about Bounded\nRational Best Response\nDavid H. Wolpert\xe2\x80\x99\nNASA Ames Research Center, Moffett Field, CA, 94035, USA\nd h d e m a i l .a r c . nasa .gov\n\nSummary. Probability Collectives (PC) provides the information-theoretic extension of conventional full-rationality game theory to bounded rational games. Here\nan explicit solution to the equations giving the bounded rationality equilibrium of\na game is presented. Then PC is used to investigate games in which the players use\nbounded rational best-response strategies. Next it is shown that in the continuumtime limit, bounded rational best response games result in a variant of the replicator\ndynamics of evolutionary game theory- It is then shown that for team (shared-payoff)\ngames, this variant of replicator dynamics is identical to Newton-Raphson iterative\noptimization of the shared utility function.\n\n1 Introduction\nRecent work has used information theory [9, 121 to provide a principled extension of noncooperate conventional game theory to accommodate bounded\nrationality [25, 271. Intuitively, this extension starts with the observation that\nin the real world ascertaining a game\xe2\x80\x99s equilibrium is an exercise in statistical\ninference: one is given (or assumes) partial information about the behavior of\nthe players, and from that infers (!) what the joint mixed strategy is likely to\nbe. There are many ways to do such statistical inference. The one investigated\nin [27] is based on information theory\xe2\x80\x99s version of Occam\xe2\x80\x99s razor: Predict the\njoint mixed strategy that has as little extra information as possible beyond\nthe provided partial knowledge while being consistent with that knowledge.\nThis version of Occam\xe2\x80\x99s razor is known as the Maximum entropy (Maxent)\nprinciple [9, 121. It tells us that the mixed strategy of a game\xe2\x80\x99s equilibrium,\nq(z E X ) =\nqs(zz), the solution to a coupled set of Lagrangian functions\nis\nthat are specified by the game structure and the provided partial knowledge.\nSec. 2 reviews how information theory can be used to derive bounded\nrational noncooperative game theory. Some simple examples of the bounded\nrational equilibrium solutions of games are then presented. Sec. 3 analyzes\nscenarios in which the players use bounded rational versions of best response\n\nn,\n\n2\n\nDavid H. Wolpert\n\nstrategies. Particular attention is paid to team games, in which the players\nshare the same utility function. The analysis for this case provide insight into\nhow to optimize the sequence of moves by the players, as far as their shared\nutility is concerned. This can be viewed as a formal way to optimize the\norganization chart of a corporation.\nBest response strategies, even bounded rational ones, are poor models of\nreal-world computational players that use Reinforcement Learning (RL) [20].\nSec. 4 considers iterated games in which players use a (bounded rational)\nvariant of best response, a variant that is more realistic for computational\nplayers, and arguably for human players as well. In this variant the conditional\nexpected utilities used by player i to update her strategy, expected payoff given\nmove xz,is a decaying average of recent conditional expected utilities. This\ndecay biases the player to dampen large and sudden changes in her strategy.\nThis variant is then explored for the case of team games. The continuum\nlimit of the dynamics of such games is shown to be variant of the replicator\ndynamics. It is shown such continuum-limit bounded rational best response\nis identical to Newton-Raphson iterative optimization of the shared utility\nfunction of such games.\nThe formalism presented in this paper is a special case of the field of\nProbability Collectives (PC), a case in which the joint distribution over the\nvariables of interest is a product distribution. This special case is known as\nProduct Distribution (PD) theory [25, 27, 29, 28, 26, 71. P C has many applications beyond those considered in this paper, e.g., distributed optimization\nand control [16, 15, 2, 291. Finally, see [16] for relations to other work in game\ntheory, optimization, statistical physics, and reinforcement learning.\n\n2 Bounded Rational Noncooperative Game Theory\nIn this section we motivate PD theory as the information-theoretic formulation of bounded rational game theory. We use the integral sign\nwith the\nassociated measure implicit, i.e., it indicates sums if appropriate, Lebesgue\nintegrals over R" if appropriate, etc. In addition, the subscript (i) is used to\nindicate all index values other than i. Finally, we use P to indicate the set of\nall probability distributions over a vector space, and Q to indicate the subset of P consisting of all product distributions (i.e., the associated Cartesian\nproduct of unit simplices).\nIn noncooperative game theory one has a set of N players. Each player i\nhas its own set of allowed pure strategies. A mixed strategy is a distribution qi(zi)over player 2\'s possible pure strategies. Each player i also has a\nprivate utility function gi that maps the pure strategies adopted by all N of\nthe players into the real numbers. So given mixed strategies of all the players,\nthe expected utility of player i is E(gi) = s d x\nqj(zj)gi(z).\nIn a Nash equilibrium every player adopts the mixed strategy that maximizes its expected utility, given the mixed strategies of the other players. More\n\n(s)\n\nnj\n\n*\nI\n\nWhat Information Theory Says\n\n3\n\nnidi\n\nformally, Vz, qi = argmax,; .[dz qi\nq j ( z j ) gi(z).Perhaps the major objection that has been raised to the N&h equilibrium concept is its assumption\nof full rationality [lo, 6, 18, 4 . This is the assumption that every player i\n1\ncan both calculate what the strategies qj+ will be and then calculate its associated optimal distribution. In other words, it is the assumption that every\nplayer will calculate the entire joint distribution q(z) = ITj qj (zj).\nIn the real world, this assumption of full rationality almost never holds,\nwhether the players are humans, animals, or computational agents [5, 17,\n10, 3, 8, 1, 22, 141. This is due t o the cost of computation of that optimal\ndistribution, if nothing else. This real-world bounded rationality is a major\nimpediment to applying conventional game theory in the real world.\n\n2.1 Review o the minimum information principle\nf\nShannon was the first person to realize that based on any of several separate\nsets of very simple desiderata, there is a unique real-valued quantification of\nthe amount of syntactic information in a distribution P(y). He showed that\nthis amount of information is the negative of the Shannon entropy of that\ndistribution, S ( P ) = - J d y P(y)ln[#].\nSo for example, the distribution\nwith minimal information is the one that doesn\xe2\x80\x99t distinguish at all between\nthe various y, i.e., the uniform distribution. Conversely, the most informative\ndistribution is the one that specifies a single possible y. Note that for a product\ndistribution, entropy is additive, i.e., S(n, i ( y i ) ) =\nq\nS(qi).\nSay we given some incomplete prior knowledge about a distribution P(y).\nHow should one estimate P(y) based on that prior knowledge? Shannon\xe2\x80\x99s result tells us how to do that in the most conservative way: have your estimate\nof P ( y ) contain the minimal amount of extra information beyond that already\ncontained in the prior knowledge about P(y). Intuitively, this can be viewed\nas a version of Occam\xe2\x80\x99s razor: introduce as little extra information beyond\nthat you are provided in your inferring of P. This minimum information a p\nproach is called the maxent principle. It has proven extremely powerful in\ndomains ranging from signal processing to supervised learning [12]. In particular, it is has been successfully used in many statistics applications, including econometrics[l3]. It has even provided what many consider the cleanest\nderivation of the foundations of statistical physics [ll].\n\nCi\n\n2.2 Maxent Lagrangians\n\nMuch of the work on equilibrium concepts in game theory adopts the perspective of an external observer of a game. We are told something concerning\nthe game, e-g., its cost functions, information sets, etc., and from that wish to\npredict what joint strategy will be followed by real-world players of the game.\nSay that in addition to such information, we are told the expected utilities\nof the players. What is our best estimate of the distribution q that generated\n\n.\nDavid H. Wolpert\n\n4\n\nthose expected cost values? By the maxent principle, it is the distribution\nwith maximal entropy, subject to those expectation values.\nTo formalize this, for simplicity assume a finite number of players and of\npossible strategies for each player. To agree with the convention in fields other\nthan game theory (e.g., optimization, statistical physics, etc.), from now on\nwe implicitly flip the sign of each gi so that the associated player i wants to\nminimize that function rather than maximize it. Intuitively, this flipped g2(z)\nis the \xe2\x80\x9ccost\xe2\x80\x9d to player i when the joint-strategy is z.\nWith this convention, given prior knowledge that the expected utilities of\nthe players are given by the set of values { ~ i }the maxent estimate of the\n,\nassociated q is given by the minimizer of the Lagrangian\n\nz\n\nJ\n\n3\n\nwhere the subscript on the expectation value indicates that it evaluated under distribution q. The\nare \xe2\x80\x9cinverse temperatures\xe2\x80\x9d implicitly set by the\nconstraints on the expected utilities.\nSolving, we get the coupled equations\n\n{a}\n\nwhere the overall proportionality constant for each i is set by normalization,\nand G\npigi In Eq. 3 the probability of player i choosing pure strategy\nxi depends on the effect of that choice on the utilities of the other players.\nThis reflects the fact that our prior knowledge concerns all the players equally.\nIf we wish to focus only on the behavior of player i, it is appropriate to\nmodify our prior knowledge. First consider the case of maximal prior knowledge, in which we know the actual joint-strategy of the players, and therefore\nall of their expected costs. For this case, trivially, the maxent principle says\nwe should \xe2\x80\x9cestimate\xe2\x80\x9d q as that joint-strategy (it being the q with maximal\nentropy that is consistent with our prior knowledge). The same conclusion\nholds if our prior knowledge also includes the expected cost of player i.\nModify this maximal set of prior knowledge by removing from it specification of player i\xe2\x80\x99s strategy. So our prior knowledge is the mixed strategies of all\nplayers other than i, together with player i\xe2\x80\x99s expected cost. We can incorporate prior knowledge of the other players\xe2\x80\x99 mixed strategies directly, without\nintroducing Lagrange parameters. The resultant maxent Lagrangian is\n\nCi\n\nsolved by a set of coupled Boltxmann distributions:\n~~\n\n~\n\n\xe2\x80\x98The subscript q(i) on the expectation value indicates that it is evaluated according the distribution\nqj.\n\nnjft\n\nWhat Information Theory Says\n\n5\n\nFollowing Nash, we can use Brouwer\xe2\x80\x99s k e d point theorem t o establish that\nfor any non-negative values { p } , there must exist at least one product distribution given by the product of these Boltzmann distributions (one term in\nthe product for each i).\nThe first term in 3% minimized by a perfectly rational player. The second\nis\nterm is minimized by a perfectly zn-atzonal player, i.e., by a perfectly uniform\nmixed strategy qI. So in the maxent Lagrangian explicitly specifies the balance between the rational and irrational behavior of the player. In particular,\nfor p + 00, by minimizing the Lagrangians we recover the Nash equilibria\nof the game. More formally, in that limit the set of q that simultaneously\nminimize the Lagrangians is the set of mixed strategy equilibria of the game,\ntogether with the set of delta functions about the pure Nash equilibria of the\ngame. The same is true for Eq. 3.\nNote also that independent of information-theoretic considerations, the\nBoltzmann distribution is a reasonable (highly abstracted) model of human\nbehavior. Typically humans do some \xe2\x80\x9cexploration\xe2\x80\x9d as well as \xe2\x80\x9cexploitation\xe2\x80\x9d,\ntrying each move with probability that rises as the expected cost of the move\nfalls. This is captured in the Boltzmann distribution mixed strategy.\nOne can formalize the concept of the rationality of a player in a way that\napplies to any distrihtion, not just a B o l t n a m distribution. One does this\nwith a rationality operator which maps a q and a gt to a non-negative\nreal value measuring the rationality of player 2 in adopting strategy q, given\nprivate cost function gt and strategies q(*) of the other players. For the solution\nin Eq. 4 and private cost gl, the value of that operator is just pt [27].\nEq. 3 is just a special case of E . 4, where all player\xe2\x80\x99s share the same\nq\nprivate cost function, G. (Such games are known as team games.) This\nrelationship reflects the fact that for this case, the difference between the\nmaxent Lagrangian and the one in h. is independent of 4. Due to this\n2\n,\nrelationship, our guarantee of the existence of a solution to the set of maxent\nLagrangians implies the existence of a solution of the form E . 3. Typically\nq\nplayers will be closer to minimizing their expected cost than maximizing it.\nFor prior knowledge consistent with such a case, the pa are all non-negative.\nFor each player i define ft(z,qt(zt))P t g t ( z ) ln[q,(z,)].\nE\nThen we can\nwrite the maxent Lagrangian for player i as\n\n+\n\nNow in a bounded rational game every player sets its strategy to minimize its\nLagrangian, given the strategies of the other players. In light of Eq. 5, this\nmeans that we can interpret each player in a bounded rational game as being\nperfectly rational for a cost function that incorporates its computational cost.\nTo do so we simply need to expand the domain of \xe2\x80\x9ccost functions\xe2\x80\x9d to include\n(logarithms of) probability values as well as joint moves.\n\n6\n\nDavid H. Wolpert\n\n2.3 Examples of bounded rational equilibria\nIt can be difficult to start with a set of cost functions and associated r a t i e\nnalities ,& and then solve for the associated bounded rational equilibrium q.\nSolving for q when prior knowledge consists of expected costs ~i rather than\nrationalities can be even more tedious. (In that situation the\nare not specified upfront but instead are Lagrange parameters that we must solve for.)\nHowever there is an alternative approach to constructing examples of games\nand their bounded rational equilibria that is quite simple. In this alternative\none starts with a particular mixed strategy q and then solves for a game for\nwhich q is a bounded rational equilibrium, rather than the other way around.\nTo illustrate this, consider a 2-player single-stage game. Let each player\nhave 3 possible moves, indicated by the numerals 0,1, and 2. Say the (bounded\nrational) mixed strategy equilibrium is\nqi(0) = 1/2, qi(1) = 1/4, qi(2) = 1/4;\nq2(0) = 2/3, 42(1) = 1/4, q2(2) = 1/12 .\n\n(6)\n\nNow we know that at the equilibrium, ql(zl) e-@lE(g1Iz1), where Pi\nc\n(\nis player 1\xe2\x80\x99s rationality, and g1 is her cost function (the negative of her cost\nfunction). This means for example that\n\nPl[E(gl\n\nI 5 1 = 0) - E(g1 I 5 1 = I)] = -142).\n\n(7)\n\nA similar equation governs the remaining independent difference in expectation values for player 1. The analogous two equations for player 2 also hold.\nNow define the vectors g 2 , 3 ( . )3 gz(z, = 3 , .). So for example g1,o =\n(gl(z1 = 0,322 = O),gl(zl = 0 , =~l ) , g l ( z l = 0,52 = 2)). Then we can\nexpress our equations compactly as four dot product equalities:\nPl(gl,o - g l J ) .472 = -142) ; Pl(fXl,O - g 1 , 2 ) . 4 2 = - W ) ;\nP2(g2,0- g 2 , l ) . QI = -ln(8/3) ; Pz(g2,o - g2,2) . qi = -In(8) .\n\n(8)\n\nWe can absorb each pzinto its associated g2;all that matters is their product.\nWe can now plug in for the vectors q1 and q2 from Eq. 6 and simply write\ndown a set of solutions for the four three-dimensional vectors g 2 , J For these\n.\n{ g E }the bounded ratinal equilibrium is given by the q of Eq. 6 . If desired, we\ncan evaluate the associated expected values of the cost functions for the two\nplayers; our q is the bounded ratinal equilibrium for those expected costs.\nNote that the variables in the first pair of equalities in Eq. 8 are independent of those in the second pair. In other words, whereas the Boltzmann\nequations giving q for a specified set of g, are a set of coupled equations, the\n\nWhat Information Theory Says\n\n7\n\nequations giving the gz for a specified q are not coupled. Note also that our\nequations for the g,,] are (extremeiyj underculsil&td\nX-ii&,iZk+i I G\nI T\ncompressive the mapping kom the gZ to the associated equilibrium q is. Bear\nin mind though that that mapping is also multi-valued in general; in general\na single set of cost functions can have more than one equilibrium, just like it\ncan have more than one Nash equilibrium.\nThe generalization of this example to arbitrary numbers of players with\narbitrary move spaces is immediate. As before, indicate the moves of every\nplayer by an associated set of integer numerals starting at 0. Recall that the\nsubscript (2) on a vector indicate all components but the i\xe2\x80\x99th one. Also absorb\nthe rationalities\ninto the associated 9%.\nNow specify q and the vectors gz(zz= O,.) (one vector for each i) to be\nanything whatsoever. Then for all players i, the only associated constraint on\nthe i\xe2\x80\x99th cost function concerns certain projections of the vectors g,(z, > 0, .)\n(one projection for each value z, > 0). Concretely, Qi,z,> 0,\n\nAll the terms on the right-hand side are specified, a well as the q(i) term on\ns\nthe left-hand side. Any g i ( z i , - ) that obeys the associated equation has the\nspecified q as a bounded rational equilibrium.\nSee [27] for discussion of alternative interpretations of this informationtheoretic formulation of bounded rationality. That reference also discusses\nkinds of prior knowledge that do not result in the Maxent Lagrangian, in\nparticular knowledge based on finite data sets (Bayesian inference). A scalarvalued quantification of the rationality of a player is also presented there.\n\n3 Bounded rational versions of best response\nOne crude way to try to iind the q given by Eq. 4 would be an iterative process akin to the best-response scheme of game theory [lo]. Given any current\ndistribution q, in this scheme all agents i simultaneously replace their current\ndistributions. In this replacement each agent i replaces q, with the distribution given in Eq. 4 based on the current q(i). This scheme is the basis of the\nuse of Brouwer\xe2\x80\x99s k e d point theorem to prove that a solution to l3q. 4 exists.\nAccordingly, it is called parallel Brouwer updating. (This scheme goes by\nmany names in the literature, from Boltzmann learning in the RL community\nto block relaxation in the optimization community.)\nSometimes conditional expected costs for each agent can be calculated explicitly at each iteration. More generally, they must be estimated. This can\n\n.\n\nI\n\n8\n\nDavid H. Wolpert\n\nbe done via Monte Carlo sampling, iterated across a block of time. During\nthat block the agents all repeatedly and jointly IID sample their (unchanging)\nprobability distributions to generate joint moves, and the associated cost values are recorded. These are then use to estimate all the conditional expected\ncosts, which then determine the parallel Brouwer update \xe2\x80\x99.\nThis is exactly what is done in RL-based schemes in which each agent\nmaintains a data-based estimate of its cost for each of its possible moves,\nand then chooses its actual move stochastically, by sampling a Boltzmann\ndistribution of those estimates. (See [25] for ways to get accurate MC estimates\nmore efficiently than in this simple scheme, e.g., by exploiting the bias-variance\ntradeoff of statistics.)\nOne alternative to parallel Brouwer updating is serial Brouwer updating,\nwhere we only update one qz at a time. This is analogous to a Stackelberg\ngame, in that one agent makes its move and then the other(s) respond [4,\n61. In a team game, any serial Brouwer updating must reduce the common\nLagrangian, in contrast to the case with parallel Brouwer updating.\nThere are many versions of serial updating. In cyclic serial Brouwer u p\ndating, one cycles through the i in order. In random serial Brouwer updating,\none cycles through them in a random fashion.\nIn greedy serial Brouwer updating, instead of cycling through all i, at each\niteration we choose what single player to update based on the associated drop\nin the common Lagrangian. Those drops can be evaluated without calculating\nthe associated Boltzmann distributions. To see how, use N, t o indicate the\nnormalization constant of Eq. 4. Then define the Lagrangian gap at q for\n+Jdzzqz(z,)Eq,,,(gz ,) J d ~ z q , ( ~ z ) l n [ q z (This is. how\nI2\n~z>l\nplayer as 1n[NZ1\nmuch 9 reduced if only qz undergoes the Brouwer update \xe2\x80\x99.\nis\nAnother obvious variant of these schemes is mixed serial/parallel Brouwer\nupdating, in which one subset of the players moves in synchrony, followed by\nanother subset, and so on. Such updating in a team game can be viewed as\na simple model of the organization chart of the players. For example, this is\nthe case when the players are a corporation, with G being a common cost\nfunction based on the corporation\xe2\x80\x99s performance.\n\n+\n\n\xe2\x80\x99Parallel Brouwer updating has minimal memory requirements on the agents. Say\nagent i has just made a particular move, getting cost T , and that the most recent\nprevious time it made that time was T iterations ago. Then the new estimated cost\nfor that move, E\xe2\x80\x98, is related to the previous one, E , by E\xe2\x80\x99 =\nwhere k is\na constant less than 1, and a is initially set to 1, while itself also being updated\naccording to a+ = kT. So agent i only needs to keep a running tally of E , a , and T\nfor each of its possible moves to use data-aging, rather than a tally of all historical\ntime-cost pairs\n3Proof outline: Write the entropy after the update as a sum of non-a entropies\n(which are unchanged by the update) plus 2\xe2\x80\x99s new entropy. Then expand i\xe2\x80\x99s new\nentropy. This gives the value of the new Lagrangian as -ln[N,]. Then do the subtraction.\n\nT:!kTe,\n\n.\n\n9\n\nwhat Information Theory Says\n\nSay we observe the functioning of such an organization over time, and view\nthose observations as Monte Carlo sampling of its beiiaviui. TZeii %-e c x i -isc\nthose samples to statistically estimate how best to do serial/parallel Brouwer\nupdating, for the purpose of minimizing the shared cost function G. This can\nbe viewed as a way to optimize the organization chart coupling the players.\n\n4 Parallel Brouwer with data-aging is Nearest Newton\nThis section considers a variant of best-response that is more realistic (more\naccurately modeling a b a s e d computational players that are actually used\nin machine learning, and arguably more accurately modeling human players\na well). In this variant the expected cost used by each player to update her\ns\nstrategy is a decaying average of recent expected utilities;_this decay reflects\na conservative preference for dampening large changes in strategy.\nSuch a bias is used (implicitly or otherwise) in most multi-player l algw\nU\nrithms. For example, in the COIN framework each agent i collects a data set\nof pairs of what value its private cost function has at timestep t together with\nthe move it made then. It then estimates its cost for move x as a weighted\ni\naverage of all the cost values in its data set for that move. The weights are\nexponentially decaying functions of how long ago the associated observation\nwas made. This data-aging is crucial to reflect the non-stationarity of agent\n2\xe2\x80\x99s environment, i.e., that the other agents are changing their strategies with\ntime. Arguably, humans use similar modifications to best response. Indeed, in\nidealized learning rules like ficticious play, such dampening is crucial.\n4.1 The dynamics of Brouwer updating\n\nConsider a multi-stage game where at the end of iteration t , each player i\nupdates her distribution qi(., t ) t o\n\nThis is a generalization of parallel Brouwer updating, where the function being\nexponentiated can be Q values (as in Q-learning[24]), single-instant reward\nvalues, distorted versions of these (e.g., to incorporate data-aging), etc.\nAs an example, for single-instant rewards (i.e., conventional parallel Brouwer),\n@i(xi,t) is player i\xe2\x80\x99s estimate of (pi times) her conditional expected cost for\ntaking move x at time t - 1. If that estimate were exact, this would mean\ni\n\n@*(%t) P%7*\n=\n\n1 =P\n4\n\n1\n\n&.(*)q*)(qz),t\n\n- 1)gzhq*)).\n\n(11)\n\nAs another example, for Q-learning, one player is Nature and her distribution\nis always a delta function. In this case @i(zi,t)is the Q-value for player i\n\n10\n\nDavid H. Wolpert\n\ntaking action x,,when the state of Nature is as specified by the associated\ndelta function in q ( . ,t - 1).\nNote that there\xe2\x80\x99s no Monte Carlo sampling being done here, as there is\nin most real-world RL; this is a somewhat abstracted version of such FIL.\nAlternatively, the analysis here becomes exact when @a is evaluated closed\nform, or (as when @a is an empirical expectation value) there\xe2\x80\x99s enough samples\nin a Monte Carlo block so that empirical averages effectively give US exact\nvalues of expected quantities.\nAt this point we have to say something about how @a evolves with time.\nConsider the case where @$ is an estimate of some function\nformed by\nexponential aging of the previous q5 values. In our case (since everything is\nevaluated closed form) assuming there have been an infinite number of preceding timesteps, this is the same as geometric data-aging:\n@a(za,t)=a+a(xa,q(t- I))+ (1-a)@t(za>t-1)\n\n(12)\n\nfor some appropriate function q5a \xe2\x80\x98. For example, in parallel Brouwer updating,\nq 5 a ( z z , t ) = PE(ga I ~ % , q ( ~ ) ( t ) ) , Q a ( x a , t )is a geometric average of the\nwhile\nprevious values of q5(zZ).\n4.2 The continuum-time limit\n\nTo go to the continuum-time limit, let t be a real variable, and replace the\ntemporal delay value of 1 in Eq. 12 with 6 and a with a b (we\xe2\x80\x99ll eventually\ntake b -+ 0). In addition differentiate Eq. 10 with respect to t to get\n\nIn the b + 0 limit, assuming q is a continuous function o f t , Eq. 12 becomes\n.\n\nwhere from now on the t variable is being suppressed for clarity.\nIf we knew the dynamics of 4i, we could solve Eq. 14 via integrating factors,\ninto Eq. 13.\nin the usual way. Instead, here we\xe2\x80\x99ll plug that equation for\nThen use Eq. 10 to write @ i ( z i , q ) = constant -ln(qi(q)). The result is\n\n4 T see this is exponential data-aging with exponent y set y = -ln(l - a)).\n~\n\n- .\n\n.\n\nWhat Information Theory Says\n\n4.3 Ee!&cn\n\n11\n\nwith Nearest Newton descent and replicator\n\ndynamics\n\nA s mentioned previously, there axe many ways to find equilibria, and in particular many distributed algorithms for doing so. This is especially so in team\ngames. where h d i n g such equilibria reduces to descending a single overarching Lagrangian. One natural idea for descent in such games is to use\nthe Newton-Raphson descent algorithm. However that algorithm cannot be\napplied directly to search across q in a distributed fashion, due to the need\nto invert matrices coupling the agents. As an alternative, one can consider\nwhat new distribution p the Newton algorithm would step to if there was no\nrestriction that p be a product distribution. One can then ask what product\ndistribution is closest to p , according to Kullback-Leibler distance[9]. It turns\nout that one can solve for that optimal product distribution. The associated\nupdate rule is called the Nearest Newton algorithm[29].\nIt turns out that when one writes down the Nearest Newton update rule,\nit says to replace each component q,(z,) with the exact quantity appearing\non the right-hand side of Eq. 15, where Q is the stepsize of the update, and\nq5,(xt,t) = PE(G I z,,q(,)(t)), as in parallel Brouwer updating for a team\ngame In other words, in team games, the continuum limit of having each\nplayer using (bounded rational) best response is identical to the continuum\nlimit of the Newton-Raphson algorithm for descending the Lzgiangim, with\nthe data-aging parameter Q giving the stepsize.\nEq. 15 arises in other yet other contexts as well. In particular, say @, is\nconditional expected rewards (i.e., #,(z,,t - 1) = E(g, I q(..t - 1). Then\n))\nthe p + 00 limit of Eq. 15 reduces to a simplified form of the replicator\ndynamics equation of evolutionary game theory[21, 231. (If the stepsize a is\nan appropriately increasing function of E(G) other versions of that dynamics\narise.) This is because in that limit the I term disappears, and the righthand\nn\nside of Eq. 15 involves only the difference between player i\'s expected cost\nand the average expected cost of all players. This Sway connection suggests\nusing some of the techniques for solving replicator dynamics to expedite either\nparallel Brouwer or Nearest Newton.\n\n\'.\n\n4.4 Convergence and equilibria\n\nBy Eq. 15, at equilibrium, for each i, qi(xi)[q5i(xi,q)+In(qi(zi))]\nmust be independent of i. One way this can occur is if it equals 0. However qi(zi)can never\nbe 0, by Eq. 10. This means we have an equilibrium at q i ( z i ) c e-4i(zaiq).\nx\nIntuitively, this is exactly what we want, according to Eq. 10 and our interpretation of &(xi, q) as an estimate of $*(xi,q). Note also that this solution\nmeans that q5i(xi, q ) = @i(zi, so that (according to Eq. 14) @i(zi, ) has\nq),\nq\nalso reached an equilibrium.\n5More generally, Nearest Newton uses this update rule with +;(z;,t) PE(gi\n=\nzi, c i ) ( t ) )where each g;(z) = G(z) - D ( z ( ; ) ) some fundion D.see [29].\nq\nfor\n\nI\n\n12\n\nDavid H. Wolpert\n\nWhen our equilibrium has q,(z2)[q5,(x,,\nq)\nq2\n\n(xz)\n\n+ ln(q,(z,))] = A # 0, we have\n\ne - d 4 4 * (Gdf\n\n(16)\n\nIn light of Eq. 10, this means that @,(x,,q ) # +,(z,, 4). So by Eq. 14, @,(z,, q)\nhasn\'t reached an equilibrium in this case:\n\nIf both q,(z,) and &(x,, q ) were frozen at this point, this solution for @,(z,, q )\nwould not obey Eq. 12. So either q,(x,) and/or 4,(xa1) cannot be frozen. In\nq\nfact, if 4,(z,,q) varies with time, then we know by Eq. 15 that q,(z,)varies\nas well. So in either case q a ( z z )\nmust vary, i.e., this equilibrium is not stable.\nAlthough the dynamics has the desired k e d point, it may take a long time\nto converge there. There are several ways to analyze that: One is to examine\nthe second derivatives (with respect to time) of the q, and/or the @,. Another\nis to examine the time-dependence of the residual error,\n\nThe next subsection includes a convergence analysis involving residual errors,\nbut for a different variant of Brouwer from the ones considered so far.\n4.5 Other variants of Brouwer updating\n\nData-aging can be viewed as moving only part-way from the current Q2 to\nwhat it should be (i.e. to 4,). An alternative is to dispense with the @, and 4z\naltogether, and instead step part-way from the current q to what it should be,\ni.e., partially move to the (bounded rational) best response mixed strategy.\nFormally, this means replacing Eq. 10 so that the update is not implicit] in\nhow djZ(zz, depends on the past value of q(t - 1) (Eq. 12), but explicit:\nt)\nq z ( x a , t ) = ~z(xz, - 1)\nt\n\n+ a[hz(xaiq(a)(t - 1))- qz(xz, t - I)]\n\n(19)\n\nis ( t )\nwhere / ~ , ( x ~ ] q ( ~ )the) Boltzmann distribution of what q a ( x , , t )would be,\nunder ideal circumstances, and we implicitly have small stepsize a.\nThe only fixed point of this updating rule is where q, = h, Vz. So just\nlike with continuum-limit parallel Brouwer, we have the correct equilibrium.\nTo investigate how fast the update rule of Eq. 19 arrives at that equilibrium]\nwrite its error at time t as the residual\n\nWhat Information Theory Says\n\n13\n\nw&rs =-e hz-;\xe2\x80\x99:~~ r s l i m ~ d all all players other than i are updating themthat\nselves in the same that i does (i.e., via Eq. 19), and h(,)(q(t i ) j means &\n\xe2\x80\x98\nvector of the values of all h3+,(zJ)\nevaluated for q(t - 1).\nWith obvious notation, rewrite Eq. 20 a\ns\nT,st(%t)\n\n- 1)[1 4\nah,(z,,q(z)(t 1))\n-\n\n= qz(G,t\n\n+\n\n- ha[zz, q ( z ) ( t - 1)- ar(a)(t 111-\n\n(21)\n\nNow use the fact that cy is small to expand the last h, term on the righthand\nside to k t order in its second (vector-valued) argument, getting the result\ns\n\nwhere the gradient of hi is with respect to the vector components of its-second\nargument. Accordingly, if rst(z1)\nstarts much larger than the other residuals,\nit will be pushed down to their values. Conversely, if it starts much smaller\nthan them, it will rise.\nThere are other ways one can reduce a stochastic game to a deterministic\ncontinuum-time process. In particular, this can be done in closed form for\nficticious play games and some simple variants of it [19, lo].\nAcknowledgements:I would like to thank Stefan Bieniawski, Bill Macready,\nGeorge Judge, Chris Heme, and Ilan Kroo for helpful discussion.\n\nReferences\n1. AL-NAJJAR,N. I., and R. SMORODINSKY,\n\xe2\x80\x9cLarge nonanonymous repeated\ngames\xe2\x80\x9d, Game and Economic Behavior 37, 2639 (2001).\nN.,\nI.\nand D. H. WOLPERT,\n\xe2\x80\x9cFleet assignment\n2. ANTOINE, S. BIENIAWSKI,KROO,\nusing collective intelligence\xe2\x80\x9d, Proceedings of 4 Z n d Aerospace Sciences Meeting,\n(2004), AIAA-2004-0622.\n3. ARTHUR, W. B., \xe2\x80\x9cComplexity in economic theory: Inductive reasoning and\nbounded rationality\xe2\x80\x9d, American Economic Review 84, 2 (May 1994), 406-411.\nR.J., and S. HAW, Handbook of Game Theory with Economic Appli4. AUMANN,\ncations, North-Holland Press (1992).\n5. AXELROD, The Evolution of Coopemtion, Basic Books NY (1984).\nR.,\nT.,\n6. BASAR, and G.J. OLSDER,Dynamic Noncoopemtive Game Theory, Siam\nPhiladelphia, PA (1999), Second Edition.\n7. BIENIAWSKI, and D. H. WOLPERT,\xe2\x80\x9cAdaptive, distributed control of conS.,\nstrained multi-agent systems\xe2\x80\x9d, Proceedings of A A M A S 04, (2004).\n8. BOUTILIER, Y . SHOHAM, M. P. WELLMAN,\nC.,\nand\n\xe2\x80\x9cEditorial: Economic principles of multi-agent systems\xe2\x80\x9d, Artificial Intelligence Journal 94 (1997), 1-6.\n9. COVER, T., and J. THOMAS, Elements of Information Theory, WileyInterscience New York (1991).\n\n14\n\nDavid H. Wolpert\n\n10. FUDENBERG, D., and D. K. LEVINE,The Theory of Learning in Games, MIT\nPress Cambridge, MA (1998).\n11. JAYNES, T., \xe2\x80\x9cInformation theory and statistical mechanics\xe2\x80\x9d, Physical Review\nE.\n106 (1957), 620.\nE.\nProbability Theory : The Logic of\n12. JAYNES, T., and G. Larry BRETTHOFGT,\nScience, Cambridge University Press (2003).\n13. JUDGE,G., D. MILLER,and W. CHO, \xe2\x80\x9cAn information theoretic approach\nto ecological estimation and inference\xe2\x80\x9d, Ecological Inference:New methodological Strategies (KING,ROSEN,\nAND TANNER\neds.), Cambridge University Press\n(2004).\n14. KAHNEMAN,D., \xe2\x80\x9cA psychological perspective an economics\xe2\x80\x9d, American Economic Review (Proceedings) 93:2 (2003), 162-168.\n15. LEE, C. Fan, and D. H. WOLPERT,\xe2\x80\x9cProduct distribution theory for control of\nmulti-agent systems\xe2\x80\x9d, Proceedings of A A M A S 04, (2004).\n16. MACREADY, William, and David H. WOLPERT, \xe2\x80\x9cDistributed constrained optimization with semi-coordinate transformations\xe2\x80\x9d, submitted (2004).\n17. NEYMAN,A., \xe2\x80\x9cBounded complexity justifies cooperation in the finitely repeated\nprisoner\xe2\x80\x99s dilemma\xe2\x80\x9d, Economics Letters 19 (1985), 227-230.\nA\n18. OSBORNE, and A. RUBENSTEIN, Course in Game Theory, MIT Press\nM.,\nCambridge, MA (1994).\nJ.S., and G. ARSLAN, \xe2\x80\x9cDynamic fictitious play, dynamic gradient\n19. SHAMMA,\nplay, and distributed convergence to nash equilibria\xe2\x80\x9d , submitted (2004).\n20. SUTTON, S., and A. G. BARTO,Reinforcement Learning: An Introduction,\nR.\nMIT Press Cambridge, MA (1998).\nA.\n\xe2\x80\x9cExtended replicator\n21. Tuns, K., D. HEYTENS, NOWE,and B. MANDERICK,\ndynamics as a key to reinforcement learning in multi-agent systems\xe2\x80\x9d, Lecture\nNotes in Artificial Intelligence, LNAI, (ECML 2003), (2003).\n22. TVERSKY, and D. KAHNEMAN,\nA.,\n\xe2\x80\x9cAdvances in prospect theory: Cumulative\nrepresentation of uncertainty\xe2\x80\x9d, Journal of Risk and Uncertainty 5 (1992), 297323.\n\xe2\x80\x9cCoordinated exploration in stochas23. VERBEECK, A. NOWE,and K. TUYLS,\nK.,\ntic common interest games\xe2\x80\x9d, Proceedings of A A M A S - 3 . University of Wales,\nAberystuyth, (2003).\n24. WATKINS, and P. DAYAN,\xe2\x80\x9cQ-learning\xe2\x80\x9d, Machine Learning 8, 3/4 (1992),\nC.,\n279-292.\n25. WOLPERT, H., \xe2\x80\x9cFactoring a canonical ensemble\xe2\x80\x9d, cond-mat/0307630.\nD.\n26. WOLPERT,\nDavid H., \xe2\x80\x9cFinding bounded rational equilibria part 1: Iterative\nfocusing\xe2\x80\x9d, Proceedings of the International Society of Dynamic Games Conference, 2004, (2004), in press.\n27. WOLPERT, H., \xe2\x80\x9cInformation theory - the bridge connecting bounded ratioD.\nnal game theory and statistical physics\xe2\x80\x9d, Complex Engineering Systems (A. M.\nD. BRAHA\nAND Y. BAR-YAM\neds.), (2004).\nD.\n\xe2\x80\x9cAdaptive distributed control: beyond\n28. WOLPERT, H., and S. BIENIAWSKI,\nsingle-instant categorical variables\xe2\x80\x9d, Proceedings of MSRAS04 (A. s. ET AL\ned.), Springer Verlag (2004).\n\xe2\x80\x9cDistributed control by lagrangian s t e e p\n29. WOLPERT, H., and S. BIENIAWSKI,\nD.\nest descent\xe2\x80\x9d, Proceedings of CDC 0 4 , (2004).\n\n'