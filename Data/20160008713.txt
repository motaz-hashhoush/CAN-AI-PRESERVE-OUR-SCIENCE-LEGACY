b'FLIGHT TEST SERIES 3\nFlight Test Report\nMike Marston, NASA AFRC Operations Engineer\nDaniel Sternberg, NASA AFRC Operations Engineer\nSteffi Valkov, NASA AFRC Operations Engineer\n\nNASA Armstrong Flight Research Center\nUAS-NAS IT&E Subproject\nOctober 2015\n\nRELEASE:\nREVISION B\n\nOverview\nThis document is a flight test report from the Operational perspective for Flight Test Series\n3, a subpart of the Unmanned Aircraft System (UAS) Integration in the National Airspace\nSystem (NAS) project. Flight Test Series 3 testing began on June 15, 2015, and\nconcluded on August 12, 2015.\nParticipants included NASA Ames Research Center, NASA Armstrong Flight Research\nCenter, NASA Glenn Research Center, NASA Langley Research center, General Atomics\nAeronautical Systems, Inc., and Honeywell.\nKey stakeholders analyzed their System Under Test (SUT) in two distinct Configurations.\nConfiguration 1, known as Pairwise Encounters, was subdivided into two parts: 1a,\ninvolving a low-speed UAS ownship and intruder(s), and 1b, involving a high-speed\nsurrogate ownship and intruder. Configuration 2, known as Full Mission, involved a\nsurrogate ownship, live intruder(s), and integrated virtual traffic.\nTable 1 is a summary of flights for each Configuration, with data collection flights\nhighlighted in green.\nSection 2 and 3 of this report give an in-depth description of the Flight Test Period, Aircraft\ninvolved, Flight Crew, and Mission Team.\nOverall, Flight Test 3 gathered excellent data for each SUT. We attribute this successful\noutcome in large part from the experience that was acquired from the ACAS Xu SS flight\ntest flown in December 2014. Configuration 1 was a tremendous success, thanks to the\ntraining, member participation, integration/testing, and in-depth analysis of the flight\npoints. Although Configuration 2 flights were cancelled after 3 data collection flights due\nto various problems, the lessons learned from this will help the UAS in the NAS project\nmove forward successfully in future flight phases.\n\nii\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nTable 1. Overview of Flights from Flight Test Series 3 for UAS in the NAS.\n\nFlight\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\nDate\nJune 15, 2015\nJune 17, 2015\nJune 18, 2015\nJune 22, 2015\nJune 24, 2015\nJune 26, 2015\nJuly 7, 2015\nJuly 9, 2015\nJuly 10, 2015\nJuly 21, 2015\nJuly 22, 2015\nJuly 24, 2015\n\nFlight\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\nDate\nJuly 13, 2015\nJuly 16, 2015\nJuly 28, 2015\nJuly 29, 2015\nJuly 29, 2015\nJuly 30, 2015\nAugust 3, 2015\nAugust 4, 2015\nAugust 4, 2015\nAugust 5, 2015\nAugust 6, 2015\nAugust 7, 2015\nAugust 10, 2015\nAugust 11, 2015\nAugust 12, 2015\n\nConfiguration 1\nHours Encounters\n6.9\n0\n5.0\n15\n4.9\n23\n4.5\n20\n4.7\n20\n4.6\n16\n4.8\n22\n4.8\n23\n4.6\n20\n4.8\n20\n3.4\n17\n3.2\n16\n56.2\n212\nConfiguration 2\nHours Encounters\n3.1\n0\n1.3\n0\n3.1\n2\n2.0\n0\n1.9\n0\n1.1\n0\n3.3\n4\n2.8\n0\n2.9\n2\n3.0\n5\n1.1\n0\n2.8\n1\n2.8\n8\n2.7\n8\n2.5\n8\n36.35\n38\n\niii\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nAircraft\nNASA 870\nNASA 870, N3GC\nNASA 870, N3GC\nNASA 870, N3GC\nNASA 870, N3GC\nNASA 870, N3GC, NASA 865\nNASA 870, N3GC\nNASA 870, N3GC\nNASA 870, N3GC, NASA 865\nNASA 870, NASA 850, NASA 865\nNASA 870, NASA 865\nNASA 870, NASA 865\n\nAircraft\nNASA 608\nNASA 608\nNASA 608, NASA 865\nNASA 608\nNASA 608\nNASA 608\nNASA 608, NASA 7, N3GC\nNASA 608\nNASA 608, N3GC\nNASA 608, N3GC\nNASA 608, NASA 865, N3GC\nNASA 608, NASA 865, N3GC\nNASA 608, NASA 865, N3GC\nNASA 608, NASA 865, N3GC\nNASA 608, NASA 865, N3GC\n\nTable of Contents\nOVERVIEW ............................................................................................................................... II\n1\n\nINTRODUCTION ................................................................................................................ 1\n1.1\nSCOPE ............................................................................................................................................. 1\n1.2\nPURPOSE.......................................................................................................................................... 2\n1.3\nSTAKEHOLDERS, PARTICIPANTS, AND RESPONSIBILITIES ............................................................................ 3\n1.4\nOPERATIONS WORKING GROUP ........................................................................................................... 4\n\n2\n\nFLIGHT TEST PERIOD...................................................................................................... 5\n\n3\n\nFLIGHT CREW AND MISSION TEAM ............................................................................... 6\n\n4\n\nSYSTEM CONFIGURATION ............................................................................................. 7\n\n5\n\nFLIGHT TEST ADMINISTRATIVE INFORMATION ........................................................... 8\n5.1\nOPERATING AREA .............................................................................................................................. 8\n5.2\nAIRCRAFT STAGING AND LOCATIONS ..................................................................................................... 9\n5.3\nWEATHER ...................................................................................................................................... 10\n5.4\nSTAND ALONE FACILITY MISSION CONTROL ROOM ............................................................................... 11\n5.4.1\nControl Displays................................................................................................................... 12\n5.4.2\nTest Coordinator Position .................................................................................................... 12\n5.5\nMISSION INFORMATION.................................................................................................................... 12\n5.6\nTRAINING AND QUALIFICATIONS......................................................................................................... 15\n5.6.1\nPre-Test Coordination and Training .................................................................................... 15\n5.6.2\nSystem Under Test Training ................................................................................................ 16\n5.7\nALTIMETER CALIBRATION .................................................................................................................. 17\n5.7.1\nConfiguration 1 ................................................................................................................... 17\n5.7.2\nConfiguration 2 ................................................................................................................... 19\n5.8\nSAFETY AND MISSION RULES ............................................................................................................. 19\n\n6\n\nFLIGHT EXECUTION .......................................................................................................23\n6.1\nCONFIGURATION 1: PAIRWISE ENCOUNTERS ........................................................................................ 23\n6.1.1\nConfiguration 1 Nomenclature Development ..................................................................... 23\n6.1.2\nConfiguration 1a Low Speed Ownship ................................................................................ 24\n6.1.3\nConfiguration 1b High Speed Ownship ............................................................................... 47\n6.1.4\nMatrix Development ........................................................................................................... 47\n6.1.5\nFlight Card Description ........................................................................................................ 54\n6.2\nCONFIGURATION 2: FULL MISSION ENCOUNTERS .................................................................................. 60\n6.2.1\nFireline Route Development ................................................................................................ 61\n6.2.2\nConstraints and Limitations ................................................................................................ 62\n6.2.3\nFlight Card Description ........................................................................................................ 63\n\n7\n\nFLIGHT SUMMARY ..........................................................................................................69\n7.1\nCONFIGURATION 1........................................................................................................................... 69\n7.1.1\nFlight 1: June 17, 2015 ........................................................................................................ 70\n7.1.2\nFlight 2: June 18, 2015 ........................................................................................................ 71\n7.1.3\nFlight 3: June 22, 2015 ........................................................................................................ 72\n7.1.4\nFlight 4: June 24, 2015 ........................................................................................................ 73\niv\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.1.5\nFlight 5: June 26, 2015 ........................................................................................................ 75\n7.1.6\nFlight 6: July 7, 2015............................................................................................................ 76\n7.1.7\nFlight 7: July 9, 2015............................................................................................................ 77\n7.1.8\nFlight 8: July 10, 2015.......................................................................................................... 78\n7.1.9\nFlight 9: July 21, 2015.......................................................................................................... 79\n7.1.10 Flight 10: July 22, 2015........................................................................................................ 80\n7.1.11 Flight 11: July 24, 2015........................................................................................................ 81\n7.2\nCONFIGURATION 2........................................................................................................................... 82\n7.2.1\nFlight 1: July 13, 2015.......................................................................................................... 83\n7.2.2\nFlight 2: July 16, 2015.......................................................................................................... 83\n7.2.3\nFlight 3: July 28, 2015.......................................................................................................... 83\n7.2.4\nFlight 4: July 29, 2015.......................................................................................................... 84\n7.2.5\nFlight 5: July 29, 2015.......................................................................................................... 84\n7.2.6\nFlight 6: July 30, 2015.......................................................................................................... 84\n7.2.7\nFlight 7: August 3, 2015 ...................................................................................................... 85\n7.2.8\nFlight 8: August 4, 2015 ...................................................................................................... 85\n7.2.9\nFlight 9: August 4, 2015 ...................................................................................................... 85\n7.2.10 Flight 10: August 5, 2015 .................................................................................................... 86\n7.2.11 Flight 11: August 6, 2015 .................................................................................................... 87\n7.2.12 Flight 12: August 7, 2015 .................................................................................................... 87\n7.2.13 Flight 13: August 10, 2015 \xe2\x80\x93 Data Collection 1 ................................................................... 88\n7.2.14 Flight 14: August 11, 2015 \xe2\x80\x93 Data Collection 2 ................................................................... 89\n7.2.15 Flight 15: August 12, 2015 \xe2\x80\x93 Data Collection 3 ................................................................... 90\n8\n\nOBSERVATIONS AND ISSUES .......................................................................................91\n8.1\nMAJOR IMPACTS ............................................................................................................................. 91\n8.1.1\nAn intruder was within 1 NM and less than 500 ft vertical separation without being visual\non Ikhana. ........................................................................................................................................... 91\n8.1.2\nFull mission Configuration 2 flights only completed 3 of 10 planned data collection flights\nbefore being cancelled. ...................................................................................................................... 92\n8.1.3\nConfiguration 1b was not attempted. ................................................................................. 92\n8.2\nMINOR IMPACTS AND LESSONS LEARNED ............................................................................................ 92\n8.2.1\nConfiguration 1 and Configuration 2 flight tests are distinct and separate. ...................... 92\n8.2.2\nMultiple operating/staging locations decreased efficiency in test execution..................... 93\n8.2.3\nLow priority within R-2515 resulted in missed flight test opportunities. ............................ 93\n8.2.4\nPlanning for nominal and off-nominal conditions was not clear or distinct enough. ......... 94\n8.2.5\nHaze, clouds and winds aloft............................................................................................... 94\n8.2.6\nUnderstanding success criteria and training operators was critical to mission success. .... 95\n8.2.7\nA separate truth source for positional data, TSPI, from each aircraft was not available for\npost flight analysis.............................................................................................................................. 95\n8.3\nRESEARCHER OBSERVATIONS ............................................................................................................. 96\n8.3.1\nFT3 Configuration 1 successfully completed the major objectives for all SUT. ................... 96\n8.3.2\nBoth ARC and LaRC requested that all test aircraft keep heading and airspeed more stable\nduring future test events. ................................................................................................................... 96\n8.3.3\nVSCS displays did not function for ARC and LaRC. .............................................................. 97\n8.3.4\nRadar vertical speed indications were not filtered. ............................................................ 97\nv\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n8.3.5\n\nAn FT4 data collection plan is desired. ................................................................................ 97\n\n9\n\nACRONYMS .....................................................................................................................98\n\n10\n\nREFERENCES ................................................................................................................102\n\n11\n\nAPPENDIX A: DEFINITION OF TERMS .........................................................................103\n\n12 APPENDIX B: REDLINED FLIGHT CARDS ...................................................................104\n12.1 FLIGHT 1 REDLINED FLIGHT CARDS ................................................................................................... 105\n12.2 FLIGHT 2 REDLINED FLIGHT CARDS ................................................................................................... 119\n12.3 FLIGHT 3 REDLINED FLIGHT CARDS ................................................................................................... 142\n12.4 FLIGHT 4 REDLINED FLIGHT CARDS ................................................................................................... 163\n12.5 FLIGHT 5 REDLINED FLIGHT CARDS ................................................................................................... 180\n12.6 FLIGHT 6 REDLINED FLIGHT CARDS ................................................................................................... 202\n12.7 FLIGHT 7 REDLINED FLIGHT CARDS ................................................................................................... 224\n12.8 FLIGHT 8 REDLINED FLIGHT CARDS ................................................................................................... 247\n12.9 FLIGHT 9 REDLINED FLIGHT CARDS ................................................................................................... 280\n12.10\nFLIGHT 10 REDLINED FLIGHT CARDS ............................................................................................. 300\n12.11\nFLIGHT 11 REDLINED FLIGHT CARDS ............................................................................................. 318\n\nvi\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nList of Tables\nTable 1. Overview of Flights from Flight Test Series 3 for UAS in the NAS. ................................................iii\nTable 2. Flight Test Series 3 Flight Dates. .................................................................................................... 5\nTable 3. Flight Test Series 3 Aircraft and Flight Crew. ................................................................................. 6\nTable 4. Mission Team. ................................................................................................................................. 7\nTable 5. Aircraft Equipage............................................................................................................................. 7\nTable 6. Configuration 1 Mission Rules. ..................................................................................................... 20\nTable 7. Configuration 2 Mission Rules. ..................................................................................................... 22\nTable 8. Configuration 1 SUT Summary. .................................................................................................... 23\nTable 9. ARC Wind Adjust Matrix. .............................................................................................................. 30\nTable 10. CPDS Objective and Encounter Overview. ................................................................................ 38\nTable 11. Pairwise Encounters Scenario Matrix. ........................................................................................ 52\nTable 12. Config 1 Flight 1 Data. ................................................................................................................ 70\nTable 13. Config. 1 Flight 2 Data. ............................................................................................................... 71\nTable 14. Config. 1 Flight 3 Data. ............................................................................................................... 72\nTable 15. Config. 1 Flight 4 Data. ............................................................................................................... 73\nTable 16. Config. 1 Flight 5 Data. ............................................................................................................... 75\nTable 17. Config. 1 Flight 6 Data. ............................................................................................................... 76\nTable 18. Config. 1 Flight 7 Data. ............................................................................................................... 77\nTable 19. Config. 1 Flight 8 Data. ............................................................................................................... 78\nTable 20. Config. 1 Flight 9 Data. ............................................................................................................... 79\nTable 21. Config. 1 Flight 10 Data. ............................................................................................................. 80\nTable 22. Config. 1 Flight 11 Data. ............................................................................................................. 81\nTable 23. Configuration 2 Flights. ............................................................................................................... 82\nTable 24. Config 2 Flight 1 Data. ................................................................................................................ 83\nTable 25. Config 2 Flight 2 Data. ................................................................................................................ 83\nTable 26. Config 2 Flight 3 Data. ................................................................................................................ 83\nTable 27. Config 2 Flight 4 Data. ................................................................................................................ 84\nTable 28. Config 2 Flight 5 Data. ................................................................................................................ 84\nTable 29. Config 2 Flight 6 Data. ................................................................................................................ 84\nTable 30. Config 2 Flight 7 Data. ................................................................................................................ 85\nTable 31. Config 2 Flight 8 Data. ................................................................................................................ 85\nTable 32. Config 2 Flight 9 Data. ................................................................................................................ 85\nTable 33. Config 2 Flight 10 Data. .............................................................................................................. 86\nTable 34. Config 2 Flight 11 Data. .............................................................................................................. 87\nTable 35. Config 2 Flight 12 Data. .............................................................................................................. 87\nTable 36. Config 2 Flight 13 Data. .............................................................................................................. 88\nTable 37. Config 2 Flight 14 Data. .............................................................................................................. 89\nTable 38. Config 2 Flight 15 Data. .............................................................................................................. 90\n\nvii\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nList of Figures\nFigure 1. Configuration 1 Flight Test Area. .................................................................................................. 8\nFigure 2. Configuration 2 Flight Test Area. .................................................................................................. 9\nFigure 3. Aircraft Staging Areas. ................................................................................................................. 10\nFigure 4. SAF Control Room Displays. ....................................................................................................... 11\nFigure 5. Configuration 1 Timeline. ............................................................................................................. 14\nFigure 6. Configuration 2 Timeline. ............................................................................................................. 15\nFigure 7. Altimeter Calibration Flight Card. ................................................................................................. 18\nFigure 8. Configuration 1 Pairwise Encounters Nomenclature. .................................................................. 24\nFigure 9. ARC Pairwise Encounter Angles 1. ............................................................................................. 25\nFigure 10. ARC Pairwise Encounter Angles 2. ........................................................................................... 26\nFigure 11. ARC Pairwise Encounter Vertical Profiles. ................................................................................ 27\nFigure 12. ARC High Speed Intruder Pairwise Encounter Angles. ............................................................. 28\nFigure 13. LaRC Pairwise Encounter Angles.............................................................................................. 31\nFigure 14. LaRC Pairwise Encounter Vertical Profiles 1. ........................................................................... 32\nFigure 15. LaRC Pairwise Encounter Vertical Profiles 2. ........................................................................... 32\nFigure 16. LaRC Pairwise Multi-ship Encounters 1. ................................................................................... 33\nFigure 17. LaRC Pairwise Multi-ship Encounters 2. ................................................................................... 34\nFigure 18. LaRC Pairwise Multi-ship Encounters 3. ................................................................................... 34\nFigure 19. LaRC High Speed Intruder Pairwise Encounter Angles. ........................................................... 35\nFigure 20. LaRC High-Speed Pairwise Multi-ship Encounters 1. ............................................................... 36\nFigure 21. LaRC High-Speed Pairwise Multi-ship Encounters 2. ............................................................... 36\nFigure 22. GA-ASI CPDS Pairwise Encounters. ......................................................................................... 39\nFigure 23. GA-ASI CPDS Multi-ship Pairwise Encounters. ........................................................................ 40\nFigure 24. GA-ASI Low-Altitude Pairwise Radar Encounters. .................................................................... 41\nFigure 25. GA-ASI Radar CBDR Pairwise Encounters (110\xc2\xb0). ................................................................... 42\nFigure 26. GA-ASI Radar CBDR Pairwise Encounters (90\xc2\xb0) ...................................................................... 42\nFigure 27. GA-ASI Radar Zig-Zag Pairwise Encounter. ............................................................................. 43\nFigure 28. GA-ASI TCAS Mitigated Pairwise Encounters 1. ...................................................................... 44\nFigure 29. GA-ASI TCAS Mitigated Pairwise Encounters 2. ...................................................................... 44\nFigure 30. GA-ASI TCAS Sequential Pairwise Encounters. ....................................................................... 45\nFigure 31. GA-ASI TCAS Sequential Pairwise Encounters Vertical Profile 1. ............................................ 46\nFigure 32. GA-ASI TCAS Sequential Pairwise Encounters Vertical Profile 2. ............................................ 46\nFigure 33. ARC High Speed Ownship Pairwise Encounter Angles. ........................................................... 47\nFigure 34. Pairwise Encounters Ownship Example Test Card. .................................................................. 58\nFigure 35. Pairwise Encounters Intruder Example Test Card. .................................................................... 59\nFigure 36. Fireline Ownship Route with Live Intruder Intercepts. ............................................................... 60\nFigure 37. Fireline Routing, Ownship and Live/Virtual Intruders. ............................................................... 61\nFigure 38. Full Mission Fireline Route and Encounters. ............................................................................. 62\nFigure 39. Full Mission Ownship Test Card. ............................................................................................... 66\nFigure 40. Full Mission Intruder 1 Test Card. .............................................................................................. 67\nFigure 41. Full Mission Intruder 2 Test Card. .............................................................................................. 68\nFigure 42. Prioritization of and Flown Encounters. ..................................................................................... 69\n\nviii\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n1 Introduction\nThe desire and ability to fly Unmanned Aircraft Systems (UAS) in the National Airspace\nSystem (NAS) is of increasing urgency. The application of unmanned aircraft to perform\nnational security, defense, scientific, and emergency management are driving the critical\nneed for less restrictive access by UAS to the NAS. UAS represent a new capability that\nwill provide a variety of services in the government (public) and commercial (civil) aviation\nsectors. The growth of this potential industry has not yet been realized due to the lack of\na common understanding of what is required to safely operate UAS in the NAS.\nNASA\xe2\x80\x99s UAS Integration into the NAS Project is conducting research in the areas of\nSeparation Assurance/Sense and Avoid Interoperability, Human Systems Integration\n(HSI), and Communication to support reducing the barriers of UAS access to the NAS.\nThis research is broken into two research themes namely, UAS Integration and Test\nInfrastructure. UAS Integration focuses on airspace integration procedures and\nperformance standards to enable UAS integration in the air transportation system,\ncovering Sense and Avoid (SAA) performance standards, command and control\nperformance standards, and human systems integration. The focus of Test Infrastructure\nis to enable development and validation of airspace integration procedures and\nperformance standards, including the integrated test and evaluation. In support of the\nintegrated test and evaluation efforts, the Project will develop an adaptable, scalable, and\nschedulable relevant test environment capable of evaluating concepts and technologies\nfor unmanned aircraft systems to safely operate in the NAS.\nTo accomplish this task, the Project will conduct a series of Human-in-the-Loop and Flight\nTest activities that integrate key concepts, technologies and/or procedures in a relevant\nair traffic environment. Each of the integrated events will build on the technical\nachievements, fidelity and complexity of the previous tests and technical simulations,\nresulting in research findings that support the development of regulations governing the\naccess of UAS into the NAS.\n\n1.1 Scope\nThe integrated Flight Test 3 (FT3) test period was divided into two distinct configurations.\nConfiguration 1 test execution began on 15 June, completed 27 July and comprised 12\nflights and more than 200 test points. Configuration 2 began concurrently on 16 July and\nwas concluded early on 12 August. The Configuration 2 phase conducted three test\nsorties and numerous systems integration sorties but was unable to achieve the desired\nperformance needed by the final authority and was therefore concluded early. Further\ndetails about each configuration are describe in the main body of this test report.\nPer ITE-FT3-01 Section 5 \xe2\x80\x9cTest Reporting\xe2\x80\x9d this test report will include details on any\ndeficiencies with either the system under test or the test equipment and methods as well\nas preliminary test results and analysis. This report may also be considered a general\nprogress report to the Integrated Test and Evaluation (IT&E) sub-project.\n1\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n1.2 Purpose\nFT3 gathered data for UAS researchers for their development and evaluation of a\nCommunication system, Sense and Avoid (referred to as Detect and Avoid (DAA) in the\nRTCA SC 228 ToR) algorithms and pilot displays for candidate UAS systems in a relevant\nenvironment. The technical goals of FT3 were to: 1) perform end to end traffic encounter\ntest of pilot guidance generated by Self Separation algorithms (aircraft sensor to wind,\nTraffic Collision and Avoidance System (TCAS) II, and latency uncertainties to Ground\nControl Station (GCS) display); and 2) conduct flight test of prototype Communication\nsystem as part of an integrated DAA system; 3) collect data to inform the preliminary draft\nof the Minimum Operational Performance Standards (MOPS) for DAA and Command and\nControl (C2), to include display and human performance standards in both MOPS.\nFurthermore, FT3 increased the team\xe2\x80\x99s capabilities and reduce the risks associated with\nbuilding a pertinent flight test environment moving towards the final flight tests FT4 and\nCapstone.\nThe UAS-NAS project support and participation on the 2014 flight test of ACAS Xu and\nSelf Separation (SS) significantly contributed to building up infrastructure and developing\nprocedures for FT3. The FT3 experiment was divided into two distinct test configurations,\neach focusing on different aspects of the primary technical goals. The first (described as\nPairwise Encounters) looked at the SS and Collision Avoidance algorithms to support the\ndefinition of well-clear and TCAS integration onto the ownship platform. The second\n(described as Full Mission, or FM, flights) focused on UAS pilot response times to, and\nacceptability of, the same SAA alerts, resolutions, and GCS displays under real world\nuncertainties.\nThe two test planned baseline configurations were conducted in two phases. The\nPairwise Encounters (Configuration 1) were conducted at NASA Armstrong from 17 June\nuntil 27 July. NASA Armstrong provided the primary \xe2\x80\x9cownship\xe2\x80\x9d test aircraft for this\nconfiguration. The Full Mission flights (Configuration 2), likewise conducted from NASA\nArmstrong, started integration efforts on 16 July and concluded on 12 August. NASA\nGlenn provided the primary \xe2\x80\x9cownship\xe2\x80\x9d test aircraft for this configuration as well as the\nCommunication system under test. The test period did not exceed original planned\nschedule estimates, even though Configuration 1 took longer than expected and the\nConfiguration 2 was concluded early.\nTesting facilities are Government owned, managed, leased, or under agreement and fall\ninto two categories:\n1.\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\nDevelopment Facilities:\nDistributed System Research Laboratory (DSRL) at NASA Ames\nFlight Deck Display Research Laboratory (FDDRL) at NASA Ames\nResearch Aircraft Integration Facility (RAIF) at NASA Armstrong\nUAS Sense and Avoid Research Lab at Stinger Gaffarian Technologies (SGT,\noutside of NASA Langley)\n2\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\nAircraft Operations Research Hangar at NASA GRC\nCommunication Laboratory at NASA GRC\nGA-ASI Grey Butte Flight Test Facility\nGA-ASI System Integration Lab\n\n2.\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\nTest Facilities:\nCrew Vehicle Simulation Research Facility (CVSRF) at NASA Ames\nDSRL at NASA Ames\nRAIF at NASA Armstrong\nDryden Aeronautical Test Range (DATR) at NASA Armstrong\nStand Alone Facility (SAF) at NASA Armstrong\nThe Radio Frequency (RF) Communications facility at NASA Armstrong\nEdwards R-2508 Complex\n\n1.3 Stakeholders, Participants, and Responsibilities\nNASA Integrated Aviation Systems Program (IASP) provides direction for the UAS in the\nNAS project. The project office had the overall responsibility for FT3 flight test. NASA\nAmes, NASA Armstrong, NASA Glenn, NASA Langley, GA-ASI and Honeywell supported\nthe project and were participants in the FT3 activity. The following is a brief description of\nresponsibilities:\n\xe2\x80\xa2\n\n\xe2\x80\xa2\n\n\xe2\x80\xa2\n\n\xe2\x80\xa2\n\nNASA Ames Research Center (ARC): NASA Ames provided the HSI research\nrequirements for subject pilot evaluation based on performance during scenario\nevents. Subject pilots performed scenario tests from the Research Ground Control\nStation (RGCS) located at NASA Armstrong. ARC also provided\nAutoResolver/Java Architecture for DAA Extensibility and Modeling (JADEM), one\nof the Self Separation algorithms used during Configuration 1.\nNASA Armstrong Flight Research Center (AFRC): NASA Armstrong was the\nresponsible test organization for all test missions flown from AFRC. AFRC\nprovided the RGCS for subject pilot evaluation. Further, AFRC hosted the Live\nVirtual Constructive (LVC) infrastructure for data distribution between NASA\nAmes, Glenn, and Langley. AFRC also provided the live unmanned aircraft used\nas intruders for both configurations. NASA 870 (\xe2\x80\x9cIkhana\xe2\x80\x9d) was the unmanned\naircraft ownship platform for Configuration 1 encounters within the R-2515\nairspace.\nNASA Glenn Research Center (GRC): NASA Glenn was the participating test\norganization for all test missions flown with the NASA 608 T-34C. GRC provided\nthe communication and control system interface and the UAS Surrogate ownship\naircraft for use during Full Mission flights. Although initially planned, the NASA\nGlenn S-3B Viking aircraft was not available for use as a \xe2\x80\x9chigh\xe2\x80\x9d speed ownship\nduring Configuration 1 testing.\nNASA Langley Research Center (LaRC): NASA Langley provided a SelfSeparation algorithm (Stratway+) that was displayed and evaluated by subject\npilots during flight encounters.\n3\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xe2\x80\xa2\n\n\xe2\x80\xa2\n\nGeneral Atomics Aeronautical Systems Inc. (GA-ASI): Provided hardware,\nsoftware and integration support on the Ikhana UAS and specifically the Due\nRegard Radar (DRR). GA-ASI also provided pairwise encounter requirements for\nautonomous aircraft response maneuvers to TCAS alerting, as well as a SelfSeparation Algorithm, CPDS (Conflict Prediction and Display System), for\nevaluation.\nHoneywell: Honeywell provided the software for the Surveillance Tracking Module\n(STM) prototype that contained the Honeywell Fusion Tracker. Honeywell also\nprovided a second TCAS II equipped intruder aircraft (N3GC) in support of both\nconfigurations. N3GC had onboard TCAS recording capability, and that recorded\ndata was made available to the rest of the FT3 test team to support their data\nanalysis.\n\n1.4 Operations Working Group\nThe Operations Working Group (OWG) was a collaborative meeting between all\nstakeholders and participants for FT3. The working group, which met weekly on Tuesdays\nand bi-weekly on Fridays prior to testing, discussed all FT3 ground and flight operations\ntopics. The working group was responsible for flight planning and coordination, assigning\nactions items, safety concerns which would feed into the System Safety Working Groups\n(SSWG), hardware integration and testing discussion, training, and readiness. The OWG\nwas new for FT3 and was responsible for a large part of the successes in FT3.\n\n4\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n2 Flight Test Period\nThe flight test period spanned from 15 June 2015 to 12 August 2015. The flight days are\ndetailed in Table 2.\nTable 2. Flight Test Series 3 Flight Dates.\n\nFlight\n\nFlight Date\n\nSUT\nConfiguration 1\n\nTypes\n\n0\n\n15 June 2015\n\nIkhana\n\nSystem check-out\n\n1\n\n17 June 2015\n\nAutoResolver\n\nFly-throughs\n\n2\n\n18 June 2015\n\nAutoResolver\n\nFly-throughs\n\n3\n\n22 June 2015\n\nAutoResolver\n\nFollow display\n\n4\n\n24 June 2015\n\nCPDS\n\nTCAS, Radar\n\n5\n\n26 June 2015\n\nCPDS\n\nLow-altitude, Multi-ship, TCAS sequential, TCAS\n\n6\n\n7 July 2015\n\nStratway+\n\nFollow display\n\n7\n\n9 July 2015\n\nStratway+\n\nFollow display\n\n8\n\n10 July 2015\n\nStratway+/CPDS\n\nFollow display, Multi-ship / Radar\n\n9\n\n21 July 2015\n\nStratway+\n\nHigh-speed intruder, Multi-ship\n\n10\n\n22 July 2015\n\nAutoResolver\n\nFly-throughs\n\n11\n\n24 July 2015\n\nCPDS\n\nGRC1\n\n13 July 2015\n\nCNPC\n\nCharacterization flight (GRC only)\n\nCST1\n\n16 July 2015\n\nCNPC\n\nCheck-out\n\nCST2\n\n28 July 2015\n\nCNPC\n\nCheck-out\n\nGRC2\n\n29 July 2015\n\nINS\n\nCheck-out (GRC only)\n\nCST3\n\n29 July 2015\n\nCNPC\n\nCheck-out\n\nGRC3\n\n30 July 2015\n\nCNPC\n\nCheck-out (GRC only)\n\nREH1\n\n3 August 2015\n\nCNPC\n\nFull mission rehearsal\n\nGRC4\n\n4 August 2015\n\nCNPC\n\nCheck-out (GRC only)\n\nCST4\n\n4 August 2015\n\nCNPC\n\nCheck-out\n\nCST5\n\n5 August 2015\n\nCNPC\n\nCheck-out\n\nREH2\n\n6 August 2015\n\nRGCS/HSI\n\nCancelled \xe2\x80\x93 weather\n\nREH3\n\n7 August 2015\n\nRGCS/HSI\n\nFull mission rehearsal\n\n1\n\n10 August 2015\n\nRGCS/HSI\n\nData collect 1\n\n2\n\n11 August 2015\n\nRGCS/HSI\n\nData collect 2\n\n3\n\n12 August 2015\n\nRGCS/HSI\n\nData collect 3\n\nTCAS, Zig-Zag, Radar\nConfiguration 2\n\n5\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n3 Flight Crew and Mission Team\nNASA 870, known as Ikhana, served as the ownship for the Configuration 1 flight test\nseries. NASA 608, the GRC T-34C, served as the ownship for the Configuration 2 flight\ntest series. Flight test encounter setups included a single ownship vs. a single intruder or\nsingle ownship vs. multiple intruders. The intruder role was supported by multiple aircraft\ndue to availability and crew rest considerations. The aircraft and flight crew required to\ncomplete the test series are identified in Table 3.\nTable 3. Flight Test Series 3 Aircraft and Flight Crew.\n\nAircraft\n\nNASA 870 MQ-9\n(Ikhana)\n\nRole\n\nConfig. 1\nOwnship\n\nPosition\nNASA Pilots\nAir National Guard\nPilots\n\nFlight Crew\nHowe, Posada, Less\nLtCol Reiss, Maj Deveroux, Hinton\nMaj Rhodes, Maj Baughman, SSgt\nCade\n\nDET3\nNASA Mission\nDirector\nHoneywell Pilots\nHoneywell FTEs\n\nBuoni, Howell, Valeri\nDubbury, McMahon, Walker\nSingh, Dougherty\n\nN3GC C90 King Air\n\nConfig. 1/2\nIntruder\n\nNASA 865 T-34C\n\nConfig. 1/2\nIntruder\n\nNASA Pilots\n\nPurifoy, Miller, Newton, Howe,\nBroce\n\nNASA 850 F-18A\n\nConfig. 1 HighSpeed Intruder\n\nNASA Pilots\n\nLarson\n\nNASA 7 King Air\n\nConfig. 2\nIntruder\n\nNASA Pilots\n\nHowe, Williams\n\nNASA 608 T-34C\n\nConfig. 2\nSurrogate\nOwnship\n\nGRC Pilots\n\nDemers, Micklewright\n\nGRC FTEs\n\nGriner\n\nAir National Guard\nPilots\n\nLtCol Shaw, Maj Brooks\n\nGlobal Vigilance CTF\n\nLtCol Allen\n\nRGCS\n\nConfig. 2 GCS\n\nAlong with the aircraft and flight crew assets, an operations mission control team was\nutilized to manage the overall test effort. The test conductor was responsible for overall\nmission success and the coordination of all test assets. The test director provided flight\nsafety oversight and supported the test conductor by performing all back channel and\nengineering channel coordination. The test coordinator acted as the scribe and performed\ncontrol room supporting tasks. The mission control team was located in the SAF for all\ntest missions performed during FT3. Table 4 provides a listing of the mission team.\nFurthermore, engineering coordinators in the LVC, Ikhana GCS, and Ames virtual Air\nTraffic Control (ATC)/pilot coordination facility supported the operation.\n\n6\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nTable 4. Mission Team.\n\nMission Team\nNASA Test Conductor\nNASA Test Directors\nNASA Test Coordinator\nLVC Engineering Coordinator\n\nMarston (Config. 1), Sternberg (Config. 2)\nSternberg (Config. 1), Marston (Config. 2)\nValkov (Config. 1, 2)\nKim, Willhite\n\nGCS Engineering Coordinator\nAmes Ghost Controller\n\nLoera\nBridges\n\n4 System Configuration\nAll aircraft that participated in this flight test were equipped with navigation systems that\nuse a Global Positioning System (GPS). All manned intruder aircraft were equipped with\nTCAS, and the Honeywell C90 King Air was equipped with TCAS II version 7.1.\nA high level summary of the equipage installed on each aircraft is found in Table 5.\nTable 5. Aircraft Equipage.\n\nCFG 1\n\nCFG 2\n\nCFG 1\n\nCFG 1, 2\n\nCFG 1, 2\n\nCFG 2\n\n7\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n5 Flight Test Administrative Information\n5.1 Operating Area\nThe operating area for Configuration 1 flight test occurred in the Restricted Airspace, R2515, located at Edwards Air Force Base (EAFB), along with the Buckhorn Military\nOperating Area (MOA), with operations scheduled and coordinated through the Air Force\nTest Center (AFTC). Specific airspace scheduled each day during these flight tests\nincluded the Four Corners Area, Mercury Spin Area, overflight of the Precision Impact\nRange Area (PIRA) East/West, and the Buckhorn MOA. These areas within R-2515 are\ndepicted within the yellow shaded area shown in Figure 1.\nThis operating area was adequate for the majority of the Configuration 1 encounters.\nHowever, there were some encounters that required either or both the intruder and\nownship to extend north or west, remaining within R-2515, of the airspace. The\nextensions were required to either start or complete these encounters. For those\nencounters where an extension was required to accomplish the test encounter, approval\nfrom the controlling agency (SPORT) was required. In some cases the extension was not\npermitted and the encounter was either terminated early or skipped. The Buckhorn MOA\nwas used by the manned intruder aircraft only for many of the test encounters.\n\nFigure 1. Configuration 1 Flight Test Area.\n\nThe operating area for Configuration 2 flight test required both R-2508 and R-2515 due\nto the length of the mission plan, and therefore, included additional organizational cross\ntalk between High Desert TRACON and SPORT controlling agencies. The Isabella MOA\nblock 10,000 ft MSL to 16,000 ft MSL and the western portion of R-2515 were used for\nthis configuration. The primary working area for this configuration is shown in Figure 2.\nThe \xe2\x80\x98fire mission\xe2\x80\x99 route (depicted in green on Figure 2) was the ownship route of flight.\nGiven that ownship transited between the Isabella MOA and R-2515, prior coordination\n8\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nwas required to ensure a seamless communication flow from one agency to the next.\nAlthough on some of the sorties extra coordination was required, for the majority of these\nflights, no significant issues were noted.\nIt is important to note that NASA operations within R-2515 are subject to the priority of\nUnited States Air Force (USAF) programs under the AFTC. Generally speaking, NASA is\ngiven a general priority that is superseded by most other AFTC operations. Configuration\n2 flights were impacted by this low prioritization and required several mitigations that were\nin one case acceptable, and in two other cases, were inadequate. On one occasion the\nsortie start time was changed to avoid an airspace conflict. This was acceptable to the\ntest team. On two other occasions the ownship route of flight and Intruder\xe2\x80\x99s route needed\nmodification to avoid R-2515 altogether. This was not adequate for the test team.\n\nFigure 2. Configuration 2 Flight Test Area.\n\n5.2 Aircraft Staging and Locations\nDuring FT3 both intruders and ownship aircraft operated from three different locations.\nThe Honeywell N3GC aircraft operated from Van Nuys, KVNY, for both configurations,\nand NASA 608 operated from both Palmdale, KPMD (during systems check flights) and\nBakersfield, KBFL, for the three Configuration 2 mission flights. All other aircraft, Ikhana\nand intruders, operated from Armstrong Flight Research Center, KEDW. The\ngeographical separation resulted in some operational challenges that were overcome with\nmission planning. Further details on the challenges are listed in later sections of this\nreport. Figure 3 depicts the staging locations for each aircraft.\n\n9\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nKBFL\nNASA 608\n\nKEDW\nNASA 870\nNASA 865\nNASA 7\nNASA 850\n\nKVNY\nN3GC\n\nFigure 3. Aircraft Staging Areas.\n\n5.3 Weather\nFT3 mission rules, FT3-1 and FT3-2, plus Ikhana Standard-8, required all tests to be\nconducted in Visual Meteorological Conditions (VMC) conditions and 3 statute miles of\nvisibility with at least 1,000 feet of cloud clearance above and below planned block\naltitude. Additionally, Ikhana Standard mission rules 9 and 10 prohibited flight into known\nicing conditions as well as transit through visible moisture for repositioning operations. All\nother participating aircraft complied with their operating limitations as defined by be their\nrespective flight manuals for repositioning operations.\nNo Configuration 1 flights were cancelled due to weather and all flights completed as\noriginally scheduled. On Flight 5 the visibility was reduced (due to smoke/haze generated\nby the Lake fire), although still likely greater than 3 miles; this condition was considered\na contributing factor for a mission rule violation when an encounter occurred without the\nintruder having visual on Ikhana.\nConfiguration 2 was impacted by weather with one cancellation on 6 August 2015 when\nbroken cloud layers impacted the northern half of the desired operating airspace. All\nother Configuration 2 flights were completed as scheduled; however, high winds aloft,\n10\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\ngreater than 30 kts, were noted on several occasions and needed to be accounted for\nwhen controlling each individual intercept. High winds aloft resulting in incorrect\ncompensations were a contributing factor to two or possibly more individual encounters\nnot achieving desired alerting results.\n\n5.4 Stand Alone Facility Mission Control Room\nThe SAF, located at the NASA AFRC main building, was used by the operations team to\ncoordinate, manage, and execute the flight test. The room was configured with three\nworkstations and multiple support stations; one of the three workstations was dedicated\nto UAS-NAS operations while the other two were used to support other programs (though\nnot concurrently). Each work station was configured with a DICES III voice\ncommunication system and several display monitors, shown in Figure 4. Zeus, Test and\nEvaluation Command and Control System (TECCS), Ikhana video camera, and Vigilant\nSpirit Control Station (VSCS) traffic displays provided SA and two-way voice capability to\nthe control room team for test execution.\n\nFigure 4. SAF Control Room Displays.\n\n11\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n5.4.1 Control Displays\nThe Raytheon Solipsys Zeus system acted as the primary control display for both Test\nConductor (TC) and Test Director (TD) positions. The Zeus system used surveillance\nradar information to provide Time-Space-Position Information (TSPI) to the test team. The\nimbedded range and bearing tool were used to maintain range Situational Awareness\n(SA) in order to ensure mission rule FT3-9 visual required compliance when applicable.\nThe use of Zeus as the SA and mission control tool for FT3 was considered a must-have\nsystem and the test was completed with no safety incidents and one mission rule violation.\nThe upper right two displays (see Figure 4) were repeaters of the SUT data sourced from\nthe LVC. Using these displays the LVC would pipe JADEM, Stratway+, CPDS, and Radar\ninformation to the SAF. The upper left two displays were used as test support displays\nand included a repeater of the Ikhana Heads-Up Display (HUD) video, which provided\nTCAS advisory information as well as state information for Ikhana such as altitude,\nheading and airspeed. Additional information was available on the upper left displays and\nthis included Long Range Optics (LRO) (if active) and access to commercial weather\nsources.\n5.4.2 Test Coordinator Position\nThroughout the FT3 event an additional position was matured to support data collection.\nThis Test Coordinator position (located to the right of the image in Figure 4) was a simple\nwork station that included a desk, large monitor, and computer docking station. The work\nstation was not restricted to the secure intranet that the TC and TD position required, and\nenabled access to the external network. The team was able to use public access tools\nsuch as Flightradar24\xc2\xae, AviationWeather.gov, SkyVector\xc2\xae, FalconView\xc2\xae, and standard\nMicrosoft Office\xc2\xae products not available on the TC/TD computers.\nThe primary role for Test Coordinator was to collect test data for use during debrief as\nwell as this report. However, being able to access the aforementioned websites proved\nto be invaluable as it provided information Zeus could not, such as: take off and land TSPI\nfor KVNY, KBFI and KPMD, screen captures, simple encounter replay, weather\ninformation, access to all planning material, and other administrative data. The operations\nteam found this workstation, mission control room position, and external tools as\ninvaluable to supporting the overall flight test execution.\n5.5 Mission Information\nExecuting FT3 flights required a large amount of coordination. Configuration 1 flights were\nplanned at a rate of three flights per week due to the duration of each sortie and the\namount of test cards executed per sortie. Furthermore, flying the encounters was user\nwork load intensive, with 10 minutes allocated to each encounter, and setup required in\nbetween runs. Further discussion on user workload is found in later sections.\nConfiguration 2 work load was considerably less intensive for the airborne participants as\nthe subject pilot under test was now located in the RGCS and the overall design of the\n12\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFull Mission test provided a more controlled environment than is available to a pilot under\ntest operating from an airborne aircraft.\nAs a pre-requisite to executing Configuration 1 flight tests, a T-1 crew briefing was\naccomplished the day prior to the event. The T-1 briefing covered, in detail, the following\naspects related to the upcoming flight:\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nRoll Call\nMission Summary\nMission Timeline\nWeather / NOTAMs\nUAS Status\nMission Information\nGCS Status\nAirspace / Airfield\nSupport Assets\nContingencies\nMiscellaneous\nFlight Card Review\n\nA flight could be delayed or postponed based on information discussed during the T-1\nbriefing. All team members were required to participate in the briefing either in-person or\nremotely.\nConfiguration 2 flights did not require a T-1 for each event. Given the static nature of the\nFireline route and intruder routes a T-1 for each flight was considered excessive and any\nchanges were briefed at the pre-flight briefing before conducted 2.5 hours prior to takeoff.\nChanges were minimal and largely constituted discussion of contingency plans for loss\nof airspace or weather effects.\nAll FT3 Configuration 1 flights started at 0600 (local) Pacific Time. Subsequently, the\nmorning brief was held at 0415L. The morning briefings covered at a higher level the\nsame information with emphasis on any changes from the previous T-1 briefing. The\nintent was for this briefing to be about 15 minutes in length. A final go/no-go decision was\nmade at this briefing. After the brief, the team was dismissed to prepare for the flight and\nin some cases additional crew familiarity training was conducted for each display SUT.\nThe SAF was manned at approximately 0545L to support any systems troubleshooting or\ncoordination efforts required by the supporting aircraft teams.\nIn general, flights were planned for approximately 5 hours. One hour of that total flight\ntime was allocated to transit, altitude calibration, and systems startup procedures. The\nlimiting factor for the Ikhana was frequency coordination and typically required an OFF\ntime of 1100L for both Satcom and Line of Sight (LOS) frequencies. For the intruders, fuel\n\n13\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\navailable was the limiting factor. TheT-34C aircraft could only support up to 3.5 hours of\nflight time while the King Air aircraft could support 4.0 hours although on a few occasions\nN3GC dropped into Victorville for fuel and then returned to KVNY thereafter, which\nprovided the test team an additional 20 minutes of play time. Moreover, the longer transit\ntimes required by N3GC due to operating out of KVNY had to be considered for fuel\nplanning.\nA flight debrief was mandatory in order to discuss the day\xe2\x80\x99s flight events, identify any\naircraft discrepancies, and discuss test inefficiencies that may have decreased the\nnumber of encounters and test objectives achieved. Action items were assigned for\nissues and lessons learned that needed to be closed out prior to the next flight. A postflight test card review and high level data analysis was conducted as well. If the next\nflight occurred on the following day, a T-1 was then conducted to review test objectives\nfor that next flight, otherwise the T-1, as appropriate, prior to the next test opportunity.\nThe following figures show the basic Configuration 1 and 2 timelines previously\ndiscussed. Start of test day for Configuration 1 flights was typically 0415L and completed\naround 1415L. While the Configuration 2 flights started at 1200L and end of test day was\napproximately 1945L.\n\nFigure 5. Configuration 1 Timeline.\n\n14\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 6. Configuration 2 Timeline.\n\n5.6 Training and Qualifications\nAll visiting flight crew team members were required to participate in local area\nfamiliarization briefing and conduct a local area familiarization flight. In addition, all flight\ncrew and mission team members were required to have obtained current Crew Resource\nManagement (CRM) training. The TC and TD were required to obtain a formal approval\nfrom the NASA Armstrong Director of Flight Operations in order to serve in that capacity.\nThe requirements for the FT3 test conductor were derived from NASA Armstrong DCPO-003, Mission Control Procedure. The requirements were tailored from the mission\ncontroller section.\n5.6.1 Pre-Test Coordination and Training\nA training event was conducted approximately one month prior to the start of test.\nRepresentatives from all aircraft participants, SUT stakeholders, and IT&E operations\nwere present. The following is a list of training and coordination conducted.\nAdmin / Motherhood\n\xef\x82\xb7\n\nR-2508 annual refresher training \xe2\x80\x93 Conducted by R-2508 representatives\n15\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nR-2515 detailed training \xe2\x80\x93 Conducted by R-2515 Sport representatives\nInternational Traffic in Arms Regulations (ITAR) constraints\nBasic flight admin expectations\n\nTest Admin\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nMission Rules and Go/No-Go criteria\nRoles and responsibilities\nTerminology\nGeneral flight day timeline\nCommunication plan\nAltimeter check\nSuccess criteria\nContingencies, aborts and lost links\n\nTest Execution\n\xef\x82\xb7\n\nSpecific encounters and test card overview\n\nAt the completion of this training event all stakeholders and aircrew were considered\nprepared and ready to support the FT3 events in the planned airspace, by all oversight\norganizations.\n5.6.2 System Under Test Training\nSUT training was conducted closer to the actual flight date. Aircrew availability and\nSubject Matter Expert (SME) availability were the two primary reasons that the training\nwas conducted either as part of the T-1 briefing or following the flight briefing at 0415L.\nThis training was used to inform the aircrew who would be executing the flight what each\nSME was expecting from them. The following questions were addressed during these\nexchanges\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nTest configuration. What does the SME/Researcher want; ON, OFF or deenergized.\nManeuver type. Maneuvering (mitigated) or Non-maneuvering (unmitigated).\nGuidance type. Will the SUT provide directive or descriptive guidance?\nDisplay under test familiarity. What display will the aircrew be using to gain SA and\nmake a maneuver decision?\nMiscellaneous expectations. Is there anything specific to a particular SUT that the\naircrew need to know?\n\nGiven that there were multiple SUT being evaluated during FT3, variations were observed\nin the quantity and quality of training that the aircrew received. Training conducted by the\nSME on the CPDS system was considered the most thorough, timely, and informative.\nThe CPDS training was conducted the day prior to the flight in a separate presentation\n16\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nwith just the operations team present. Representations of CPDS in both picture format as\nwell as a replay of previous simulated events were presented to the crew. In some cases,\ntraining and explanation of the SUT were required during the conduct of the flight. Had\nthe training sessions been more thorough, using the CPDS training as an example, more\nefficiencies would have been gained by the test team and potentially better data collected.\nThis is due to the flight crew would have better understood the SUT and expectations on\nits use during flight.\n5.7 Altimeter Calibration\nAn altimeter calibration was required for all encounters where the vertical separation\nbetween intruders and ownship was less than 500 ft (MR FT3-20). The mission rule was\nenforced for Configuration 1 flights; however during Configuration 2, it was waived and\nnot required for reasons explained in Section 5.7.2.\n5.7.1 Configuration 1\nThe altimeter calibration was designed to take out the standard errors found within the\npitot-static systems in order to ensure the planned vertical separation was as close to\nplanned as possible. According to FAR 91.411 and Appendix E of Part 43 aircraft pitot\nsystems must be within 75 ft of field elevation when dialed into the local altimeter setting.\nAdditional errors come with changes in altitude and airspeed. Since some of the planned\nencounters were with a 200 ft vertical separation, it was possible to be much closer with\nthe errors identified above if they were not mitigated with the calibration.\nThe calibration was conducted at a flight condition that closely approximated all the\nplanned encounters; 140 KIAS and 13,000 ft MSL. The Ikhana platform set standard\n29.92 inHg and the other aircraft adjusted their altimeter settings to show a 13,000 ft\naltitude. At those conditions each participating aircraft observed the same difference from\nIkhana. N3GC consistently showed 60 ft high and NASA865 100 ft high.\nThe altimeter calibration was performed prior to each flight that required it using the flight\ncard shown in Figure 7.\n\n17\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFT3 ALTIMETER CALIBRATION\n\nFigure 7. Altimeter Calibration Flight Card.\n\n18\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n5.7.2 Configuration 2\nAlthough the Configuration 2 encounters were planned at 400 ft altitude separation, the\ntest team received approval to waive the altimeter calibration as the conduct of such\nwould require approximately 30 min to complete, with limited technical and no safety\nvalue. For the following reasons the team elected to exclude the altimeter calibration:\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nAll aircraft set local altimeter settings.\nAll participating aircraft are both properly maintained (24 month certification) and\nperform a ground altimeter check daily based off of known field elevation with local\naltimeter set (\xc2\xb175 ft) prior to takeoff. (AIM 7-2-3, Part 43 Appendix E)\nAll aircraft are TCAS equipped.\nOwnship will be maneuvering (lateral or vertical) away from conflict.\nConfiguration 2 encounters are self-separation alerting encounters where the\nalerting threshold is 120 seconds prior to Closest Point of Approach (CPA).\nAll participating aircraft are manned and all encounters require Visual Identification\n(VID) by 1 nautical mile (NM) between aircraft based off TCAS (MR FT3-9).\nAltimeter calibration data was not required for user interface SUT.\n\nThe decision to not conduct an altimeter calibration during Configuration 2 flights did have\nan impact on safe operations. Positively, the team was able to conduct two Fireline route\ndata collection runs per test day. Had an altimeter calibration been conducted, the ability\nto conduct two runs may have been limited or not possible. However, deviations closer\nthan 400 ft of separation were prohibited, and in multiple instances, that separation was\ntoo great and failed to trigger the appropriate SUT alerting. Had the runs been conducted\nat 300 or 200 ft of separation, the alerting would likely have been what was desired by\nthe researchers. The team elected to not attempt any encounters less than 400 ft without\nconducting a calibration.\n5.8 Safety and Mission Rules\nAll operations were conducted in accordance with NASA AFRC safety policies outlined in\nDCP-S-001 and 002. A safety representative was present for all operations planning and\nwas responsible for chairing the SSWG. All encounters and configurations were\nconcurred with by the safety representative. The following Mission Rules were developed\nin a coordinated effort with the need to maintain approved levels of safety paramount.\nTable 6 Mission Rules were used during Configuration 1 flights. They comprised two\nsections: the first section is the standard to Ikhana project rules and independent of the\nsupported project. The second being specific to FT3.\n\n19\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nTable 6. Configuration 1 Mission Rules.\nRule #\n\nRationale/Hazard\nReport\n\nRule Description \xe2\x80\x93 Flight\n\nIkhana Standard\n1\n2\n\n3a\n\n3b\n\n4\n\n5\n6\n8\n\nAny team member may call an abort for pilots to\nabort the flight at any time for safety reasons\nOnly authorized AFRC employees and AFRC\napproved pilots are permitted to operate the\naircraft\nOnly authorized AFRC employees and AFRC\napproved System Monitors (Sys Mons) are\npermitted to operate the GCS\nOnly authorized AFRC employee and AFRC\napproved Flight Systems Engineers (FSEs) are\npermitted to assist the PIC and operate Payloads\nfrom PSO2\nIkhana will be operated according to FAA, AFRC,\nUSAF, CBP, and General Atomics standard and\nemergency procedures\nFlight will remain within R-2515 and in accordance\nwith any applicable COA restrictions for Predator\nB\nNo envelope expansion tests will be performed\nFlights will be conducted in VMC conditions of no\nmore than moderate turbulence.\n\n9\n\nNo flight in known icing conditions\n\n10\n\nNo flight through opaque clouds, nor sustained\nflight through translucent clouds\n\n11\n\nMonitor weather forecasts for icing conditions\n\n12\n\nNo flight over densely populated areas\n\n13\n\nNo flights above 45,000 ft\n\n14\n\n15\n\n16\n\nInside R-2515, the AFRC/EAFB RSO has realtime directive authority, including vehicle\ndestruction, to the Ikhana pilot. Note: RSO\nresponsibility and authority may be delegated to\nthe Ikhana PIC as a result of RSO\nrecommendation and Tech Brief Committee\nconcurrence.\nIf returning with a controllable, but compromised\naircraft, to the best degree possible, the flight path\nwill avoid over flight of populated areas and major\nroads to the maximum extent possible\nIf generator out, land at EDW 22/04 with battery\nbus voltage at least 23.6V, otherwise, make a\nlakebed landing.\n\n20\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nSOP, FT3-01, FT3-02,\nFT3-07\nRestrict pilots/operators\nto the authorized group\nRestrict System Monitors\n(Sys Mons) to the\nauthorized group\nRestrict Flight Systems\nEngineers (FSEs) to the\nauthorized group\nSOP, AFRC Aircraft\nChecklist for Hosting;\nSpace Act Agreement,\nFT3-01, FT3-03\nSOP\nFT3-05, FT3-06, FT3-13\nSOP\nCOA requirement - VMC\nStandard Hazard\nMitigation\nNo de-icing capability\nStandard Hazard\nMitigation\nNo de-icing capability\nLaminar flow wing\nStandard Hazard\nMitigation\nNo de-icing capability\nStandard Hazard\nMitigation, Dryden Range\nMission Rule\nDEEC envelope\nclearance has only been\nconducted to 45,000 ft\n\nRange Safety Plan\n\nAFRC process for Range\nSafety Requirement to\navoid population areas,\nFT3-14\nProvides at least >=20\nminutes of battery power\nto land, taxi, and\nshutdown aircraft\n\nNotes\n\nFT3 Specific\nFT3-1\n\nFT3-2\n\nFT3-4\n\nFT3-5\n\nFT3-6\n\nTest runs will be conducted in VMC with inflight\nvisibility at least 3 statute miles.\nTest runs will be conducted clear of clouds and\nwith at least 1000\xe2\x80\x99 cloud clearance above and\nbelow the planned test block, including abort\nmaneuvers.\nAt the beginning of each test run, pilots will check\ntheir navigational system accuracy; runs with less\nthan 500\xe2\x80\x99 vertical separation will be aborted if the\npredicted error exceeds 0.1 nm.\nAll participating aircraft will maintain at least 1000\xe2\x80\x99\nvertical separation from other participating aircraft\nbetween test runs unless visual. Maintain\ndeconfliction altitude noted on each test card or\nas instructed.\nIntruder aircraft will not climb/descend to scenario\naltitude until Ikhana has reestablished its\ndeconfliction altitude.\n\nStandard Hazard\nMitigation\nFTP, Standard Hazard\nMitigation\n\nFTP, Mission Rule, FT301, FT3-02\n\nFTP, Standard Hazard\nMitigation, FT3-03\n\nFTP, Mission Rule, FT303\n\nFT3-7\n\nThe test run will be aborted if a UAS loses LOS\nLink.\n\nHazard Mitigation FT3-03\n\nFT3-8\n\nThe test run will be reset if timing constraints of \xc2\xb15\nto \xc2\xb110 seconds (as defined for each test\nencounter) cannot be met by a minimum of 120\nseconds prior to CPA.\n\nFTP, Hazard Mitigation\n\nFT3-9\n\nFT3-10\n\nFT3-11\n\nFT3-12\n\nFT3-13\n\nFT3-14\nFT3-15\nFT3-16\nFT3-17\n\nFor all test encounters where vertical separation is\nless than 500\xe2\x80\x99, the test run will be aborted if all\nmanned aircraft do not have visual on all\nparticipating aircraft at any point inside 1 nm\nseparation.\nDuring auto TCAS runs, the test run will be\naborted if Ikhana begins an automatic maneuver\nin the opposite direction than expected for that\ntest encounter.\nThe test run will be aborted if any aircraft is offtrack by more than 0.1NM inside 1 minute to CPA.\nThe manned intruder pilot will not follow a TCAS\ncontrary to the pre-briefed abort procedures\nunless they have reason to believe the RA is\ngenerated by non-participating traffic and they\nhave SA on ownship position.\nUpdate of the appropriate Ikhana Lost Link\nMission variables (Entry Waypoint, ILLH, ILLA)\nwill be verified, by both aircrew and the Mission\nDirector, prior to commencing each test run.\nWhen not on a test run, Ikhana crew will ensure\nSAAP Maneuver Mode is set to ADVISORY or\nOFF.\nAnytime below 5000\xe2\x80\x99 AGL, SAAP auto\nmaneuvering modes will be ADVISORY or OFF.\nConfirm each aircraft\xe2\x80\x99s nav system time matches\nthe UTC time hack given in the pre-flight brief.\nThe test run will be aborted if any aircraft is off\naltitude by more than 50\xe2\x80\x99 toward other aircraft\ninside 1 minute to CPA.\n\n21\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nAcceptable error is\n0.1 nm\n\nThis MR is not\napplicable during low\naltitude test runs.\nNumber of seconds is\ntest run/point specific\nSome test runs/points\nwill be greater than\n120 secs of steady\nstate variables.\n\nFTP, Hazard Mitigation\n\nHazard Mitigation, FT301\nFTP, Mission Rule, FT301\n\nFTP, Hazard Mitigation\n\nFTP, Hazard Mitigation,\nFT3-01, FT3-03\n\nMission Rule, FT3-02\nMission Rule, FT3-02,\nFT3-19\nMission Rule\nFTP, Standard Hazard\nMitigation, FT3-01\n\nLost link mission\nidentified on each test\ncard.\n\nFT3-18\nFT3-19\n\nFT3-20\n\nA 200\xe2\x80\x99 foot min vertical separation shall be\nmaintained for all test geometries.\nFlight operations outside of the approved mission\nflight envelope for Ikhana are prohibited during\ntest encounters\nA side-by-side altitude calibration will be\nperformed between aircraft for any mission that\nincludes an encounter less than 500 ft vertical\nseparation.\n\nMission Rule\nFT3-14\n\nMission Rule\n\nApplicable to\nConfiguration 1a and\n1b\n\nTable 7 lists the Mission Rules applicable to Configuration 2 flights. Since Ikhana was not\nparticipating the list is significantly reduced and supplanted with platform specific flight\nmanual operational limitations.\nTable 7. Configuration 2 Mission Rules.\nRule #\n\nRationale/Hazard\nReport\n\nRule Description \xe2\x80\x93 Flight\n\nNotes\n\nFT3 Configuration 2 Specific\nFT3-1\n\nFT3-2\n\nFT3-4\n\nFT3-9\n\nFT3-12\n\nFT3-16\nFT3-18\nFT3-21\n\nTest runs will be conducted in VMC with inflight\nvisibility at least 3 statute miles.\nTest runs will be conducted clear of clouds and\nwith at least 1000\xe2\x80\x99 cloud clearance above and\nbelow the planned test block, including abort\nmaneuvers.\nAt the beginning of each test run, pilots will check\ntheir navigational system accuracy; runs with less\nthan 500\xe2\x80\x99 vertical separation will be aborted if the\npredicted error exceeds 0.1 nm.\nFor all test encounters where vertical separation is\nless than 500\xe2\x80\x99, the test run will be aborted if all\nmanned aircraft do not have visual on all\nencounter aircraft at any point inside 1 nm\nseparation.\nThe manned intruder pilot will not follow a TCAS\ncontrary to the pre-briefed abort procedures\nunless they have reason to believe the RA is\ngenerated by non-participating traffic and they\nhave SA on ownship position.\nConfirm each aircraft\xe2\x80\x99s nav system time matches\nthe UTC time hack given in the pre-flight brief .\nA 200\xe2\x80\x99 foot min vertical separation shall be\nmaintained for all test geometries.\nIntentional ownship vertical maneuvers towards\nintruder aircraft shall not be made within 60\nseconds of CPA during live intruder encounters.\n\nStandard Hazard\nMitigation\nFTP, Standard Hazard\nMitigation\n\nFTP, Mission Rule, FT301, FT3-02\n\nAcceptable error is\n0.1 nm\n\nFTP, Hazard Mitigation\n\nLateral and Vertical\nvectors may be\napplied prior to an\nabort to maintain\napplicable separation.\n\nFTP, Hazard Mitigation\n\nMission Rule\nMission Rule\nMission Rule\n\nAdditionally, during all FT3 flights a Senior Operations Representative (SOR) was present\nin the SAF. The SOR acted as a spokesperson for the NASA AFRC Director of Flight\nOperations and their responsibility was to monitor general conduct of the flight test\noperations, monitor the team\xe2\x80\x99s real-time decisions, and initiate the Aircraft Incident\nResponse Procedure (DCP-S-001) in the case of an aircraft mishap.\n\n22\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n6 Flight Execution\nThe Flight Test Series 3 was split into two distinct phases: Pairwise Encounters\n(Configuration 1) and Full Mission Encounters (Configuration 2). The following sections\ndescribe the flight execution activities related to each phase.\n\n6.1 Configuration 1: Pairwise Encounters\nThis test configuration evaluated the advisories generated by the Self Separation and\nCollision Avoidance Algorithms fed by data from live aircraft during flight. Flight Test\nConfiguration 1 was further defined into two distinct groups, Configuration 1a and 1b.\nConfiguration 1a involved flight test encounters using NASA\xe2\x80\x99s Ikhana aircraft as the low\nspeed ownship. Configuration 1b was planned for use of a high-speed ownship aircraft.\nHowever, the aircraft planned for Configuration 1b ultimately could not support the FT3\nevent.\nIn these tests the Ikhana ownship aircraft was flown against either one or two manned\nintruder aircraft. Both Self Separation and Collision Avoidance algorithms were evaluated.\nThe SS algorithms were evaluated using both mitigated and unmitigated encounters.\nUnmitigated, also known as fly-through and non-maneuvering encounters, were designed\nfor each aircraft to fly the route as planned all the way to CPA regardless of alerting\ndisplayed. These encounters evaluated each SUT\xe2\x80\x99s ability to maintain the correct alerting\nthresholds. The mitigated encounters were designed for the test aircrew to maneuver the\nownship aircraft away from the Collision Avoidance Threshold (CAT) or Near Mid-Air\nCollision (NMAC) thresholds and maintain a well clear distance between intruder and\nownship. Table 8 categorizes each SUT.\nTable 8. Configuration 1 SUT Summary.\n\nSUT\nJADEM\nStratway+\nCPDS\nRadar\nTCAS\n\nResearcher/Developer\nAmes Research Center\nLangley Research Center\nGA \xe2\x80\x93 ASI & TU Delft\nGA \xe2\x80\x93 ASI\nGA \xe2\x80\x93 ASI\n\nSS\nPrimary\nPrimary\nPrimary\nPrimary\nSecondary\n\nCA\nSecondary\nSecondary\nSecondary\nSecondary\nPrimary\n\nMitigated\nYes\nYes\nYes\nNo\nYes\n\nUnmitigated\nYes\nYes\nNo\nYes\nNo\n\n6.1.1 Configuration 1 Nomenclature Development\nBased on requirements from researchers, a nomenclature was developed to capture the\nneeds of each scenario. Each encounter\xe2\x80\x99s name included four parts:\n1. Type of encounter. Low speed players, high speed players, or multi-ship\nencounter\n2. Altitude offset between the ownship and intruder(s).\n3. Vertical profile.\n4. Angle of the intruder flying relative to ownship\xe2\x80\x99s path. For some letter indices, two\nangles were defined to accommodate multi-ship encounters.\n\n23\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nThe full set of nomenclature definition is shown in Figure 8.\n\nFigure 8. Configuration 1 Pairwise Encounters Nomenclature.\n\nThis nomenclature would help define and distinguish encounters, and give a quick, simple\nassessment of the type of encounter being performed.\n6.1.2 Configuration 1a Low Speed Ownship\nConfiguration 1a encounters used a low speed ownship (<210 KGS). Some of these\nencounters (noted) consisted of a high speed intruder (\xe2\x89\xa5210 KGS).\n6.1.2.1 Ames (AutoResolver) Pairwise Encounter Geometries\n\nAmes\xe2\x80\x99 low-speed ownship Pairwise Encounters were designed to test the\nAutoResolver/JADEM algorithm. The algorithm was further divided into two,\nAutoResolver 1 and 2, with different alerting thresholds. Ames\xe2\x80\x99 encounters were divided\nin three types: fly-through/maneuvering climbs/descents/level (Figure 9), fly-through\nalerting TCAS (Figure 10), and high-speed intruder (Figure 12). The vertical profiles for\nthe fly-through/maneuvering encounters is shown in Figure 11. Below is an explanation\nof each.\n24\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nAircraft in the fly-through scenarios fly towards a target CPA of 0 NM horizontal offset.\nActual CPA is not critical since any CPA can be compared to the CPA predictions. It is\ndesirable, however, to fly close enough to trigger a self-separation alert. The maneuver\nencounters are scenarios in which the pilot maneuvers the aircraft as directed by the\nspecific self-separation display.\nThese fly-through/maneuvering encounters tested various angles into: 0\xc2\xb0, 45\xc2\xb0, 90\xc2\xb0, 110\xc2\xb0,\n180\xc2\xb0, and a 45\xc2\xb0 and 90\xc2\xb0 blunder into. All encounters had a minimum 1,000 ft vertical\nseparation that was offset artificially within the algorithm, so as to make the ownship and\nintruder appear co-altitude.\n\nFigure 9. ARC Pairwise Encounter Angles 1.\n\nThe TCAS/Self Assurance/Sense and Avoid Interoperability (SSI) encounters were\ndesigned to evaluate interoperability between TCAS and self-separation systems. Selfseparation systems are expected to keep the ownship well clear of an intruder. Although\nwell clear is not specifically defined to avoid alerting the intruder\xe2\x80\x99s TCAS, alerting TCAS\ncan generally be considered not well clear. Ideally, the self-separation alert would trigger\nlong before the TCAS alert.\nThese encounters were designed simply as a fly-through to gather data. Because the\nvertical profile was planned to go as close as 300 ft, a \xe2\x80\x9cbuild-up\xe2\x80\x9d approach was used; the\nencounters were flown first at 1,000 ft, then 500 ft, and finally, 300 ft. To stay consistent,\n\n25\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nall of these encounters employed the 3,000 ft lateral separation (although only the 300 ft\nencounter required the lateral offset). These encounters had an angle into of 0\xc2\xb0, 45\xc2\xb0, and\n90\xc2\xb0.\n\nFigure 10. ARC Pairwise Encounter Angles 2.\n\nThe vertical profiles for all of Ames\xe2\x80\x99 low-speed intruder encounters is shown in Figure 11.\nFor the fly-through/maneuver encounters, angles 0\xc2\xb0, 45\xc2\xb0, and 90\xc2\xb0 required climb/descent\nairspeed as opposed to groundspeed, and are explained in detail in Section 6.1.2.1.1.\nAngles 110\xc2\xb0, 180\xc2\xb0, turning 45\xc2\xb0, and turning 90\xc2\xb0 had 1,000 ft vertical separation. Flythrough alerting TCAS had 1,000 ft, 500 ft, and 300 ft separation.\n\n26\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 11. ARC Pairwise Encounter Vertical Profiles.\n\nAs mentioned, Ames required cases with a high-speed intruder to mimic jet aircraft\nconditions. The angles for these encounters were 0\xc2\xb0, 45\xc2\xb0, 90\xc2\xb0, and overtaking 180\xc2\xb0.\nThese encounters were done both as a fly-through and a maneuver performed with\nrequired 1,000 ft vertical separation, shown in Figure 12.\n\n27\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 12. ARC High Speed Intruder Pairwise Encounter Angles.\n\n6.1.2.1.1 ARC Indicated Airspeed Encounter Wind Matrix\n\nTo validate ARC\xe2\x80\x99s simulation results for AutoResolver/JADEM, a slightly different\napproach was required for their climbing and descending encounters. Because their\nsimulations were built using indicated airspeed (KIAS) as opposed to knots groundspeed\n(KGS), the researcher requested that all of these climb/descent encounters be flown with\na constant airspeed.\nHowever, this posed a problem for CPA timing. All other encounters were designed with\ngroundspeed in mind, so that daily variable winds aloft would not affect the timing of the\nencounter. To mitigate this problem for ARC\xe2\x80\x99s climb/descent encounters, a wind adjust\nmatrix was built to help with timing and variable winds.\nAs depicted in Table 9, the Wind Adjust Matrix used a spreadsheet format to calculate\nrelative timing from the Initial Point (IP). Each series of climb/descent encounters (L13,\nL14, L15, and L16) were designed to start at a specific altitude. Using this altitude, a true\nairspeed (KTAS) was calculated for the desired airspeed. Once current winds were input\n(green boxes, wind speed and wind direction) simple trigonometry was used to show the\nheadwind the aircraft would experience for that encounter based on encounter angle.\nDistance adjust value was then calculated (more or less distance traveled for the\nencounter) based on headwind and the leg length. Although the researcher originally\n28\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nwished to communicate this distance adjust to the pilots, operationally it made more\nsense to communicate a time adjust. Therefore, two additional values were calculated:\nthe groundspeed the aircraft would experience based on the headwind and true airspeed\nat altitude, and finally a time adjust was calculated using the simple formula time =\ndistance / rate, using the distance adjust and groundspeed.\nFor each of the encounters of this type, the TC would ask the winds aloft for the climbing\nor descending aircraft. The values were input into the matrix and the spreadsheet would\nautomatically calculate a time adjust from the IP. The TC would then communicate this\nvalue back to the airborne players. A positive value indicated to the aircraft to begin their\ndescent that many seconds after IP crossing. A negative value indicated to the aircraft to\nbegin their descent that many seconds before the IP crossing.\nDuring later flights and for simplicity, values were adjusted to be called out in increments\nof 5 seconds since the aircrew found this solution to be easier to implement while\nperforming the dynamic encounters.\nThis compromise led to successful climb/descent encounters and the researcher received\ngood data where the matrix was used correctly.\n\n29\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nTable 9. ARC Wind Adjust Matrix.\nTIME BASED WIND DRIFT ADJUSTMENT FOR KIAS CLIMB/DESCENT ENCOUNTERS\nVersion: 22Jun2015\nWind Speed (kt)\n10\n+\nlater IP crossing\nWind Direction (deg)\n258\nearlier IP crossing\nLeg Length (min)\n3\n\nL13 Series: Climbing Intruder, 140 KIAS, Initial Alt = 12500 ft\nKTAS at Initial Alt =\n169\n\nEncounter Angle\nA - 0 deg\nC - 45 deg\nD - 90 deg\n\nAircraft\nHeading\n(deg)\n78\n33\n258\n\nHeadwind (kt)\n-10.0\n-7.1\n10.0\n\nDistance\nTime\nGround\nAdjust\nAdjust\nSpeed (kt)\n(nmi)\n(sec)\n0.5\n179.0\n10.1\n0.4\n176.1\n7.2\n-0.5\n159.0\n-11.3\n\nL14 Series: Descending Intruder, 140 KIAS, Initial Alt = 16000 ft\nKTAS at Initial Alt =\n178.6\n\nEncounter Angle\nA - 0 deg\nC - 45 deg\nD - 90 deg\n\nAircraft\nHeading\n(deg)\n78\n33\n258\n\nHeadwind (kt)\n-10.0\n-7.1\n10.0\n\nDistance\nTime\nGround\nAdjust\nAdjust\nSpeed (kt)\n(nmi)\n(sec)\n0.5\n188.6\n9.5\n0.4\n185.7\n6.9\n-0.5\n168.6\n-10.7\n\nL15 Series: Climbing Ownship, 120 KIAS, Initial Alt = 12000 ft\nKTAS at Initial Alt =\n143.8\n\nEncounter Angle\nA - 0 deg\nC - 45 deg\nD - 90 deg\n\nAircraft\nHeading\n(deg)\n258\n258\n348\n\nHeadwind (kt)\n10.0\n10.0\n0.0\n\nDistance\nTime\nGround\nAdjust\nAdjust\nSpeed (kt)\n(nmi)\n(sec)\n-0.5\n133.8\n-13.5\n-0.5\n133.8\n-13.5\n0.0\n143.8\n0.0\n\nL16 Series: Descending Ownship, 120 KIAS, Initial Alt = 16000 ft\nKTAS at Initial Alt =\n153.3\n\nEncounter Angle\nA - 0 deg\nC - 45 deg\nD - 90 deg\n\nAircraft\nHeading\n(deg)\n258\n258\n348\n\nHeadwind (kt)\n10.0\n10.0\n0.0\n\n30\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nDistance\nTime\nGround\nAdjust\nAdjust\nSpeed (kt)\n(nmi)\n(sec)\n-0.5\n143.3\n-12.6\n-0.5\n143.3\n-12.6\n0.0\n153.3\n0.0\n\n6.1.2.2 Langley (Stratway+) Pairwise Encounter Geometries\n\nNASA Langley designed self-separation encounters to test and collect data on their selfseparation algorithm Stratway+ (now called Detect & AvoID Alerting Logic for Uncrewed\nSystems (DAIDALUS)) using maneuvering flight test encounters. This series of scenarios\nwas designed to collect data to validate CPA predictions and validate the Stratway+\nsolution well clear band data during live flight test conditions. The encounters also\noperated on the edge of the TCAS Resolution Advisory (RA) envelope and ensured\nStratway+ guidance provided maneuver bands to operate outside the RA envelope of\nTCAS II. Most encounters were set at 3,000 ft lateral planned CPA with 400 or 500 ft\nvertical offset.\nAll of Langley\xe2\x80\x99s encounters required a lateral maneuver and were divided into low-speed\nownship level/climb/descent/double blunder (Figure 13, Figure 14, and Figure 15), lowspeed intruder multi-ship (Figure 16, Figure 17, and Figure 18), high-speed intruder\n(Figure 19), and high-speed intruder multi-ship (Figure 20 and Figure 21).\nEncounter geometries were flown at 0\xc2\xb0, 20\xc2\xb0, 45\xc2\xb0, 90\xc2\xb0, and 135\xc2\xb0 angle into. The 135\xc2\xb0\ngeometry was of particular interest to evaluate the effectiveness of Stratway+ in a late\nintruder discovery scenario where radar is operating at the edge of its azimuth. Multiple\nruns were conducted with varying sensor selection.\n\nFigure 13. LaRC Pairwise Encounter Angles.\n\n31\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nVertical maneuvers represented further evaluation of the Stratway+ algorithm\nperformance and also engaged the TCAS II RA envelope. Additionally, a double blunder\nencounter was added for FT3, to gauge the solution space of Stratway+. Climb/descent\nencounters had a 500 ft vertical separation and required a 1,000 fpm climb or descent.\n\nFigure 14. LaRC Pairwise Encounter Vertical Profiles 1.\n\nFigure 15. LaRC Pairwise Encounter Vertical Profiles 2.\n\n32\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nThe multiple intruder series of scenarios were designed to constrain the solution space\npresented to the pilot and to evaluate the Stratway+ solution well clear band data.\nStratway+ was designed to present well clear maneuver space as the union of all threats\nand a solution space which provides guidance well clear of all intruders. These scenarios\nincreased the complexity of the solution band data presented to the pilot as there were\nsolutions which were constrained to either side of the aircraft\xe2\x80\x99s course. For an SAA system\nto operate effectively in the NAS, it must be able to solve a multiple intruder scenario even\nthough this may be a very low probability scenario.\nAll multi-ship encounters had a 500 ft vertical separation between ownship and intruder\nand all were level flight/level maneuvers to introduce the first stage of this complexity\nwhich is planned to be continued in FT4.\nThe multi-ship encounters included several permutations: 0\xc2\xb0/0\xc2\xb0, 20\xc2\xb0/-20\xc2\xb0, 0\xc2\xb0/45\xc2\xb0, 45\xc2\xb0/90\xc2\xb0,\n0\xc2\xb0/90\xc2\xb0, 0\xc2\xb0/135\xc2\xb0, and 90\xc2\xb0/135\xc2\xb0 angles into, as depicted in Figure 16, Figure 17, and Figure\n18.\n\nFigure 16. LaRC Pairwise Multi-ship Encounters 1.\n\n33\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 17. LaRC Pairwise Multi-ship Encounters 2.\n\nFigure 18. LaRC Pairwise Multi-ship Encounters 3.\n\n34\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nThe high speed intruder encounter series of scenarios were designed to evaluate the\neffectiveness of the Stratway+ algorithm when engaging intruders operating at speeds\ntypically encountered with commercial jet transport aircraft transiting below Class A\nairspace. The increased intruder speed shortened the available pilot reaction time and\nprovided faster closure while the ownship started to execute the maneuver to remain well\nclear. It was also of interest to evaluate if alerting times effective at lower closure rates\nwith slower intruders would remain sufficient with higher closure speeds.\nFor Stratway+, high-speed intruder encounters were performed at 400 ft vertical\nseparation and at angles into of 0\xc2\xb0, 45\xc2\xb0, 90\xc2\xb0, and 135\xc2\xb0 (Figure 19).\n\nFigure 19. LaRC High Speed Intruder Pairwise Encounter Angles.\n\nAdditionally, multi-ship encounters were performed with a high-speed intruder with 500 ft\nvertical separation. These encounters\xe2\x80\x99 permutations include: 0\xc2\xb0/45\xc2\xb0, 0\xc2\xb0/90\xc2\xb0, and 0\xc2\xb0/135\xc2\xb0.\nFigure 20 and Figure 21 detail the high-speed and low-speed intruder for each.\n\n35\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 20. LaRC High-Speed Pairwise Multi-ship Encounters 1.\n\nFigure 21. LaRC High-Speed Pairwise Multi-ship Encounters 2.\n\n36\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n6.1.2.3 General Atomics CPDS Pairwise Encounter Geometries\n\nThe first group of encounters that was provided from General Atomics was to test the selfseparation algorithm CPDS.\nFive single intruder encounters were provided for CPDS: one at 400 ft vertical separation\nand four at 500 ft vertical separation. Each of these encounters, all low-speed, had the\nintruder performing a 45\xc2\xb0 standard rate turn blunder into the ownship\xe2\x80\x99s path.\nCPDS also required three multi-ship encounters. Each featured an intruder performing a\n45\xc2\xb0 standard rate turn blunder into the ownship\xe2\x80\x99s path, as well as a second intruder\nconstraining the solution space. These encounters were designed with the first intruder\nat 400 ft above ownship and the second at 500 ft below ownship.\nBelow is a detailed description of the encounters and their main objectives, as well as a\ntable (Table 10) to summarize/\nCPDS Objectives\n1. Test the current system / algorithms beyond the situation in which an intruder traverses\nthe various self-separation alert states in the way that would happen if the conflict\ngeometry already exists outside of the temporal limit defining Self-Separation Proximate\nTraffic (SSPT).\na. Test the system for situations in which Well Clear is resolved by intruder maneuver\nwhile having status CSSA (Corrective Self-Separation Alert) (before Self-Separation\nWarning Alert (SSWA) occurs).\ni. Loss of well clear is predicted and after the intruder alert status has become SelfSeparation Corrective Alert (SSCA), the intruder maneuvers in such a way that well\nclear will not be violated.\nb. Test the system for situations in which an intruder becomes CSSA due to a\nmaneuver within the 75 -25 seconds to the well clear boundary.\ni. In these situations the predictability in terms of time remaining until SSWA cannot\nbe deduced from the time the yellow band intersected ownship track or the time\ntraffic became SSCA.\nc. Test the system for situations in which the intruder self-separation alert state due to\na maneuver cycles from normal to CSSA to normal.\n2. Test the current system / algorithms beyond the situation in which an intruder traverses\nthe various self-separation alert states in the way that would happen if the conflict\ngeometry already exists outside of the temporal limit defining SSPT and an additional\nconstraint on the solution space.\na. Test the system for situations in which well clear is resolved by intruder maneuver\nwhile having status CSSA (before SSWA occurs).\ni. Loss of well clear is predicted and after the intruder alert status has become SSCA,\nthe intruder maneuvers in such a way that well clear will not be violated.\n37\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nb. Test the system for Situations in which an intruder becomes CSSA due to a\nmaneuver within the 75 -25 seconds to the well clear boundary.\ni. In these situations the predictability in terms of time remaining until SSWA cannot\nbe deduced from the time the yellow band intersected ownship track or the time\ntraffic became SSCA.\nc. Test the system for situations in which the intruder self-separation alert state due to\na maneuver transitions from normal to CSSA to normal.\n3. Test the conflict probe function for the most opposite impacts of wind on the same\nconflict geometry.\nTable 10. CPDS Objective and Encounter Overview.\n\nDesired UAS Pilot Performance\nThe desired UAS pilot performance in the task of remaining well clear comprises two\naspects:\n1.Timely detection of all conflicts (future loss of well clear) that will require a maneuver to\nprevent them from occurring unless the intruder resolves it in time, and appropriate\nexecution of the maneuver (timing and magnitude) that prevents the otherwise occurring\nwell clear violation.\n2. A minimum of unnecessary maneuvering. This comprises the prevention of:\na. Situations in which the pilot initiates a maneuver to remain well clear whereas the\ncontinuation of the current direction and velocity would not have resulted in a loss of\nwell clear.\nb. Situations in which ownship maneuvers due to a temporary predicted loss of well\nclear outside the 85 second threshold use for the SSPT.\nc. Situations in which the maneuver performed by the pilot to remain well clear is far\nmore severe than necessary.\nRequirements for CPDS Conflict Geometries (Figure 22)\nGiven the objectives, the following three types of encounters are needed:\n\n38\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n1. To meet objective 1a and 2a: Encounters that are predicted to result in a loss of well\nclear at a time to CPA larger than 120 seconds, but are resolved by the intruder\nmaneuvering between 75 and (25+TBD margin) seconds to well clear.\n2. To meet objective 1b and 2b: Encounters in which the intruder maneuvers within 110\nseconds to CPA in such a way that the predicted distance at CPA crosses the well clear\ndmod threshold.\n3. To meet objective 1c and 2c: Encounters that only during the maneuver of the intruder\ncause a predicted loss of well clear with a time to CPA that always remains above 60\nseconds.\n\nFigure 22. GA-ASI CPDS Pairwise Encounters.\n\nRequirements for Second Intruder (Figure 23)\nTo meet objective 2, the trajectory for the second intruder must meet the following\nrequirements:\n4. The second intruder is not used to generate a geometry which causes a predicted loss\nof well clear.\n5. The second intruder is not intended to maneuver, unless necessitated by an\n(unplanned) maneuver of Ikhana.\n\n39\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n6. The second intruder is to be positioned in such a way that during the encounter with\nintruder 1 (with \xe2\x80\x98during\xe2\x80\x99 defined as the period Ikhana being at least from 120 seconds to\nmoment until the predicted loss of well clear with intruder 1 occurs) the intruder will be\nPreventative Self-Separation Alert (PSSA) (using the proposed update to the PSSA\nspecification).\n7. The second intruder is to be positioned in such a way that within 10 seconds of the\nstart of a rate-one turn to the left of Ikhana, the PSSA becomes CSSA.\n\nFigure 23. GA-ASI CPDS Multi-ship Pairwise Encounters.\n\n6.1.2.4 General Atomics Radar Pairwise Encounter Geometries\n\nThe next set of encounters provided by General Atomics were designed to test the\nEngineering Development Module (EDM) DRR.\nTest encounter geometries provided by GA-ASI collected data on the performance of the\ncompany provided EDM radar system and to help inform the SC-228 radar working group\nMOPS. The EDM radar performance operating at low altitudes was unknown, thus during\nFT3, test encounters were planned to explore how the radar performs at low altitude with\nground clutter effecting target resolution. Figure 24 depicts the planned low altitude radar\nflight test geometries. The minimum test altitude was 1,000 ft AGL based off the highest\nground feature located along the flight path of the encounter. Both the ownship (Ikhana)\nand the intruder performed 1,000 ft AGL runs but at no time did an encounter participant\noperate below 1,000 ft. Eight low altitude radar runs were planned.\n40\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nAll low-altitude encounters performed had a vertical separation of 1,000 ft, 3,000 ft lateral\noffset, and were all flown head-on.\n\nFigure 24. GA-ASI Low-Altitude Pairwise Radar Encounters.\n\nFurther, GA-ASI required performance testing of the EDM radar to determine targeting\ncapabilities at the azimuth limits of the radar system (Figure 25), performance of the\nsystem of the radar when the intruder is persisting on the beam (Figure 26), as well as,\nsystem performance of the radar during intruder acceleration maneuvering called \xe2\x80\x9cZigZag\xe2\x80\x9d encounter (Figure 27).\nThe Constant Bearing, Decreasing Range (CBDR) encounters for radar held the intruder\nat a relative angle of either 110\xc2\xb0 or 90\xc2\xb0 bearing to the ownship. These encounters were\nlonger than most, being flown for 5 or 6 minutes to collect further radar data. The\nencounters included climbs and descents with 500 ft vertical separation, and were\nperformed in level flight with a minimum of 300 ft vertical separation.\nThe \xe2\x80\x9cZig-Zig\xe2\x80\x9d encounter, although depicted with defined angles in the figure, could change\nduring the flight; as long as the constant acceleration was in place, researchers would be\nreceiving the data they required. This encounter was flown with 1,000 ft vertical\nseparation.\n41\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 25. GA-ASI Radar CBDR Pairwise Encounters (110\xc2\xb0).\n\nFigure 26. GA-ASI Radar CBDR Pairwise Encounters (90\xc2\xb0)\n\n42\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 27. GA-ASI Radar Zig-Zag Pairwise Encounter.\n\n6.1.2.5 General Atomics TCAS Pairwise Encounter Geometries\n\nThe Collision Avoidance (CA) performance encounters were designed to test the full\nrange of TCAS Resolution Advisories (i.e., preventive and corrective) and when executed\nautomatically, to test the performance of the vehicle response in a real world environment.\nClimbing/descending ownship and intruders were included to capture realistic encounter\ndynamics of the Phase I DAA MOPS definition of "transition". These encounters also\nserved to capture Radar performance data all the way through a CA maneuver.\nFigure 28 and Figure 29 depict the mitigated single intruder TCAS runs that were\ndesigned to further investigate the threshold between collision avoidance and selfseparation boundaries. Runs were planned in a variety of geometries and used a buildup\napproach starting with 500 ft vertical separation and building up to 300 ft vertical\nseparation encounters, running in advisory and then AUTO (automatic) mode. Vertical\nblunder type maneuvers were planned with ownship maneuvers, intruder maneuvers, and\nsome encounters where both ownship and intruder perform vertical maneuvering toward\neach other with a minimum of 500 ft separation at the completion of the encounter.\nAngles into for these encounters included 0\xc2\xb0, 20\xc2\xb0, 45\xc2\xb0, 90\xc2\xb0, 135\xc2\xb0, and 160\xc2\xb0.\n\n43\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 28. GA-ASI TCAS Mitigated Pairwise Encounters 1.\n\nFigure 29. GA-ASI TCAS Mitigated Pairwise Encounters 2.\n\n44\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nSeveral of the CA performance encounters included multiple threat aircraft. While not\ntraditional "multi-threat" encounters as defined by TCAS, these encounters were\ndesigned to generate TCAS RA\'s one at a time or sequentially. These did not directly test\nthe TCAS multi-threat logic, but were designed to test the dynamics of multiple TCAS\nRAs generated in different directions. The encounter was timed to induce one TCAS RA,\nfollowed by a "clear of conflict", followed by another RA in the opposite direction to the\nfirst. These encounters were the most complex to be tested during FT3. A buildup\napproach was used for these encounters starting at 300 ft vertical separation with Ikhana\noperating in advisory mode. Once the 300 ft encounter had been cleared in advisory, the\nencounter would be performed in AUTO mode at 300 ft vertical separation. Once 300 ft\nencounters were cleared, 200 ft vertical separation would be tested using the same\nbuildup approach. As mentioned later in this report, researchers stated on the flight day\nthat advisory 300 ft encounters were enough for their data collection (thus only M67Q and\nM68Q were performed, and in advisory mode).\nThe figures below show the planned top view (Figure 31) of these TCAS multi-ship\nsequential encounters as well as the vertical profile views (Figure 32, Figure 33).\n\nFigure 30. GA-ASI TCAS Sequential Pairwise Encounters.\n\n45\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 31. GA-ASI TCAS Sequential Pairwise Encounters Vertical Profile 1.\n\nFigure 32. GA-ASI TCAS Sequential Pairwise Encounters Vertical Profile 2.\n\n46\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n6.1.3 Configuration 1b High Speed Ownship\nConfiguration 1b encounters required a high speed ownship (\xe2\x89\xa5210 KGS). Unfortunately\ndue to time constraints and unavailability of the high speed ownship, these encounters\nwere not completed.\n6.1.3.1 Ames (AutoResolver) Pairwise Encounter Geometries\n\nAmes required some encounters where a high-speed ownship (210 and 250 KGS) was\nnecessary to test the limits of their algorithm, with a low-speed (130 KGS) intruder. The\nplanned angles into for this configuration were 0\xc2\xb0, 45\xc2\xb0, 90\xc2\xb0, and an overtaking 180\xc2\xb0, seen\nin Figure 33. The high-speed encounters would test the fly-through (\xe2\x80\x9cunmitigated\xe2\x80\x9d) case,\nas well as maneuvering based on algorithm directive (\xe2\x80\x9cdisplay maneuver\xe2\x80\x9d). All high-speed\nownship scenarios had planned 1,000 ft vertical separation for safety.\n\nFigure 33. ARC High Speed Ownship Pairwise Encounter Angles.\n\n6.1.4 Matrix Development\nA comprehensive flight test matrix was built for FT3 based on researcher requirements\nand the geometries described in Section 6.1. The encounters were grouped into sections\nby their encounter angle or type: head-on, 45\xc2\xb0, 90\xc2\xb0, 135\xc2\xb0, high-speed participant, radar,\nTCAS, and CPDS.\nOriginally, researchers from Langley Research Center, Ames Research Center, and\nGeneral Atomics each provided their own set of maneuvers and requirements in\n47\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nspreadsheet form. These requirements were consolidated into one matrix by personnel\nat Langley, and this spreadsheet was inherited by Armstrong Flight Research Center and\nextensively expanded upon. Flight planning by Armstrong began with this version \xe2\x80\x93\ngathering requirements for the type of maneuver being performed, altitudes,\ngroundspeeds, and sensors.\nThe purpose of the matrix was to provide a centralized spreadsheet to input all required\nresearch geometries, plan their flight in the approved airspace, and populate the flight\ncards with information and requirements for each encounter.\nThe matrix was built in Microsoft Excel\xc2\xae and used Visual Basic for Applications (VBA) for\ncalculating pertinent values, such as GPS coordinates in multiple formats. Look-up tables\nwere used on several occasions and especially to populate Ikhana Lost Link mission for\neach encounter based on CPA. Additionally, Excel was useful for calculating IP to\nManeuver Point (MP) and CPA using dead-reckoning equations. Here is a breakdown of\nthe parameters, some of which (bolded) are shown in the snapshot Table 11.\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\nScenario Number (S/N): The Scenario Number was the most critical number for\neach encounter. This number served as an identification number for the unique\ngeometry, flight altitude, and other relevant information. Although encounters could\nhave the same scenario name, the number was unique. S/N was used throughout\nthe entire flight test, from the matrix for building the encounter, to encounter\nprioritization, and finally, for creating the flight test cards.\nScenario Name: Scenario names were based from the Pairwise Encounters\nNomenclature (see Section 6.1). The Scenario Name was a quick reference to\ngain SA on what type of encounter was being performed. Scenario Number could\nbe the same for two or more encounters.\nScenario Name (Old Nomenclature): Previously, encounters had a naming\nconvention developed from researchers at Ames. Later, this was refined and\nmodified to the version currently being used. The old nomenclature was kept on\nthe matrix as a trace to the original required encounter.\nOWN True Course: True Course of the ownship. This value was used to calculate\nGPS coordinates (magnetic course was later calculated on the flight cards\nthemselves).\nLeg time: Time for the encounter from Commence Exercise (COMEX). Leg time\nincluded some buffer for setting up the encounter geometry. Most encounters had\n3 minute legs, with some maneuvering encounters (3.25, 3.5), low-altitude radar\n(3.5), and CBDR radar (5, 6).\nMin. Vertical Separation: Smallest vertical separation between ownship and\nintruder(s) for the encounter at CPA. If the vertical separation necessary for an\nencounter was \xe2\x89\xa4500 ft, a lateral offset was required for safety.\n\n48\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nAngle Into: Relative angle of the intruder(s) into the ownship for that geometry.\nMore can be seen about the angles into in Section 6.1. This value was used to\ncalculate GPS coordinates.\nLateral Offset: A lateral offset of half a nautical mile (~3,000 ft) was calculated\ninto the geometry for encounters with a vertical separation of \xe2\x89\xa4500 ft. This was to\nensure that if visual was not acquired according to mission rule, there would still\nbe a safety buffer.\nGS OWN: Groundspeed of the ownship. Depending on what the researcher\nwanted to see, the groundspeed varied between encounters. Most low-speed\nencounters had a GS of 130 or 150 KGS. High-speed encounters required an\nownship GS of \xe2\x89\xa5210 KGS. Groundspeed was preferred for calculations since it did\nnot have to take wind into account. For some encounters, an airspeed of 120 KIAS\nwas required. GPS coordinates were provided for these encounters from the\nresearcher. More discussion can be seen in Section 6.1.2.1.1 on how these unique\nencounters were performed.\nGS INT1: Most encounters required the intruder(s) to fly at 150 or 180 KGS. For\nhigh-speed encounters, intruders were required to fly \xe2\x89\xa5210 KGS. For some\nencounters, an airspeed of 140 KIAS was required.\nGS INT2: Multi-ship encounters only had low-speed intruders for Flight Test Series\n3. Thus, all intruder 2 groundspeeds were 150 or 180 KGS.\nOwnship Initial Altitude: Altitudes chosen for each encounter took Ikhana and\nintruder flight performance into consideration, as well as airspace. Encounters\nbegan 10K-20K ft MSL. Low-altitude radar encounters took the highest point on\nthe terrain (3,200 ft MSL) and added 1,000 ft for the flight level (thus 4,200 ft MSL).\nOwnship Vertical Velocity: For some encounters, a climb or descent was\nrequired by the ownship. Rates required were either 1,000 fpm (climb) or -1,000\nfpm (descent).\nOwnship Final Altitude: Once more, the final altitude was within the block of 10K20K ft MSL or 1,000 ft above the highest terrain point.\nIntruder 1 Initial Altitude: Identical to ownship.\nIntruder 1 Vertical Velocity: Identical to ownship.\nIntruder 1 Final Altitude: Identical to ownship.\nIntruder 2 Initial Altitude: Identical to ownship.\nIntruder 2 Vertical Velocity: Identical to ownship.\nIntruder 2 Final Altitude: Identical to ownship.\nCPA OWN: The CPA of the ownship was one of its most important parameters.\nThe CPA was the point where the ownship and intruder(s) would be nearest in\nspace for each encounter. Within R-2515, 11 different ownship CPAs were chosen\nfor all Pairwise Encounters. CPAs were chosen to accommodate for the 3 minute\nlegs in the airspace (as well as the longer radar legs), plan sun angles for manned\n49\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\nintruders, and were used to build the Ikhana Lost Link mission. Additionally, the\nCPAs made it easier to group encounters based on matching CPA when building\nthese geometries in Zeus for SAF SA. Finally, the CPAs were used in a lookup\ntable to build GPS coordinates for all geometries.\nCPA OWN Lat/Lon: Chosen latitude and longitude for each CPA in Decimal\nDegrees (DD) format. The CPA latitude/longitude was found using FalconView.\nIP OWN: The IP of the ownship was chosen to fit within the airspace and to\naccommodate for the 3 min (or more) legs. The IP served as the point where the\nencounter would start and where the aircraft needed to be at the COMEX. Each IP\nhad an identification number based on its coordinates, and for encounters that\nused the same IP, an identical IP ID was used. The IP was also used on the flight\ncards for reference on the top view (see Figure 34) and coordinates.\nIP OWN Lat/Lon: Calculated latitude and longitude of the ownship IP from the CPA\nusing dead reckoning equations, in DD format.\nIP OWN DME: Calculated distance in NM from the CPA to the IP for ownship.\nIP INT: The same procedure was used for intruder IP as for ownship.\nIP INT Lat/Lon: Calculated latitude and longitude of the initial point for intruder from\nthe CPA in DD format.\nIP INT DME: Calculated distance in NM from the CPA to the IP for intruder.\nCPA INT: Similar to the ownship, the CPAs for the intruder were also grouped\nbased on GPS coordinates. However, since the geometries for the intruders were\nbuilt around those for the ownship, there were many more CPAs for intruders than\nfor the ownship (due to various angles into, groundspeeds, etc.).\nCPA INT Lat/Lon: Calculated latitude and longitude of the intruder CPA in DD. The\nCPA for the intruder was either the same as the ownship (\xcb\x83500 ft vertical\nseparation) or calculated to be 3,000 ft away (\xe2\x89\xa4500 ft vertical separation) from\nownship CPA using the relative angle into.\nMP INT: For some encounters, a maneuver was required in the middle of the\nencounter for the intruder to create a \xe2\x80\x9cblunder\xe2\x80\x9d type scenario. Maneuver points\nonce again held the same ID if they had the same GPS coordinates.\nMP INT Lat/Lon: Calculated latitude and longitude in DD that the intruder was\nexpected to begin their standard rate turn to the CPA.\nOn Condition: Each encounter required that the aircraft be on condition a certain\nnumber of seconds from CPA. This was to ensure that the researcher\xe2\x80\x99s algorithm\nwould have enough time to pick up the aircraft in the encounter for their required\nconditions (speeds, altitudes, vertical speed, etc.). Most encounters had a\nrequirement of being on condition for 2.5 min (0.5 min for setting up) for the total 3\nmin legs required.\nTolerance: A carryover from ACAS Xu, the tolerance for an encounter was the\nnumber of seconds that each player could be away from the CPA and still achieve\n50\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\nthe correct alerting for that encounter. Most encounters had a timing of \xc2\xb15, 8, or\n10 seconds. This value was determined from simulation by the researchers or from\nprevious experience. As time went on, it became apparent that tolerance was not\nas critical for FT3 (especially for the maneuvering encounters) as much as\nachieving stable conditions.\nIkhana Lost Link: In the case that Ikhana would lose link, a Lost Link mission was\nprogrammed into its flight computer. The Lost Link mission was based on the CPA\nthe ownship would be heading to for that encounter. For this reason, it was critical\nfor the Ikhana team to have all CPAs prior to flight testing so they could build this\nmission. The Lost Link mission was input in the flight matrix using a lookup table\nbased on CPA.\n\nIt\xe2\x80\x99s important to note that once all the geometries were built, they were geo-referenced in\nFalconView to ensure they fit in the assigned airspace or gave enough maneuverability\nspace for all aircraft participants. If they did not, the CPA and GPS coordinates were\niterated until a suitable geometry was achieved.\nThe Flight Test Matrix also included a section showing Scenario Number, priority, and\nwhat sensors would be required for that encounter. Encounters for a particular flight day\nwere chosen in this way.\nThus the matrix provided the basic requirements for execution, safety mitigation, and\nprioritization for planned FT3 scenarios.\n\n51\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nTable 11. Pairwise Encounters Scenario Matrix.\n\nHigh Speed Level Encounters\n0/45/90/135/180\n\n135\xc2\xb0 Overtaking\n90\xc2\xb0 / 110\xc2\xb0 Crossing Level/Ascending/Descending Left- 45\xc2\xb0 Crossing Level/Ascending /Descending Left-toLevel/Ascending\nto-Right\nRight\n/Descending Left-\n\nHead-On Level/Ascending/Descending\n\nScenario\nNumber\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n\nScenario\nName\nL42A\nL42B\nL55A\nL55B\nL56A\nL56B\nL57A\nM59Q\nM59U\nL13A\nL13A\nL14A\nL14A\nL15A\nL15A\nL16A\nL16A\nL12A\nL52A\nL32A\nL42C\nL55C\nL56C\nL57C\nL53C\nL54C\nM59R\nM59V\nL12C\nL52C\nL32C\nL13C\nL13C\nL14C\nL14C\nL15C\nL15C\nL16C\nL16C\nL12M\nL12M\nL42D\nL55D\nL56D\nL57D\nL53D\nL54D\nM59S\nL12D\nL52D\nL32D\nL13D\nL13D\nL14D\nL14D\nL15D\nL15D\nL16D\nL16D\nL12E\nL12E\nL12N\nL12N\nL42F\nL55F\nL56F\nL57F\nL53F\nL54F\nM59T\nM59W\nH42A\nH42C\nH42D\nH42F\nM59R\nM59S\nM59T\nH12A\nH12A\nH12A\nH12A\nH12A\nH12A\nH12A\n\nLeg Time\n(minutes)\n\nMin Vertical\nSeparation\n(ft)\n\nAngle\nInto\n\nLateral\nOffset\n(ft)\n\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3.25\n3.25\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3.5\n3.5\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n\n400\n400\n500\n500\n500\n500\n500\n500\n500\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n500\n300\n400\n500\n500\n500\n500\n500\n500\n500\n1000\n500\n300\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n400\n500\n500\n500\n500\n500\n500\n1000\n500\n300\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n400\n500\n500\n500\n500\n500\n500\n500\n400\n400\n400\n400\n500\n500\n500\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n\n0\n20\n0\n20\n0\n20\n0\n0\n20/-20\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n45\n45\n45\n45\n45\n45\n0/45\n45/90\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n45\n90\n90\n90\n90\n90\n90\n0/90\n90\n90\n90\n90\n90\n90\n90\n90\n90\n90\n90\n110\n110\n90\n90\n135\n135\n135\n135\n135\n135\n0/135\n90/135\n0\n45\n90\n135\n0/45\n0/90\n0/135\n0\n0\n0\n0\n0\n0\n0\n\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n0\n0\n0\n0\n0\n0\n0\n0\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n0\n0\n0\n0\n0\n0\n0\n\nGS OWN GS INT1\n150\n150\n130\n130\n130\n130\n130\n150\n150\n150\n150\n150\n150\n120\n120\n120\n120\n150\n150\n150\n150\n130\n130\n130\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n120\n120\n120\n120\n150\n150\n150\n130\n130\n130\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n120\n120\n120\n120\n150\n150\n150\n150\n150\n130\n130\n130\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n130\n130\n130\n130\n210\n210\n250\n\n180\n180\n180\n180\n180\n180\n180\n180\n180\n140\n140\n140\n140\n150\n150\n150\n150\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n140\n140\n140\n140\n150\n150\n150\n150\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n140\n140\n140\n140\n150\n150\n150\n150\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n300\n300\n300\n300\n300\n300\n300\n210\n210\n250\n250\n130\n130\n130\n\nGS INT2\n\nOwnship\nInitial\nAltitude\n\nOwnship\nVertical\nVelocity\n\nOwnship\nFinal\nAltitude\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n150\n150\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n150\n150\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n150\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n150\n150\nNA\nNA\nNA\nNA\n150\n150\n150\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n12000\n12000\n11000\n11000\n16000\n16000\n11000\n13000\n13000\n16500\n16500\n12000\n12000\n12000\n12000\n16000\n16000\n12000\n12000\n12000\n12000\n11000\n16000\n11000\n14500\n12000\n13000\n13000\n12000\n12000\n12000\n16500\n16500\n12000\n12000\n12000\n12000\n16000\n16000\n12000\n12000\n12000\n11000\n16000\n11000\n14500\n12000\n13000\n12000\n12000\n12000\n16500\n16500\n12000\n12000\n12000\n12000\n16000\n16000\n12000\n12000\n12000\n12000\n12000\n11000\n16000\n11000\n14500\n12000\n13000\n13000\n12000\n12000\n12000\n12000\n13000\n13000\n13000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n\n0\n0\n1000\n1000\n-1000\n-1000\n1000\n0\n0\n0\n0\n0\n0\n1000\n1000\n-1000\n-1000\n0\n0\n0\n0\n1000\n-1000\n1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1000\n1000\n-1000\n-1000\n0\n0\n0\n1000\n-1000\n1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1000\n1000\n-1000\n-1000\n0\n0\n0\n0\n0\n1000\n-1000\n1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n12000\n12000\n14000\n14000\n13000\n13000\n14000\n13000\n13000\n16500\n16500\n12000\n12000\n15000\n15000\n13000\n13000\n12000\n12000\n12000\n12000\n14000\n13000\n14000\n14500\n12000\n13000\n13000\n12000\n12000\n12000\n16500\n16500\n12000\n12000\n15000\n15000\n13000\n13000\n12000\n12000\n12000\n14000\n13000\n14000\n14500\n12000\n13000\n12000\n12000\n12000\n16500\n16500\n12000\n12000\n15000\n15000\n13000\n13000\n12000\n12000\n12000\n12000\n12000\n14000\n13000\n14000\n14500\n12000\n13000\n13000\n12000\n12000\n12000\n12000\n13000\n13000\n13000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n\n52\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nIntruder Intruder Intruder Intruder Intruder Intruder\nCPA\nOn\nIkhana\n1 Initial 1 Vertical 1 Final 2 Initial 2 Vertical 2 Final Tolerance Condition\nLost Link\nAltitude Velocity Altitude Altitude Velocity Altitude\n(sec)\n(min)\n12400\n12400\n14500\n14500\n12500\n12500\n17500\n13500\n13500\n12500\n12500\n16000\n16000\n16000\n16000\n12000\n12000\n13000\n12500\n12300\n12400\n14500\n12500\n17500\n11000\n15500\n13500\n13500\n13000\n12500\n12300\n12500\n12500\n16000\n16000\n16000\n16000\n12000\n12000\n13000\n13000\n12400\n14500\n12500\n17500\n11000\n15500\n13500\n13000\n12500\n12300\n12500\n12500\n16000\n16000\n16000\n16000\n12000\n12000\n13000\n13000\n13000\n13000\n12400\n14500\n12500\n17500\n11000\n15500\n13500\n13500\n12400\n12400\n12400\n12400\n13500\n13500\n13500\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n\n0\n0\n0\n0\n0\n0\n-1000\n0\n0\n1000\n1000\n-1000\n-1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1000\n1000\n-1000\n0\n0\n0\n0\n0\n1000\n1000\n-1000\n-1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1000\n1000\n-1000\n0\n0\n0\n0\n1000\n1000\n-1000\n-1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1000\n1000\n-1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n12400\n12400\n14500\n14500\n12500\n12500\n14500\n13500\n13500\n15500\n15500\n13000\n13000\n16000\n16000\n12000\n12000\n13000\n12500\n12300\n12400\n14500\n12500\n14500\n14000\n12500\n13500\n13500\n13000\n12500\n12300\n15500\n15500\n13000\n13000\n16000\n16000\n12000\n12000\n13000\n13000\n12400\n14500\n12500\n14500\n14000\n12500\n13500\n13000\n12500\n12300\n15500\n15500\n13000\n13000\n16000\n16000\n12000\n12000\n13000\n13000\n13000\n13000\n12400\n14500\n12500\n14500\n14000\n12500\n13500\n13500\n12400\n12400\n12400\n12400\n13500\n13500\n13500\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n12500\n12500\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n12500\n12500\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n12500\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n12500\n12500\nNA\nNA\nNA\nNA\n12500\n12500\n12500\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n0\nNA\nNA\nNA\nNA\n0\n0\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n12500\n12500\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n12500\n12500\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n12500\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n12500\n12500\nNA\nNA\nNA\nNA\n12500\n12500\n12500\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb18\n\xc2\xb18\n\xc2\xb18\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb15\n\xc2\xb15\n\xc2\xb15\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 2\nLL WPT 5\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 8\nLL WPT 10\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 9\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 9\nLL WPT 9\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 6\nLL WPT 10\nLL WPT 2\nLL WPT 10\nLL WPT 10\nLL WPT 1\nLL WPT 6\nLL WPT 6\nLL WPT 6\nLL WPT 6\nLL WPT 6\nLL WPT 6\nLL WPT 6\nLL WPT 9\nLL WPT 9\nLL WPT 9\n\nHigh Speed Level Encounters 0/45/90/135/180\nRadar Runs for GA\nTCAS Mitigated Runs for GA\nCPDS Runs for GA\n\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n\nH12A\nH12C\nH12C\nH12C\nH12C\nH12C\nH12C\nH12C\nH12C\nH12D\nH12D\nH12D\nH12D\nH12D\nH12D\nH12D\nH12D\nH12H\n(Deleted)\nH12H\n(Deleted)\nL12A (1)\nL12A (2)\nL12A (3)\nL12A (4)\nL11A (1)\nL11A (2)\nL11A (3)\nL11A (4)\nL32G (110)\nL31G (110)\nL53G (110)\nL55G (110)\nL54G (110)\nL56G (110)\nL32G (90)\nL31G (90)\nL53G (90)\nL55G (90)\nL54G (90)\nL56G (90)\nL12P\nL32A\nL32C\nL32D\nL32F\nL31A\nL31C\nL31D\nL31F\nL53A\nL53C\nL53D\nL53F\nL55A\nL55C\nL55D\nL55F\nL54A\nL54C\nL54D\nL54F\nL56A\nL56C\nL56D\nL56F\nL32B\nL32G\nL32H\nL31B\nL31G\nL31H\nL57A\nL57D\nM67Q\nM68Q\nM27Q\nM28Q\nL42M\nL52M (1)\nL52M (2)\nL52M (3)\nL52M (4)\nM79X (1)\nM79X (2)\nM79X (3)\n\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n5\n5\n5\n5\n5\n5\n6\n6\n6\n6\n6\n6\n5\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3.1\n3.1\n3.2\n3.2\n3\n3\n3\n3\n3\n3\n3\n3\n\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n300\n300\n500\n500\n500\n500\n300\n300\n500\n500\n500\n500\n1000\n300\n300\n300\n300\n300\n300\n300\n300\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n300\n300\n300\n300\n300\n300\n500\n500\n300/700\n300/700\n200/700\n200/700\n400\n500\n500\n500\n500\n400/500\n400/500\n400/500\n\n0\n45\n45\n45\n45\n45\n45\n45\n45\n90\n90\n90\n90\n90\n90\n90\n90\n180\n180\n180\n180\n0\n0\n0\n0\n0\n0\n0\n0\n160\n160\n160\n160\n160\n160\n160\n160\n160\n160\n160\n160\n65/115\n0\n45\n90\n135\n0\n45\n90\n135\n0\n45\n90\n135\n0\n45\n90\n135\n0\n45\n90\n135\n0\n45\n90\n135\n20\n160\n180\n20\n160\n180\n0\n90\n0\n0\n0\n0\n45\n45\n45\n45\n45\n45\n45\n45\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n0\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n14000\n14000\n12150\n3000\n3000\n14000\n12150\n\n250\n130\n130\n130\n130\n210\n210\n250\n250\n130\n130\n130\n130\n210\n210\n250\n250\n210\n210\n250\n250\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n150\n\n130\n210\n210\n250\n250\n130\n130\n130\n130\n210\n210\n250\n250\n130\n130\n130\n130\n130\n130\n130\n130\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n160\n160\n160\n160\n160\n160\n150\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n180\n160\n180\n180\n180\n160\n160\n180\n180\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n180\n180\n180\n180\nNA\nNA\nNA\nNA\nNA\n180\n180\n180\n\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n4200\n5200\n6200\n7200\n5200\n6200\n7200\n8200\n12000\n12300\n16500\n11000\n11000\n16500\n12000\n12300\n17500\n11000\n11000\n17500\n12000\n12000\n12000\n12000\n12000\n12300\n12300\n12300\n12300\n14500\n14500\n14500\n14500\n11000\n11000\n11000\n11000\n12000\n12000\n12000\n12000\n15500\n15500\n15500\n15500\n12000\n12000\n12000\n12300\n12300\n12300\n11000\n11000\n13300/13700\n13700/13300\n13200/13700\n13700/13200\n12000\n12000\n12000\n12000\n12000\n13000\n13000\n13000\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1000\n0\n-1000\n0\n0\n0\n1000\n0\n-1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1000\n1000\n1000\n1000\n0\n0\n0\n0\n-1000\n-1000\n-1000\n-1000\n0\n0\n0\n0\n0\n0\n1000\n1000\n1000/-1000\n-1000/1000\n1000/-1000\n-1000/1000\n0\n0\n0\n0\n0\n0\n0\n0\n\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n12000\n4200\n5200\n6200\n7200\n5200\n6200\n7200\n8200\n12000\n12300\n16500\n16000\n11000\n11500\n12000\n12300\n17500\n17000\n11000\n11500\n12000\n12000\n12000\n12000\n12000\n12300\n12300\n12300\n12300\n14500\n14500\n14500\n14500\n14000\n14000\n14000\n14000\n12000\n12000\n12000\n12000\n12500\n12500\n12500\n12500\n12000\n12000\n12000\n12300\n12300\n12300\n14000\n14000\n13700/13000\n13300/14000\n13700/13000\n13200/13900\n12000\n12000\n12000\n12000\n12000\n13000\n13000\n13000\n\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n5200\n6200\n7200\n8200\n4200\n5200\n6200\n7200\n12300\n12000\n11000\n16500\n16500\n11000\n12300\n12000\n11000\n17500\n17500\n11000\n13000\n12300\n12300\n12300\n12300\n12000\n12000\n12000\n12000\n11000\n11000\n11000\n11000\n14500\n14500\n14500\n14500\n15500\n15500\n15500\n15500\n12000\n12000\n12000\n12000\n12300\n12300\n12300\n12000\n12000\n12000\n17500\n17500\n14000\n14000\n13900\n13900\n12400\n12500\n12500\n12500\n12500\n13400\n13400\n13400\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1000\n0\n-1000\n0\n0\n0\n1000\n0\n-1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1000\n1000\n1000\n1000\n0\n0\n0\n0\n-1000\n-1000\n-1000\n-1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1000\n-1000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n13000\n5200\n6200\n7200\n8200\n4200\n5200\n6200\n7200\n12300\n12000\n16000\n16500\n11500\n11000\n12300\n12000\n17000\n17500\n11500\n11000\n13000\n12300\n12300\n12300\n12300\n12000\n12000\n12000\n12000\n14000\n14000\n14000\n14000\n14500\n14500\n14500\n14500\n12500\n12500\n12500\n12500\n12000\n12000\n12000\n12000\n12300\n12300\n12300\n12000\n12000\n12000\n14500\n14500\n14000\n14000\n13900\n13900\n12400\n12500\n12500\n12500\n12500\n13400\n13400\n13400\n\nAlgorithm\n\nType\n\nSpecial Cases\n\nJADEM\nStratway+\nCPDS\n\nMultiship\n\nKIAS\nLow Altitude\nHigh-Speed O/S\n\n53\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n13000\n13000\n13000\n13000\nNA\nNA\nNA\nNA\nNA\n12500\n12500\n12500\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n0\n0\n0\nNA\nNA\nNA\nNA\nNA\n0\n0\n0\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n13000\n13000\n13000\n13000\nNA\nNA\nNA\nNA\nNA\n12500\n12500\n12500\n\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n\n\xc2\xb1 10\n\n2.5\n\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\xc2\xb1 10\n\n3\n3\n3\n3\n3\n3\n3\n3\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n3.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n2.5\n\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 10\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 7\nLL WPT 3\nLL WPT 7\nLL WPT 3\nLL WPT 7\nLL WPT 3\nLL WPT 7\nLL WPT 3\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 1\nLL WPT 8\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 1\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 1\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 1\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 1\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 1\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 1\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 9\nLL WPT 10\nLL WPT 6\nLL WPT 6\nLL WPT 6\nLL WPT 6\nLL WPT 8\nLL WPT 8\nLL WPT 2\nLL WPT 8\nLL WPT 8\nLL WPT 6\nLL WPT 6\nLL WPT 6\n\n6.1.5 Flight Card Description\nFlight cards for Configuration 1 were developed based on cards created during ACAS Xu\nby personnel from Massachusetts Institute of Technology (MIT) Lincoln Laboratory.\nThese flight cards, which were atypical, were used for similar type encounters during that\nflight test.\nDue to the success of that program and card clarity, a similar format was used for Flight\nTest Series 3. With the collaborative effort of FT3 Ikhana Operations, Armstrong IT&E\nOperations, and researcher input, the product was designed to provide a simple, easyto-use, and easily modifiable card that met researcher requirements for mission success.\nThe cards also presented a familiar format to that of an instrument approach plate which\nenabled the aircrew to quickly determine test parameters and critical flight information.\nAn additional factor that was taken into the design was that of Human Factors: throughout\nall scenario cards for Configuration 1, a standard color format was selected:\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nOwnship and middle altitude aircraft was green\nIntruder 1 and higher altitude aircraft was red\nIntruder 2 and lower altitude aircraft was blue\n\nThis color scheme was chosen for its quick SA for the customer and easy cockpit use.\nThe issue of color-blindness was considered, but since all of the test pilots had been\nthrough physical examination that checks for color blindness, it was a non-factor.\nThe flight cards were a tremendous success, both visually and for being highly informative\nand practical. Kudos were received from Ikhana, intruder aircrew, and industry\nstakeholders alike.\n6.1.5.1 Production\n\nFlight Cards were built in Excel and were directly linked to the Flight Test Matrix. The\nmatrix had the capability of auto-populating much of the information for the card based\non look-up tables from the scenario number: IP/CPA names and coordinates, altitudes,\nheadings, velocities, distances, groundspeeds, lost link mission for Ikhana, on-condition\ntiming, and CPA tolerances. Manual input was required for the sensor selection,\ndeconfliction altitudes, notes, and abort procedures.\nThe top down view is a geo-referenced image that was built in FalconView, and\nfurthermore auto-generated by a program called Excel2FV. Excel2FV plotted and created\nFalconView files automatically by taking user-grouped identical geometries from Excel as\nan input. Once the geometries/files were created, IP/CPA icons were manually added\nonto the FalconView files and a normal image file was created. These image files were\nmoved to Microsoft PowerPoint\xc2\xae and aircraft icons added. The top-down views were then\npasted onto the flight cards.\n\n54\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nVertical profiles were built in PowerPoint. Groups of similar encounters were easily pasted\nonto the flight cards.\nThe cards were designed to fit on an 8.5\xe2\x80\x9dx11\xe2\x80\x9d sheet of paper, with one half dedicated to\nownship and the other to intruder. This allowed users to either cut the deck in half or fold\ntheir card to the one of interest. For multi-ship, an additional sheet of standard size paper\nwas required.\nSince there were over 200 test points, over 200 unique flight cards were created in this\nfashion for Configuration 1. Each flight card had its own spreadsheet and the cards were\nlater converted into PDF, packaged into a document for that particular flight day, and\ndistributed in soft- and hard-copy form to all FT3 participants.\n6.1.5.2 Breakdown\n\nFigure 34 and Figure 35 below show an example of the Flight Cards used for\nConfiguration 1. The following is a breakdown of card sections.\n6.1.5.2.1 Ownship\n\nOwnship cards showed the aircraft route of flight as green. Here are the main cards\nsections, listed alphabetically, and a brief explanation for each:\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\nAbort Alt: This altitude is defined as the safe escape altitude the aircraft were\nexpected to immediately hold and maintain if an abort is called. For the ownship,\nabort altitudes normally required no maneuver other than to remain level, or during\nclimbs and descents to level off at the identified level off altitude. The encounter\nwould end if an abort was called. The abort altitude provided each participating\naircraft at least 1,000 ft of vertical separation.\nAbort Heading: Heading to maintain or fly to in the case of an abort. The\nencounter would end if an abort was called.\nAircraft Role: This section specified whether the aircraft was Ownship, Intruder 1,\nor Intruder 2.\nAircraft: Callsign of the aircraft for that test card.\nCard Number: The card numbers were chosen the day of flight and represented\nthe flight order.\nCard Type Version: Cards went through several iterations before reaching their\nfinal version (9). The reason for versions was to keep track of all format updates\nmade during the production of cards. Changes from version to version included\nadditional information added to top view, notes section added, updated encounter\ninstructions, etc.\nCPA: The CPA was the predicted point where the ownship and intruder(s) would\nbe closest vertically and laterally. For encounters less than 500 ft in separation,\nthis CPA had a 0.5 NM lateral offset. Each CPA had a unique number (i.e. CPA7)\nand if GPS coordinates for a CPA repeated, the same CPA number would be used.\n55\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\nThe coordinates listed were in two forms for use by the Ikhana operators and\nintruders. CPAs were denoted by a triangle symbol.\nCOMEX: Each encounter had a COMEX \xe2\x80\x93 a time where all participants needed to\nbe positioned at the IP or ready to commence the run.\nConfiguration: Flight Test Series Configuration. For Configuration 1, two types\nexisted: 1a (low-speed ownship) and 1b (high-speed ownship).\nCPA Tolerance: Tolerance was based on requirements from the researchers\xe2\x80\x99\nsimulations. The tolerance required the aircraft to be at the CPA within that number\nof seconds from each other. Typically the value was \xc2\xb15, 8, or 10 seconds.\nDeconfliction Alt: Altitude aircraft would fly in between test points.\nEncounter Instructions: These instructions were radio instructions as well as\nwhat to expect from the TC. The instructions also showed at least how much time\nthe aircraft needed to be on condition for that run (step 3). On condition was\ntypically 2.5, 3, or 3.5 min depending on the encounter length.\nFinal Alt: Expected altitude at the CPA.\nGroundspeed: Encounter horizontal velocity parameter for all participating\naircraft. For Flight Test Series 3, all speeds were constant (no acceleration). As\nmentioned, some encounters required airspeed \xe2\x80\x93 this was specified on those\nunique cards and highlighted.\nIkhana Lost Link: Each Ikhana CPA had a single waypoint on a Lost Link Mission\nthat the aircraft would go to in the event of this condition.\nIP: The IP was the point in space where the aircraft would need to be for COMEX.\nEach IP had a unique number (i.e. IP16) and if GPS coordinates for an IP repeated,\nthe same IP number would be used. The coordinates listed were in two forms for\nuse by the Ikhana operators and intruders. IPs were denoted by a square symbol.\nIP to CPA in NM: The distance between IP and CPA in nautical miles, for\nreference.\nMagnetic Course: Expected Magnetic Course (MC) between the IP to CPA.\nO/S Pilot Instructions: Instructions specific to that geometry/SUT that the pilot\nhad to execute.\nProfile View: Side view of the encounters, showing initial and final altitudes at IP\nand CPA respectively, abort procedures (dashed lines), and vertical separation.\nScenario Name: This name was a quick reference to the type of encounter being\nexecuted, using the Scenario Nomenclature. For more information, see Figure 8.\nScenario Number: This number was a unique to each encounter. In fact, it carried\nthrough from the Flight Test Matrix and was critical to building the cards based on\nlookup tables. For a full list of scenario numbers, see Table 11.\nSensor Selected: The Ikhana had several sensors that could be selected or\ndeselected based on researcher requirements for that encounter. \xe2\x80\x9cSelected\xe2\x80\x9d\n\n56\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\nmeant that track data was being fed into the SUT. ADS-B, Radar, or TCAS data\ncould be selected, as well as \xe2\x80\x9cTracker\xe2\x80\x9d, which would fuse all three.\nStart Alt: Aircraft starting altitude at the IP.\nSUT: Display for that particular encounter. The choices were AutoResolver 1 or 2,\nStratway+, or CPDS.\nTCAS Mode: The TCAS mode would be selected depending on if the Ikhana\nshould perform a maneuver based on TCAS alerts. \xe2\x80\x9cOFF\xe2\x80\x9d meant that no TCAS\nalerts would be received, \xe2\x80\x9cAdvisory\xe2\x80\x9d would show the alerts to the pilot and let them\ndecide whether to maneuver based on the guidance, and \xe2\x80\x9cAUTO\xe2\x80\x9d would enable\nthe flight control computer to automatically take control and maneuver based on\nTCAS alerts.\nTime Adjust: The time adjust was useful for airspeed encounters. See Section\n6.1.2.1.1 for more information.\nTime Hack: A time hack (based off of UTC as displayed in the SAF) was performed\nprior to starting encounters. The time hack allowed all participants to sync up their\nclocks prior to COMEX. Time hack was called by the TC, and although each card\nhad a section for this, it was only performed once for each flight day.\nTop View: Geo-referenced top view of the encounter (based off of FalconView)\nshowing what the ideal case would look like. CPA and IP are shown for the aircraft\nfor that particular card.\nVertical Velocity: Climb or descent rate of the aircraft in feet per minute.\nVID Notice: If an encounter was <500 ft in vertical separation, a notice was\ndisplayed on the top view to warn participants that a VID was required by 1 NM\nlateral separation or an abort would be called.\n\n6.1.5.2.2 Intruder\n\nIntruder cards showed the aircraft route of flight as either red (Intruder 1) or blue (Intruder\n2). However, the cards were almost identical to the ownship cards:\n\xef\x82\xb7\n\xef\x82\xb7\n\nNo Sensor Select: The intruder aircraft did not have the same sensor selection\ncapabilities for inputting to the SUT; thus, this section was not required.\nIntruder Pilot Instructions: These instructions varied from the ownship and gave\nSA to the pilot and how the ownship would be performing.\n\nNot depicted on example card:\n\xef\x82\xb7\n\nManeuver Point: The MP was the point the intruder aircraft was expected to fly to\nand then perform a standard rate turn to another set of coordinates. The MP was\ndenoted by a circle symbol.\n\n57\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 34. Pairwise Encounters Ownship Example Test Card.\n\n58\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 35. Pairwise Encounters Intruder Example Test Card.\n\n59\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n6.2 Configuration 2: Full Mission Encounters\nFull Mission flight encounters, also identified as Configuration 2, followed a pre-planned\nflight plan that represents a fictitious Fireline route mission flown in Oakland Center Class\nE airspace (ZOA) that has been previously used for Integrated Human in the Loop (IHITL)\nand Full Mission simulation exercises. FT3 Full Mission would gather real flight data to\nimprove simulation.\nThese missions involved a single ownship aircraft (UAS Surrogate T-34C) navigating a\nflight plan, two live intruder aircraft performing flight encounters that were generally\nscripted but had flexibility in execution to accommodate real-time changes during the test\nruns, and finally, multiple virtual intruders that were not displayed to the airborne aircrew\nbut the ownship aircraft maneuvered to avoid based off inputs executed by the subject\npilot located in the RGCS. Each live intruder encounter with UAS Surrogate ownship were\n1v1 encounters. Figure 36 shows the ownship Fireline route and the expected paths and\nintercepts of the two live intruders (Intruder 1 \xe2\x80\x93 red, Intruder 2 \xe2\x80\x93 blue). Figure 37 shows\nthe ownship, Intruder 1, and Intruder 2 routes overlaid, as well as expected live and virtual\nencounters.\n\nFigure 36. Fireline Ownship Route with Live Intruder Intercepts.\n\n60\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 37. Fireline Routing, Ownship and Live/Virtual Intruders.\n\nNASA 608 acted as a surrogate UAS for this configuration. The aircraft was directly\ncontrolled laterally by the RGCS pilot, and NASA 608\xe2\x80\x99s pilot would perform other\nmaneuvers such as airspeed and altitude changes, received by the research computer\nfrom VSCS, and relayed to an on-board tablet. To the RGCS pilot, the route appeared to\nbe in ZOA airspace on the VSCS display.\n6.2.1 Fireline Route Development\nThe following stakeholders helped develop the Configuration 2 event:\n\xef\x82\xb7\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\nARC \xe2\x80\x93 IT&E team members. These members represented the virtual ATC team\nand the ARC HSI interests. They were responsible for integrating the route\ndepicted above into ZOA airspace (Figure 38), and with the restrictions required\nfor flight in R-2508.\nARC \xe2\x80\x93 HSI team members were the primary research team. The encounter\nintercepts they developed would put the test pilot under heavy working conditions\nin order to evaluate their display.\nAFRC \xe2\x80\x93 IT&E team members were responsible for local R-2508 and R-2515\ncoordination as well as overall flight execution. AFRC was responsible for relaying\n\n61\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\nthe information to ARC HSI/IT&E to create a Temporary Flight Restriction (TFR)\nand slightly modify the Fireline route on waypoint 6.\nR-2508 CCB \xe2\x80\x93 The Complex Control Board (CCB) is the governing body for the R2508 complex and as such represented High Desert TRACON as well as USAF\nR-2515 interests.\nGRC \xe2\x80\x93 Communication \xe2\x80\x93 The Glenn team was responsible for the T-34 NASA 608\nduring Configuration 2.\n\nFigure 38. Full Mission Fireline Route and Encounters.\n\n6.2.2 Constraints and Limitations\nThe Configuration 2 ownship was controlled by the RGCS, whose subject pilot under test\nwas immersed in a virtual ATC, ZOA, and environment. Where aircrew in Configuration 1\nflights maneuvered according to the display, aircrew in Configuration 2 coordinated with\nATC as if they were actually operating an aircraft in Oakland Center\xe2\x80\x99s airspace.\nFurthermore, ownship maneuvers were less predictable in Configuration 2 and resulted\nin missed encounters as discussed in Section 8 of this report.\nTransiting from High Desert TRACON (Joshua) control to SPORT control needed prior\ncoordination and in some cases imposed minor delays, on the order of minutes, while the\ncontrollers conducted hand overs. Although entry and use of the Isabella MOA was\n62\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nessentially guaranteed for participating aircraft with a Sage or Pancho clearance, entry\ninto R-2515 was not guaranteed as NASA aircraft are assigned a lower priority than many\nUSAF programs.\n6.2.3 Flight Card Description\nDue to its unique nature, Full Mission flight cards were completely developed by\nArmstrong IT&E Operations for Flight Test Series 3. Because of the distinctive and\nrepeating route for each flight day, one set of cards was created that was used every day:\nOwnship, Intruder 1, and Intruder 2.\nThese cards were produced to be easy to use by the airborne users and other ground\nparticipants, not designed for the pilot using the SUT (RGCS). The cards were made to\nclearly show where live encounters were to occur, from what waypoints and holding\npatterns the intruder pilots should maneuver, and from where the ownship should expect\nfused reality virtual encounters.\nUsing the same color scheme but reversed altitudes from Configuration 1:\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nOwnship and middle altitude aircraft was green\nIntruder 1 and lower altitude aircraft was red\nIntruder 2 and higher altitude aircraft was blue\n\n6.2.3.1 Production\n\nAs in Configuration 1, Configuration 2 cards were built in Microsoft Excel\xc2\xae. However,\nsince all coordinates and geometries were provided by Ames IT&E Operations (and there\nwas only one Fireline route), the cards were created manually. The geo-referenced top\nview images were again created in FalconView, but manually.\n6.2.3.2 Breakdown\n\nThe following is a breakdown and description of unique elements on each of the\nConfiguration 2 flight cards. Figure 39, Figure 40, and Figure 41 depict the flight cards\nused for Configuration 2.\n6.2.3.2.1 Ownship\n\nOwnship card depicted the aircraft route of flight as green. Here are the main cards\nsections, listed alphabetically, and a brief explanation for each:\n\xef\x82\xb7\n\nAbort Procedure: For Configuration 2, the word \xe2\x80\x9cabort\xe2\x80\x9d did not have the same\nmeaning as in Configuration 1. Since the tracks for Configuration 2 were much\nlonger (~40min), an abort simply meant that the intruder aircraft would \xe2\x80\x9cincrease\nvertical separation\xe2\x80\x9d (to obtain a separation of at least 500 ft between aircraft) and\ncontinue the encounter. If visual was not acquired within the 1 NM range, the pilots\nwould call \xe2\x80\x9cblind\xe2\x80\x9d and the TC would instruct them to hold level, climb, or descend,\nas required.\n\n63\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\n\xef\x82\xb7\n\nAircraft Role: This section specified whether the aircraft was Ownship, Intruder 1,\nor Intruder 2.\nAircraft: Callsign of the aircraft for that test card.\nCOMEX: Although there are several sections to input COMEX, the Fireline route\nonly had one COMEX at the beginning of the mission. This section was therefore\nused to show at what time an intruder would push to CPA instead.\nConfiguration: Flight Test Series Configuration. Configuration 2 had one Full\nMission.\nDeconfliction Alt: Altitude aircraft would fly in between test points. A carry-over\nfrom Configuration 1, ultimately participants stayed at their encounter altitude in\nbetween runs for efficiency.\nDistance: The distance between IP WP and encounter WP in nautical miles, for\nreference.\nFly-to WP: The Fly-to WP is the point the aircraft is expected to fly-to if there is no\nencounter. For a couple of encounters (3, 4) this is the case since the Live\nEncounter WP is not on the flight plan for ownship.\nHolding Pattern: The ownship\xe2\x80\x99s holding pattern before COMEX and the start of\nthe Fireline route.\nLive Encounter #: Denotes which Live Encounter on the Fireline is being shown.\nThe route had 4 live encounters (1, 3 \xe2\x80\x93 intruder 1 and 2, 4 \xe2\x80\x93 intruder 2).\nLive Encounter WP: The WP at which the intruder is heading to, and the live\nencounter, will occur.\nLive Encounter: Live encounters occur with one ownship and one intruder (no\nmulti-ship for Configuration 2).\nMagnetic Course: Expected MC at waypoint.\nO/S Enc. Alt: Expected altitude of the ownship at the live encounter.\nO/S Enc. Mag. Course: Expected magnetic course of the ownship at the live\nencounter.\nOwnship Airspeed: Expected airspeed of the ownship at the live encounter.\nProfile View: Side view of the encounter showing expected altitudes and vertical\nseparation for ownship and intruder.\nTFR: A virtual TFR was added to the Fireline route to keep all aircraft west of\nspecific test areas (per a request from airspace coordination).\nTop View: Geo-referenced top view of the Fireline route showing what the ideal\ncase would look like. The top view shows both the Live and Virtual Encounters, as\nwell as the TFR and holding patterns.\nVID Notice: Since all live encounters were <500 ft for Configuration 2, a notice\nwas displayed on the top view to warn participants that a VID was required by 1\nNM lateral separation or an abort would be called.\n\n64\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\nVirtual Encounter: These encounters were only visible to the RGCS pilot. They\nwere displayed to the aircraft pilots to show where a maneuver would be expected\nby ownship from RGCS.\nWaypoints: List of waypoints for the aircraft to follow for the Fireline.\n\n6.2.3.2.2 Intruder 1\n\n\xef\x82\xb7\n\n\xef\x82\xb7\n\xef\x82\xb7\n\nWaypoint IP: The waypoint the intruder aircraft was expected to push out from.\nThis was denoted as a square symbol and labeled so that the intruder could have\nquick SA on where to push from.\nHolding Pattern 1: Intruder 1 had two separate holding patterns. This one is\nbefore encounter 1 and is located in the southwest.\nHolding Pattern 2: Holding pattern before encounter 3 on the Fireline (intruder 1\xe2\x80\x99s\nsecond encounter) located in the northeast.\n\n6.2.3.2.3 Intruder 2\n\n\xef\x82\xb7\n\xef\x82\xb7\n\nHolding Pattern 1: Holding pattern before encounter 2 on the Fireline (intruder 2\xe2\x80\x99s\nfirst encounter) located in the southeast.\nHolding Pattern 2: Holding pattern before encounter 4 on the Fireline (intruder 2\xe2\x80\x99s\nsecond encounter) located in the north.\n\n65\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 39. Full Mission Ownship Test Card.\n\n66\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 40. Full Mission Intruder 1 Test Card.\n\n67\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFigure 41. Full Mission Intruder 2 Test Card.\n\n68\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7 Flight Summary\nThe following is a summary of each flight day for Flight Test Series 3.\n\n7.1 Configuration 1\nConfiguration 1 flight points were conducted from June 17, 2015, to July 24, 2015. Figure\n42 shows the prioritization of flights and test points. Scenarios were prioritized in the\nfollowing ways:\n\xef\x82\xb7 Numbering scheme based on researcher input (Priority 1, 2, 3, 4).\n\xef\x82\xb7 Build-up approach (higher to lower vertical separation, advisory to AUTO, etc.).\n\xef\x82\xb7 Ease of flight (airspace transition from one encounter to next).\n\xef\x82\xb7 Repeating encounters grouped.\n\xef\x82\xb7 Sun angle consideration.\nTest points not completed are grayed out. Test points that are crossed out were removed\nfrom the flight schedule by the researcher (deemed unnecessary for mission success).\n\nFigure 42. Prioritization of and Flown Encounters.\n\nThe subjective analysis is according to Armstrong Ops and may differ from researchers\xe2\x80\x99\nopinions. Test points are shown in order flown. Flight duration is based on the ownship\xe2\x80\x99s\ntime in the air. COMEX is written in local time. Traffic Advisories (TAs) and RAs are noted.\nAltitudes are flight level MSL and at CPA or maneuver start. Sensors selected for that\n69\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nencounter are shown (ADS-B, Radar, TCAS, Tracker). The last three columns are timing\nfrom CPA (+ is to arrive after, - is to arrive before) or maneuver suggested by display (if\ntype is \xe2\x80\x9cFollow\xe2\x80\x9d). Boxes with a \xe2\x80\x9c-\xe2\x80\x9d denote missing data. [R] is repeat.\n7.1.1 Flight 1: June 17, 2015\nTable 12. Config 1 Flight 1 Data.\nFlight\n\n1\n\nSUT\n\nAutoResolver\n\nDuration\n\n5 hours\n\nIntruder(s)\n\nN3GC\n#\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nTA/RA\n\nADS-B\n\nRDR\n\nTCAS\n\nTRC\n\nO/S\n\nInt1\n\nInt2\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n-10\n\n-10\n\nN/A\n\n125\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n0\n\n-2\n\nN/A\n\n123\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+10\n\n+8\n\nN/A\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+2\n\n+10\n\nN/A\n\n0843\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n0\n\n+6\n\nN/A\n\nFly-through, AR1\n\n0855\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+10\n\n+10\n\nN/A\n\n7 40-L12M\n\nFly-through, AR1\n\n0910\n\n140\n\n150\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+15\n\n+23\n\nN/A\n\n8 62-L12N\n\nFly-through, AR1\n\n0923\n\n140\n\n150\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+0\n\n+1\n\nN/A\n\n9 12-L14A\n\nFly-through, AR1\n\n0938\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+8\n\n-7\n\nN/A\n\n10 34-L14C\n\nFly-through, AR1\n\n0950\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+6\n\n-20\n\nN/A\n\n11 54-L14D\n\nFly-through, AR1\n\n1000\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n-1\n\n+5\n\nN/A\n\n12 30-L52C\n\nFly-through, AR1\n\n1013\n\n120\n\n125\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n-\n\n-\n\nN/A\n\n13 30-L52C [R]\n\nFly-through, AR1\n\n1023\n\n120\n\n125\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n-2\n\n-5\n\nN/A\n\n14 50-L52D\n\nFly-through, AR1\n\n1033\n\n120\n\n125\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n-20\n\n+9\n\nN/A\n\n15 50-L52D [R]\n\nFly-through, AR1\n\n1043\n\n120\n\n125\n\nN/A\n\nN/A\n\nX\n\nX\n\n-10\n\n+6\n\nN/A\n\n1 18-L12A\n\n0758\n\n120\n\n130\n\n2 19-L52A\n\nFly-through, AR1\n\n0810\n\n120\n\n3 20-L32A\n\nFly-through, AR1\n\n0820\n\n120\n\n4 29-L12C\n\nFly-through, AR1\n\n0834\n\n5 49-L12D\n\nFly-through, AR1\n\n6 60-L12E\nData\n\nFly-through, AR1\n\nRA\n\nTA\n\nTA\n\nNotes: Altimeter calibration performed (N3GC +50 ft). Level acceleration performed.\nArtificial offset was applied for 1,000 ft runs. No 300 ft non-head-on encounters allowed\ndue to no ADS-B in SAF. Used wind matrix for climb/descent encounters.\nEncounters: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 O/S RA, good, 4 \xe2\x80\x93 int1 start off angle ~30deg MC,\ngood, 5 \xe2\x80\x93 good, 6 \xe2\x80\x93 good, 7 \xe2\x80\x93 run 3 min instead of 3.25, int1 turn late, bad, 8 \xe2\x80\x93 good, 9 \xe2\x80\x93\nwind adjust -10s, TA to climb rather than descend (O/S), good, 10 \xe2\x80\x93 wind adjust +7s, int1\nlate, bad, 11 \xe2\x80\x93 wind adjust -12s, good, 12 \xe2\x80\x93 ABORT lost VID, 13 \xe2\x80\x93 good, 14 \xe2\x80\x93 O/S late,\nbad, 15 - good\nAirspace: Requested Buckhorn, SPORT raised flight test to FL140 then FL150\n(temporary) due to MQ-9 lasing in West Range. Affected runs 7, 8.\nBaro/Vis: 29.92, clear\nWind: 1 \xe2\x80\x93 O/S 251/20, int1 220/10, 8 \xe2\x80\x93 O/S 205/18, int1 195/20, 10 \xe2\x80\x93 int1 256/10, 11 \xe2\x80\x93\nint1 210/17\nBottom Line: Overall, test points were conducted well. The wind matrix had some errors\nand went through its first iteration. Although ADS-B was not functional in the SAF, the test\nwas still able to continue for 300 ft \xe2\x80\x9chead-on\xe2\x80\x9d (0 degree angle into) encounters.\nAdditionally, Fusion (Tracker) was seeing problems with tracks. Nonetheless the\nresearcher was pleased with data since there was no \xe2\x80\x9creal world\xe2\x80\x9d data previous to this. A\ndecision was made to audibly announce TA/RA alerts for proceeding flights (all\nplatforms).\n\n70\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.1.2 Flight 2: June 18, 2015\nTable 13. Config. 1 Flight 2 Data.\nFlight\n\n2\n\nSUT\n\nAutoResolver\n\nDuration\n\n4.9 hours\nN3GC\n\nIntruder(s)\n\nInt2 Alt.\n\nTA/RA\n\nADS-B\n\nO/S\n\nInt1\n\nInt2\n\n1 31-L32C\n\nFly-through, AR1\n\n0643\n\n120\n\nTA/RA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\n-2\n\n-2\n\nN/A\n\n2 51-L32D\n\nFly-through, AR1\n\n0653\n\n120\n\nTA/RA\n\n123\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\n+8\n\n+6\n\nN/A\n\n3 10-L13A\n\nFly-through, AR1\n\n0704\n\n165\n\n155\n\nN/A\n\nN/A\n\n0\n\n-10\n\nN/A\n\n4 32-L13C\n\nFly-through, AR1\n\n0715\n\n165\n\n155\n\nN/A\n\nN/A\n\n+8\n\n-30\n\nN/A\n\n5 52-L13D\n\nFly-through, AR1\n\n0725\n\n165\n\n155\n\nN/A\n\nN/A\n\n+4\n\n+10\n\nN/A\n\n6 16-L16A\n\nFly-through, AR1\n\n0735\n\n136\n\n120\n\nN/A\n\nN/A\n\nX\n\n0\n\n+10\n\nN/A\n\n7 38-L16C\n\nFly-through, AR1\n\n0745\n\n130\n\n120\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n-2\n\n+1\n\nN/A\n\n8 58-L16D\n\nFly-through, AR1\n\n0755\n\n130\n\n120\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+7\n\n+2\n\nN/A\n\n9 14-L15A\n\nFly-through, AR1\n\n0805\n\n150\n\n160\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+18\n\n-2\n\nN/A\n\n10 36-L15C\n\nFly-through, AR1\n\n0817\n\n150\n\n160\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+5\n\n+4\n\nN/A\n\n11 56-L15D\n\nFly-through, AR1\n\n0826\n\n150\n\n160\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+5\n\n+7\n\nN/A\n\n12 10-L13A\n\nFly-through, AR2\n\n0835\n\n165\n\n155\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+9\n\n-7\n\nN/A\n\n13 32-L13C\n\nFly-through, AR2\n\n0845\n\n165\n\n155\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+5\n\n-15\n\nN/A\n\n14 52-L13D\n\nFly-through, AR2\n\n0855\n\n165\n\n155\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n0\n\n+28\n\nN/A\n\n15 16-L16A\n\nFly-through, AR2\n\n0905\n\n130\n\n120\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+6\n\n-1\n\nN/A\n\n16 38-L16C\n\nFly-through, AR2\n\n0916\n\n130\n\n120\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+3\n\n+3\n\nN/A\n\n17 58-L16D\n\nFly-through, AR2\n\n0925\n\n130\n\n120\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n-15\n\n+8\n\nN/A\n\n18 12-L14A\n\nFly-through, AR2\n\n0935\n\n120\n\n135\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+1\n\n-20\n\nN/A\n\n19 34-L14C\n\nFly-through, AR2\n\n0945\n\n120\n\nTA\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+4\n\n-10\n\nN/A\n\n20 54-L14D\n\nFly-through, AR2\n\n0955\n\n120\n\nTA\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+5\n\n+17\n\nN/A\n\n21 60-L12E\n\nFly-through, AR2\n\n1005\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n-\n\n-\n\nN/A\n\n22 60-L12E [R]\n\nFly-through, AR2\n\n1011\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+1\n\n+4\n\nN/A\n\n23 40-L12M\n\nFly-through, AR2\n\n1020\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-3\n\n-4\n\nN/A\n\n#\n\nData\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nTA\n\nRDR\n\nTCAS\n\nTRC\n\nX\n\nX\nX\nX\n\nX\n\nNotes: Altimeter calibration performed (N3GC +60 ft). Artificial offset was applied for\n1,000 ft runs. Used wind matrix climb/descent encounters. O/S collected additional ADSB data after run 23.\nEncounters: 1 \xe2\x80\x93 O/S RA, good, 2 \xe2\x80\x93 O/S RA descend, int1 RA climb, good, 3 \xe2\x80\x93 wind adjust\n+13s, int1 off course ~130 KIAS, good, 4 \xe2\x80\x93 no wind adjust comm., relax VSI, bad, 5 \xe2\x80\x93\nraised int1 start alt FL130, wind adjust -20s, relax VSI, good, 6 \xe2\x80\x93 wind adjust +15s, good,\n7 \xe2\x80\x93 wind adjust +8s, wind died to 16kts by end, set 1,100 VSI for Ikhana = 1,000 VSI,\ngood, 8 \xe2\x80\x93 wind adjust -19s, good, 9 \xe2\x80\x93 wind adjust +20s, O/S sped up KIAS, good, 10 \xe2\x80\x93\nwind adjust +5s, O/S sped up KIAS, good, 11 \xe2\x80\x93 wind adjust -10s, sped up KIAS, good,\n12 \xe2\x80\x93 no wind adjust, good, 13 \xe2\x80\x93 wind adjust +12s, good, 14 \xe2\x80\x93 wind adjust -26s, climb too\nearly (should have reset), bad, 15 \xe2\x80\x93 wind adjust +20s, good, 16 \xe2\x80\x93 wind adjust +8s, good,\n17 \xe2\x80\x93 decision: no wind adjust, O/S early/int1 late (whole run), bad, 18 \xe2\x80\x93 wind adjust +12s,\nlate start descent w/overshoot, bad, 19 \xe2\x80\x93 wind adjust +5s, good, 20 \xe2\x80\x93 wind adjust +18s\n(opposite of wind matrix), good, 21 \xe2\x80\x93 reset, int1 too slow to make CPA, 22 \xe2\x80\x93 good, 23 good\nAirspace: No Buckhorn below FL130 and above FL200 (temporary). Did not affect runs.\nBaro/Vis: 29.92, clear\n\n71\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nWind: 1 \xe2\x80\x93 O/S 274/23, int1 274/27, 3 \xe2\x80\x93 O/S 282/15, int1 296/17, 4 \xe2\x80\x93 int1 289/20, 5 \xe2\x80\x93 int1\n265/18, 6 \xe2\x80\x93 O/S 282/15, 7 \xe2\x80\x93 O/S 285/25, 8 \xe2\x80\x93 O/S 270/15, 9 \xe2\x80\x93 O/S 285/20, 10 \xe2\x80\x93 O/S\n290/19, 11 \xe2\x80\x93 O/S 300/10, 12 \xe2\x80\x93 int1 252/21, 13 \xe2\x80\x93 O/S 270/20, int1 252/16, 14 \xe2\x80\x93 int1 281/23,\n15 \xe2\x80\x93 O/S 280/22, 16 \xe2\x80\x93 O/S 285/22, 17 \xe2\x80\x93 O/S 260/11, 18 \xe2\x80\x93 int1 264/13, 19 \xe2\x80\x93 int1 288/21,\n20 \xe2\x80\x93 int1 256/16\nBottom Line: Overall, test points were conducted well. Additional errors were present in\nthe wind matrix that were corrected after this flight. Team was getting into a flow that\nhelped with obtaining more test runs this day.\n7.1.3 Flight 3: June 22, 2015\nTable 14. Config. 1 Flight 3 Data.\nFlight\n\n3\n\nSUT\n\nAutoResolver\n\nDuration\n\n4.5 hours\nN3GC\n\nIntruder(s)\n#\n\nInt2 Alt.\n\nTA/RA\n\nADS-B\n\nO/S\n\nInt1\n\nInt2\n\n1 61-L12E\n\nFollow AR1\n\n0650\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nR007\n\n0\n\nN/A\n\n2 41-L12M\n\nFollow AR1\n\n0659\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nR007\n\n+5\n\nN/A\n\n3 63-L12N\n\nFollow AR1\n\n0710\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nR007\n\n+10\n\nN/A\n\n4 13-L14A\n\nFollow AR1\n\n0722\n\n120\n\n135\n\nN/A\n\nN/A\n\nX\n\nR007\n\n+4\n\nN/A\n\n5 35-L14C\n\nFollow AR1\n\n0731\n\n120\n\n140\n\nN/A\n\nN/A\n\nX\n\nR277\n\n-\n\nN/A\n\n6 55-L14D\n\nFollow AR1\n\n0742\n\n120\n\n150\n\nN/A\n\nN/A\n\nX\n\nR287\n\n+3\n\nN/A\n\n7 15-L15A\n\nFollow AR1\n\n0753\n\n135\n\n160\n\nN/A\n\nN/A\n\nX\n\nL227\n\n0\n\nN/A\n\n8 37-L15C\n\nFollow AR1\n\n0803\n\n140\n\n160\n\nN/A\n\nN/A\n\nX\n\nL217\n\n-2\n\nN/A\n\n9 57-L15D\n\nFollow AR1\n\n0812\n\n150\n\n160\n\nN/A\n\nN/A\n\nX\n\nNone\n\n+2\n\nN/A\n\n10 11-L13A\n\nFollow AR1\n\n0822\n\n165\n\n145\n\nTA\n\nN/A\n\nN/A\n\nX\n\nR277\n\n+16\n\nN/A\n\n11 33-L13C\n\nFollow AR1\n\n0833\n\n165\n\n135\n\nTA\n\nN/A\n\nN/A\n\nX\n\nR277\n\n-7\n\nN/A\n\n12 53-L13D\n\nFollow AR1\n\n0844\n\n165\n\n150\n\nN/A\n\nN/A\n\nX\n\nR277\n\n+14\n\nN/A\n\n13 17-L16A\n\nFollow AR1\n\n0854\n\n145\n\n120\n\nN/A\n\nN/A\n\nX\n\nR287\n\n0\n\nN/A\n\n14 39-L16C\n\nFollow AR1\n\n0905\n\n148\n\n120\n\nN/A\n\nN/A\n\nX\n\nR287\n\n-5\n\nN/A\n\n15 59-L16D\n\nFollow AR1\n\n0917\n\n148\n\n120\n\nN/A\n\nN/A\n\nX\n\nNone\n\n0\n\nN/A\n\n16 61-L12E\n\nFollow AR2\n\n0928\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nL317\n\n-90\n\nN/A\n\n17 41-L12M\n\nFollow AR2\n\n0940\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nR277\n\n+15\n\nN/A\n\n18 63-L12N\n\nFollow AR2\n\n0950\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nR277\n\n+12\n\nN/A\n\n19 13-L14A\n\nFollow AR2\n\n1002\n\n120\n\n145\n\nN/A\n\nN/A\n\nX\n\nR277\n\n-20\n\nN/A\n\n20 35-L14C\n\nData\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nRDR\n\nTCAS\n\nTRC\n\nFollow AR2\n\n1013\n\n120\n\n143\n\nN/A\n\nN/A\n\nX\n\nR267\n\n-15\n\nN/A\n\nNotes: Altimeter calibration performed (N3GC +60 ft). Artificial offset was applied for\n1,000 ft runs. Used wind matrix climb/descent encounters. All runs were ADS-B only (due\nto poor Fusion performance).\nEncounters: 1 \xe2\x80\x93 int1 ground speed 10-15 low, good, 2 \xe2\x80\x93 seems excessive turn, bad, 3 \xe2\x80\x93\ngood, 4 \xe2\x80\x93 wind adjust +5s, good, 5 \xe2\x80\x93 wind adjust +5s, good, 6 \xe2\x80\x93 wind adjust -5s, good, 7\n\xe2\x80\x93 wind adjust -25s, good, 8 \xe2\x80\x93 29.97 (ground), wind adjust -20s, good, 9 \xe2\x80\x93 wind adjust +5s,\nO/S slow and late, bad, 10 \xe2\x80\x93 wind adjust +10s, good, 11 \xe2\x80\x93 wind adjust +10s, good, 12 \xe2\x80\x93\nwind adjust -20s, int1 request reset, good, 13 \xe2\x80\x93 wind adjust -5s, maneuver and level off,\ngood, 14 \xe2\x80\x93 wind adjust -5s, maneuver and level off, good, 15 \xe2\x80\x93 wind adjust +5s, maneuver\nand level off, O/S request reset (too fast), bad, 16 \xe2\x80\x93 CBDR, merged, bad, 17 \xe2\x80\x93 O/S request\nreset, good, 18 \xe2\x80\x93 CBDR, merged, bad, 19 \xe2\x80\x93 wind adjust 0s, int1 request reset, good, 20\n\xe2\x80\x93 wind adjust +10s, int1 request reset, good\n72\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nAirspace: R-2515 FL200 and below, Buckhorn FL100 and above, R-2515 SPORT stay\nFL120 and above. Did not affect runs.\nBaro/Vis: 29.95, clear\nWind: 3 \xe2\x80\x93 int1 221/19, 4 \xe2\x80\x93 int1 209/7, 5 \xe2\x80\x93 int1 207/5, 6 \xe2\x80\x93 int1 294/8, 7 \xe2\x80\x93 O/S 257/18, int1\n204/13, 8 \xe2\x80\x93 O/S 237/15, int1 160/3, 9 \xe2\x80\x93 O/S 217/8, int1 217/10, 10 \xe2\x80\x93 O/S 203/6, int1\n194/22, 11 \xe2\x80\x93 O/S 224/10, int1 232/13, 12 \xe2\x80\x93 O/S 167/6, int1 225/20, 13 \xe2\x80\x93 O/S 220/8, int1\n223/21, 14 \xe2\x80\x93 O/S 217/8, int1 256/12, 15 \xe2\x80\x93 O/S 195/7, int1 232/18, 19 \xe2\x80\x93 O/S 237/18, int1\n180/10, 20 \xe2\x80\x93 O/S 244/15, int1 232/14\nBottom Line: Overall, test points were conducted well, but maneuvers were only of\n\xe2\x80\x9cacceptable\xe2\x80\x9d quality. Display of VSCS (seen in SAF) was showing identical maneuvers\nfor different encounters. This was thought odd as each encounter was of a new geometry.\nWind matrix worked well for this flight day. This flight was the first case where it seemed\nCPA timing was not as important as intent for maneuvering encounters \xe2\x80\x93 algorithm only\nneeded to be alerted, followed by O/S maneuver.\n7.1.4 Flight 4: June 24, 2015\nTable 15. Config. 1 Flight 4 Data.\nFlight\n\n4\n\nSUT\n\nCPDS\n\nDuration\n\n4.7 hours\nN3GC\n\nIntruder(s)\n#\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nTA/RA\n\nADS-B\n\nRDR\n\nTCAS\n\nO/S\n\nInt1\n\nInt2\n\n137\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-2\n\n+5\n\nN/A\n\nTA/RA\n\n140\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-5\n\n-6\n\nN/A\n\n145\n\nTA/RA\n\n140\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+25\n\n0\n\nN/A\n\n0720\n\n120\n\nTA\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-12\n\n-8\n\nN/A\n\nTCAS Advisory\n\n0730\n\n120\n\nTA/RA\n\n125\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n0\n\n+5\n\nN/A\n\n6 146-L54D\n\nTCAS AUTO\n\n0741\n\n120\n\nTA/RA\n\n125\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n0\n\n0\n\nN/A\n\n7 140-L55A\n\nTCAS Advisory\n\n0753\n\n135\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+5\n\n+3\n\nN/A\n\n8 140-L55A [R]\n\nTCAS Advisory\n\n0804\n\n140\n\nTA/RA\n\n145\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+13\n\n+5\n\nN/A\n\n9 140-L55A\n\nTCAS AUTO\n\n0813\n\n140\n\nTA/RA\n\n145\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-2\n\n+3\n\nN/A\n\n10 151-L56F\n\nTCAS Advisory\n\n0823\n\n125\n\nTA/RA\n\n120\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-7\n\n-2\n\nN/A\n\n11 151-L56F\n\nTCAS AUTO\n\n0833\n\n125\n\nTA/RA\n\n120\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-\n\n0\n\nN/A\n\n12 115-L32G\n\nRadar CBDR (110)\n\n0841\n\n120\n\nTA/RA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\n-40\n\n-30\n\nN/A\n\n13 121-L32G\n\nRadar CBDR (90)\n\n0855\n\n120\n\nTA/RA\n\n123\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\n-\n\n-30\n\nN/A\n\n14 115-L32G [R]\n\nRadar CBDR (110)\n\n0909\n\n120\n\nTA/RA\n\n124\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\n-30\n\n-60\n\nN/A\n\n15 117-L53G\n\nRadar CBDR (110)\n\n0923\n\n165\n\nTA/RA\n\n160\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\n-30\n\n-30\n\nN/A\n\n16 124-L55G\n\nRadar CBDR (90)\n\n0936\n\n170\n\nTA/RA\n\n175\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\n-30\n\n-40\n\nN/A\n\n17 128-L32A\n\nTCAS Advisory\n\n0955\n\n120\n\nTA/RA\n\n123\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+7\n\n-1\n\nN/A\n\n18 128-L32A\n\nTCAS AUTO\n\n1005\n\n120\n\nTA/RA\n\n123\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+6\n\n+5\n\nN/A\n\n19 129-L32C\n\nTCAS Advisory\n\n1015\n\n120\n\nTA/RA\n\n123\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-7\n\n-4\n\nN/A\n\n20 129-L32C\n\nTCAS AUTO\n\n1025\n\n120\n\nTA/RA\n\n123\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-3\n\n-7\n\nN/A\n\n1 137-L53C\n\n0640\n\n145\n\n2 137-L53C [R]\n\nTCAS Advisory\n\n0650\n\n145\n\n3 137-L53C\n\nTCAS AUTO\n\n0702\n\n4 146-L54D\n\nTCAS Advisory\n\n5 146-L54D [R]\n\nData\n\nTCAS Advisory\n\n145\n\nTRC\n\nNotes: Altimeter calibration performed (N3GC +80 ft).\nEncounters: 1 \xe2\x80\x93 no RA, bad, 2 \xe2\x80\x93 raised int1 start alt FL115, O/S RA climb, good, 3 \xe2\x80\x93\nraised int1 start alt FL115, O/S RA climb, good, 4 \xe2\x80\x93 multiple rolex, no RA, bad, 5 \xe2\x80\x93 lowered\nint1 start alt FL150, O/S RA descend, good, 6 \xe2\x80\x93 lowered int1 start alt FL150, O/S RA\ndescend, good, 7 \xe2\x80\x93 no RA, bad, 8 \xe2\x80\x93 raised O/S start alt FL115, O/S RA descend, good,\n9 \xe2\x80\x93 raised O/S start alt FL115, O/S RA do not climb, good, 10 \xe2\x80\x93 O/S RA do not descend,\n73\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\ngood, 11 \xe2\x80\x93 O/S RA do not descend, good, 12 \xe2\x80\x93 O/S RA do not climb, int1 fast, bad, 13 \xe2\x80\x93\nO/S RA descend, int1 slow, bad, 14 \xe2\x80\x93 angle ~106\xc2\xb0, good, 15 \xe2\x80\x93 raised int1 start alt FL120,\nint1 descend 1100 fpm, good, 16 \xe2\x80\x93 raised O/S alt FL120, O/S RA do not climb, good, 17\n\xe2\x80\x93 O/S RA do not climb, good, 18 \xe2\x80\x93 O/S RA descend, good, 19 \xe2\x80\x93 O/S RA descend, did not\ndescend, bad, 20 \xe2\x80\x93 O/S RA descend, good\nAirspace: Buckhorn active FL200 and below, later cleared FL120-200. Did not affect\nruns.\nBaro/Vis: 29.91, clear\nWind: 1 \xe2\x80\x93 O/S 167/17, int1 160/16, 2 \xe2\x80\x93 O/S 152/17, int1 141/20, 3 \xe2\x80\x93 O/S 168/9, int1\n156/19, 4 \xe2\x80\x93 O/S 147/13, int1 195/8, 5 \xe2\x80\x93 O/S 134/13, int1 231/6, 6 \xe2\x80\x93 O/S 134/13, int1 278/7,\n7 \xe2\x80\x93 O/S 184/12, int1 188/12, 8 \xe2\x80\x93 O/S 185/10, int1 192/10, 9 \xe2\x80\x93 O/S 190/10, int1 calm, 10\n\xe2\x80\x93 O/S 200/10, int1 240/19, 11 \xe2\x80\x93 O/S 280/6, int1 calm, 13 \xe2\x80\x93 O/S 188/12, int1 188/17, 14 \xe2\x80\x93\nO/S 192/12, int1 178/18, 15 \xe2\x80\x93 O/S 222/13, 219/15, 16 \xe2\x80\x93 O/S 169/16, int1 188/11, 17 \xe2\x80\x93\nO/S 205/16, int1 155/13, 18 \xe2\x80\x93 O/S 208/17, int1 calm, 19 \xe2\x80\x93 O/S 205/15, int1 161/11, 20 \xe2\x80\x93\nO/S 209/15, int1 176/21\nBottom Line: Some starting altitudes needed to be raised or lowered (500 ft) real-time in\norder for aircraft to achieve desired performance and trigger RAs. Once this occurred, the\nTCAS system was successfully tested and Ikhana performed maneuvers that were\nexpected for the particular encounter, and in the milestone AUTO mode. For radar CBDR\nencounters, N3GC attempted to use a bearing tool on board their aircraft \xe2\x80\x93 although the\ntool itself was effective, the encounter angle itself was not completely understood by the\ncrew. Thus, several of the radar encounters\xe2\x80\x99 relative angle was incorrect. For the\nencounters where the angle was correct, the radar data was deemed good by the\nresearcher. Altitude redlines were made to proceeding flights cards to meet aircraft\nperformance based on this flight\xe2\x80\x99s outcome. CBDR cards were modified for proceeding\nflights to better highlight how to run this type of encounter.\n\n74\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.1.5 Flight 5: June 26, 2015\nTable 16. Config. 1 Flight 5 Data.\nFlight\n\n5\n\nSUT\n\nCPDS\n\nDuration\n\n4.6 hours\n\nIntruder(s)\n\nN3GC, NASA865\n#\n\nInt2 Alt.\n\nTA/RA\n\n1 108-L12A\n\nScenario\n\nRadar low alt.\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n0637\n\n052\n\n062\n\nN/A\n\nN/A\n\n2 107-L12A\n\nRadar low alt.\n\n0647\n\n042\n\n052\n\nN/A\n\nN/A\n\n3 112-L11A\n\nRadar low alt.\n\n0657\n\n062\n\n052\n\nN/A\n\n4 111-L11A\n\nRadar low alt.\n\n0708\n\n052\n\n042\n\nN/A\n\n5 169-M79X\n\nFly-through, CPDS\n\n0822\n\n130\n\n6 170-M79X\n\nFly-through, CPDS\n\n0834\n\n130\n\n7 171-M79X\n\nFly-through, CPDS\n\n0846\n\n130\n\nTA\n\n134\n\n8 160-M67Q\n\nTCAS Advisory\n\n0858\n\n133\n\nTA/RA\n\n9 161-M68Q\n\nTCAS Advisory\n\n0909\n\n137\n\nTA/RA\n\n10 165-L52M\n\nFly-through, CPDS\n\n0924\n\n120\n\n11 166-L52M\n\nFly-through, CPDS\n\n0937\n\n120\n\n12 167-L52M\n\nFly-through, CPDS\n\n0947\n\n13 168-L52M\n\nFollow CPDS\n\n14 164-L42M\n\n134\n\nTA/RA\n\nRDR\n\nO/S\n\nInt1\n\nInt2\n\nX\n\nTCAS\n\nTRC\n\n-2\n\n+2\n\nN/A\n\nX\n\nX\n\n-8\n\n+1\n\nN/A\n\nN/A\n\nX\n\nX\n\n0\n\n+2\n\nN/A\n\nN/A\n\nX\n\nX\n\n+9\n\n0\n\nN/A\n+5\n\n125\n\nX\n\nX\n\nRT\n\n+10\n\n125\n\nX\n\nX\n\n-\n\n-\n\n-\n\nTA\n\n125\n\nX\n\nX\n\n+2\n\n0\n\n-4\n\n14\n\nTA/RA\n\n13\n\nTA\n\nX\n\nX\n\nX\n\n-\n\n-\n\n-\n\n14\n\nTA/RA\n\n13\n\nTA\n\nX\n\nX\n\nX\n\n0\n\n-\n\n+8\n\nTA\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\n+10\n\n+15\n\nN/A\n\nTA\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\n+10\n\n0\n\nN/A\n\n120\n\nTA\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\n-9\n\n0\n\nN/A\n\n0957\n\n120\n\nTA/RA\n\n125\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nRT\n\n-3\n\nN/A\n\nFollow CPDS\n\n1007\n\n120\n\nTA/RA\n\n125\n\nRA\n\nN/A\n\nN/A\n\nX\n\nX\n\nLT\n\n0\n\nN/A\n\n15 132-L31A\n\nTCAS Advisory\n\n1018\n\n123\n\n120\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-\n\n-\n\nN/A\n\n16 132-L31A\n\nData\n\nTA/RA\n\nADS-B\n\nTCAS AUTO\n\n1028\n\n123\n\n120\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n0\n\n+5\n\nN/A\n\n134\n\nNotes: Altimeter calibration performed (N3GC +60 ft, NASA865 +190 ft). Additional 200\nft encounters were planned but not performed (TCAS sequential) \xe2\x80\x93 alerting achieved with\n300 ft separation (runs 8, 9).\nEncounters: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93 O/S was not supposed to\nmaneuver (+not called out), int1 RA climb, good, 6 \xe2\x80\x93 int1 heading wrong on maneuver,\ngood, 7 \xe2\x80\x93 good, 8 \xe2\x80\x93 O/S RA climb, about 20 seconds then O/S RA descend, NO VID int2\non O/S, good, 9 \xe2\x80\x93 O/S RA descend, about 8 seconds then O/S RA climb, good, 10 \xe2\x80\x93\ngood, 11 \xe2\x80\x93 good, 12 \xe2\x80\x93 good, 13 \xe2\x80\x93 RA int1 monitor vertical speed, good, 14 \xe2\x80\x93 int1 RA\nadjust vertical speed, good, 15 \xe2\x80\x93 O/S RA climb, int1 RA adjust vertical speed, good, 16 \xe2\x80\x93\nO/S RA climb, int1 RA descend, good\nAirspace: Received Buckhorn early FL100-200.\nBaro/Vis: 29.99, hazy (due to Lake wildfire)\nWind: 12 \xe2\x80\x93 O/S 160/13, int1 158/19, 14 \xe2\x80\x93 O/S 169/14, int1 150/11\nBottom Line: Considering the challenging geometries and conditions (weather), this flight\ncollection day was excellent. The first multi-ship live UAS encounter in flight test history\nwas performed (runs 5, 6, 7). First low-altitude radar runs were performed to test DRR on\nIkhana (runs 1, 2, 3, 4). The TCAS multi-ship sequential encounters (runs 8, 9) ran\nsmoothly and safely, all triggering expected alerting in advisory mode. Although a mission\nrule was violated (run 8, no VID), at no point did the pilots or any other team member feel\nunsafe or that the flight could not continue. Additional, directive guidance was given to\npilots concerning VID after run 8 and in proceeding flights.\n\n75\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.1.6 Flight 6: July 7, 2015\nTable 17. Config. 1 Flight 6 Data.\nFlight\n\n6\n\nSUT\n\nStratw ay+\n\nDuration\n\n4.8 hours\nN3GC\n\nIntruder(s)\n#\n\nInt2 Alt.\n\nTA/RA\n\n1 25-L53C\n\nFollow Stratw ay+\n\n0645\n\n145\n\nTA\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\n2 46-L53D\n\nFollow Stratw ay+\n\n0655\n\n145\n\nTA/RA\n\n135\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\n3 68-L53F\n\nFollow Stratw ay+\n\n0707\n\n145\n\nN/A\n\nN/A\n\n4 26-L54C\n\nFollow Stratw ay+\n\n0717\n\n120\n\nTA\n\n145\n\nTA\n\nN/A\n\nN/A\n\n5 47-L54D\n\nFollow Stratw ay+\n\n0727\n\n120\n\nTA\n\n135\n\nTA\n\nN/A\n\nN/A\n\n6 69-L54F\n\nFollow Stratw ay+\n\n0736\n\n120\n\nN/A\n\nN/A\n\n7 5-L56A\n\nFollow Stratw ay+\n\n0746\n\n142\n\nTA\n\n125\n\nTA\n\nN/A\n\nN/A\n\n8 6-L56B\n\nFollow Stratw ay+\n\n0756\n\n141\n\nTA/RA\n\n125\n\nTA/RA\n\nN/A\n\nN/A\n\n9 23-L56C\n\nFollow Stratw ay+\n\n0808\n\n-\n\nTA/RA\n\n125\n\nTA/RA\n\nN/A\n\nN/A\n\n10 44-L56D\n\nFollow Stratw ay+\n\n0817\n\n150\n\nTA\n\n125\n\nTA\n\nN/A\n\nN/A\n\n11 66-L56F\n\nFollow Stratw ay+\n\n0825\n\n130\n\n125\n\nN/A\n\nN/A\n\n12 66-L56F [R]\n\nFollow Stratw ay+\n\n0842\n\n140\n\n125\n\nN/A\n\n13 1-L42A\n\nFollow Stratw ay+\n\n0852\n\n120\n\nTA/RA\n\n124\n\nTA/RA\n\n14 2-L42B\n\nFollow Stratw ay+\n\n0902\n\n120\n\nTA/RA\n\n124\n\n15 21-L42C\n\nFollow Stratw ay+\n\n0912\n\n120\n\nTA/RA\n\n16 42-L42D\n\nFollow Stratw ay+\n\n0922\n\n120\n\nTA\n\n17 64-L42F\n\nFollow Stratw ay+\n\n0940\n\n120\n\n18 20-L32A\n\nFollow Stratw ay+\n\n0950\n\n120\n\nTA\n\n124\n\n19 31-L32C\n\nFollow Stratw ay+\n\n1005\n\n130\n\nTA/RA\n\n20 51-L32D\n\nFollow Stratw ay+\n\n1015\n\n130\n\nTA\n\n21 68-L53F\n\nFollow Stratw ay+\n\n1027\n\n145\n\n22 69-L54F\n\nData\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nO/S\n\nInt1\n\nInt2\n\nRT\n\n0\n\nN/A\n\nX\n\nRT\n\n0\n\nN/A\n\nX\n\nLT\n\n0\n\nN/A\n\nX\n\nX\n\nRT\n\n0\n\nN/A\n\nX\n\nX\n\nRT\n\n+3\n\nN/A\n\nX\n\nLT\n\n+7\n\nN/A\n\nX\n\nX\n\nRT\n\n-2\n\nN/A\n\nX\n\nX\n\nLT\n\n0\n\nN/A\n\nX\n\nX\n\nX\n\nNone\n\n-10\n\nN/A\n\nX\n\nX\n\nX\n\nRT\n\n+3\n\nN/A\n\nX\n\nNone\n\n+7\n\nN/A\n\nN/A\n\nX\n\nLT\n\nearly\n\nN/A\n\nN/A\n\nN/A\n\nX\n\nRT\n\n0\n\nN/A\n\nRA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nLT\n\n-13\n\nN/A\n\n124\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nLT\n\n0\n\nN/A\n\n124\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nLT\n\n0\n\nN/A\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nLT\n\n0\n\nN/A\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nRT\n\n+2\n\nN/A\n\n133\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nLT\n\n+2\n\nN/A\n\n133\n\nTA\n\nN/A\n\nN/A\n\nX\n\nLT\n\n0\n\nN/A\n\n125\n\nN/A\n\nN/A\n\nX\n\nX\n\nLT\n\n-1\n\nFollow Stratw ay+\n\n1037\n\n120\n\nN/A\n\n145\n\nN/A\n\nN/A\n\nX\n\nX\n\nLT\n\nearly\n\nN/A\n\n120\n\n137\n\n124\n\nADS-B\n\nRDR\n\nTCAS\n\nX\n\nX\n\nX\n\nTRC\n\nNotes: Altimeter calibration performed (N3GC +60 ft). What looked like a level\nacceleration was performed (should not have been). All climb/descent leg altitudes were\nredlined prior to flight to achieve \xe2\x80\x9crun-in\xe2\x80\x9d type encounters for Stratway+.\nEncounters: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 odd run (O/S maneuver into int1), good, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 VSCS\nsplit track, band issues (almost terminate run), bad, 5 \xe2\x80\x93 vertical velocity noise in\nStratway+, good, 6 \xe2\x80\x93 good, 7 \xe2\x80\x93 split track, good, 8 \xe2\x80\x93 int1 RA descend, good, 9 \xe2\x80\x93\nmaintained heading, O/S RA do not descend, int1 RA descend, bad, 10 \xe2\x80\x93 split track, good,\n11 \xe2\x80\x93 maintained heading, O/S fast, bad, 12 \xe2\x80\x93 TC push int1 faster (210 KGS), terminate\nearly due to north airspace activity, good, 13 \xe2\x80\x93 O/S RA descend, int1 RA climb, good, 14\n\xe2\x80\x93 O/S RA descend, int1 RA climb, good, 15 \xe2\x80\x93 O/S RA do not climb, int1 RA monitor\nvertical speed, good, 16 \xe2\x80\x93 good, 17 \xe2\x80\x93 changed Stratway+ sensitivity, laptop problem and\nmultiple rolex, good, 18 \xe2\x80\x93 int1 ended at higher altitude (+100 ft), good, 19 \xe2\x80\x93 good, 20 \xe2\x80\x93\ngood, 21 \xe2\x80\x93 good, 22 - good\nAirspace: Multiple calls to SPORT that were unanswered. Stay above FL105 (did not\naffect runs). SPORT called combat laser, request stay above FL130 (affected runs 19,\n20). Later stay above FL100 (did not affect runs).\nBaro/Vis: 29.97, clear\n\n76\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nWind: 4 \xe2\x80\x93 O/S 170/25, int1 180/19, 7 \xe2\x80\x93 O/S 189/7, int1 175/18, 10 \xe2\x80\x93 O/S 193/6.5, int1\n207/24, 13 \xe2\x80\x93 O/S 160/17, int1 176/20, 18 \xe2\x80\x93 O/S 178/13, int1 169/20, 19 \xe2\x80\x93 O/S 191/19,\nint1 134/19, 21 \xe2\x80\x93 O/S 176/10, int1 191/13\nBottom Line: The bulk of the data was good but the \xe2\x80\x9csplit tracks\xe2\x80\x9d that kept occurring on\nVSCS were thought too distracting/incorrect for subsequent days. Thus, in proceeding\nflights the native Stratway+ display was used in the Ikhana GCS instead of the algorithm\nbeing fed through VSCS.\n7.1.7 Flight 7: July 9, 2015\nTable 18. Config. 1 Flight 7 Data.\nFlight\n\n7\n\nSUT\n\nStratw ay+\n\nDuration\n\n4.8 hours\nN3GC\n\nIntruder(s)\n#\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nTA/RA\n\nO/S\n\nInt1\n\nInt2\n\nN/A\n\nN/A\n\nX\n\nR275\n\n-\n\nN/A\n\n145\n\nN/A\n\nN/A\n\nX\n\nR050\n\n+2\n\nN/A\n\n145\n\nN/A\n\nN/A\n\nL060\n\n+20\n\nN/A\n\nTA\n\nN/A\n\nN/A\n\nX\n\nR265\n\n0\n\nN/A\n\nTA\n\nN/A\n\nN/A\n\nX\n\nL330\n\n+5\n\nN/A\n\nN/A\n\nN/A\n\nL060\n\n+10\n\nN/A\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nR270\n\n-\n\nN/A\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nL200\n\n0\n\nN/A\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nL225\n\n+1\n\nN/A\n\n124\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nR260\n\n0\n\nN/A\n\nTA\n\n124\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nL330\n\n+5\n\nN/A\n\nTA\n\n124\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nR130\n\n0\n\nN/A\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nR285\n\n0\n\nN/A\n\nTA/RA\n\nN/A\n\nN/A\n\nL230\n\n-\n\nN/A\n\n140\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nR030\n\n+4\n\nN/A\n\n145\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nR270\n\n0\n\nN/A\n\n145\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nR060\n\n0\n\nN/A\n\n145\n\nN/A\n\nN/A\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nTA\n\n145\n\nTA\n\nN/A\n\nN/A\n\n145\n\nTA\n\n130\n\nTA\n\nN/A\n\nN/A\n\n145\n\nTA/RA\n\n125\n\nTA\n\nN/A\n\nN/A\n\n1 22-L55C\n\n0653\n\n132\n\n2 43-L55D\n\nFollow Stratw ay+\n\n0703\n\n125\n\n3 65-L55F\n\nFollow Stratw ay+\n\n0713\n\n127\n\n4 24-L57C\n\nFollow Stratw ay+\n\n0725\n\n122\n\nTA\n\n167\n\n5 45-L57D\n\nFollow Stratw ay+\n\n0735\n\n118\n\nTA\n\n162\n\n6 67-L57F\n\nFollow Stratw ay+\n\n0743\n\n119\n\n7 7-L57A\n\nFollow Stratw ay+\n\n0755\n\n120\n\n8 3-L55A\n\nFollow Stratw ay+\n\n0807\n\n131\n\n9 4-L55B\n\nFollow Stratw ay+\n\n0819\n\n120\n\nTA\n\n145\n\n10 21-L42C\n\nFollow Stratw ay+\n\n0830\n\n120\n\nTA/RA\n\n11 42-L42D\n\nFollow Stratw ay+\n\n0840\n\n120\n\n12 64-L42F\n\nFollow Stratw ay+\n\n0848\n\n120\n\n13 26-L54C\n\nFollow Stratw ay+\n\n0900\n\n120\n\n14 47-L54D\n\nFollow Stratw ay+\n\n0909\n\n120\n\nTA/RA\n\n126\n\n15 47-L54D\n\nFollow Stratw ay+\n\n0920\n\n120\n\nTA\n\n16 22-L55C\n\nFollow Stratw ay+\n\n0930\n\n131\n\nTA\n\n17 43-L55D\n\nFollow Stratw ay+\n\n0940\n\n127\n\n18 65-L55F\n\nFollow Stratw ay+\n\n0948\n\n-\n\n19 24-L57C\n\nFollow Stratw ay+\n\n0958\n\n124\n\nTA\n\n168\n\n20 45-L57D\n\nFollow Stratw ay+\n\n1007\n\n-\n\nTA/RA\n\n21 67-L57F\n\nFollow Stratw ay+\n\n1016\n\n140\n\n22 25-L53C\n\nFollow Stratw ay+\n\n1026\n\n23 46-L53D\n\nData\n\nFollow Stratw ay+\n\nTA/RA\n\n145\n\nFollow Stratw ay+\n\n1036\n\nTA/RA\n\n16\nTA\n\n158\n\nTA\n\n145\n\n132\n\nADS-B\n\nRDR\n\nTCAS\n\nTRC\n\nX\n\nX\n\nX\n\nX\n\nL050 early\n\nN/A\n\nX\n\nR270\n\n0\n\nN/A\n\nX\n\nR055\n\n+5\n\nN/A\n\nNone\n\n-\n\nN/A\n\nX\n\nR270\n\n-9\n\nN/A\n\nX\n\nR045\n\n0\n\nN/A\n\nX\n\nNotes: Altimeter calibration performed (N3GC +60 ft). All climb/descent leg altitudes were\nredlined prior to flight to achieve \xe2\x80\x9cmerge intent\xe2\x80\x9d type encounters for Stratway+. Used\nnative Stratway+ display in Ikhana GCS.\nEncounters: 1 \xe2\x80\x93 O/S RA descend, int1 RA climb, TC slowed int1 10kts, good, 2 \xe2\x80\x93 O/S\nlarge deviation from CPA, good, 3 \xe2\x80\x93 TC slowed int1 10kts, good, 4 \xe2\x80\x93 performed maneuver\nbut did not turn back, bad, 5 \xe2\x80\x93 good, 6 \xe2\x80\x93 TC slowed int1 10kts, resumed speed at turn,\ngood, 7 \xe2\x80\x93 good, 8 \xe2\x80\x93 good, 9 \xe2\x80\x93 good, 10 \xe2\x80\x93 O/S RA do not climb, int1 RA monitor speed,\n11 \xe2\x80\x93 good, 12 \xe2\x80\x93 good, 13 \xe2\x80\x93 good, 14 \xe2\x80\x93 O/S RA do not climb, int1 RA maintain vertical\nspeed, good, 15 \xe2\x80\x93 late turn back on course, good, 16 \xe2\x80\x93 good, 17 \xe2\x80\x93 good, 18 \xe2\x80\x93 terminate\nearly due to no airspace north, good, 19 \xe2\x80\x93 good, 20 \xe2\x80\x93 200 ft excursion, O/S RA descend,\ngood, 21 \xe2\x80\x93 no banding, bad, 22 \xe2\x80\x93 good, 23 \xe2\x80\x93 O/S RA climb, good\n77\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nAirspace: SPORT stay FL105-200 (did not affect runs), C-17 FL060 and below (did not\naffect runs). FT3 multiple spill-outs north of airspace (coordinated minutes prior with\nSPORT).\nBaro/Vis: 29.81, haze layer, good vis. at altitude\nWind: 1 \xe2\x80\x93 O/S 220/15.5, int1 235/12, 4 \xe2\x80\x93 O/S 191/17, int1 228/24, 7 \xe2\x80\x93 O/S 193/16, int1\n233/26, 8 \xe2\x80\x93 O/S 206/15, int1 238/17, 11 \xe2\x80\x93 O/S 174/11, int1 209/9, 13 \xe2\x80\x93 O/S 180/19, int1\n250/16, 15 \xe2\x80\x93 O/S 200/12, int1 234/16, 16 \xe2\x80\x93 O/S 205/15, int1 195/16, 18 \xe2\x80\x93 O/S 190/11,\nint1 218/7, 19 \xe2\x80\x93 O/S 200/14, int1 237/19, 22 \xe2\x80\x93 O/S 220/20, int1 180/21\nBottom Line: Overall, encounters were good and using the native Stratway+ display\nhelped the O/S pilots better understand the banding which they needed to fly. Intruder\naircraft had timing issues this day (caused several rolex calls) but did not affect runs.\n7.1.8 Flight 8: July 10, 2015\nTable 19. Config. 1 Flight 8 Data.\nFlight\n\n8\n\nSUT\n\nStratw ay+, CPDS\n\nDuration\n\n4.6 hours\n\nIntruder(s)\n\nN3GC, NASA865\n#\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nTA/RA\n\nADS-B\n\nRDR\n\nTCAS\n\nTA\n\n135\n\nTA\n\n125\n\n2 8-M59Q\n\nFollow Stratw ay+\n\n0656\n\n130\n\nTA\n\n135\n\nTA\n\n125\n\n3 28-M59V\n\nFollow Stratw ay+\n\n0705\n\n130\n\nTA\n\n135\n\nTA\n\n125\n\nTA\n\n4 28-M59V\n\nFollow Stratw ay+\n\n0715\n\n130\n\nTA\n\n135\n\nTA\n\n125\n\nTA\n\nX\n\n5 71-M59W\n\nFollow Stratw ay+\n\n0725\n\n130\n\nTA\n\n135\n\nTA\n\n125\n\nTA\n\nFollow Stratw ay+\n\n0735\n\n130\n\nTA\n\n135\n\n125\n\nTA\n\nX\n\n7 9-M59U\n\nFollow Stratw ay+\n\n0745\n\n130\n\n135\n\n125\n\nTA\n\n8 27-M59R\n\nFollow Stratw ay+\n\n0755\n\n130\n\nTA/RA\n\n135\n\nTA/RA\n\n125\n\n9 48-M59S\n\nFollow Stratw ay+\n\n0805\n\n130\n\nTA\n\n135\n\nTA\n\n125\n\nTA\n\n10 70-M59T\n\nFollow Stratw ay+\n\n0815\n\n130\n\nTA/RA\n\n135\n\nTA/RA\n\n125\n\nTA\n\nX\n\n11 9-M59U\n\nFollow Stratw ay+\n\n0827\n\n130\n\nTA/RA\n\n135\n\n125\n\nTA\n\nX\n\nX\n\nX\n\n12 27-M59R\n\nFollow Stratw ay+\n\n0854\n\n130\n\nTA\n\n135\n\nTA\n\n125\n\nX\n\nX\n\n13 48-M59S\n\nFollow Stratw ay+\n\n0905\n\n130\n\nTA\n\n135\n\nTA\n\n125\n\nTA\n\nX\n\nX\n\n14 70-M59T\n\nFollow Stratw ay+\n\n0915\n\n130\n\nTA\n\n135\n\nTA\n\n125\n\nTA\n\nX\n\n15 63-L12N\n\nFollow Stratw ay+\n\n0927\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\n16 63-L12N\n\nFollow Stratw ay+\n\n0935\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\n17 63-L12N\n\nFollow Stratw ay+\n\n0943\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\n18 122-L31G\n\nRadar CBDR (90)\n\n0953\n\n120\n\nTA/RA\n\n123\n\nTA/RA\n\nN/A\n\nN/A\n\nX\n\n19 122-L31G [R]\n\nRadar CBDR (90)\n\n1010\n\n120\n\nTA/RA\n\n123\n\nTA/RA\n\nN/A\n\nN/A\n\n20 125-L54G\n\nRadar CBDR (90)\n\n1022\n\n110\n\nTA/RA\n\n115\n\nTA/RA\n\nN/A\n\nN/A\n\nTRC\n\nX\n\n6 71-M59W\n\nData\n\n130\n\nX\nX\n\nX\n\nX\n\nX\n\nX\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nInt2\n\n+1\n\n+3\n\nL040\n\n+2\n\n+4\n\n0\n\n+8\n\nR290\n\n+5\n\n+4\n\nL370\n\n0646\n\nInt1\n\nR290\n\nFollow Stratw ay+\n\nO/S\nL330\n\n1 8-M59Q\n\n+4\n\n+11\n\nL300\n\n+3\n\n+5\n\nX\n\nR370\n\n+1\n\n0\n\nX\n\nR310\n\n+6\n\n0\n\nX\n\nR330\n\n+1\n\n0\n\nL270\n\n+10\n\n0\n\nX\n\nR300\n\n0\n\n+5\n\nX\n\nX\n\nR270\n\n+1\n\n-4\n\nX\n\nX\n\nR285\n\n+2\n\n0\n\nX\n\nX\n\nX\n\nR303\n\n+4\n\n+5\n\nR330\n\n-20\n\nN/A\n\nX\n\nX\n\nX\n\nR302\n\n-15\n\nN/A\n\nR300\n\n-15\n\nN/A\n\nX\n\n-40\n\n-50\n\nN/A\n\nX\n\nX\n\n-\n\n-\n\nN/A\n\nX\n\nX\n\n-\n\n-\n\nN/A\n\nNotes: Altimeter calibration performed (N3GC +60 ft, NASA865 +100 ft). All\nclimb/descent leg altitudes were redlined prior to flight to achieve \xe2\x80\x9cmerge intent\xe2\x80\x9d type\nencounters for Stratway+. Used native Stratway+ display in Ikhana GCS. TCAS on\nNASA865 showed Ikhana 300 ft high. Runs 18, 19, 20 radar runs with CPDS display.\nEncounters: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93 good, 6 \xe2\x80\x93 good, 7 \xe2\x80\x93 good, int2\nlate TA from int1, 8 \xe2\x80\x93 int1 RA monitor vertical speed, good, 9 \xe2\x80\x93 good, int2 late TA from\nint1, 10 \xe2\x80\x93 O/S RA maintain level, int1 monitor/adjust vertical speed, int2 TA from int1,\ngood, 11 \xe2\x80\x93 O/S RA do not descend, int2 TA from int1, 12 \xe2\x80\x93 good, 13 \xe2\x80\x93 good, int2 TA from\nint1, 14 \xe2\x80\x93 good, int2 TA from int1, 15 \xe2\x80\x93 good, 16 \xe2\x80\x93 good, 17 \xe2\x80\x93 good, 18 \xe2\x80\x93 int1 RA descend,\n78\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nint1 (fast) ahead of O/S, bad, 19 \xe2\x80\x93 angle about 80\xc2\xb0 instead of 90\xc2\xb0, O/S RA climb, int1 RA\ndescend, good, 20 \xe2\x80\x93 angle 95\xc2\xb0 most of run and 110\xc2\xb0 at the end, O/S RA descend, int1\nRA monitor vertical speed, good\nAirspace: Stay below FL230 Buckhorn (did not affect runs). Spin aircraft FL110 and\nabove 45min (did not affect runs).\nBaro/Vis: 29.86, clear\nWind: 1 \xe2\x80\x93 O/S 270/3, 9 \xe2\x80\x93 O/S 265/11, int1 223/5, int2 calm, 15 \xe2\x80\x93 O/S 145/10, int1 194/15\nBottom Line: Overall data was good. Second day of multi-ship encounters went smoothly\nas the first. Adjusting the vertical profile caused a lot more alerting for Stratway+ (good).\nRuns 18, 19, 20 gathered good radar data due to change in altitude and better\nunderstanding of angle requirements from pilots.\n7.1.9 Flight 9: July 21, 2015\nTable 20. Config. 1 Flight 9 Data.\nFlight\n\n9\n\nSUT\n\nStratw ay+\n\nDuration\n\n4.8 hours\n\nIntruder(s)\n\nNASA850, NASA865\n#\n\nTA/RA\n\nRDR\n\nTCAS\n\nTRC\n\nO/S\n\nInt1\n\nInt2\n\nFollow Stratw ay+\n\n0705\n\n120\n\nTA\n\n124\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nL055\n\n+20\n\nN/A\n\n2 72-H42A\n\nFollow Stratw ay+\n\n0715\n\n120\n\nTA/RA\n\n124\n\nN/A\n\nN/A\n\nX\n\nX\n\nL045\n\n-\n\nN/A\n\n3 73-H42C\n\nFollow Stratw ay+\n\n0723\n\n120\n\nTA/RA\n\n124\n\nN/A\n\nN/A\n\nX\n\nX\n\nR270\n\n+25\n\nN/A\n\n4 73-H42C\n\nFollow Stratw ay+\n\n0733\n\n120\n\nTA/RA\n\n124\n\nN/A\n\nN/A\n\nX\n\nX\n\nL215\n\n-6\n\nN/A\n\n5 74-H42D\n\nFollow Stratw ay+\n\n0741\n\n120\n\nTA\n\n124\n\nN/A\n\nN/A\n\nX\n\nX\n\nL330\n\n-\n\nN/A\n\n6 74-H42D\n\nFollow Stratw ay+\n\n0751\n\n120\n\nTA\n\n124\n\nN/A\n\nN/A\n\nX\n\nX\n\nL320\n\n-\n\nN/A\n\n7 75-H42F\n\nFollow Stratw ay+\n\n0800\n\n120\n\n124\n\nN/A\n\nN/A\n\nX\n\nX\n\nL045\n\n-5\n\nN/A\n\n8 75-H42F\n\nFollow Stratw ay+\n\n0808\n\n120\n\nTA/RA\n\n124\n\nN/A\n\nN/A\n\nX\n\nX\n\nL010\n\n0\n\nN/A\n\n9 23-L56C\n\nFollow Stratw ay+\n\n0835\n\n145\n\nTA/RA\n\n125\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nR270\n\n-\n\nN/A\n\n10 44-L56D\n\nFollow Stratw ay+\n\n0843\n\n142\n\nTA\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nL335\n\n-10\n\nN/A\n\n11 66-L56F\n\nFollow Stratw ay+\n\n0851\n\n152\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nL060\n\n-\n\nN/A\n\n12 31-L32C\n\nFollow Stratw ay+\n\n0901\n\n120\n\nTA/RA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nL220\n\n-5\n\nN/A\n\n13 51-L32D\n\nFollow Stratw ay+\n\n0910\n\n120\n\nTA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\nR020\n\n0\n\nN/A\n\n14 23-L56C\n\nFollow Stratw ay+\n\n0926\n\n145\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nL200\n\n-20\n\nN/A\n\n15 44-L56D\n\nFollow Stratw ay+\n\n0935\n\n152\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nL330\n\n+5\n\nN/A\n\n16 66-L56F\n\nFollow Stratw ay+\n\n0944\n\n145\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nL045\n\n-\n\nN/A\n\n17 66-L56F\n\nFollow Stratw ay+\n\n0955\n\n152\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nL030\n\n-20\n\nN/A\n\n18 76-M59R\n\nFollow Stratw ay+\n\n1015\n\n130\n\nTA/RA\n\n135\n\nTA\n\n125\n\nTA\n\nX\n\nX\n\nX\n\nL220\n\n-\n\n-\n\n19 76-M59R\n\nFollow Stratw ay+\n\n1025\n\n130\n\nTA\n\n135\n\n125\n\nTA\n\nX\n\nX\n\nR295\n\n-\n\n-\n\n20 77-M59S\n\nData\n\nInt2 Alt.\n\n1 72-H42A\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nFollow Stratw ay+\n\n1035\n\n130\n\nTA\n\n135\n\n125\n\nTA\n\nX\n\nX\n\nR340\n\n+10\n\n0\n\nTA/RA\n\nTA\n\nTA\n\nADS-B\n\nX\n\nX\n\nX\n\nX\n\nNotes: Altimeter calibration performed (NASA850 +140 ft, NASA865 +100 ft). All\nclimb/descent leg altitudes were redlined prior to flight to achieve \xe2\x80\x9cmerge intent\xe2\x80\x9d type\nencounters for Stratway+. Used native Stratway+ display in Ikhana GCS. Runs 1-12 int1\nNASA850, runs 9-17 int1 NASA865, runs18-20 int1 NASA850, int2 NASA865.\nEncounters: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 O/S RA do not climb, good, 5 \xe2\x80\x93 slow to\ndevelop and int1 lagging, bad, 6 \xe2\x80\x93 good, 7 \xe2\x80\x93 became tail chase run, good, 8 \xe2\x80\x93 good, 9 \xe2\x80\x93\nTC increase int1 10 kts, int1 started FL120 instead of 125, good, 10 \xe2\x80\x93 O/S fast, int1 started\nFL123 instead of 125, bad, 11 \xe2\x80\x93 int1 off course on IP, bad, 12 \xe2\x80\x93 O/S RA do not climb,\ngood, 13 \xe2\x80\x93 good, 14 \xe2\x80\x93 good, 15 \xe2\x80\x93 int1 fast TC request go card speed, good, 16 \xe2\x80\x93 O/S RA\n79\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\ndo not descend, O/S fast, int1 increased speed, good, 17 \xe2\x80\x93 good, 18 \xe2\x80\x93 good, 19 \xe2\x80\x93 good,\n20 \xe2\x80\x93 good\nAirspace: Fly below FL200 (did not affect runs), received Buckhorn FL100-200.\nBaro/Vis: 29.97, clear\nWind: 12 \xe2\x80\x93 O/S 212/5\nBottom Line: This was another successful flight day for Stratway+ and the first ever highspeed multi-ship encounters. As the previous multi-ship encounters, these went\nsurprisingly smoothly and gave good data for the researcher. Although int1 was late/early\nto the CPA for many of the encounters, numerous of these runs were successful due to\nthe \xe2\x80\x9cintent\xe2\x80\x9d of the intruder \xe2\x80\x93 Stratway+ displayed good alerting for the O/S pilot.\n7.1.10 Flight 10: July 22, 2015\nTable 21. Config. 1 Flight 10 Data.\nFlight\n\n10\n\nSUT\n\nAutoResolver\n\nDuration\n\n3.4 hours\nNASA865\n\nIntruder(s)\n#\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nTA/RA\n\nADS-B\n\nO/S\n\nInt1\n\nInt2\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\n280\n\n-15\n\nN/A\n\nTA\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\n280\n\n-20\n\nN/A\n\n120\n\nTA\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\nR010\n\n+5\n\nN/A\n\n0705\n\n120\n\nTA\n\n144\n\nTA\n\nN/A\n\nN/A\n\nX\n\nR280\n\n-8\n\nN/A\n\nFollow AR1\n\n0715\n\n120\n\nTA\n\n147\n\nTA\n\nN/A\n\nN/A\n\nX\n\nR280\n\n-3\n\nN/A\n\n6 55-L14D\n\nFollow AR1\n\n0725\n\n120\n\nTA\n\n147\n\nTA\n\nN/A\n\nN/A\n\nX\n\nL320\n\n-\n\nN/A\n\n7 15-L15A\n\nFollow AR1\n\n0735\n\n138\n\n160\n\nN/A\n\nN/A\n\nX\n\nL230\n\n0\n\nN/A\n\n8 37-L15C\n\nFollow AR1\n\n0745\n\n135\n\n160\n\nN/A\n\nN/A\n\nX\n\nR270\n\n-25\n\nN/A\n\n9 57-L15D\n\nFollow AR1\n\n0755\n\n132\n\n160\n\nN/A\n\nN/A\n\nX\n\nL330\n\n+8\n\nN/A\n\n10 11-L13A\n\nFollow AR1\n\n0805\n\n165\n\nN/A\n\nN/A\n\nX\n\nR270\n\n0\n\nN/A\n\n11 33-L13C\n\nFollow AR1\n\n0815\n\n165\n\nN/A\n\nN/A\n\nX\n\nR280\n\n+3\n\nN/A\n\n12 53-L13D\n\nFollow AR1\n\n0825\n\n165\n\nX\n\nL330\n\n+12\n\nN/A\n\n13 18-L12A\n\nFly-through, AR1\n\n0835\n\n14 29-L12C\n\nFly-through, AR1\n\n15 49-L12D\n\n1 41-L12M\n\n0635\n\n120\n\n2 63-L12N\n\nFollow AR1\n\n0645\n\n120\n\n3 61-L12E\n\nFollow AR1\n\n0655\n\n4 13-L14A\n\nFollow AR1\n\n5 35-L14C\n\nData\n\nFollow AR1\n\nTA\n\n140\n\nTA\n\n138\nTA\n\nRDR\n\nTCAS\n\nTRC\n\n140\n\nTA\n\nN/A\n\nN/A\n\n120\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nSome\n\n-\n\nN/A\n\n0845\n\n120\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nNone\n\n-4\n\nN/A\n\nFly-through, AR1\n\n0855\n\n120\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\n-30\n\n+9\n\nN/A\n\n16 18-L12A\n\nFly-through, AR1\n\n0905\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+12\n\n+10\n\nN/A\n\n17 29-L12C\n\nFly-through, AR1\n\n0915\n\n120\n\n130\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\nX\n\n+5\n\n-4\n\nN/A\n\nTA\n\nNotes: No altimeter calibration (all runs >500 ft). Artificial offset was applied for 1,000 ft\nruns.\nEncounters: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 wind adjust 0s, good, 5 \xe2\x80\x93 wind adjust\n+20s, good, 6 \xe2\x80\x93 wind adjust -10s, good, 7 \xe2\x80\x93 wind adjust -10s, O/S kept climbing after\nmaneuver, good, 8 \xe2\x80\x93 wind adjust -20s, O/S kept climbing after maneuver, good, 9 \xe2\x80\x93 wind\nadjust +20s, good, 10 \xe2\x80\x93 wind adjust -5s, int1 max performance on climb, good, 11 \xe2\x80\x93 wind\nadjust +20s, good, 12 \xe2\x80\x93 wind adjust -10s, good, 13 \xe2\x80\x93 intermittent headings, fly-through,\nbad, 14 \xe2\x80\x93 no headings, fly-through, bad, 15 \xe2\x80\x93 both aircraft too fast, O/S arrive early, int1\ncorrected, bad, 16 \xe2\x80\x93 O/S airspeed low at start of run, good, 17 \xe2\x80\x93 good\nAirspace: Buckhorn received FL100-200, F-35 at FL220 (did not affect runs). Later in\nday stay FL110-170 (did not affect runs).\nBaro/Vis: 29.86, clear\n\n80\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nWind: 4 \xe2\x80\x93 O/S 215/15, int1 168/38, 5 \xe2\x80\x93 O/S 220/14, int1 180/26, 6 \xe2\x80\x93 O/S 175/13, int1\n187/30, 7 \xe2\x80\x93 O/S 210/15, int1 180/20, 8 \xe2\x80\x93 O/S 210/20, int1 180/25, 9 \xe2\x80\x93 O/S 170/15, int1\n230/24, 10 \xe2\x80\x93 O/S 210/20, int1 160/30, 11 \xe2\x80\x93 in1 180/25, 12 \xe2\x80\x93 O/S 200/20, int1 190/22\nBottom Line: Overall it was a good, smooth day of data collection. This day had no\naborts, rolex calls, or resets \xe2\x80\x93 a first for FT3. The wind matrix worked well for the\nclimb/descent encounters.\n7.1.11 Flight 11: July 24, 2015\nTable 22. Config. 1 Flight 11 Data.\nFlight\n\n11\n\nSUT\n\nCPDS\n\nDuration\n\n3.2 hours\nNASA865\n\nIntruder(s)\n#\n\nInt2 Alt.\n\nTA/RA\n\nADS-B\n\nO/S\n\nInt1\n\nInt2\n\n1 168-L52M\n\nFollow CPDS\n\n0633\n\n120\n\nTA\n\n125\n\nTA\n\nN/A\n\nN/A\n\nX\n\nR275\n\n-2\n\nN/A\n\n2 164-L42M\n\nFollow CPDS\n\n0643\n\n120\n\nTA/RA\n\n124\n\nTA\n\nN/A\n\nN/A\n\nX\n\nL250\n\n+13\n\nN/A\n\n3 159-L57D\n\nTCAS Advisory\n\n0652\n\n140\n\nTA/RA\n\n145\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+7\n\n+7\n\nN/A\n\n4 130-L32D\n\nTCAS Advisory\n\n0702\n\n120\n\nTA/RA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+10\n\n+16\n\nN/A\n\n5 131-L32F\n\nTCAS Advisory\n\n0710\n\n120\n\nTA/RA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+7\n\n-3\n\nN/A\n\n6 152-L32B\n\nTCAS Advisory\n\n0720\n\n120\n\nTA/RA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+10\n\n+7\n\nN/A\n\n7 153-L32G\n\nTCAS Advisory\n\n0730\n\n120\n\nTA/RA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+2\n\n+5\n\nN/A\n\n8 154-L32H\n\nTCAS Advisory\n\n0738\n\n120\n\nTA/RA\n\n123\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+15\n\n+15\n\nN/A\n\n9 155-L31B\n\nTCAS Advisory\n\n0747\n\n123\n\nTA/RA\n\n120\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n-\n\n-\n\nN/A\n\n10 156-L31G\n\nTCAS Advisory\n\n0756\n\n123\n\nTA/RA\n\n120\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+5\n\n+1\n\nN/A\n\n11 157-L31H\n\nTCAS Advisory\n\n0805\n\n123\n\nTA/RA\n\n120\n\nN/A\n\nN/A\n\nX\n\nX\n\nX\n\n+5\n\n+9\n\nN/A\n\n12 127-L12P\n\nRadar Zig Zag\n\n0815\n\n120\n\n13 116-L31G\n\nRadar CBDR (110)\n\n0824\n\n123\n\n14 135-L31F\n\nTCAS Advisory\n\n0832\n\n15 126-L56G\n\nRadar CBDR (90)\n\n16 120-L56G\n\nData\n\nScenario\n\nType\n\nCOMEX O/S Alt. TA/RA Int1 Alt. TA/RA\n\nRDR\n\nTCAS\n\nTRC\n\nRadar CBDR (110)\n\n130\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\n+3\n\n+10\n\nN/A\n\nRA\n\n120\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\n-\n\n-\n\nN/A\n\n123\n\nTA/RA\n\n120\n\nTA\n\nN/A\n\nN/A\n\n+8\n\n+1\n\nN/A\n\n0842\n\n115\n\nTA/RA\n\n110\n\nTA\n\nN/A\n\nN/A\n\nX\n\nX\n\n-30\n\n-32\n\nN/A\n\n0856\n\n115\n\nRA\n\n110\n\nN/A\n\nN/A\n\nX\n\nX\n\n-20\n\n-30\n\nN/A\n\nX\n\nX\n\nX\n\nNotes: Altimeter calibration performed (NASA865 +100 ft).\nEncounters: 1 \xe2\x80\x93 virtual offset 500 ft good, 2 \xe2\x80\x93 O/S RA descend, good, 3 \xe2\x80\x93 O/S RA do not\nclimb, good, 4 \xe2\x80\x93 O/S RA do not climb, good, 5 \xe2\x80\x93 O/S RA do not climb, good, 6 \xe2\x80\x93 O/S RA\ndo not climb, good, 7 \xe2\x80\x93 O/S RA descend (before run start), O/S RA descend, int1 fly\n190kts per TC, 8 \xe2\x80\x93 O/S RA do not climb, good, 9 \xe2\x80\x93 O/S RA climb to FL130, good, 10 \xe2\x80\x93\nO/S RA climb FL130, int1 on O/S camera, good, 11 \xe2\x80\x93 O/S RA do not descend (before run\nstart), O/S RA climb to FL130, good, 12 \xe2\x80\x93 lagged slightly, good, 13 \xe2\x80\x93 O/S RA climb,\noutside radar field of view, bad, 14 \xe2\x80\x93 O/S RA climb, good, 15 \xe2\x80\x93 O/S RA do not descend,\ngood, 16 \xe2\x80\x93 O/S RA do not descend, int1 hold 200kts per TC, good\nAirspace: Buckhorn received FL100-200.\nBaro/Vis: 29.92, slight haze\nWind: Alt Cal \xe2\x80\x93 O/S 230/25\nBottom Line: An additional day of data collection of CPDS and radar encounters was\nwell-received. Due to the previous practice of doing these types of encounters, the\naircrews and ops team were well-prepared and the data collected good. Although the\n\xe2\x80\x9cZig-Zag\xe2\x80\x9d (run 12) encounter was expected to be more of an S-turn, the researchers were\nstill pleased with the data received. The extra TCAS encounters also allowed all\n81\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\ngeometries shown on the \xe2\x80\x9cpinwheel\xe2\x80\x9d (Figure 28 and Figure 29) to be flown. This particular\nday also used the \xe2\x80\x9cwing flash\xe2\x80\x9d (run 13) technique to acquire VID, something that was\nconsidered for future testing.\n\n7.2 Configuration 2\nConfiguration 2 flights were conducted from July 13, 2015, to August 12, 2015. Table 23\nshows a summary of these flight days. A total of 12 preparation flights (Combined\nSystems Test (CST), Inertial Navigation System (INS), Rehearsal type flights) and 3 data\ncollection flights were performed. As mentioned, the system was not ready for test and\ndid not meet perceived requirements after these 3 runs. Thus, additional data runs were\ncancelled for Configuration 2 only.\nFor the rehearsal runs, a truncated route was flown on some flight days. Data runs flew\nthe entire route. Additionally, an altimeter calibration was not performed for Configuration\n2 since the aircraft were simulating a normal flight environment.\nGRC flights are Glenn only as the participant (NASA608); CSTs, Rehearsals, and Data\nFlights involved Glenn, Armstrong, and Ames.\nTable 23. Configuration 2 Flights.\nFlight\n\nDate\n\nGRC 1\nCST 1\nCST 2\nGRC 2\nCST 3\nGRC 3\nRehearsal 1\nGRC 4\nCST 4\nCST 5\nRehearsal 2\nRehearsal 3\nData 1\nData 2\nData 3\nData 4\nData 5\nData 6\nData 7\nData 8\nData 9\nData 10\n\n13-Jul-15\n16-Jul-15\n28-Jul-15\n29-Jul-15\n29-Jul-15\n30-Jul-15\n3-Aug-15\n4-Aug-15\n4-Aug-15\n5-Aug-15\n6-Aug-15\n7-Aug-15\n10-Aug-15\n11-Aug-15\n12-Aug-15\n13-Aug-15\n17-Aug-15\n18-Aug-15\n19-Aug-15\n20-Aug-15\n21-Aug-15\n24-Aug-15\n\nDay of\nWeek\nM\nTh\nTu\nW\nW\nTh\nM\nTu\nTu\nW\nTh\nF\nM\nTu\nW\nTh\nM\nTu\nW\nTh\nF\nM\n\nSystem Under\nPlanned LIVE Flown LIVE LIVE Points Virtual Points\nEncounter Types\nTest\nEncounters Encounters Achieved\nFlown\nCNPC\n(None)\n0\n0\n0\n0\nCNPC\nTargets of Opportunity\n0\n0\n0\n0\nCNPC\nLive\n4\n2\n0\n0\nINS\n(None)\n0\n0\n0\n0\nCNPC\n(None)\n0\n0\n0\n0\nCNPC\n(None)\n0\n0\n0\n0\nCNPC\nLive\n8\n4\n0\n0\nCNPC\n(None)\n0\n0\n0\n0\nCNPC\nLive, Virtual\n4\n2\n0\n2\nCNPC\nLive, Virtual\n6\n5\n0\n9\nCNPC\n(None)\n8\n0\n0\n0\nCNPC\nLive\n8\n1\n0\n0\nHSI/RGCS\nLive, Virtual\n8\n8\n7\n11\nHSI/RGCS\nLive, Virtual\n8\n8\n6\n9\nHSI/RGCS\nLive, Virtual\n8\n8\n5\n10\nCANCELLED\nHSI/RGCS\nCANCELLED\nHSI/RGCS\nCANCELLED\nHSI/RGCS\nCANCELLED\nHSI/RGCS\nCANCELLED\nHSI/RGCS\nCANCELLED\nHSI/RGCS\nHSI/RGCS\nCANCELLED\nTotals\n62\n38\n18\n41\n\n82\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.2.1 Flight 1: July 13, 2015\nTable 24. Config 2 Flight 1 Data.\nGRC 1\n\nFlight\nSUT\n\nCNPC\n\nDuration\n\n3.1 hours\n\nIntruder(s)\n\nNone\n\nNotes: RF Characterization Flight, NASA608 flew the Fireline route once as planned to\ngather data on the Control and Non-Payload Communication (CNPC) RF radiation\npattern. The Fireline route was flown a second time with a 10-mile buffer to account for\nexpected deviations during the Configuration 2 Flight Test.\n7.2.2 Flight 2: July 16, 2015\nTable 25. Config 2 Flight 2 Data.\nCST 1\n\nFlight\nSUT\n\nCNPC\n\nDuration\n\n1.3 hours\n\nIntruder(s)\n\nNone\n\nNotes: Initial system checkout flight to verify command and control function with RGCS\nand establish valid data flow to the LVC environment from the ADS-B system using\ntargets of opportunity. Issues were observed: 1. Up to 15 second delay from RGCS\ncommand to NASA608 response. 2. ADS-B targets were incorrectly displayed on the\nVSCS and LVC systems. ADS-B traffic was stacked in columns.\n7.2.3 Flight 3: July 28, 2015\nTable 26. Config 2 Flight 3 Data.\nCST 2\n\nFlight\nSUT\n\nCNPC\n\nDuration\n\n3.1 hours\n\nIntruder(s)\n\nNASA865\n\nNotes: Flight to verify ADS-B issues were fixed. Issues observed: 1. Remote GRC team\nunavailable to start Vehicle Specific Module (VSM) scripts. 2. INS state information stale.\n3. Research computer C2 script crashed multiple times. 4. ADS-B traffic data exhibits\nincorrect conversion to lat/lon. 5. Airspeed not shown on VSCS. 6. LVC Gateway crashed\nafter 1 minute of initializing the GRC VSM script.\nEncounters: NASA865 practiced first leg of route for int2 at FL144 while troubleshooting\nissues for NASA608. COMEX 0820 performed encounter for WP2 as int1. Performed\nencounter for WP15 as int1.\nAirspace: Some activity, moved NASA865 to int1 holding and route.\nBaro/Vis: 29.92, hazy but workable\nWind: WP2 encounter int1 \xe2\x80\x93 320/11\nBottom Line: For the intruder, it was easier to skip through to waypoints and save time\ninstead of following the pattern on the flight cards. Calls were being made in minutes and\n\n83\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nseconds to CPA, but it was mentioned by the pilots they preferred whole minutes (which\nwas difficult to implement). From an operational perspective, all participants need to be\nready at flight time. This was mitigated for future flights.\n7.2.4 Flight 4: July 29, 2015\nTable 27. Config 2 Flight 4 Data.\nGRC 2\n\nFlight\nSUT\n\nINS\n\nDuration\n\n2 hours\n\nIntruder(s)\n\nNone\n\nNotes: GRC led flight test to troubleshoot INS issues observed during last flight.\nEncounters: None.\n7.2.5 Flight 5: July 29, 2015\nTable 28. Config 2 Flight 5 Data.\nCST 3\n\nFlight\nSUT\n\nCNPC\n\nDuration\n\n1.85 hours\n\nIntruder(s)\n\nNone\n\nNotes: RGCS/LVC systems connected for 30 minutes at the end of the flight. Issues\nobserved: 1. LVC Gateway software crashed multiple times at around 1 minute after GRC\nVSM traffic script was started. 2. ARC Multi Aircraft Control System (MACS) observer\nstation never received O/S data.\nEncounters: None.\n7.2.6 Flight 6: July 30, 2015\nTable 29. Config 2 Flight 6 Data.\nGRC 3\n\nFlight\nSUT\n\nCNPC\n\nDuration\n\n1.1 hours\n\nIntruder(s)\n\nNone\n\nNotes: GRC led flight test to troubleshoot LVC issues. Afternoon system checkout flight\ncancelled due to weather.\nEncounters: None.\n\n84\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.2.7 Flight 7: August 3, 2015\nTable 30. Config 2 Flight 7 Data.\nFlight\n\nRehearsal 1\n\nSUT\n\nCNPC\n\nDuration\n\n3.3 hours\nNASA7, N3GC\n\nIntruder(s)\n#\n\nScenario\n\nType\n\nPush\n\nO/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nResult\n\nTA/RA\n\nFireline 1 - COMEX 1637\n1 ENC1\n\nLive\n\n16:38:58\n\n140\n\n136\n\nTA\n\nN/A\n\n2 ENC2\n\nLive\n\n16:45:36\n\n140\n\nN/A\n\nN/A\n\n144\n\n3 ENC3\n\nLive\n\n17:06:00\n\n140\n\n136\n\nTA\n\nN/A\n\nN/A\n\n16:57:29 Left turn\n\n4 ENC4\n\nData\n\nN/A\n\n-\n\nLive\n\n17:07:30\n\n140\n\nN/A\n\nN/A\n\n144\n\nTA\n\n17:10:40 Left turn\n\nTA/RA 16:46:15 Left turn, int2 RA climb 100ft\n\nNotes: No virtual encounters, LVC gateway problems at beginning of flight day. Multiple\nsystem restarts. NASA608 originally could not go into nav. mode for any runs.\nEncounters: 1 \xe2\x80\x93 no lateral offset, bad, 2 \xe2\x80\x93 no VID, bad, 3 \xe2\x80\x93 good, 4 - good\nAirspace: No data.\nBaro/Vis: 29.88, changed to 29.85 at 1630\nWind: O/S 220/20, int1 250/13, int2 197/11\nBottom Line: Although it was useful to practice the live encounters, to do a full\nrehearsal, virtual traffic would have been required as well.\n7.2.8 Flight 8: August 4, 2015\nTable 31. Config 2 Flight 8 Data.\nGRC 4\n\nFlight\nSUT\n\nCNPC\n\nDuration\n\n2.8 hours\n\nIntruder(s)\n\nNone\n\nNotes: Good O/S data and traffic from the VSM. Heading/altitude/speed changes looked\ngood.\nEncounters: None.\n7.2.9 Flight 9: August 4, 2015\nTable 32. Config 2 Flight 9 Data.\nFlight\n\nCST 4\n\nSUT\n\nCNPC\n\nDuration\n\n2.9 hours\n\nIntruder(s)\n\nN3GC\n#\n\nScenario\n\nType\n\nPush\n\nO/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nTA/RA\n\nResult\n\nFireline 1 - COMEX 1707\nData\n\n1 ENC2\n\nLive\n\n17:13:45\n\n140\n\nN/A\n\nN/A\n\n144\n\n2 ENC4\n\nLive\n\n17:36:00\n\n140\n\nN/A\n\nN/A\n\n144\n\nTA/RA 17:12:43 Left turn, int2 RA climb\nTA\n\n17:31:59 Left turn\n\nN/A\n\nAll virtual encounters\n\nFireline 2 - COMEX 1808\n1 VE1\n\nVirtual\n\n140\n\n(140)\n\nN/A\n\nNotes: Good live traffic, bad virtual. Inertial Measurement Unit (IMU) data was stale on\nmultiple occasions. NASA608 was able to go into nav. mode.\nEncounters: Fireline1: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good; Fireline 2: all virtual, only for testing scripts\n85\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nAirspace: Predator FL100-210, F-18s, did not affect runs.\nBaro/Vis: 29.89, ENC4 29.90, hazy\nWind: int2 237/27, ENC4 int2 226/30\nBottom Line: Actual performance for NASA608 was approximately 500 fpm (as opposed\nto required 1,000 fpm). On this flight day, it became absolutely clear there was disconnect\nbetween the systems here at AFRC and those at/near GRC. What should have been the\nsecond day of data collection was the 9th check flight.\n7.2.10 Flight 10: August 5, 2015\nTable 33. Config 2 Flight 10 Data.\nFlight\n\nCST 5\n\nSUT\n\nCNPC\n\nDuration\n\n3 hours\n\nIntruder(s)\n\nN3GC\n#\n\nScenario\n\nType\n\nPush\n\nO/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nTA/RA\n\nResult\n\nFireline 1 - COMEX 1528\n1 VE1\n\nVirtual\n\n2 ENC1\n\nLive\n\n3 ENC2 (virt)\n\nVirtual\n\n4 ENC3\n\nLive\n\n5 VE2\n\n140\n\nVirtual\n\n136\n(144)\n\n140\n\n136\n\n140\n\n15:42:05\n\n-\n\n140\n140\n\n15:31:14\n\nN/A\n\n-\n\nVirtual intruder passed too early.\n\nN/A\n\n15:33:34 Left turn (due to WP or command?)\n\nN/A\n\n15:38:30 Right turn\n\nN/A\n\nN/A\n\n15:39:18 Left turn, climb to 145 (later)\n\nN/A\n\nN/A\n\nFly-through WP7 to WP9\n\nN/A\n\nTA\n\nN/A\n\nN/A\nN/A\n\nTA\n\nN/A\n\nFly-through WP1 to WP2\n\nN/A\n\nN/A\n\n16:07:14 Right turn and climb\n\nFireline 2 - COMEX 1603\n1 VE1\n\nVirtual\n\n2 ENC1\n\nLive\n\n3 ENC2 (virt)\n4 VE2\n\nData\n\n140\n\n(145)\n\n140\n\n136\n\nVirtual\n\n148\n\n(144)\n\nN/A\n\nN/A\n\nDescend 140, turn left\n\nVirtual\n\n140\n\n-\n\nN/A\n\nN/A\n\nFly-through\n\nN/A\n\nN/A\n\nFly-through\n\nN/A\n\nN/A\n\n16:40:46 Expect climb\n\n16:05:58\n\nTA\n\nFireline 3 - COMEX 1636\n1 VE1\n\nVirtual\n\n2 ENC1\n\nLive\n\n140\n\n(145)\n\n141\n\n136\n\n3 VE2\n\nVirtual\n\n138\n\n(144)\n\nN/A\n\nN/A\n\n16:45:50 Left turn, climb to 140\n\n4 VE3\n\nVirtual\n\n140\n\n(144)\n\nN/A\n\nN/A\n\nRight turn\n\n5 ENC4\n\nLive\n\n140\n\nN/A\n\n16:40:02\n\n16:53:00\n\nTA\n\nN/A\n\n154\n\n16:57:01 Right turn\n\nNotes: Stale tracks on LVC system. Even after reboot, VSCS stale data. NASA608 was\nreceiving commands from RGCS late or did not see them. N3GC acted as int1 for\nFireline1, in1 for Fireline2, and both int1/int2 for Fireline3. At end of flight, NASA608 was\nasked to fly a cardinal direction (due west) but this did not look correct in any of the ground\ndisplays.\nEncounters: Fireline1: 1 \xe2\x80\x93 stale data, bad, 2 \xe2\x80\x93 int1 late, bad, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93\nRGCS did not have control of NASA608, bad, Fireline2: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 good, 4\n\xe2\x80\x93 good, Fireline3: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93 int2 climbed to check\nvertical velocity\nAirspace: F-35s with tanker coming in to R-2515, may not use airspace for up to 2 hours\n(did not affect runs).\nBaro/Vis: 29.91, good\nWind: int1 227/29, Fireline2 ENC1 int1 199/22, Fireline3 ENC1 int1 225/24\nBottom Line: Due to the fuel capacity of the O/S, 3 full runs would not be possible for\nany of the data runs as planned. Either the Fireline had to be truncated (as was done this\nflight day) or less runs could be performed per day.\n86\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.2.11 Flight 11: August 6, 2015\nTable 34. Config 2 Flight 11 Data.\nFlight\n\nRehearsal 2\n\nSUT\n\nCNPC\n\nDuration\n\n1.1 hours\n\nIntruder(s)\n\nNASA865, N3GC\n\nNotes: Encounters were cancelled due to weather. All aircraft flew to altitude but could\nnot establish VMC. At end of flight, NASA608 flew on its own to troubleshoot magnetic\ncourse problem.\nEncounters: None.\nAirspace: No data.\nBaro/Vis: 29.95, virga/rain, clouds at FL145\nWind: No data.\nBottom Line: The weather was too poor to perform encounters this day. This was\nunfortunate since it seemed that this would be the first day where all systems are\nfunctional.\n7.2.12 Flight 12: August 7, 2015\nTable 35. Config 2 Flight 12 Data.\nFlight\n\nRehearsal 3\n\nSUT\n\nCNPC\n\nDuration\n\n2.8 hours\n\nIntruder(s)\n\nNASA865, N3GC\n\nNotes: Began as a rehearsal flight but turned more into a CST. Ames was showing traffic\nsomewhere close to the North Pole.\nEncounters: N3GC RTB 1641. Only flew a partial encounter due to problems with scripts.\nCOMEX was set to 1731 and a 500 fpm descent (FL140 to 136) by NASA865 at ENC2\nwas performed. TA received.\nAirspace: F-35 NOTAM, but cancelled (did not affect runs). Affected takeoff time: 1530\nas opposed to 1430.\nBaro/Vis: 29.84\nWind: 265/37\nBottom Line: Continuing troubleshooting entire system since software changes were\nmade on NASA608 computer without informing other team members. During the\nencounter, TC/TD were uncertain if visual had been picked up since a \xe2\x80\x9cnegative visual\xe2\x80\x9d\nwas called. A visual was picked up after by the pilots, but there was still uncertainty on\nthe ground. Thus it was communicated to only call positive visuals during flights.\n\n87\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.2.13 Flight 13: August 10, 2015 \xe2\x80\x93 Data Collection 1\nTable 36. Config 2 Flight 13 Data.\nData Collect 1\n\nFlight\nSUT\n\nHSI\n\nDuration\n\n2.8 hours\nNASA865, N3GC\n\nIntruder(s)\n#\n\nType\n\nScenario\n\nPush\n\nO/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nResult\n\nTA/RA\n\nFireline 1 - COMEX 1510\n1 VE1\n\n140\n\n(145)\n\nN/A\n\nN/A\n\nClear of conflict 15:12:56\n\nLive\n\n15:45:15\n\n141\n\n136\n\nTA\n\nN/A\n\nN/A\n\n15:16:03 O/S turned right\n\n3 ENC2\n\nLive\n\n15:17:15\n\n141\n\nN/A\n\nN/A\n\n145\n\nTA\n\n15:19:39 turning left\n\n4 VE2\n\nVirutal\n\n141\n\n-\n\nN/A\n\nN/A\n\nLeft turn\n\n5 VE3\n\nVirtual\n\n141\n\n(145)\n\nN/A\n\nN/A\n\n-\n\n6 VE3.5\n\nVirtual\n\n141\n\n-\n\nN/A\n\nN/A\n\nFly-through\n\n7 VE4\n\nVirtual\n\n141\n\n(143)\n\nN/A\n\nN/A\n\n15:36:07 L273\n\n8 VE5\n\nVirtual\n\n141\n\n(135)\n\nN/A\n\nN/A\n\nFly-through\n\n9 ENC3\n\nLive\n\n15:41:37\n\n141\n\n137\n\nTA\n\nN/A\n\nN/A\n\n17:04:01 RT\n\n10 ENC4\n\nLive\n\n15:42:39\n\n141\n\nN/A\n\nN/A\n\n145\n\n1 VE1\n\nVirtual\n\nN/A\n\nN/A\n\nFly-through\n\n2 ENC1\n\nLive\n\n16:12:48\n\n141\n\nTA\n\n137\n\nTA\n\nN/A\n\nN/A\n\n16:15:06 R168\n\n3 ENC2\n\nLive\n\n16:17:12\n\n141\n\nTA\n\nN/A\n\nN/A\n\n144\n\nTA\n\n16:21:43 R129\n\n4 VE2\n\nVirtual\n\n141\n\n(145)\n\nN/A\n\nN/A\n\n16:29:32 L107\n\n5 VE3\n\nVirtual\n\n141\n\n(144)\n\nN/A\n\nN/A\n\n16:34:33 L263\n\n6 VE4\n\nVirtual\n\n141\n\n(145)\n\nN/A\n\nN/A\n\nFly-through\n\n7 VE5\n\nVirtual\n\n141\n\n(135)\n\nN/A\n\nN/A\n\nFly-through\n\n8 ENC3\n\nLive\n\n16:38:14\n\n141\n\n(137)\n\nTA\n\nN/A\n\nN/A\n\n16:41:46 R326, South 319\n\n9 ENC4\n\nData\n\nVirtual\n\n2 ENC1\n\nLive\n\n16:40:49\n\n141\n\nN/A\n\nN/A\n\n145\n\nTA\n\n16:47:13 R344\n\nTA/RA 15:50:42 L248, int2 O/S RA climb 100ft\n\nFireline 2 - COMEX 1610\n(145)\n\n141\n\nNotes: NASA865 was seeing NASA608 200 ft above.\nEncounters: Fireline1: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 course correction, bad, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93\ngood, 6 \xe2\x80\x93 good, 7 \xe2\x80\x93 good, 8 \xe2\x80\x93 good, 9 \xe2\x80\x93 good, 10 \xe2\x80\x93 some stale data on RGCS, int2 RA\nclimb 100 ft, good; Fireline2: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93 good, 6 \xe2\x80\x93 good,\n7 \xe2\x80\x93 good, 8 \xe2\x80\x93 good, 9 \xe2\x80\x93 good\nAirspace: F-16 south of HWY58, could not use WP5 for short time (did not affect runs).\nBaro/Vis: 29.86, clear at FL140, haze at FL120 and below\nWind: int2 152/19, Fireline2 int2 ENC2 201/28\nBottom Line: Overall, it was a good flight day. A few problems were discovered, like\nturning prior to a WP 5 versus of flying into it \xe2\x80\x93 what the researchers expected and what\nwas done did not coincide. Additionally, ENC4 was expected to be more of a \xe2\x80\x9cbeak to\nbeak\xe2\x80\x9d encounter. A correction was made to the flight cards for a different intruder heading\nto create this geometry.\n\n88\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.2.14 Flight 14: August 11, 2015 \xe2\x80\x93 Data Collection 2\nTable 37. Config 2 Flight 14 Data.\nData Collect 2\n\nFlight\nSUT\n\nHSI\n\nDuration\n\n2.7 hours\nNASA865, N3GC\n\nIntruder(s)\n#\n\nType\n\nScenario\n\nPush\n\nO/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nResult\n\nTA/RA\n\nFireline 1 - COMEX 1510\n1 VE1\n\n141\n\n(145)\n\nN/A\n\nN/A\n\nFly-through\n\nLive\n\n15:12:47\n\n141\n\n136\n\nN/A\n\nN/A\n\n15:14:59 L111\n\n3 ENC2\n\nLive\n\n15:18:55\n\n141\n\nN/A\n\n144\n\nTA\n\nNo maneuver\n\n4 VE2\n\nVirutal\n\n141\n\n(139)\n\nN/A\n\nN/A\n\n15:25:38 R148\n\n5 VE3\n\nVirtual\n\n141\n\n(145)\n\nN/A\n\nN/A\n\nFly-through\n\n6 VE4\n\nVirtual\n\n141\n\n(135)\n\nN/A\n\nN/A\n\nFly-through\n\n7 ENC3\n\nLive\n\n15:38:34\n\n141\n\n136\n\nTA\n\nN/A\n\nN/A\n\n15:41:39 R301\n\n8 ENC4\n\nLive\n\n15:43:00\n\n141\n\nN/A\n\nN/A\n\n145\n\n1 VE1\n\nVirtual\n\nN/A\n\nN/A\n\n16:08:28 L111\n\n2 ENC1\n\nLive\n\n16:08:48\n\n141\n\nTA\n\n136\n\nTA\n\nN/A\n\nN/A\n\n16:11:06 L115\n\n3 ENC2\n\nLive\n\n16:14:34\n\n141\n\nTA\n\nN/A\n\nN/A\n\n145\n\nTA\n\n16:17:56 R113, R133\n\n4 VE2\n\nVirtual\n\n141\n\n(140)\n\nN/A\n\nN/A\n\nLeft\n\n5 VE3\n\nVirtual\n\n141\n\n(145)\n\nN/A\n\nN/A\n\n16:25:10 L076\n\n6 VE4\n\nVirtual\n\n141\n\n-\n\nN/A\n\nN/A\n\nFly-through\n\n7 VE5\n\nVirtual\n\n141\n\n(135)\n\nN/A\n\nN/A\n\n16:33:29 R334\n\n8 ENC3\n\nLive\n\n16:34:50\n\n141\n\n136\n\nTA\n\nN/A\n\nN/A\n\nNo maneuver\n\n9 ENC4\n\nData\n\nVirtual\n\n2 ENC1\n\nLive\n\n16:37:47\n\n141\n\nN/A\n\nN/A\n\n145\n\nTA\n\nN/A\n\nTA/RA 15:47:39 L272\n\nFireline 2 - COMEX 1606\n(145)\n\n141\n\nTA\n\nTA/RA 16:42:27 L262\n\nNotes: N3GC was showing 100 ft high on Zeus at one point.\nEncounters: Fireline1: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 bad, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93 good, 6 \xe2\x80\x93 good, 7 \xe2\x80\x93\nO/S seemed to head too much south, int1 RA descend 100 ft, good, 8 \xe2\x80\x93 good; Fireline2:\n1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93 good, 6 \xe2\x80\x93 good, 7 \xe2\x80\x93 good, 8 \xe2\x80\x93 int1 not enough\nrun-in, bad, 9 \xe2\x80\x93 int2 RA climb\nAirspace: Temporarily restricted from using WP 5 (did not affect runs).\nBaro/Vis: Fireline1: 29.87, Fireline2: 29.86, clear\nWind: int1 190/25, int2 182/36, Fireline2: ENC1 int1 158/25, ENC2 int2 189/29\nBottom Line: Another seemingly good flight day. Although the researchers mentioned\nthat only 6 of the 8 encounters were usable, this was still real flight data and was still\nconsidered beneficial. The heading changed employed a day prior for one of the\nencounters was also more successful.\n\n89\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n7.2.15 Flight 15: August 12, 2015 \xe2\x80\x93 Data Collection 3\nTable 38. Config 2 Flight 15 Data.\nData Collect 3\n\nFlight\nSUT\n\nHSI\n\nDuration\n\n2.5 hours\nNASA865, N3GC\n\nIntruder(s)\n#\n\nScenario\n\nType\n\nPush\n\nO/S Alt. TA/RA Int1 Alt. TA/RA\n\nInt2 Alt.\n\nResult\n\nTA/RA\n\nFireline 1 - COMEX 1455\n1 VE1\n\n139\n\n(145)\n\nN/A\n\nN/A\n\n-\n\nLive\n\n14:58:18\n\n139\n\n136\n\nN/A\n\nN/A\n\n14:59:57 R179\n\n3 ENC2\n\nLive\n\n15:03:02\n\n139\n\n4 VE2\n\nVirutal\n\n139\n\n(147)\n\nN/A\n\nN/A\n\nRight\n\n5 VE3\n\nVirtual\n\n139\n\n(132)\n\nN/A\n\nN/A\n\n15:12:18 L078\n\n6 VE4\n\nVirtual\n\n139\n\n(142)\n\nN/A\n\nN/A\n\n15:21:23 L322\n\n7 VE5\n\nVirtual\n\n139\n\n(135)\n\nN/A\n\nN/A\n\n-\n\n8 ENC3\nData\n\nVirtual\n\n2 ENC1\n\nLive\n\n15:24:11\n\n139\n\nTA\n\n134\n\nTA\n\nN/A\n\nN/A\n\nNo alert on VSCS\n\n9 ENC4\n\nLive\n\n15:26:05\n\n139\n\nTA\n\nN/A\n\nN/A\n\n143\n\nTA\n\nNo alert on VSCS\n\nTA\n\nN/A\n\nN/A\n\n143\n\nTA/RA 15:07:07 R124\n\nFireline 2 - COMEX 1555\n1 VE1\n\nVirtual\n\n2 ENC1\n\nLive\n\n15:58:12\n\n139\n139\n\n3 ENC2\n\nLive\n\n16:01:40\n\n4 VE2\n\n(145)\n\nN/A\n\nN/A\n\nFly-through\n\n136\n\nTA\n\nN/A\n\nN/A\n\n16:00:50 L173\n\n139\n\nN/A\n\nN/A\n\n144\n\nTA\n\n16:06:14 R124\n\nVirtual\n\n139\n\n(143)\n\nN/A\n\nN/A\n\nFly-through\n\n5 VE3\n\nVirtual\n\n139\n\n(145)\n\nN/A\n\nN/A\n\nFly-through\n\n6 VE4\n\nVirtual\n\n139\n\n(147)\n\nN/A\n\nN/A\n\n16:18:20 L289\n\n7 VE5\n\nVirtual\n\n139\n\n(135)\n\nN/A\n\nN/A\n\n16:22:27 R308\n\n8 ENC3\n\nLive\n\n16:22:54\n\n139\n\n136\n\nN/A\n\nN/A\n\n16:26:21 L243\n\n9 ENC4\n\nLive\n\n16:25:01\n\n139\n\n143\n\nTA\n\n16:31:18 L300\n\nTA\n\nTA\n\nN/A\n\nN/A\n\nNotes: Problem observed with RGCS only sending a WP command once when off\nautopilot. This day marked the highest barometric pressure seen during Flight Test Series\n3, topping out at 30.02 inHg. This was showing NASA608 100 ft low on Zeus. During the\nphase of no alerting during Fireline1, ADS-B targets were showing frozen for Ghost\ncontroller.\nEncounters: Fireline1: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93 good, 3 - int2 RA climb, good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93 good,\n6 \xe2\x80\x93 good, 7 \xe2\x80\x93 good, 8 \xe2\x80\x93 no alerting, bad, 9 \xe2\x80\x93 no alerting, bad; Fireline2: 1 \xe2\x80\x93 good, 2 \xe2\x80\x93\ntarget freeze on NASA608, int1 RA descend 100 ft, bad, 3 \xe2\x80\x93 good, 4 \xe2\x80\x93 good, 5 \xe2\x80\x93 good, 6\n\xe2\x80\x93 good, 7 \xe2\x80\x93 good, 8 \xe2\x80\x93 good, 9 \xe2\x80\x93 altitude seemed incorrect, bad\nAirspace: No data.\nBaro/Vis: Fireline1: 30.02, Fireline2: 30.00, clear but slight haze\nWind: int1 185/20, int2 185/20, Fireline2: int1 180/28, int2 227/22\nBottom Line: This day did not seem successful as others, due to only achieving 5 of the\n8 planned live intruder points. Problems with the RGCS/VSCS and also barometric\npressure affected system performance. Flight 15 was also the final flight day before\nadditional Configuration 2 flights were cancelled. The researchers recognized that they\nwere not getting the type of data and alerting they required, and also, the system was not\nacting the way it was thought to (VSGC, etc.). The decision was made to end test and\ngather lessons learned to make improvements to the Fireline and further flight testing of\nthe Full Mission.\n\n90\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n8 Observations and Issues\nThe many months of planning and coordination was a key factor in the success of the\nFlight Test Series 3 Configuration 1. Although successful, several lessons learned can be\ndrawn and built into future similar flight testing, especially the lessons learned from the\ndecision to forgo Configuration 1b and additional Configuration 2 flights. Those lessons\nlearned are described below.\n\n8.1 Major Impacts\nThe following issues significantly impacted the flights or resulted in mission rule violations.\n8.1.1 An intruder was within 1 NM and less than 500 ft vertical separation without\nbeing visual on Ikhana.\nBackground Data: During Flight 5 of Configuration 1, while conducting a challenging multiintruder sequential TCAS encounter, the first intruder did not acquire a visual acquisition\n(or callout) of Ikhana and reported this after the encounter had been completed. Further,\npersonnel within the SAF did not recognize the missed call or query the intruder aircrew\nto callout the visual. The flight was completed without further incident. This violated FT39 mission rule.\nAnalysis: After post flight discussion it was determined that hazy conditions, test point\ncomplexity and an inside cockpit focus were significant contributors to the mission rule\nviolation. It was also determined that pre-planned routing which maintained a 0.5 NM\nlateral and 300 ft vertical separation was able to maintain an adequate level of safety,\nalthough should not be relied upon as a the sole mitigation.\nMission Relation: Should any manned aircraft fail to acquire visual during FT4 or\nsubsequent tests, the consequence may lead to a closer than planned encounter (either\nlaterally or vertically) and may increase the chances of a midair collision.\nRecommendation/Conclusion: The test team developed CRM procedures to help aircrew\ndetermine when the visual boundary, 1 NM, was approaching:\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\xef\x82\xb7\n\nThe new procedures became a mandatory briefing item\nThe TC was responsible for notifying all participants that the next encounter would\nrequire a visual\nThe TC would provide range calls over the radio and;\nAt approximately 1.5 NM separation a \xe2\x80\x9cCheck Visual\xe2\x80\x9d call was made giving the\naircrew ~5 seconds to acquire a visual or abort the encounter.\n\nNo further violations occurred after Flight 5 and these additional procedures should be\ncarried forward into future flight test events.\n\n91\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n8.1.2 Full mission Configuration 2 flights only completed 3 of 10 planned data\ncollection flights before being cancelled.\nBackground Data: On 12 August Configuration 2 flight test was terminated with only 3 of\n10 data collection flights completed. Although 15 flights in total had been completed, 12\nof them were system check and troubleshooting events. Additional details on the\neffectiveness and suitability of the Configuration 2 system are subject to ongoing analysis\nand not available at the time of drafting this report.\nAnalysis: ARC-HSI determined that the flight test data would not adequately suit their\nobjectives and recommended termination of the remaining flights.\nRecommendation/Conclusion: Ensuring adequate development time prior to FT4 or\nsubsequent flight tests is paramount. The addition of system check periods with a long\nenough time to allow system enhancements, changes, or fixes, should be mandatory.\n8.1.3 Configuration 1b was not attempted.\nBackground Data/Analysis: The aircraft planned for this test event, the GRC S-3B, was\nnot able to meet the development time constraints and was not in the desired flight test\nconfiguration for FT3.\nRecommendation/Conclusion: The other research teams felt that the data acquired with\nthe Ikhana ownship was adequate and their Configuration 1 objectives were met.\n\n8.2 Minor Impacts and Lessons Learned\nThe following observations resulted in minor impacts to FT3 or are lessons learned to\nimpart upon FT4 and subsequent flight test events.\n8.2.1 Configuration 1 and Configuration 2 flight tests are distinct and separate.\nThe two flight test configurations were considered as a single test event. A limited amount\nof time was available between the two configurations to complete all necessary planning,\nintegrating, and approval briefings. The following highlights the distinctions between the\ntwo configurations:\nThe workload was significantly different. Configuration 1 flights were workload intensive.\nAs many as 30 encounters were planned during a flight and each encounter was allotted\nonly 10 minutes to complete including setup. Aircrew needed to navigate to new\ncoordinates, understand the upcoming encounter geometry, and setup to make the IP\nwithin 5 to 10 seconds of the COMEX time. Each encounter was distinct and required the\nabove mentioned steps to be re-executed each time. Additionally, up to 3 early morning\nflights were performed per week, adding to crew rest considerations.\nConfiguration 2 flights were significantly less intensive for the airborne participants. The\nmajority of the test decisions were made by the RGCS, a ground based resource. The\n\n92\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nairborne participants simply ensured the aircraft was on the planned parameters while the\nground element directed the ownship maneuvers. Furthermore, from one run to the next\nthe encounters did not change. Each intruder executed the same two encounters while\nthe ownship expected the same four encounters changed only by the ground element\xe2\x80\x99s\nresponse and basic environmental factors such as winds aloft. Configuration 2 flights\nwere afternoon flights flow daily.\nThat being said, the ownship maneuvers were less predictable in this configuration. The\npilot in the RGCS changed from one flight to the next which resulted in a variation of\nresponses to the presented encounters. On one occasion a pilot skipped an encounter\naltogether having requested and been granted approval by the virtual ATC controller.\nRecommendation/Conclusion: Decrease the number of flights per week for early morning\nsorties (or increase each sortie\xe2\x80\x99s duration). Reduce the planned number of test\nencounters per sortie in order to provide more time to execute each run. Increase the\nnumber of test sorties in order to complete the desired test points.\nAllocate more time between configurations. The additional time will help ensure success\ncriteria is clearly understood. Clearly communicate the differences between the two\nconfigurations more as test phases than simply configurations.\n8.2.2 Multiple operating/staging locations decreased efficiency in test execution.\nOperating from KEDW, KVNY, KPMD, and KBFL was a challenge to ensure efficient test\nexecution. On multiple occasions, supporting aircraft were held at their staging locations\nfor ATC clearances. Additionally, the offsite aircraft needed a higher bingo fuel in order\nto return to their staging location. During Configuration 1 events, one or possibly two more\nencounters per flight could have been completed if all aircraft were co-located.\nDuring Configuration 2 flights the remote staging location of NASA 608 was planned to\nadd the ability for a 3rd fire line run; however, the T-34 was too fuel limited and could not\nattempt a 3rd run regardless of staging location, and therefore negated the primary benefit\nof being located off site. Furthermore, the amount of troubleshooting conducted with\nNASA 608, LVC, and RGCS would have benefitted significantly from being collocated.\nRecommendation: Co-locate at AFRC for FT4 and subsequent flight test events.\n8.2.3 Low priority within R-2515 resulted in missed flight test opportunities.\nBoth test configurations were impacted by low prioritization. Configuration 2 operations\nwere more significantly impacted since the routes of flight for both ownship and intruder\naircraft utilized more of the airspace than Configuration 1 required and were therefore\nmore in conflict with other airspace users. Additionally, Configuration 2 flights started at\n1500L which was an impacted time of day (although this operation time was chosen at\nthe recommendation of USAF airspace management personnel).\n\n93\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nConfiguration 1 operations within the Mercury Spin and Four Corners work areas were\nmore contained within known UAS working areas and were therefore less impacted by\nother users. However, on some occasions the FT3 participating aircraft were restricted to\ncertain altitude blocks or lateral bounds which decreased the amount of encounters\ncompleted while the team waited for the traffic to clear. Configuration 1 flights started at\n0600L and, in general, operations occurring between 0600L to 0800L tend to experience\nlittle to no conflicts with other traffic.\nRecommendation: For complex routing like the Configuration 2 route, it would increase\nthe mission success rate if the route of flight remains clear of R-2515. Additionally,\nplanning for early morning operations (from 0600L to 0800L) will likely increase the\nsuccess rate for executing and completing the test as planned.\n8.2.4 Planning for nominal and off-nominal conditions was not clear or distinct enough.\nSome discussion in brief and de-brief was spent on the differences between Lost Link\nAltitude and Deconfliction Altitude during the briefing and de-brief. This resulted in some\nconfusion on which altitude the aircraft should fly after the encounter. Here are the\nexpectations and recommendations from the operations group:\nDeconfliction Altitude: This is the planned altitude listed on the card that all aircraft should\nbe at after completing the encounter and with TC coordination. This is nominal condition.\nLost Link Altitude: This is the altitude Ikhana will be at in a contingency event where the\naircraft loses link. It is designed to keep the aircraft predictable for the other intruder\naircraft for a short period of time prior to Ikhana proceeding to lost link holding points. This\nis an off-nominal condition and the altitude does not need to mirror that of nominal\nconditions.\n8.2.5 Haze, clouds and winds aloft.\nEnvironmental factors impacted FT3 in the following ways:\nHaze due to smoke from southern California wildfires reduced the visibility at the aircraft\noperating altitude enough that it was a contributing factor to the mission rule violation. It\nis of note that later in the flight visibility at higher altitudes was significantly better.\nClouds, broken layers, caused the cancellation of one Configuration 2 mission rehearsal\nflight. No other flights were significantly impacted.\nWinds aloft were sometimes greater than 30kts. Station keeping was affected for\nConfiguration 1 flights and made arriving at the IP on time challenging. For Configuration\n2 flights, the intercept was significantly impacted by high winds and in some cases caused\nmissed encounters.\n\n94\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nRecommendation: It is important for aircrew to assess the environmental conditions and\nmake recommendations to the TC. Should a haze or cloud layer decrease the visuals the\nTC needs to be informed and all participants can make accurate assessments and\ndecisions. In most cases a shift in test altitude would mitigate the haze layer problem. A\nmitigation for high winds aloft is to develop a tool for the TC/TD to determine push times\nthat accounts for winds aloft.\n8.2.6 Understanding success criteria and training operators was critical to mission\nsuccess.\nOver the course of FT3, the test team seemed to struggle to understand what the exact\nsuccess criteria was for each SUT. Since there were multiple SUTs, the success criteria\nwas not the same from one to another, and this situation was not always clear to the\nteam. For Configuration 1 the following were SUT: JADEM, Stratway+, Radar, TCAS, and\nCPDS. Each system utilized a different display. Training was conducted by each SUT\nresearcher, and with the researcher present during their respective flights, mission\nsuccess was maximized. It is noteworthy to point out that the training conducted by the\nCPDS researcher was the most effective and required the least amount of intervention\nduring the flight itself. In the other system cases, the researcher had to provide instruction\nin real-time to ensure correct data was being generated.\nRecommendation: In order to emulate the desired training, the operations team will\nprovide a template for future test events that identifies what training is required, a format\nfor presenting the material and a schedule of when it will be accomplished. Additionally,\na clear understanding of what the researcher expects out of the SUT will be explained to\nthe test team. An example of why proper training in required is in one case during FT3 a\nsystem was de-energized by the test team when the team, in fact, wanted to de-select a\ncomponent of the system.\n8.2.7 A separate truth source for positional data, TSPI, from each aircraft was not\navailable for post flight analysis.\nPost flight analysis of TSPI information was not a requirement for FT3. However, for\nhigher fidelity evaluation of the Radar, TCAS, and CPDS, a TSPI truth source such as\nDifferential Global Positioning System (DGPS) would have been beneficial. Additionally,\nthe time and geo-location sync from one data source to another was not easily and clearly\ndetermined resulting in significant post processing to sync all data sources.\nRecommendation: A truth data source should be considered standard equipment for any\nflight test operation. Incorporate DGPS or suitable TSPI data source on each intruder and\nownship aircraft. Ensure all data being collected is time synced.\n\n95\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n8.3 Researcher Observations\nThe following observations were noted and discussed with each research team. The\nfindings below are preliminary and may become outdated once FT3 data has been fully\nanalyzed.\n8.3.1 FT3 Configuration 1 successfully completed the major objectives for all SUT.\nAll stakeholders considered this phase a success and in many cases a first for their SUT.\nARC team members collected good data that will be used to update their simulation model\nand support future test efforts including PT6 (Part Task 6, V&V of MOPS) and FT4. The\ndata supports operational concept developments for aircraft in the cruise phase of flight.\nLaRC team members collected more data in one flight test event than had been collected\nin past simulated events. The data will be used to update their simulation model and help\ndevelop Phase I MOPS for SC-228 and eventually the FAA. Additionally, FT3 was the\nfirst time a multi-intruder encounter was conducted for these purposes. For LaRC, many\nsuccessful scenarios were completed and all objectives were met, and they will continue\nto find areas for improvement.\nCPDS teams consider FT3 a success at collecting several corner case scenarios that\nchallenge both the algorithm and aircrew judgment and decision making based off the\nCPDS displays. All data analyzed to date appears to correlate with simulation.\nThe TCAS and Radar stakeholders from GA-ASI both report good data collected for their\nsystems and intend to implement enhancements based off the data collected. The TCAS\nalerts presented to the crews were within TCAS specifications, but crews recommended\nsome user interface changes that better help them get instant SA once a TCAS message\nis displayed.\nThe HSI teams were not able to use the data from Configuration 2 flights for technical\nreasons still being analyzed.\n8.3.2 Both ARC and LaRC requested that all test aircraft keep heading and airspeed\nmore stable during future test events.\nBoth teams noted in preliminary data analysis that in some cases the aircraft would\nchange airspeed or heading in order to arrive at CPA at the planned time. Although this\nwas intended and permitted, up to 90 seconds from CPA the changes in aircraft state\nperturbed the researcher\xe2\x80\x99s desired data to a minimal extent. In follow-on test events the\nresearch teams request aircrew accept the aircraft conditions at the IP as long as they\nare within timing tolerances. One way to mitigate this is to add real-time monitoring of the\nownship and intruder speeds, heading, altitude, predicted CPA, etc. Further planning is\nrequired for FT4 to ensure accurate understanding of the success criteria and oncondition parameters will be constrained to help mitigate perturbations.\n96\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n8.3.3 VSCS displays did not function for ARC and LaRC.\nFor both teams the planned SUT display did not function correctly. The LaRC team\ndetermined that there was not enough granularity in the display for crews to make\naccurate heading change decisions. They reverted to the Stratway+ native displays on\nsubsequent flight days.\nThe ARC team used VSCS for all of their planned encounters, but for the encounters that\nprovided directive guidance, the display did not function correctly. Integration issues\nbetween AutoResolver and the VSCS display gave inaccurate headings and turn\ndirections to the crews, which resulted in skewed data for those runs that required the\naircrew to maneuver per the guidance. The ARC team did not have an alternative display\navailable. For FT4, resources should be assigned to ensure proper integration between\nAutoresolver and VSCS.\n8.3.4 Radar vertical speed indications were not filtered.\nThe SUT stakeholders that used the radar as a contributing senor were expecting filtered\nradar data. Their algorithms noted significant shifts in vertical speed that would have been\ndampened out if filtered. However, the GA-ASI radar team understood that filtering to be\ninherent to the system that used the data, as opposed to imbedded with the radar itself.\nSince there are no clear requirements for what a certified radar system should do, GAASI intends to implement a filtered data stream for FT4.\n8.3.5 An FT4 data collection plan is desired.\nSC-228 representatives as well as the FT3 research teams noted that the data collection\nplan and implementation for FT3 resulted in some inconsistencies that may be reduced\nwith a more detailed collection effort. As noted in Section 8.2.7, multiple data sources with\ndifferent time syncs needed to be post processed. Specifically, the data from N3GC was\na relative time sync as opposed to a GPS time. Planning for a common time sync and\ninstallation of a truth source data system for test aircraft should help mitigate this issue.\n\n97\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n9 Acronyms\nACAS\nAFRC\nAFTC\nAGL\nAIM\nARC\nATC\nAUTO\nC2\nCA\nCAT\nCBDR\nCCB\nCFG\nCNPC\nCOA\nCOMEX\nCPA\nCPDS\nCRM\nCSSA\nCST\nCTF\nCVSRF\nDAA\nDAIDALUS\nDATR\nDCP\nDD\nDEEC\nDET3\nDGPS\nDICES\nDME\nDRR\nDSRL\nEAFB\nEDM\nENC\n\nAirborne Collision Avoidance System\nArmstrong Flight Research Center\nAir Force Test Center\nAbove Ground Level\nAeronautical Information Manual\nAmes Research Center\nAir Traffic Control\nAutomatic\nCommand and Control\nCollision Avoidance\nCollision Avoidance Threshold\nConstant Bearing Decreasing Range\nComplex Control Board\nConfiguration\nControl and Non-Payload Communication\nCertificates of Authorization\nCommence Exercise\nClosest Point of Approach\nConflict Prediction and Display System\nCrew Resource Management\nCorrective Self-Separation Alert\nCombined Systems Test\nCombined Test Force\nCrew Vehicle Simulation Research Facility\nDetect And Avoid\nDetect and AvoID Alerting Logic for Uncrewed Systems\nDryden Aeronautical Test Range\nDryden Centerwide Procedure\nDecimal Degrees\nDigital Electronic Engine Control\nDetachment 3\nDifferential Global Positioning System\nType of Radio Panel used at NASA Armstrong\nDistance Measuring Equipment\nDue Regard Radar\nDistributed System Research Laboratory\nEdwards Air Force Base\nEngineering Development Module\nEncounter\n\n98\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nFAR\nFDDRL\nFINEX\nFL\nFM\nFSE\nFT3\nFT4\nFTE\nFV\nGA-ASI\nGCS\nGPS\nGRC\nGS\nHSI\nHUD\nHWY\nIASP\nID\nIFR\nIHITL\nILLA\nILLH\nIMU\nINS\nINT\nIP\nIT&E\nITAR\nJADEM\nKBFI\nKBFL\nKBFL\nKEDW\nKGS\nKIAS\nKPMD\nKTAS\nKVCV\nKVNY\nLaRC\nLOS\n\nFederal Aviation Regulation\nFlight Deck Display Research Laboratory\nFinish Exercise\nFlight Level\nFull Mission\nFlight Systems Engineer\nFlight Test 3\nFlight Test 4\nFlight Test Engineer\nFalconView\nGeneral Atomics Aeronautical Systems, Inc.\nGround Control Station\nGlobal Positioning System\nGlenn Research Center\nGroundspeed\nHuman Systems Integration\nHeads-Up Display\nHighway\nIntegrated Aviation Systems Program\nIdentification\nInstrument Flight Rules\nIntegrated Human In The Loop\nIkhana Lost Link Altitude\nIkhana Lost Link Heading\nInertial Measurement Unit\nInertial Navigation System\nIntruder\nInitial Point\nIntegrated Test and Evaluation\nInternational Traffic in Arms Regulations\nJava Architecture for DAA Extensibility and Modeling\nICAO airfield code. King Field in Seattle\nICAO airfield code. Meadows Field in Bakersfield\nICAO airfield code. Meadows Field in Bakersfield\nICAO airfield code. Edwards AFB\nKnots Groundspeed\nKnots Indicated Airspeed\nICAO airfield code. Palmdale airfield\nKnots True Airspeed\nICAO airfield code. Victorville\nICAO airfield code. Van Nuys\nLangley Research Center\nLine Of Sight\n99\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nLRO\nLVC\nMACS\nMC\nMIT\nMOA\nMOPS\nMP\nMR\nMSL\nNAS\nNASA\nNM\nNMAC\nNOTAM\nOWG\nOWN\nPIC\nPIRA\nPSSA\nPT6\nRA\nRAIF\nREH\nRF\nRGCS\nRSO\nRTB\nRTCA\nS/N\nSA\nSAA\nSAAP\nSAF\nSC-228 ToR\nSGT\nSME\nSOP\nSOR\nSPORT\nSS\nSSCA\nSSI\n\nLong Range Optics\nLive Virtual Constructive\nMulti Aircraft Control System\nMagnetic Course\nMassachusetts Institute of Technology\nMilitary Operating Area\nMinimum Operational Performance Standards\nManeuver Point\nMission Rule\nMean Sea Level\nNational Airspace System\nNational Aeronautics and Space Administration\nNautical Miles\nNear Mid-Air Collision\nNotice To Airmen\nOperations Working Group\nOwnship\nPilot In Command\nPrecision Impact Range Area\nPreventive Self-Separation Alert\nPart Task 6\nResolution Advisory\nResearch Aircraft Integration Facility\nRehearsal\nRadio Frequency\nResearch Ground Control Station\nRange Safety Officer\nReturn To Base\nRadio Technical Commission for Aeronautics\nScenario Number\nSituational Awareness\nSense And Avoid\nSense And Avoid Processor\nStand Alone Facility\nSpecial Committee-228 Terms of Reference\nStinger Gaffarian Technologies\nSubject Matter Expert\nStandard Operating Procedures\nSenior Operations Representative\nCall Sign for AFTC Radar Control Facility\nSelf-Separation\nSelf-Separation Corrective Alert (CSSA)\nSelf Assurance/Sense and Avoid Interoperability\n100\n\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\nSSPT\nSSWA\nSSWG\nSTM\nSUT\nTA\nTBD\nTC\nTCAS\nTD\nTECCS\nTFR\nTRACON\nTSPI\nUAS\nUSAF\nUTC\nV&V\nVBA\nVFR\nVID\nVMC\nVSCS\nVSM\nWP\nZOA\n\nSelf-Separation Proximate Traffic\nSelf-Separation Warning Alert\nSystem Safety Working Group\nSurveillance Tracking Module\nSystem Under Test\nTraffic Advisory\nTo Be Determined\nTest Conductor\nTraffic Collision and Avoidance System\nTest Director\nTest and Evaluation Command and Control System\nTemporary Flight Restriction\nTerminal Radar Approach Control Facility\nTime-Space-Position Information\nUnmanned Aircraft System\nUnited States Air Force\nCoordinated Universal Time\nVerification and Validation\nVisual Basic for Applications\nVisual Flight Rules\nVisual Identification\nVisual Meteorological Conditions\nVigilant Spirit Control Station\nVehicle Specific Module\nWaypoint\nOakland Air Route Traffic Control Center\n\n101\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n10 References\nDocument Number\nFT3-FTP-01\nDCP-O-025\nDCP-O-003\nEAFBI 13-100\nFT3 IT&E DMP-001\nNPR 7900.3\nOIEP SRD-01\nR-2508\nTitle 14 CFR Part 91\n\nDocument Title\nFT3 Flight Test Plan\nNASA Armstrong Aircrew Flight Operations Manual\nMission Control Procedure\nEdward AFB Instruction Flying and Airfield Operations\nFlight Test 3 Data Management Plan\nAircraft Management Operations\nOwnship and Intruder Equipage & Performance SRD\nR-2508 Complex Users Handbook\nGeneral Operating and Flight Rules\n\n102\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n11 Appendix A: Definition of Terms\nBlunder\n\nA planned vertical or horizontal maneuver performed by the intruder,\nownship or both aircraft that occurs at some point during the flight test\nencounter. The blunder maneuver is a technique by which the\nresearcher uses to obtain data required to refine algorithm parametric\nlogic.\n\nConfiguration 1\n\nThis test configuration investigates the advisories generated by the\nSelf Separation and Collision Avoidance Algorithm displays provided\nby NASA Ames, NASA Langley, or GA-ASI and fed by data from live\naircraft during flight. The configuration further investigates TCAS and\nDRR alerting. Flight Test Configuration 1 is defined into two distinct\ngroups (Configuration 1a and 1b). Configuration 1a involves flight test\nencounters using a low-speed, unmanned ownship aircraft.\nConfiguration 1b involves flight test encounters using a high-speed\nmanned ownship aircraft.\n\nConfiguration 2\n\nThe Full Mission test configuration is designed to connect virtual ATC\nand constructive aircraft processes running at NASA Ames with a live\nmanned intruder aircraft and a UAS surrogate ownship aircraft\ncontrolled by the research GCS located at NASA Armstrong. The UAS\nSurrogate aircraft is flown on a Visual Flight Rules (VFR) flight plan\nwith scenarios containing a mix of two live and several virtual manned\nInstrument Flight Rules (IFR) and VFR (squawking) aircraft.\n\nIntruder\n\nIntruder aircraft (when properly equipped) provide a target solution for\nthe self-separation algorithm under test. Both low speed, high speed,\nand multi-ship encounters are planned using intruder aircraft.\n\nOwnship\n\nOwnship aircraft provide the self-separation algorithm host solution for\ntesting airborne geospatial encounters with target (intruder) aircraft.\nThe ownship may be a UAS or UAS surrogate aircraft. Self-separation\nalerting solutions are presented to the ground control station pilot who\ndetermines the best course of action based on display alerting\nevaluation.\n\nMitigated\n\nFlight test encounters that are designed for the controlling UAS pilot to\neither manually respond to a self-separation or RA alert or monitor the\naircraft response during an automatic RA alert. Mitigated test\nencounters are typically planned with vertical, lateral, and timing flight\nsafety margins designed into the flight test encounters to help minimize\nthe potential for an inflight collision.\n\nUnmitigated\n\nFlight test encounters that due to adequate vertical offsets do not\nrequire an associated lateral offset for flight safety. Unmitigated\nencounters are non-maneuvering.\n\n103\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12 Appendix B: Redlined Flight Cards\nFlight cards (divided into sections by day) that were flown for Configuration 1 are shown\nin this Appendix. For Configuration 2, flights cards for each flight day are identical, and\ncan therefore be referenced earlier in this document in Figure 39, Figure 40, and Figure\n41.\n\n104\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.1 Flight 1 Redlined Flight Cards\n\n105\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.2 Flight 2 Redlined Flight Cards\n\n119\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.3 Flight 3 Redlined Flight Cards\n\n142\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.4 Flight 4 Redlined Flight Cards\n\n163\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.5 Flight 5 Redlined Flight Cards\n\n180\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.6 Flight 6 Redlined Flight Cards\n\n202\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.7 Flight 7 Redlined Flight Cards\n\n224\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.8 Flight 8 Redlined Flight Cards\n\n247\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.9 Flight 9 Redlined Flight Cards\n\n280\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.10 Flight 10 Redlined Flight Cards\n\n300\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n12.11 Flight 11 Redlined Flight Cards\n\nL12P\n\n318\nFlight Test Series 3: Flight Test Report, October 2015 \xe2\x80\x93 REVISION B\n\n'