b'National Aeronautics and Space Administration\n\nNASA Advanced Supercomputing\nFacility Expansion\nIndustry Day\n21 February 2017\n\nwww.nasa.gov\n\nBackground\nThe NASA Advanced Supercomputing (NAS) Division enables advances in high-end\ncomputing technologies and in modeling and simulation methods to tackle some of the\ntoughest science and engineering challenges facing NASA today.\nThe name "NAS" has long been associated with leadership and innovation throughout\nthe high-end computing (HEC) community. We play a significant role in shaping HEC\nstandards and paradigms, and provide leadership in the areas of large-scale InfiniBand\nfabrics, Lustre open-source filesystems, and hyperwall technologies.\nWe provide an integrated high-end computing environment to accelerate NASA missions\nand make revolutionary advances in science. Pleiades, a petaflop-scale supercomputer,\nis used by scientists throughout the U.S. to support NASA missions, and is ranked\namong the most powerful systems in the world.\nOne of our key focus areas is in modeling and simulation to support NASA\'s real-world\nengineering applications and make fundamental advances in modeling and simulation\nmethods.\nNAS Facility Expansion - Industry Day\n\n2\n\nGoals\nProvide effective, production-level HEC resources and services to enable significant,\ntimely impacts across NASA\'s mission areas of Earth and space science, aeronautics\nresearch, and human and robotic space exploration.\nAdvance the state-of-the-art in HEC technologies and techniques to meet NASA\'s\ncontinuously growing requirements for advanced computational modeling, simulation,\nand analysis.\nDesign and conduct high-fidelity numerical simulation studies for NASA\'s aerospace\nengineering programs, supporting both mission-critical and system design decisions.\nPursue fundamental advances in numerical methodologies and algorithms, physical\nmodel enhancements, and application code development for large-scale simulations of\ninterest to NASA.\nIntegrate and operate IT systems to enable successful missions for specific NASA\nscience and engineering projects.\nNAS Facility Expansion - Industry Day\n\n3\n\nWhere We Fit In NASA\nThe NAS Division is part of the Exploration Technology Directorate at the NASA Ames\nResearch Center. The Directorate\xe2\x80\x99s mission is to create innovative and reliable\ntechnologies for NASA missions.\nNAS operates NASA\xe2\x80\x99s High-End Computing Capability Project, which is funded through\nthe agency\xe2\x80\x99s High-End Computing Program and the Space Environments Testing\nManagement Office.\n\nNAS Facility Expansion - Industry Day\n\n4\n\nNational Aeronautics and Space Administration\n\nHECC Requirements\n\nwww.nasa.gov\n\n5\n\nNASA\xe2\x80\x99s HEC Requirements: Capacity\nHEOMD (engineering-related work) require HEC resources that can handle\nlarge numbers of relatively-low CPU-count jobs with quick turnaround times.\nThe formation of vortex filaments and their rollup into a single, prominent vortex at each tip on\na Gulfstream aircraft\n\nOver 1500 simulations utilized ~ 2\nmillion processor hours to study\nlaunch abort systems on the next\ngeneration crew transport vehicle\n\nOver 4 million hours were used over a 4\nmonth project to evaluate future\ndesigned of the next generation launch\ncomplex at the Kennedy Space Center\n\nNAS Facility Expansion - Industry Day\n\n6\n\nNASA\xe2\x80\x99s HEC Requirements: Capability\nARMD and SMD (aeronautics and science related work) require HEC resources that can\nhandle high fidelity relatively-large CPU-count jobs with minimal time-to-solution. Capability\nenables work that wasn\xe2\x80\x99t possible on previous architectures.\nFor the first time, the Figure-of-Merit has been\npredicted within experimental error for the V22\nOsprey and Black Hawk helicopter rotors in\nhover, over a wide range of flow conditions\n\nNASA is looking at the oceans, running 100\xe2\x80\x99s\nof jobs on Pleiades using up to 10,000\nprocessors. Looking at the role of the oceans\nin the global carbon cycle is enabled by access\nto large processing and storage assets\n\nTo complete the Bolshoi simulation, which traces how the largest\ngalaxies and galaxy structures in the universe were formed billions of\nyears ago, astrophysicists ran their code for 18 straight days, consuming\nmillions of hours of computer time, and generating massive amounts of\ndata\n\nNAS Facility Expansion - Industry Day\n\n7\n\nNASA\xe2\x80\x99s HEC Requirements: Time Critical\nNASA also has need for HEC resources that can handle time-sensitive\nmission-critical applications on demand (maintain readiness)\n\nKEPLER\nReEntry\n\nKEPLER\nUAVSAR produces polarimetric (PolSAR)\nand interferometric (repeat-pass InSAR)\ndata that highlight different features and\nshow changes in the Earth over time\n\nHECC enables the enormous planetary transit searches to be\ncompleted in less than a day, as opposed to more than a\nmonth on the Kepler SOC systems, with significantly\nimproved accuracy and effectiveness of the software pipeline\n\nStorm Prediction\nNAS Facility Expansion - Industry Day\n\n8\n\nNational Aeronautics and Space Administration\n\nHECC Current Assets\n\nwww.nasa.gov\n\n9\n\nFacilities\n\n.\n\nN258\n\nR&D088\n\nN233A\n\nNAS Facility Expansion - Industry Day\n\n10\n\nHECC Traditional Computer Floors\n230\n16,800\n\n131\n2,080\n\nVisualization Lab\n\nISU 6\n\nBuilding 258\n\nSGI\r\nD-Rack\n\nSGI\r\nD-Rack\n\nStorage\n\nSGI\r\nD-Rack\n\nSGI\r\nD-Rack\n\nPDU\r\n37\n\nN258-Rm 125\r\n\r\n\r\n\r\n\r\nCurrent Diagram\n\nhyperwall\r\n(128-screen display)\n\nhyperwall\r\nVisualization \r\nSystems\n\n0\n\nSGI\r\nD-Rack\n\nSGI\r\nD-Rack\n\nSGI\r\nD-Rack\n\nSGI\r\nD-Rack\n\nDDN\r\nRAID\n\n2\n\n4\n6\nScale in Feet\n\n8\n\n10\n\nAMES RESEARCH CENTER\r\nMOFFETT FIELD, CA 94035\n\nN258 COMPUTER ROOM 125\nPDU\r\n23\n\nUpdated\n\nUpdated By\n\nJune 13, 2014\n\nChris Henze\r\nVisualization\n\nChris Buchanan\n\n125\n1,275\n\n190\n6,900\n\n189\n2,700\n\nJohn Parks\r\nFacilities\n\nNAS Facility Expansion - Industry Day\n\n11\n\nHECC Modular Computer Floors\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\nG\n\nH\n\nI\n\nJ\n\nK\n\nX\n\nX\n\n2800kVA\nTransformer\n13.8KV/415V\n\nSwitchgear & 5 kVA\nTransformer\n415V/120V\nW\n\nR&D 088\n16,800\n\nWater\nControl\n\nElectrical\nPanel\n\nW\n\nV\n\nV\n\nEvaporative Media & Filter Wall\nFan Wall\nUser Interface\n\nCold Aisle\n016 015 014 013 012 011 010 009 102\n\nBroadwell\n\nI/O\n\nU\n\nBlank\n\nU\n\nNAS HPC Facility\nModular Supercomputing Facility Floor Diagram\n\nHot Aisle\n\nMSF Diagram\nI/O\n\n008 007 006 005 004 003 002 001 101\n\nBlank\n\nBroadwell\n\nT\n\nT\n\nNotes:\n1. Total Modular Area = 960 sq.ft.\nComputer Room Area = 438 sq.ft.\n\nCold Aisle\n\nFan Wall\nEvaporative Media & Filter Wall\n\nS\nWater\nControl\n\nS\n\n0\n\n2\n\n4\n6\n8\nScale in Feet\n\n10\n\nAMES RESEARCH CENTER\nMOFFETT FIELD, CA 94035\nR\n\nN258 COMPUTER ROOM 230\n\nR\nUpdated\n\nUpdated By\n\nNovember 8, 2016\n\nProject Manager\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\nG\n\nCSS\n\nChris Tanner\n\nNetworks\n\nFacilities\n\nH\n\nNAS Facility Expansion - Industry Day\n\n12\n\nPleiades Specifics\n161 SGI Racks (7.58 PF; 936 TB; 32,308 SBUs/hr)*\n158 SGI Altix ICE X Racks (7.21 PF; 931 TB; 32,134 SBUs/hr)\n\xe2\x80\xa2 26 racks of ICE-X with Intel Xeon processor E5-2670 (Sandy\nBridge):\n623 TF; 59.9 TB; 3,407 SBUs/hr\n\xe2\x80\xa2 75 racks of ICE-X with Intel Xeon processor E5-2680v2 (Ivy\nBridge):\n2.419 PF; 345.6 TB; 13,608 SBUs/hr\n\xe2\x80\xa2 29 racks of ICE-X with Intel Xeon processor E5-2680v3 (Haswell):\n2.004 PF; 267.3 TB; 6,974 SBUs/hr\n\xe2\x80\xa2 28 racks of ICE-X with Intel Xeon processor E5-2680v4\n(Broadwell):\n2.167 PF; 129.0 TB; 8,145 SBUs/hr\n3 SGI Coyote Racks (371 TF; 5 TB; 175 SBUs) (note, accelerators\nare not counted in SBU numbers)\n\xe2\x80\xa2 2 racks of Intel Xeon processor E5-2670 (Sandy Bridge) and\nNvidia K40 graphic processors: 296 TF; 4 TB; 117 SBUs\n\xe2\x80\xa2 1 rack of Intel Xeon processor E5-2670 (Sandy Bridge) and Intel\nXeon Phi 5110P accelerator processor: 75 TF; 1 TB; 58 SBUs\nCores\n\xe2\x80\xa2 22,944 Intel Xeon processors (246,048 cores)\n\xe2\x80\xa2 64 Nvidia GPUs (184,320 cores)\n\xe2\x80\xa2 64 Intel Xeon Phi processors (3,804 cores)\n* 1 SBU equals 1 hour of a Pleiades Westmere 12-core node.\n\nNodes\n\xe2\x80\xa2 11,376 nodes (dual-socket blades)\n\xe2\x80\xa2 64 nodes (dual-socket + GPU)\n\xe2\x80\xa2 32 nodes (dual-socket + dual-Phi)\n\xe2\x80\xa2 14 Login nodes\nNetworks\n\xe2\x80\xa2 Internode: Dual-plane partial 11D hypercube (FDR)\n\xe2\x80\xa2 Gigabit Ethernet Management Network\n\nNAS Facility Expansion - Industry Day\n\n13\n\nElectra Specifics\n16 SGI Racks (1.24 PF; 147 TB; 4,654 SBUs/hr)\n\xe2\x80\xa2 16 racks of ICE-X with Intel Xeon processor E5-2680v4\n(Broadwell): 1.24 PF; 147 TB; 4,654 SBUs/hr\nCores\n\xe2\x80\xa2 2,304 Intel Xeon processors (32,256 cores)\nNodes\n\xe2\x80\xa2 1,152 nodes (dual-socket blades)\nNetworks\n\xe2\x80\xa2 Internode: Dual-plane fully-populated 7D hypercube\n(FDR)\n\xe2\x80\xa2 Gigabit Ethernet Management Network\n\xe2\x80\xa2 Metro-X IB extenders for shared storage access\n\nNAS Facility Expansion - Industry Day\n\n14\n\nMerope Specifics\n56 SGI Altix ICE X \xc2\xbd Racks (252 TF; 86 TB; 1,792 SBUs)\n\xe2\x80\xa2 56 \xc2\xbd-racks of 8400EX with Intel Xeon processor E5670\n(Westmere): 252 TF; 86 TB; 1,792 SBUs\n3,584 Intel Xeon processors (21,504 cores)\n\xe2\x80\xa2 3,584 six-core Westmere\n\xe2\x80\xa2 2.93 GHz processors (21,504 cores)\n\nNAS Facility Expansion - Industry Day\n\n15\n\nEndeavour Specifics\n32 TF constellation-class supercluster\n2 SGI Ultra Violet 2 nodes with Intel Xeon E5-4650L 2.6\nGHz processors\n\xe2\x80\xa2 One 512-core node with 2 TB globally addressable\nRAM (Endeavour1)\n\xe2\x80\xa2 One 1,024-core node with 4 TB globally addressable\nRAM (Endeavour2)\nInterconnect\n\xe2\x80\xa2 Intranode: NUMALink-6 (enable large SSI)\n\xe2\x80\xa2 Dual-Plane QDR InfiniBand connectivity into Pleiades\ninfrastructure\n\xc2\xbb 1 connection from each node into IB0 for TCP traffic\n(pbs, login, \xe2\x80\xa6)\n\xc2\xbb IB1 is for I/O traffic to the Lustre file systems.\nEndeavour1 has 3 connections and Endeavour2\nhas 4 connections.\n\xe2\x80\xa2 10 Gb Ethernet can be used for WAN traffic\n\nNAS Facility Expansion - Industry Day\n\n16\n\nAdvanced Visualization: hyperwall and CV\nSupercomputing-scale visualization system to handle\nmassive size of simulation results and increasing\ncomplexity of data analysis\n\xe2\x80\xa2 8x16 LCD tiled panel display (23 feet x 10 feet)\n\xe2\x80\xa2 245 million pixels\n\xe2\x80\xa2 Debuted as #1 resolution system in the world\n\xe2\x80\xa2 In-depth data analysis and software\nTwo primary modes\n\xe2\x80\xa2 Single large high definition image\n\xe2\x80\xa2 Sets of related images (e.g. parameter study)\nHigh-bandwidth to HEC resources\n\xe2\x80\xa2 Concurrent Visualization: Runtime data streaming allows\nvisualization of every simulation time step - ultimate\ninsight into simulation code without increase in traditional\ndisk I/O\n\xe2\x80\xa2 Traditional Post-Processing: Direct read/write access to\nPleiades filesystems eliminates nee for copying large\ndatasets\nGPU-based computational acceleration R&D for appropriate\nNASA codes\nNAS Facility Expansion - Industry Day\n\n17\n\nQuantum Computing: D-Wave Two\xc3\xa4 System\nCollaboration between NASA / Google / USRA\nD-Wave 2X Installed at NAS\n\xe2\x80\xa2 Washington processor \xe2\x80\x93 1,097 qubits (quantum bits \xe2\x80\x93\nniobium superconducting loops encoding 2 magnetic\nstates)\n\xe2\x80\xa2 Physical characteristic\n\xc2\xbb 10 kg of metal in vacuum at 15 mK\n\xc2\xbb Magnetic shielding to 1 nanoTesla (50,000x less than\nEarth\xe2\x80\x99s magnetic field)\n\xc2\xbb Uses 12 kW electrical power\n\xe2\x80\xa2 Focused on solving discrete optimization problems via\nquantum annealing\n\nNAS Facility Expansion - Industry Day\n\n18\n\nStorage and Archive\nLustre File Systems (39.6 PB in 7 file systems)\n\xe2\x80\xa2 DDN\n\xc2\xbb 14 DDN RAID Systems, 9.9 PB total, 3 facility-wide file systems\n\xe2\x80\xa2 NetApp\n\xc2\xbb 62 RAID Systems, 29.7 PB total, 4 facility-wide file systems\n\nNFS File Systems\n\xe2\x80\xa2 3 home file systems 3.7 TB total\n\xe2\x80\xa2 2 facility-wide scratch file systems 59 TB & 1 PB\n\xe2\x80\xa2 .4 PB for NEX\n\nArchive System\n\xe2\x80\xa2 490 PB Maximum Capacity\n\xe2\x80\xa2 6 T950 Spectra Logic Libraries\n\nAll storage systems are available to all of the\nproduction assets\n\nNAS Facility Expansion - Industry Day\n\n19\n\nHECC Growth\nHECC Growth\n7000\n6000\n5000\n\nLargest HECC\nLINPACK Result\n\n4000\n3000\n2000\n1000\n0\n\nNAS Facility Expansion - Industry Day\n\n20\n\nNational Aeronautics and Space Administration\n\nHECC Services\n\nwww.nasa.gov\n\nHECC Conducts Work in Four Major Technical Areas\nSupercomputing\nSystems\n\nProvide computational power, mass\nstorage, and user-friendly runtime\nenvironment through continuous\ndevelopment of management tools, IT\nsecurity, systems engineering\nHECC\n\nData Analysis and\nVisualization\n\nCreate functional data analysis and\nvisualization software to enhance\nengineering decision support and\nscientific discovery by\nincorporating advanced\nvisualization technologies\n\nApplication\nPerformance and User\nProductivity\nFacilitate advances in science and\nengineering for NASA programs by\nenhancing user productivity and code\nperformance of high-end computing\napplications of interest\n\nNetworking\nProvide end-to-end high-performance\nnetworking analysis and support to\nmeet massive modeling and\nsimulation distribution and access\nrequirements of geographically\ndispersed users\n\nSupporting Tasks\nFacility, Plant Engineering, and Operations: Necessary engineering and facility support to ensure the safety of HECC assets and staff\nInformation Technology Security: Provide management, operation, monitoring, and safeguards to protect information and IT assets\nUser Services: Account management and reporting, system monitoring and operations, first-tier 24x7 support\nInternal Operations: NASA Division activities hat support and enhance the HECC Project areas\nNAS Facility Expansion - Industry Day\n\n2\n2\n\nNational Aeronautics and Space Administration\n\nNAS Facility Expansion\n\nwww.nasa.gov\n\nNAS Facility Expansion - Industry Day\n23\n\nWhy are We Doing This\nThe calculation used to be very simple\xe2\x80\xa6\n\xe2\x80\xa2 When the cost of maintaining a group of nodes for three years exceeded the cost to\nreplace those nodes with fewer nodes that did the same work, we replaced them.\nNow, not so much\xe2\x80\xa6\n\xe2\x80\xa2 We look at the total computing our users get and procure new nodes within our budget\nand remove enough nodes to power and cool the new nodes.\n\xe2\x80\xa2 This means that we are not able to actually realize all of the expansion we are paying for.\n\nNAS Facility Expansion - Industry Day\n\n24\n\nBut That\xe2\x80\x99s Not All\nOur computer floor is limited by power and cooling\nOur Current Cooling System\n\xe2\x80\xa2 Open Air Cooling Tower with 4 50HP pumps\n\xe2\x80\xa2 4 450 Ton Chillers\n\xe2\x80\xa2 7 pumps for outbound chilled water\n\xe2\x80\xa2 4 pumps for inbound warm water\nOur Electrical System\n\xe2\x80\xa2 Nominally the facility is limited to 6MW\n\xe2\x80\xa2 20% - 30% is used for cooling\n\xe2\x80\xa2 4MW \xe2\x80\x93 5MW for computing\nNAS Facility Expansion - Industry Day\n\n25\n\nN258 Cooling Flow Chart\nTypical Operation:\nAir Handlers for Bldg AC\n162.5 Hp Total, Supply &\nReturn Fans\n\nAHU-2\n48000 cfm\n\nTypical Operation:\n12 ISUs Operate,\n5 ISUs Redundant\n22.5 Hp each\n30000 CFM each\n\nSGI\nICEX\nRack\n\nISU-1 to ISU-4\n3400 CFM\n\nAHU-3\n18000 cfm\n\nSGI\nICEX\nRack\n\nTypical Operation:\n163 Compute Racks with\nDoor Mounted Radiators\n25 KW/Rack\n10 GPM/Rack\n\nSGI\nICEX\nRack\n\nISU-8\n30000 CFM\n\nBldg A/C\n\n-THRU-\n\nTypical Operation:\n1 ISU Operates,\n22.5 Hp\n24000 CFM\n\nISU-24\n30000 CFM\n\nRoom 230\nMain Computer Floor\n\nRooms 107,143,\n205,257\n\nCHWP-5\n15 Hp\n\nCHWP-9\n50 Hp\n\nCHWP-6\n15 Hp\n\nCHWP-11\n50 Hp\n\nCHWP-10\n50 Hp\n\nCHWP-7\n50 Hp\n\nTypical Operation:\n2 ISUs Operate,\n3 Hp each\n8250 CFM each\n\nTypical Operation:\n2 ISUs Operate,\n2 Hp each\n4700 CFM each\n\nISU-6A\n24000 CFM\n\nISU-5 & 5A\n8250 CFM\n\nISU-7 & 30\n4700 CFM\n\nRoom 125\nHyperwall\n\nRoom 133\nCOMM Room\n\nRoom 226\nFishbowl\n\nCHWP-8\n50 Hp\n\n1 MWh\n50,000 gallons/day\nOne Pump Operates,\nOne Pump Redundant\n\nOne Pump Operates,\nOne Pump Redundant\n\nTwo Pumps Operate,\nOne Pump Redundant\n\nCHWP-2\n25 Hp\n\nCHWP-3\n25 Hp\n\nCHWP-1\n25 Hp\n\n4200 GPM\n\nCHWP-4\n25 Hp\n\nFour Pumps Operate\n\nChiller 4\n450 Tons\n\nCTF-1\n20 Hp\n\nCTF-2\n20 Hp\n\nCFT-3\n20 Hp\n\nChiller 3\n450 Tons\n\nChiller 2\n450 Tons\n\nEvaporator\n\nCondenser\n\nEvaporator\n\nCondenser\n\nEvaporator\n\nCondenser\n\nEvaporator\n\nNAS HECC\nCondenser\n\nAHU-1\n57400 cfm\n\nTypical Operation:\n4 ISUs Operate,\n1.5 Hp each\n3400 CFM each\n\nChiller 1\n450 Tons\n\nBuilding N258\n\nTypical Operation:\nThree Chillers Operate,\nOne Chiller Redundant\n200-250 KW each\n1400 GPM ea. Evaporator\n1750 GPM ea. Condenser\nDelta T = 7F\n\nN258 Cooling Flow Chart\n\nCFT-4\n20 Hp\n\nTypical Operation:\nFour Pumps Operate,\nFour Fans Operate\n5200 GPM\nDelta T = 7F\nNew Cooling Tower\nConstruction Complete\nMay 2016-CWP pumps\nwill be 100 Hp each.\n\nAMES RESEARCH CENTER\nMOFFETT FIELD, CA 94035\n\nBuilding N258\nUpdated\n5200 GPM\nCWP-1\n50 Hp\n\nCWP-2\n50 Hp\n\nCWP-3\n50 Hp\n\nUpdated By\n\nJune 25, 2015\n\nChris Tanner\n\nCWP-4\n50 Hp\nProject Manager\n\nCSS\n\nNetworks\n\nFacilities\n\nNAS Facility Expansion - Industry Day\n\n26\n\nDCU-20\n\nNAS Facility Expansion - Industry Day\n\n27\n\nEarly Energy Impact\n16 Computer Racks\n(1152 Nodes)\nWater Utilization Per Year\nElectricity per year\n\nExisting Facility\n\nDCoD-20 Facility\n\n1,460,000 gal*\n\n8,820 gal**\n\n3,363,840 kwh\xc2\xb0\n\n2,915,328 kwh\xc2\xb0\xc2\xb0\n\n% Savings\n99.4%\n18%(overall)\n88%(cooling)\n\n* Assumes 16 racks represent 8% of facility load\n** Adiabatic cooling required 7 days for 5 hours each day\n\xc2\xb0 1.26 PUE\n\xc2\xb0\xc2\xb0 1.03 PUE\n\nNAS Facility Expansion - Industry Day\n\n28\n\nProgress to Date\nWe commissioned a study to evaluate the alternatives\nWe procured and installed prototype modular data center\nWe received approval to move forward with the full modular center\nWe\xe2\x80\x99re expanding the prototype\nWe\xe2\x80\x99ve begun the procurement activities for the full module\n\nNAS Facility Expansion - Industry Day\n\n29\n\nSchedule\nMilestone\n\nActual/Target Date(s)\n\nRFI released to vendors\n\n16 January 2017\n\nRFI responses due\n\n15 February 2017\n\nIndustry Day at NAS\n\n21 February 2017\n\n90-Minute vendor presentations\n\n22 February \xe2\x80\x93 1\nMarch 2017\n\nRelease of Draft Request for Proposal\n\n7 March 2017\n\nRelease of Request for Proposal\n\n1 May 2017\n\nProposals due\n\n3 July 2017\n\nAward made\n\n8 October 2017\n\nNAS Facility Expansion - Industry Day\n\n30\n\n16-Module Deployment\n\nNAS Facility Expansion - Industry Day\n\n3\n1\n\nSite Location\n\nNFE Site\nLocation\n\nN258\nMSF\n\nPrepared Site\n250 ft x 180 ft\n3 \xc2\xbd ft of Vertical, Engineered Fill\n(1\xc2\xbd ft Above DeFrance Road)\nSite Surrounded by DeFrance Rd or\nFire Access Road\nRamp to Top of Elevated Site from\nDeFrance Road\n25 kV Switchgear yard at Southwest\nCorner of Site\nWater Main Point of Connection at\nSouthwest Corner of Site\n\nPrepared Site Utilities\nElectrical at Site\n25 kV Switchgear Yard (40\xe2\x80\x99 x 12\xe2\x80\x99)\n\xe2\x80\xa2 Four 25 kV Vacuum Circuit Breakers will Distribute up to\n15 MVA at 24.9 kV to Step-Down Transformers used in\nImproved Site\n\xe2\x80\xa2 Power Meters installed on each Vacuum Circuit Breaker\n\xe2\x80\xa2 Site Low Voltage Power\n\xc2\xbb\n\n1 Additional 25 kV VCB for Site Power\n\n\xc2\xbb\n\n400 A Panelboard\n\n4-inch Water Main capable of 200 GPM at 40 psi\nat Point of Connection\n\xe2\x80\xa2 RPZ Backflow Preventer & Water Meter Installed in 4-inch\nWater Line\n\nSewer & Storm Drain Piping Installed to edge of\nPrepared Site\n\n150 kVA Transformer, 24.9kV/208V, 3 Phase, 4 wire\n\n\xc2\xbb\n\nWater at Site\n\nCommunications at Site\nData to N258 will be Provided by Conduits &\nManholes\nCommunication Conduits will Terminate at\nComm Manhole in Center of Prepared Site\nFiber Optic Procurement & Installation by NASA\npersonnel\n\nCurrent Approach\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\nOur goal is to provide the vendor community the flexibility to propose the solution\nthat best showcases their technology.\nThe solution must provide the facility expansion to meet the initial deployment with a\nplan to deploy up to ~18,500 nodes.\nNASA\xe2\x80\x99s key metric continues to be a measurement of work represented by SBUs. Our\ncurrent projected SBU growth by fiscal year (Pleiades has 32,578):\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\xe2\x80\xa2\n\n\xe2\x80\xa2\n\xe2\x80\xa2\n\nFY18 \xe2\x80\x93 15,481\nFY19 \xe2\x80\x93 15,481\nFY20 \xe2\x80\x93 18,725\nFY21 \xe2\x80\x93 18,725\nFY22 \xe2\x80\x93 22,650\n\nNASA will provide the interfaces to the existing file systems providing pairs of MetroX\nInfiniBand extenders.\nThere is no storage associated with this procurement.\n\nNAS Facility Expansion - Industry Day\n\n35\n\nQuestions\n\nhttp://www.nas.nasa.gov/hecc\n\nNAS Facility Expansion - Industry Day\n\n3\n6\n\n'