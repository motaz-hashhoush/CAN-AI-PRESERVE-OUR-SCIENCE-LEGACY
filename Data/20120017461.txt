b'NASA INTEGRATED MODEL-CENTRIC ARCHITECTURE {NIMA)\nMODEL USE AND RE-USE\nMike Conroy & Rebecca Mazzone, Kennedy Space Center\nWei Lin, Ames Research Center\n\nABSTRACT\nThis whitepaper accepts the goals, needs and objectives of NASA\'s Integrated Model-centric Architecture (NIMA);\nadds experience and expertise from the Constellation program as well as NASA\'s architecture development efforts;\nand provides suggested concepts, practices and norms that nurture and enable model use and re-use across\nprograms, projects and other complex endeavors. Key components include the ability to effectively move relevant\ninformation through a large community, process patterns that support model reuse and the identification of the\nnecessary meta-information (ex. history, credibility, and provenance) to safely use and re-use that information.\n\nACKNOWLEDGEMENTS\nThe authors brought decades of modeling and simulation experience to this development effort. They hold or held\nleadership roles in previous NASA design environment initiatives, Constellation\'s (CxP) system engineering and\nintegration organization, ground operations simulation, Orion modeling and simulation, NASA\'s system\narchitecture and tool development initiatives, and various engineering and development activities spanning the\ncomplete system lifecycle.\n\nThey also called heavily upon their relationships across NASA, in industry and in\n\nacademia to better explore the past as well as plan for the future. Special thanks are in order for Jamie Adams and\nLinda Bromley of the Johnson Space Center for guidance and encouragement throughout the process, and to\nMartin Steele and Gary Mosier for sharing their expertise and helping to map the necessary model-based\nprocesses to both NASA Standard 7009 and the associated guide book.\n\n1\n\nNIMA OBJECTIVES\nNASA believes that the ability to make more informed and timely decisions is a key enabler for reducing cost and\nimproving schedule, product and workforce performance. NASA also believes that the first step toward more\ninformed and timely decision making is a move from a document-centric design approach to a model-centric\ndesign architecture across the Agency.\nNASA has 4 model-centric architecture goals :\nGoa/1\n\nIncrease system affordability through use of a model-centric architecture\n\nGoa/2\n\nAchieve interoperability within and among programs, projects, centers and external\npartners through use of a model-centric architecture\n\nGoa/3\n\nInform/train invigorate workforce on model-centric architecture\n\nGoa/4\n\nImprove product quality and success through use of a model-centric architecture\n\nThe NASA Integrated Model-Centric Architecture (NIMA) initiative is charged with achieving these goals. NIMA\nspans four technical communities of practice: Model-Based Systems Engineering (MBSE), Modeling & Simulation\n(M&S), Computer Aided Design (CAD), and Product Data & Lifecycle Management (PDLM).\nNIMA will attempt to formalize and infuse a model-based design and development architecture throughout the\nNASA engineering community by incorporating successful tools and practices from industry, NASA centers and\nother government organizations into the NASA engineering culture . A key practice is the application, and informed\nreapplication, of models and simulations to meet the design and development needs of the complex lifecycles\nassociated with advanced space systems.\n\nOther components include representation frameworks, executive\n\nguidance, NASA Standard 7009, tools, supporting infrastructure, other standards, processes, training, Product Data\nManagement (PDM), Product Lifecycle Management (PLM), partnerships, and human capital management.\nThis document specifically focuses on application and reapplication of models and simulations throughout the\nsystem development lifecycle. It calls upon success in industry, academia, the space community and Defense, as\nwell as past NASA success in complex programs and the existing NASA system engineering lifecycle. It is focused on\nproviding an overall use and reuse philosophy, associated meta-methods, information access and linkage to\nincreasingly detailed information as it emerges. Specific attention is paid to NASA Standard 7009 elements that\nenable and empower informed reuse, as well as variations in reuse methodologies associated with the different\nlifecycle phases (concept development to utilization to disposal).\n\nII \xe2\x80\xa2\n\n10 Deep\n\nThought~,\n\nMODEL USE AND RE-USE GOALS\npractitioners in the fields of engineering, software development,\nmodeling, simulation, systems engineering and management.\nTheir goals were to :\n\nQ\n\nmodel\n\n"Before~ can be reusable it first\nhas to be usable."\n-- Ralph Johnson, co-author of Design Patterns:\nElements of Reusable Object-Oriented Software\n\nIdentify and\nassociated\n\n~\n\nHigh Standards\n\nThe NIMA Use and Re-Use (URU) Team is comprised of\n\n1.\n\n\'\n\nAbout Soft\xc2\xb7\xe2\x80\xa2vare\n\nclassify goals, needs and\n\nwith\n\nmodel\n\nand\n\nsimulation\n\nobjectives\nuse\n\nand\n\nsubsequent re-use, broken out by product lifecycle\nphase and leadership role, to both frame discussion and\n\nBeseJine\xe2\x80\xa2\n\nenable recommendations that will meet global needs as well as local.\n\n2\n\n2.\n\nCapture modeling and simulation processes and practices from industry, academia, professional societies\nand other government agencies in order to provide a better understanding of efforts to date and enable\nmaximum reapplication of good ideas.\n\n3.\n\nPublish the results, by leadership role and life cycle phase, to identify gaps for future research and\ndevelopment, to document our accomplishments and better plan for the next accomplishments.\n\nAs Engineers, Designers, Managers, Inventors, Technologists and Humans we use models and simulations to\nunderstand systems and support our decisions. We model things, be they systems or processes or items, to better\nunderstand what they are. During this process, we build the model to the detail necessary to meet our needs.\nOnce we have a model that seems to represent what we want to study, with sufficient detail for the study, we\nsimulate how the model, and usually several other models, behave together. This simulation should be performed\nwith a level of rigor that is commensurate with the criticality of the study. Through this process we are able to gain\nand preserve insight, increase understanding, and subsequently make better decisions. It is hoped that when we\nmake these decisions we fully understand the level of detail and fidelity, or lack thereof, applied to creating the\ninformation (models and simulations) upon which our decisions are based.\n\nThe level of detail, fidelity and\n\nprocesses used combine to represent the overall credibility of the results. NASA Standard 7009 helps formalize the\nrepresentation of this credibility so it may be incorporated into future decisions.\nComplex endeavors are often undertaken by large teams with varied relationships. Any study of model use and reuse, as applied to large teams performing complex activities, would be incomplete at best if it did not address the\nroles played by the various team members, the rules associated with those roles and how those roles change as\nthe endeavor matures. To this end, we have identified 5 major roles associated with large complex activities.\nThese roles are introduced in the table below:\nRole:\n\nAssociated With:\n\nExecutive\n\nMission and policy development, partnerships, resources, plans, overall goals of\nthe endeavor\n\nArchitecture\nDevelopment\n\nConcept development and maturation, operations concepts, initial milestones,\nneeds, parametric analysis and cost estimation for the identified alternatives\n\nProgram Development\n\nConcept of operations, systems model, interfaces, system of systems analysis,\nprogram and project milestones, programmatic objectives, cost and schedule\nperformance, overall system integration\n\nProject Development\n\nSystem engineering, systems engineering, concepts of operations, interfaces,\ncost and schedule performance, requirements\n\nEngineering\n\nDetailed system designs, system performance, cost performance, schedule\nperformance, system integration activities, deliveries, alternatives, logistics,\nmanufacturing\n\nTABLE 1: ROLES FOR COMPLEX ACTIVITIES\n\n3\n\nIf we take the above roles and map them across lifecycle phases (Pre-A to F), we can represent their participation\nin the various phases (concept through operations and disposal) as well as their viewpoint (100,000 foot level\ndown to 1 foot level), presence in the product teams, and their overall representation in the larger team. This\nmapping is shown in Figure 1. The relevant NASA gateway reviews are included at the bottom and include: the\nMission Concept Review (MCR), the System Requirements Review (SRR), the Preliminary Design Review (PDR), the\nCritical Design Review (CDR) and the Operational I Flight Readiness Review (0/FRR).\n\n)\n\nSystem Ufe Cycle Phases\n\nPre-A\n\nA\n\nc\n\n8\n\nE\n\nD\n\n:::s\n\nn\n...\nID\nAI\n\nlOOK\'\n\n:;\xc2\xb7\n"\'\n\nCIQ\n\n0\n\n~\n\nSDK\'\n\n120\nn\n0\n\n3\n\n\'til\n\n[\n\n~\n\nSK\'\n\n1\'\n~\n\n~\n\n~\n\n~\n\n~\n\nMCR\n\nSRR\n\nPDR\n\nCDR\n\n0/FRR\n\nFIGURE 1: ROLE TO LIFECYCLE PHASE MAPPING\n\nNot readily apparent above is the fact that, as we go down the list of Roles and as the product or program matures\nacross time, the number of people and complexity associated with the activity increase significantly. This creates a\nsituation where those in the architecture role that comprised the majority of the team at inception become a\nsmaller and smaller percentage of the workforce as the project matures.\nSuccessful endeavors seem to have some common behaviors associated with and around these Roles. The rules\nfall into two general areas: Communication and Scope.\n\xe2\x80\xa2\n\nCommunication:\no\n\nThe Role above you - the Parent, the customer - provides dollars, goals and measures of your\nsuccess. You need to talk to them in their language, honoring their norms, or they will find\nsomeone who will.\n\n4\n\no\n\nThe Roles below you - the Children, the suppliers\n\nprovide that which you need to satisfy your\n\no\n\nThis is called the chain of command. However, it must be recognized that miscommunication\n\ncustomer and enable your success. They need to learn your language.\nacross these interfaces can limit or destroy progress, with the onus on each Parent to ensure\noverall program and project success.\n\xe2\x80\xa2\n\nScope:\no\n\nEach Role has a set of norms that govern local behavior. These include: motivation, reward\nstructure, punishment structure, organizational physics and culture.\n\no\n\nNorms will vary from role to role, project to project, customer to customer, sometimes week to\nweek. It is advisable not to look across too many Roles and expect your norms to still apply. One\ncan look closely at another\'s role, but to be effective, one must observe as if a part of the remote\nsystem.\n\nIn addition, in order to add value beyond the local group, what is learned must be appropriately shared.\n"Appropriately Shared" is not the same as "Publish Everything". It is a value adding process that assumes the\nParents defined what is needed as well as the format in which it is required, assumes that there is sufficient local\nknowledge to identify what meets those standards and requires a place for information that supports appropriate\nsharing. For this discussion, the terms Compose and Decompose require some clarification.\n\xe2\x80\xa2\n\nCompose: Accept models, data, simulations, designs and knowledge from Child projects or systems;\nprovide an integrated view, including all of the necessary credibility information to support subsequent\nreuse or reapplication; then publish the results for use by the Parent project or systems in their analysis\nand to better inform the Child projects of how their efforts integrate into the greater whole.\n\n\xe2\x80\xa2\n\nDecompose: Accept models, data, simulations, requirements and knowledge from Parent project or\nsystem; format or organize as necessary and share with Child projects or systems, providing the necessary\ncredibility information for subsequent use; then publish for use by the Children in their design and\ndevelopment work and by the Parent to both better understand the products being developed and\nensure the teams are on the right path.\n\nThe minimum needs for sharing are met when we can compose information for effective use by the Parent side of\nthe effort, can appropriately decompose information for effective use by the Child side of the effort, and can\ndeliver the information in a reliable manner.\nAs time progresses and programs mature, the models and information exchanged will change as well. The table\nbelow describes this progression.\n\n5\n\nRole:\n\nStarts With:\n\nMoves To:\n\nExecutive\n\nEarly Artwork, Sketches, Goals, Directives\n(pictures, simple spreadsheets)\n\nParametric Data, System Models, Simulation\nProducts, Decisions\n(system simulations, cost I performance\nmodels)\n\nArchitecture\n\nEarly Concepts, System Segment\nSimulations, Parametric Data\n(pictures, animations, spreadsheets)\n\nArchitecture Models, System Simulations\n(SysML, lifecycle simulations, cost I\nperformance analysis)\n\nProgram\n\nEarly System Models, Program Simulations,\nScenarios\n(spreadsheets, databases, animations)\n\nProgram Models, Program Simulations, Cost\nand Performance Simulations\n(SysML, simulations, program models,\ndiscrete event simulations)\n\nProject\n\nEarly Concepts, System Simulations,\nParametric Analysis\n(databases, SysML reference models)\n\nSystem Models I Simulations, Process\nSimulations, Design Visualization\n(detailed SysML, PM software, DES, Cradle,\nDOORS, Catia)\n\nEngineering\n\nEarly Concepts, Sub-System Simulations,\nParametric Data, SysML Needs\n(concept sketches, requirements, goals,\nvideos)\n\nCADICAE, PDMIPLM, System Visualization\nand Simulation, Visualization\n(Pro-E, Windchill, Catia, Unigraphics, DOORS,\nCradle, FEA, .. )\n\nTABLE 2: EVOLUTION OF INFORMATION PRODUCTS\n\nMODEL USE AND RE-USE CONCEPTS\nDEFINITIONS\nThe definitions below apply to this document. They are based upon other documents, studies done over the past\ndecade, use across the Constellation Program (CxP), use across other NASA projects, discussions across the NIMA\nteams and dictionary references.\nMODEL\nA model is static, like a noun.\n\nIt is a representation of a thing, such as a device, a behavior, a\n\nphenomenon, a process or a system. A model is studied to learn about and better understand the thing\nthat it represents. Models may also contain a temporal element and could then represent the item being\nstudied over a period of time.\n\n6\n\nSIMULATION\nA simulation is dynamic, like a verb.\n\nIt is the active representation of a process or activity that has\n\noccurred, is occurring or is expected to occur. In a simulation we expect things to move and change in\nresponse to stimulus and the passage of time.\n\nA simulation is studied to learn about and better\n\nunderstand the activity. A simulation is expected to produce results that can be stored and passed to\nother simulations. As these stored forms are no longer dynamic, and do not respond to stimuli, they now\ntake on the attributes of a model for use in inspecting and understanding the simulation they represent.\nDECISION\nA decision is something that requires information, knowledge and experience. The information comes, in\npart, from models and simulations. The decision should reference the models and simulations supporting\nthe decision. And, those models and simulations must have been performed with rigor commensurate to\nthe decision they support.\n\nNASA Standard 7009 provides our framework for representing and\n\ncommunicating the rigor associated with development of the decision support products.\n\nThis guide is intended to help engineers, managers, leaders, customers and suppliers, at any phase of their system\nlife cycle, better understand both the principles associated with NASA\'s model-centric design and how those\nprinciples contribute to better, more sustainable products with lower life cycle costs.\n\n7\n\nMODEL USE AND RE-USE BENEFITS\nSCIENTISTS AND ENGINEERS\nNIMA provides a foundation that will allow for more informed requirements and support more informed design.\nModel use and model reuse are key elements to rapid and effective design and development, no matter the life\ncycle phase. This guide defines both the incoming and outgoing model expectations to ensure that teams get the\nmodel-based information (ex. needs, requirements, specifications, past work) they need to perform their job as\nwell as provide the model-based products that allow others to successfully complete theirs.\n\nPROJECT MANAGERS\nNIMA provides core and common design practices and data structures that enable project and data integration.\nThis guide provides an overview of what is needed to produce and utilize reusable model-centric data as well as\nwhat a project manager should expect in terms of reusable data throughout the lifecycle.\n\nCUSTOMERS\nNIMA formalizes and empowers relationships across the project lifecycle. This document provides customers with\nrelationship expectations regarding model-centric design and outlines for them the expected benefits associated\nwith the process. In a model-centric world the customer deals much more with use cases and simulations as\nopposed to requirements and verification steps. This makes it much easier to understand what is being built, as\nwell as inspect the interim products during the development process.\n\nSUPPLIERS AND MANUFACTURERS\nNIMA opens the design process to those who will actually produce the final products (i.e., Commercial Providers,\nManufacturers, Universities, Other Government Agencies and/or Other NASA Centers).\n\nRepresentation of the\n\nsystems and the expected performance, early in the lifecycle, in a portable format, enables inclusion of the supply\nand manufacturing community into the design process. The same way that including an operator in the design\nprocess led to more operable systems, this will allow suppliers a better understanding of what is intended and\nenable them to help guide the design process towards more producible systems.\n\nPROGRAM MANAGERS\nNIMA provides the program manager with a model-centric framework to help create and operate NASA\'s complex\nnext generation exploration systems. This document provides the program manager with an understanding of\nwhat is possible in the way of model-centric design as well as what they should look for and expect as their\nsystems (and systems of systems) move through the various lifecycle phases associated with multi-decadal\nendeavors.\n\nOPERATIONS\nNIMA provides operations with rich information on the intended systems throughout its life cycle, as well as a seat\nat the design table. This information, available early in the lifecycle, also allows the operational community to\nmake more informed decisions concerning the incorporation of new capabilities into their operational systems and\nprocesses.\n\n8\n\nRECOMMENDATIONS\n\n1 PROVIDE A STANDARD WORKPLACE THAT ENABLES\n\nMODEL USE AND RE-USE\n\nA look at work processes, project design, Constellation Best Practices, Constellation Lessons Learned, past\nPDM/PLM efforts and successful projects indicate two basic workplace needs for teams that support large\nendeavors. Teams need an internal place to work and an external place to share, or publish, what they are\nworking on.\n\xe2\x80\xa2\n\nThe internal space will contain alternatives, works-in-progress, options, failed efforts and items in the.\nreview process. Much of this data is not intended for wider consumption but is essential for the effective\noperation of the team. Internal data will be under less formal configuration control and will be more\noften aligned with the internal team structure than the overall project structure.\n\n\xe2\x80\xa2\n\nThe external space is for publishing information to be used across the Program, Project or Partners.\nInformation will be organized along strict Program lines and will be under strict configuration control.\n\nThese places are necessary from the executive leadership level on down to the smallest task team and must be\nprovided across Programs and Projects as standard services.\n\nFailure to do so will increase standup efforts,\n\nfragment processes (this team does this that way), require team specific training and ensure duplication of work.\nProviding these capabilities at Stand up will allow for common training, common processes, easier re-use of work,\nand will let teams focus on adding value as opposed to continually re-creating and re-organizing the file shares\nused for storing and sharing data in hope of finding what they need.\n\n2\n\nPROVIDE A FORMULATION/ STANDUP TEMPLATE THAT ENCOURAGES RE-USE OF MODELS\n\nAND SIMULATIONS:\nNIMA, with its focus on MBSE, SysML, Modeling, Simulation and Structured/Architected Data, provides NASA an\nunprecedented foundation upon which to conceive, build and manage new systems. As with any powerful tool;\nguidelines, instructions and processes are critical to safe and successful application.\n\nAgain, research and\n\nobservation have shown that structural and process sub-optimizations during the Program or Project Standup\nphase are exceedingly difficult and costly to correct later in the lifecycle. A Template for Program or Project\nStand up that provides the basic structure necessary for NIMA-enabled success must be provided to take advantage\nof the benefits of planned, organized and available model-centric data.\n\n3\n\nPROVIDE A LEADERSHIP PROCESS THAT RE-USES MODELS AND SIMULATIONS\n\nWith a place to appropriately store and share models and simulations for both use and re-use, and a structure that\norganizes these places across a Program or Project, what remains is the need for a formal process for moving\nmodels, simulations and knowledge up, down and across the Program(s). Teams in a Program or large project exist\nin natural hierarchy with needs coming from near the top and products coming from near the bottom. As the\nProgram or Project matures, any one Team, no matter the place in the hierarchy, will perform at least 2 major\nfunctions:\n9\n\n\xe2\x80\xa2\n\nThey will receive data or products from subordinates, add value to that data or product, and provide it to\ntheir superior.\n\n\xe2\x80\xa2\n\nThey will receive needs from their superior, expand upon those needs as necessary, and provide the\nneeds, as well as the additional data, to subordinates.\n\nBoth of these functions are governed by NASA Program and Project management processes. However, to take full\nadvantage of the opportunities provided by MBSE in general - and NIMA\'s pattern of model, use and re-use in\nparticular - a standard tailoring of these management processes is highly recommended. This tailoring would\nrecommend where and how to store and share models, provide guidelines to help establish the necessary\norganizational structure to enable this sharing, and lay out the basic structural processes to use and re-use models,\nsimulations and knowledge across the endeavor.\n\n4\n\nPROVIDE A NASA STD.\n\n7009\n\nBASED REFERENCE TO HELP GUIDE APPROPRIATE USE ANDRE-\n\nUSE:\nThe credibility of M&S results should be assessed using, at a minimum, the Credibility Assessment Scores (CAS) and\nthe processes detailed in NASA Standard 7009. This assessment process involves evaluating the M&S results in\neight areas: Verification, Validation, Input Pedigree, Results Uncertainty, Results Robustness, Use History, M&S\nManagement, and People Qualifications. NASA is providing a NASA Standard 7009 Handbook as well as NIMAspecific training and worksheets to assist in the development of credibility assessments.\n\nAdditional data on\n\ncredibility assessment and recommended CAS data for various lifecycle phases are included as an appendix to this\nwhite paper.\n\n10\n\nCONCLUSIONS\nIn order to successfully Use and Re-Use Models and Simulations we must define and meet key organizational and\nstructural needs:\n\n1.\n\nWe must understand and acknowledge all the roles and players involved from the initial need\nidentification through to the final product, as well as how they change across the lifecycle.\n\n2.\n\nWe must create the necessary structural elements to store and share NIMA-enabled information\nthroughout the Program or Project lifecycle.\n\n3.\n\nWe must create the necessary organizational processes to stand up and execute a NIMA-enabled\nProgram or Project throughout its lifecycle.\n\nNASA must meet all three of these needs to successfully use and re-use models. The ability to Reuse Models a key\ncomponent of NIMA and the capabilities inherent in NIMA are key to accomplishing NASA\'s space exploration\ngoals.\n\n11\n\nAPPENDIX A: NOTIONAL LIFECYCLE CREDIBILITY RECOMMENDATIONS\nThe CAS recommendations in this appendix build upon Role and Lifecycle discussions in the Model Use and Re-Use\nwhite paper and incorporate information from the NASA Systems Engineering Handbook on gateway reviews (SRR,\nCDR, PDR ...) as well as information and lessons from the Constellation program . They are organized along the lines\nof the planned gateway reviews and associated initial credibility levels from the Constellation Program. It should\nbe noted that Constellation was terminated prior to formalizing these recommendations and that the actual levels\napplied would have been under the purview of the Program or Project Systems Engineer and Technical Authority .\nIt should also be noted that Constellation was a multi -billion dollar multi-decadal program. These levels are likely\nnot appropriate for smaller projects. However, that decision also falls under the purview of the associated\nprogram or projects systems engineer and technical authority.\nThe figure below identifies the eight areas evaluated while performing a credibility assessment. Each area is\nranked from 0 to 4, with 0 representing low credibility in an evaluation area and 4 repr~senting high credibility.\nThe criteria for meeting each level are also provided .\n\nVerification\n\nValidation\n\nlnf:\nPed gree\n\nRelable error\n\nResU!s\n\nInputs agree\n\n~\xc2\xb7\nacross key\n\n~m\n\nwith~\ndata\n>\n\nelements\n\nsystem\n\nM&S\n\nRestjs agree\n\nInputs agree\n\nF3thod\nis\nto\nlllil\ntesting errors\n\nFIMltllble\n\n~~\nlllgressiOII\nleslilg\n\nFIMltllble\n\n*\n\nproblems of\ninterest\n\nof\n\n~data\n\nfar\nin\nfrom\n> 3.0 Slll1lliBry\nM&S\n\nRestjs agree\n\nm\ndoc.,\nor\n>2.0\n~ In=\non lllit\nM&S\nproblems\n\nConcept a\n\n=of $\n\nfar\nconcept &math\nmodels\n\n3.5 SlllllliBry\n\ntextbook\nreferents\n\n8UIIIIIIIY M&S\n\nIn~_.\nm\n\xc2\xb7\ndoc.,\nor\n\n> 1.0\n\nSU1111111Y M&S\n\nResults\nResults ]\nUncertainty Robultneu\n\nnLon\nCluantitatiYe\n\nde\n\n.\nl1llllllrical\nanalysis\n\n&\n\nSensitivity\n\n~nmt\n(al\nof the most\nsensitive cases)\n\ndeterministic\nanalysis\n\n~y\nparameters\n\nContilual\n\n~tm\n~resut\n\nrepeatabity\n\nknownT.few\npanmeters\nSensitivity\n\nbaseCI on\nanalogy\n\nExtensive\n\n:;:\nM&S practices\n\n&tool\n\n===\n=\n\nMv. degree or\n\nprac:tice\n\nmission data\n\n=\n=:\n=\n\nexpert opinion\n\n=\n\nJ\n\nM&S\nPeople\nManagement Qualification\n\nPnMous\n\nCluantitatiYe\n\nn~on\n\nUse History\n\nknov.1edge\n\nFormal M&S\n\nUsed~ far\n\n~a.\n\ndeciiiOns\n\nn operations\n\nracommellded\nprac:tice trailing\n\n:.~ R~ties\nwith\n\n:=r:\n\nsin& tools\n\nmanaged\n\nNo ~ l No ~ ] No ~ ] No ~ ] No ~ ] No ~ ] No ~ l\n\nNo ~\n\nFIGURE 2: CREDIBILITY ASSESSMENT CRITERIA\n\n12\n\n,.\n\nCREDIBILITY RECOMMENDATION FOR PEER REVIEW/ CONCEPT DEVELOPMENT\nCredibility scores during Peer Review, and Concept or Technology Development are expected to be fairly low (l\'s\nand O\'s) as these activities consist primarily of loosely structured examinations of new ideas, usually without\ncentral control and mostly oriented toward small studies. An example of minimum suggested credibility scores for\nPeer Review I Concept Development (notionally Pre Phase A) is included below. This CAS indicates that the work\nwas performed by members of the technical team and the results were in line with what was expected. The major\nbenefits of the CAS at this point in the lifecycle are to record: the fact that some basic verification was done, that\ntechnical members of the team performed the analysis and that the center items were not addressed. While it is\npossible that additional rigor could be applied and recorded in the concept phase, it was not necessary. This\nprofile also serves as a flag to subsequent users of the results that this analysis is likely not appropriate for re-use\noutside of the concept development phase, while preserving the knowledge that some work was done that could\nbe called upon if necessary.\n\nM&s Development\n\nM&S Operations\n\nSupporting Evidence\n\nFavorable\n\nevidence of\nverification for\nconcept&m..\nmodels\n\nFIGURE 3: EXAMPLE PEER REVIEW-TYPE CREDIBILITY ASSESSMENT\n\n13\n\nCREDIBILITY RECOMMENDATION FOR SYSTEM REQUIREMENTS REVIEW (SRR)\nSRR CAS scores should demonstrate maturation of both the work and the processes to ensure that the system\nrequirements are appropriate. Progression is expected in the areas of Verification, Input Pedigree, Use History and\nM&S Management, as outlined below by the darker colored boxes. The major benefits of the CAS at this point in\nthe lifecycle are related to ensuring that the foundational analyses that went into the requirements development\nprocess are understood as well as repeatable. The sample CAS below indicates that while the work was performed\nby a technical team similar to that which performed the concept studies, additional work has been done to ensure\nthe correctness of the results and implement processes to ensure that the right work was performed and the work\ncan be repeated. While it is possible that additional rigor could be applied and recorded, the suggested scores\nshould be adequate for the System Requirements phase. Also, this profile is a flag to subsequent users of the\nresults that this analysis is not appropriate for re-use outside of requirements development, while preserving the\nknowledge that some work was done that could be called upon if necessary.\n\nM&S Development\n\nM&S Operations\n\nFavorable\n\nEstablished\nprocess for\ndevelopmlri\nllld operations\n\nreds from key\nfealln lllit I\n\nresJ888ion\ntesting\n\nEngineering or\nscience.,-ee\n\nFIGURE 4: EXAMPLE SRR-TYPE CREDIBILITY ASSESSMENT\n\n14\n\nCREDIBILITY RECOMMENDATION FOR PRELIMINARY DESIGN REVIEW\n\n(PDR)\n\nPDR CAS scores should demonstrate maturation of both the work and the processes to ensure that the preliminary\nsystem designs are appropriate. Progression is expected in the areas of Validation, Input Pedigree, Uncertainty\nand Use History over the work that supported the System Requirements Review. This is outlined below by the\ndarker colored boxes. The major benefits of the CAS at this point in the lifecycle are related to ensuring that the\nanalysis that went into the preliminary designs is well understood, appropriate and repeatable. This CAS indicates\nthat while the work was still performed by a technical team similar to that which performed the Requirement\nReview studies, additional work has been done to ensure and test the correctness of the results as well as\nimplement processes to ensure that the right work was performed and that it could be repeated. While it is\npossible that additional rigor could be applied and recorded, the suggested scores should be adequate for the\nPreliminary Design Review. Also, this profile is a flag to subsequent users of the results that this analysis is not\nappropriate for re-use outside of Preliminary Design Review environment, while preserving the knowledge that\nsome work was done that could be called upon if necessary.\n\nM&S Development\n\nFavorable\nresults from key\nfeature 1111t I\nregression\ntesting\n\nConcept llld\nmalhmodels\n\n81J811with\ngeneralllld\ntextbook\nrefenris\n\nM&S Operations\n\n111l!D traceable\nto formal doc.,\norm>2.0\nllllllmiiY M&S\n\nSupporting Evidence\n\nBased on\nqualitdive\nestimates\n\nEstablished\nprocess for\ndevelopment\nand operations\n\nEng~neenng or\nsc1ence degree\n\nFIGURE 5: EXAMPLE PDR-TYPE CREDIBILITY ASSESSMENT\n\n15\n\nCREDIBILITY RECOMMENDATION FOR CRITICAL DESIGN REVIEW\n\n(CDR)\n\nCDR CAS scores should demonstrate maturation across all eight Credibility areas as outlined below and identified\nby the darker colored boxes. These CAS scores allow appropriate model use and reuse across the CDR process\nincluding system level testing. The major benefits of the CAS at this point in the lifecycle are related to ensuring\nthat the analysis is well understood, appropriate and repeatable. While additional rigor could be applied, the\nsuggested scores should be adequate to flag for subsequent users that this analysis is not appropriate for re-use\noutside of Critical Design Review environment, while preserving the knowledge that some work was done that\ncould be called upon if necessary.\n\nSupporting Evidence\n\nM&S Development\n\nVerification\n\nFormal method\nIs used to\nassess l.llit\nteall~ 811\'01\'8\n\nUse History\n\nValidation\n\nReds81J88\nwith\nelq)lrimanal\ndiU or ather\nM&Sonl.llit\nproblems\n\nI,.W8(J88\nwilhexp. diU\nfor problems of\ninierest or hm\n> 3.0 81111m;ry\nM&S\n\nBaaed on\ndetennlnialic\nanalysis or\nexpert opinion\n\nSenaitivity\n\neetim!Ud,\nqualitative,\nbased on\n\nanalogy\n\nPrevious\npredictions\n\nwerelat\xe2\x80\xa2\n\nvalidated by\nmission diU\n\nM&S\nManagement\n\nProcess\nm8111U8d for\nrepeatabllily\n\nFonnaiM&S\nlniringn\nexperience +\nrecommended\npracticelnini~\n\nFIGURE 6: EXAMPLE CDR-TYPE CREDIBILITY ASSESSMENT\n\n16\n\nCREDIBILITY RECOMMENDATION FOR OPERATIONAL READINESS REVIEW/ FLIGHT READINESS\nREVIEW\n\n(ORR/FRR)\n\nCDR CAS scores should demonstrate maturation across seven of the eight Credibility areas as outlined below.\nThese CAS scores allow appropriate model use and re-use across the ORR/FRR process, including system level\ntesting. The major benefits of the CAS at this point in the lifecycle are related to ensuring that the analysis is well\nunderstood, appropriate and repeatable. While additional rigor could be applied, the suggested scores should be\nadequate to support decisions for Operational or Flight use. It should be noted that even at this phase of the\nprogram lifecycle, the scores are not "All 4s", and a goal of "All 4s" is likely not appropriate unless deemed so by\nthe Program or Project System Engineering and Technical Authorities.\n\nM&S Development\n\nVerification\n\nValidation\n\nReliable error\n\nRedsawee\nwith\n\nestim~es used,\n\nsmall errors\nacross key\nelemem\n\nexperimtrial\ndata for\nproblems of\n\nnerest\n\nM&S Operations\n\nInput\nPedigree\n\nI,..Wawee\nwith real-world\ndata or from >\n3.5 Sllllm.y\nM&S\n\nResults\nUncertainty\n\nSupporting Evidence\n\nResults\nRobustness\n\nSensitivity\n\nknown for afew\npl1\'8111eters\n\nDe facto\natam\'d\n\nM&S\nManagement\n\nPeople\nQualifications\n\nConiooal\n\nUse History\n\nA61. dewee or\nextllllllive\n\nproc;ess\nimprovema to\nimprove reUt\nrepeatability\n\nexperience,\nrecommended\npractice\nblowledge\n\nFIGURE 7: EXAMPLE ORR/FRR-TYPE CREDIBILITY ASSESSMENT\n\n17\n\nRECOMMENDED CREDIBILITY SCORE ROLL UP, ALL ON ONE CHART\n\nThe Credibility Assessment Score is expected to increase as a Program or Project matures. The tables above\nprovide notional scores for a large manned space flight program (CxP) at various program gateway reviews. The\ntable below rolls those recommended gateway review scores up into a single format to both show the credibility\nmaturation and enable a discussion of credibility for smaller projects or programs.\n\nM&S Development\n\nM&S Operations\n\nSupporting Evidence\n\n\xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2\n\nMCR\n\nSRR\n\nPDR\n\nCDR\n\n0/FRR\n\nFIGURE 8: EXAMPLE HUMAN SYSTEM CREDIBILITY EVOLUTION\n\n18\n\nALTERNATIVE CREDIBILITY SCORE ROLL UP\nCLASS D PROJECT RECOMMENDED CREDIBILITY SCORES\n\nAn example of an alternative to the above human rated system CAS profiles, the one below has been provided by\nthe Goddard Space Flight Center as appropriate for Class D missions. These missions are of low cost and short\nduration and as such the program or project may tolerate risk in excess of that which is acceptable for higher\nclassification missions. Not only is the operational phase of the mission short, so are the formulation and\nimplementation phases. With small budgets come limited manpower, little time for "deep" analysis and software\ntesting, a streamlined Integration & Test phase, and a small likelihood of either test beds or engineering models\nbeing developed to support M&S validation prior to development of the flight hardware. Given the above, the\nfollowing Credibility Scores are representative of expectations for a notional Class D mission.\n\nM&S Development\n\nM&S Operations\n\nSupporting Evidence\n\nM&S\n\nPeople\nQualification\n\n\xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2 \xe2\x80\xa2\n\nMCR\n\nSRR\n\nPDR\n\nCDR\n\n0/FRR\n\nFIGURE 9: EXAMPLE CLASS 0 MISSION CREDIBILITY EVOLUTION\n\n19\n\n'