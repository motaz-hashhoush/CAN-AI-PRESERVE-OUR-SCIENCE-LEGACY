b"What Information Theory says about Best Response and about Binding Contracts\nDavid H. Wolpert\nN A S A Ames Research Center,\nhloffett Field, CA, 94035, USA\ndhwQemail.arc.nasa.gov\n\nProduct Distribution (PD) theory is the information-theoretic extension of conventional fullrationality game theory t o bounded rational games. Here P D theory is used to investigate games\nin which the players use bounded rational best-response strategies. This investigation illuminates\nhow to determine the optimal organization chart for a corporation, or more generally how t o order\nthe sequence of moves of the players / employees so as to optimize an overall objective function. It\nis then shown that in the continuum-time limit, bounded rational best response games result in a\nvariant of the replicator dynamics of evolutionary game theory. This variant is then investigated for\nteam games, in which the players share the same utility function, by showing that such continuumlimit bounded rational best response is identical to Newton-Raphson iterative optimization of the\nshared utility function. Next P D theory is used to investigate changing the coordinate system of\nthe game, Le., changing the mapping from the joint move of the players t o the arguments in the\nutility functions. Such a change couples those arguments, essentially by making each players\xe2\x80\x99 move\nbe an offered binding contract.\n\nI.\n\nINTRODUCTION\n\nRecent work has shown that information theory [l-31\nprovides a principled extension of noncooperate conventional game theory to accommodate bounded rationality\n[4].Intuitively, this extension is based on Occam\xe2\x80\x99s razor: Giver, only partial knowledge concerning a game\xe2\x80\x99s\n(bounded rational) equilibrium, introduce as little extra\ninformation as possible beyond that partial knowledge\nin inferring the joint mixed strategy of that equilibrium.\nThis is formalized by setting the joint mixed strategy of\nthe game\xe2\x80\x99s equilibrium, q(z E X) = n i q * ( z i ) , o the\nt\nminimizer of a set of Lagrangian functions.\nThe field of Probability Collectives concerns the o p\ntimization of distributions over the variables of interest,\nrather than the optimization of those variables directly.\nThe special case considered here, where the joint distribution over the variables of interest is a product distribution, is known as Product Distribution (PD) theory\n[5-111. This paper uses PD theory to investigate several\naspects of game theory not considered in [4]. P D theory\nis applied to games in which the players use bounded rational versions of best-response strategies. It is also used\nto investigate changing the coordinate system of a game,\ni.e., changing the mapping from the joint move of the\nplayers to the arguments in the utility functions. Such\nchanges couple those arguments, essentially by making\nthe players\xe2\x80\x99 moves be offered binding contracts. This\npaper also uses P D theory to illuminate recent work in\nadaptive distributed control.\nSec. I1 reviews how information theory can be used\nto derive bounded rational noncooperative game theory.\nSome simple examples of bounded rational games are\nthen presented.\nSec. I11 analyzes scenarios in which the players use\nbounded rational versions of best response strategies.\nParticular attention is paid to team games, in which the\nplayers share the same utility function. The analysis for\n\nthis case provide insight into how to optimize the sequence of moves by the players, as far as their shared\nutility is concerned. This can be viewed as a formal way\nto optimize the organization chart of a corporation.\nBest response strategies, even bounded rational ones,\nare poor models of real-world computational players that\nV\nuse Reinforcement Learning (RL) [12-151. Sec. l considers iterated games in which players use a (bounded\nrational) variant of best response, a variant that is more\nrealistic for computational players, and arguably for human players as well. In this variant the conditional expected utilities used by each player to update her strategy is a decaying average of recent conditional expected\nutilities; this implements a bias by the player to dampen\nlarge and sudden changes in her strategy. This variant\nis then explored for the case of team games. The continuum limit of the dynamics of such games\xe2\x80\x99is shown to\nbe variant of the replicator dynamics. It is shown such\ncontinuum-limit bounded rational best response is identical to Newton-Raphson iterative optimization of the\nshared utility function of such games.\nThe next section investigates changing the coordinate\nsystem of the game. By doing this the moves of the players get transformed, into (bounded rational) contracts\nbinding them. Some of the implications for optimal organization charts of such bounded rational contracts are\nelucidated, as well as their use to speed convergence in\nteam games.\nThis paper ends with a discussion of how these results\nrelate to other work, and with a brief overview of extensions of these resuls.\n\n11.\n\nPD THEORY AS BOUNDED RATIONAL\nNONCOOPERATIVE GAME THEORY\n\nIn this section we motivate PD theory a the\ns\ninformation-theoretic formulation of bounded rational\n\n(s)\n\ngame theory. We use the integral sign\nwith the associated measure implicit, Le., it indicates sums if appropriate, Lebesgue integrals over Rn if appropriate, etc. In\naddition, the subscript (i) is used to indicate all index\nvalues other than i. Finally, we use P to indicate the set\nof all probability distributions over a vector space, and Q\nto indicate the subset of P consisting of all product distributions (i.e., the associated Cartesian product of unit\nsimplices).\n\nA.\n\nReview of noncooperative game theory\n\nIn noncooperative game theory one has a set of N\nplayers. Each player i has its own set of allowed\npure strategies. A mixed strategy is a distribution q z ( x 2 )over player 2\xe2\x80\x99s possible pure strategies. Each\nplayer z also has a private utility function g2 that\nmaps the pure strategies adopted by all N of the players into the real numbers. So given mixed strategies\nof all the players, the expected utility of player i is\nE(gz) = S d x r I J ? ( x j ) g t ( ~ )\n1441.\nIn a Nash equilibrium every player adopts the mixed\nstrategy that maximizes its expected utility, given the\nmixed strategies of the other players. More formally,\nvi, q2 = argmax,: S dx 4:\nqJ (xJ)g2(Z). Perhaps the\nmajor objection that has been raised to the Nash equilibrium concept is its assumption of full rationality [16201. This is the assumption that every player z can both\ncalculate what the strategies\nwill be and then calculate its associated optimal distribution. In other words,\nit is the assumption that every player will calculate the\nentire joint distribution q(z) =\nq (IC~).\nJ .\nIn the real world, this assumption of full rationality\nalmost never holds, whether the players are humans, animals, or computational agents [ll,16, 21-27]. This is due\nto the cost of computation of that optimal distribution,\nif nothing else. This real-world bounded rationality is\none of the major impediments to applying conventional\ngame theory in the real world.\n\nnJZ2\n\nn\n\nB.\n\nReview of the minimum information principle\n\nShannon was the first person to realize that based\non any of several separate sets of very simple desiderata, there is a unique real-valued quantification of the\namount of syntactic information in a distribution P ( y ) .\nHe showed that this amount of information is the negative of the Shannon entropy of that distribution, S ( P ) =\n- S d y P(y)ln[%].\nSo for example, the distribution\nwith minimal information is the one that doesn\xe2\x80\x99t distinguish at all between the various y, i.e., the uniform\ndistribution. Conversely, the most informative distribution is the one that specifies a single possible y. Note\nthat for a product distribution, entropy is additive, i.e.,\nS(rIi4z(Yi))=\nS(Qi).\n\nc\nz\n\nSay we given some incomplete prior knowledge about a\ndistribution P(y). How should one estimate P ( y ) based\non that prior knowledge? Shannon\xe2\x80\x99s result tells us how to\ndo that in the most conservative way: have your estimate\nof P ( y ) contain the minimal amount of extra information beyond that already contained in the prior knowledge about P ( y ) . Intuitively, this can be viewed as a\nversion of Occam\xe2\x80\x99s razor: introduce as little extra information beyond that you are provided in your inferring\nof P. This minimum information approach is called the\nmaxent principle. It has proven extremely powerful in domains ranging from signal processing to supervised learning 121. In particular, it is has been successfully used in\nmany statistics applications, including econometrics [as].\nIt has even provided what many consider the cleanest\nderivation of the foundations of statistical physics [as].\n\nC.\n\nMaxent Lagrangians\n\nMuch of the work on equilibrium concepts in game theory adopts the perspective of an external observer of a\ngame. We are told something concerning the game, e.g.,\nits cost functions, information sets, etc., and from that\nwish to predict what joint strategy will be followed by\nreal-world players of the game. Say that in addition to\nsuch information, we are told the expected utilities of the\nplayers. What is our best estimate of the distribution q\nthat generated those expected cost values? By the maxent principle, it is the distribution with maximal entropy,\nsubject to those expectation values.\nTo formalize this, for simplicity assume a finite number of players and of possible strategies for each player.\nTo agree with the convention in fields other than game\ntheory (eg., optimization, statistical physics, etc.), from\nnow on we implicitly flip the sign of each g2so that the associated player i wants to minimize that function rather\nthan maximize it. Intuitively, this flipped gz(x) is the\n\xe2\x80\x9ccost\xe2\x80\x9d to player i when the joint-strategy is x.\nWith this convention, given prior knowledge that the\nexpected utilities of the players are given by the set of\nvalues {eL}, the ma..ent estimate of the associated q is\ngiven by the minimizer of the Lagrangian\n\nwhere the subscript on the expectation value indicates\nthat it evaluated under distribution q. The { p i } are \xe2\x80\x98\xe2\x80\x98inverse temperatures\xe2\x80\x9d implicitly set by the constraints on\nthe expected utilities.\nSolving, we find that the mixed strategies minimizing\nthe Lagrangian are related to each other via\n\n(3)\n\n3\n/\n\nwhere the overall proportionality constant for each i is\nset by normahation, and G = E, b,g, i45j. in Eq.3 the\nprobability of player i choosing pure strategg x , depends\non the effect of that choice on the utilities of the other\nplayers. This reflects the fact that our prior knowledge\nconcerns all the players equally.\nIf we wish to focus only on the behavior of player i.\nit is appropriate to modify our prior knowledge. To see\nhow t o do this, first consider the case of maximal prior\nknowledge, in which we know the actual joint-strategy of\nthe players, and therefore all of their expected costs. For\nthis case, trivially, the maxent principle says we should\n\xe2\x80\x9cestimate\xe2\x80\x9d q as that joint-strategy (it being the q with\nmaximal entropy that is consistent with our prior knowledge). The same conclusion holds if our prior knowledge\nalso includes the expected cost of player i.\nModify this maximal set of prior knowledge by removing from it specification of player i\xe2\x80\x99s strategy. So\nour prior knowledge is the mixed strategies of all players other than i, together with player i\xe2\x80\x99s expected cost.\nWe can incorporate prior knowledge of the other players\xe2\x80\x99\nmixed strategies directly, without introducing Lagrange\nparameters. The resultant maxent Lagrangian is\n\nBoltzmann distribution. One does this with a rational..\nIcy o p e r a i u r W L L L LUqs & 4 &Eda 9, t G z, Z G Z ;;cgz,t.ti-K\nreal value measuring the rationality of player 2 in adopting strategy q, given private cost function g1 and strategies q(,) of the other players. For the solution in Eq. 4\nand private cost g , , the value of that operator is just 3,\n141.\n\nEq. 3 is just a special case of Eq. 4,where all player\xe2\x80\x99s\nshare the same private cost function, G. (Such games are\nknown as team games.) This relationship reflects the\nfact that for this case, the difference between the maxent\nLagrangian and the one in Eq. 2 is independent of q,.\nDue to this relationship, our guarantee of the existence\nof a solution to the set of maxent Lagrangians implies\nthe existence of a solution of the form Eq.3. Typically\nplayers will be closer to minimizing their expected cost\nthan maximizing it. For prior knowledge consistent with\nsuch a case, the p, are all non-negative.\nFor each player i define\n\nf z ( z , q * ( ~ *=>P z g z ( z ) + W * , x * ) l )\nThen we can write the maxent Lagrangian for player i as\nZ ( q )=\n\nj\n\nsolved by a set of coupled Boltzmann distributions:\n(4)\nFollowing Nash, we can use Brouwer\xe2\x80\x99s fixed point theorem to establish that for any non-negative values {p},\nthere must exist at least one product distribution given\nby the product of these Boltzmann distributions (one\nterm in the product for each i).\nThe first term in 2i minimized by a perfectly rais\ntional player. The second term is minimized by a perfectly irrational player, i.e., by a perfectly uniform mixed\nstrategy qi. So pi in the maxent Lagrangian explicitly\nspecifies the balance between the rational and irrational\nbehavior of the player. In particular, for @ + 00, by minimizing the Lagrangians we recover the Nash equilibria\nof the game. More formally, in that limit the set of q\nthat simultaneously minimize the Lagrangians is the set\nof mixed strategy equilibria of the game, together with\nthe set of delta functions about the pure Nash equilibria\nof the game. The same is true for Eq. 3.\nNote also that independent of information-theoretic\nconsiderations, the Boltzmann distribution is a reasonable (highly abstracted) model of how human players will\nbehave. Typically humans do some \xe2\x80\x9cexploration\xe2\x80\x9d a well\ns\nas \xe2\x80\x9cexploitation\xe2\x80\x9d, trying out all moves, with frequency as\nthe expected cost of the move increases. This is captured\nin the Boltzmann distribution mixed strategy.\nOne can formalize the concept of the rationality of a\nplayer in a way that applies to any distribution, not just a\n\nJ\n\ndx q(z)fz(x,qz(xCz)).\n\n(6)\n\nNow in a bounded rational game every player sets its\nstrategy to minimize its Lagrangian, given the strategies\nof the other players. In light of Eq. 6, this means that we\ncan interpret each player in a bounded rational game as\nbeing perfectly rational for a cost function that incorpe\nrates its computational cost. To do so we simply need to\nexpand the domain of \xe2\x80\x9ccost functions\xe2\x80\x9d to include (logarithms of) probability values as well as joint moves.\nD.\n\nExamples of bounded rational equilibria\n\nIt can be difficult to start with a set of cost functions\nand associated rationalities Pi and then solve for the as\nsociated bounded rational equilibrium q. Solving for q\nwhen prior knowledge consists of expected costs ~i rather\nthan rationalities can be even more tedious. (In that situation the & are not specified upfront but instead axe\nLagrange parameters that we must solve for.) However\nthere is an alternative approach to constructing examples of games and their bounded rational equilibria that\nis quite simple. In this alternative one starts with a particular mixed strategy q and then solves for a game for\nwhich q is a bounded rational equilibrium, rather than\nthe other way around.\nTo illustrate this, consider a 2-person noncooperative\nsingle-stage game. Let each player have 3 possible moves.\nIndicate each players\xe2\x80\x99 three possible moves by the numerals 0, l, and 2 . Say the (bounded rational) mixed\nstrategy equilibrium is\n= 1/4;\na ( 0 ) = 1/27 41 ( 1 ) = 1 / 4 ,\nq2(0) = 2 / 3 , 42(1) = 1 / 4 , 426-9 = 1 / 1 2 .\n\n(7)\n\nNow we know that at the equilibrium, q l ( x l ) cx\nPI is player 1\xe2\x80\x99s rationality, and g1\nis her cost function (the negative of her cost function).\nThis means for example that\ne-PlE(g1lz1), where\n\ni.e.,\n\nWe have a similar equation for the remaining independent difference in expectation values for player 1. The\nanalogous pair of equations for player 2 also hold.\nNow define the vectors g z , j ( . ) gz(x,= j , .). So for\nexample g1,o = (gI(x1 = O , 2 2 = O),gl(xl = O,z2 =\nI), g1(xl= O,x2 = 2 ) ) . Then we can express our equations compactly as four dot product equalities:\n\ngi(x, > 0 , .) (one projection for each value xi\ncretely, V i , X, > 0,\n\n> 0 . con)\n\ni.e., Vi,xi > 0,\n\nAll the terms on the right-hand side are specified, as well\nas the q(,) term on the left-hand side. Any g2(lcz, that\n.)\nobeys the associated equation has the specified q as a\nbounded rational equilibrium.\nE.\n\nDiscussion\n\nThere are numerous alternative interpretations of the\ninformation-theoretic formulation of bounded rationality\npresented here. For example, change our prior knowledge\nto be the entropy of each player i \xe2\x80\x99 s strategy, Le., how\nunsure it is of what move to make. Now we cannot use\nNote that we can absorb each 0 into its associated 9,;\n,\ninformation theory to make our estimate of q. Given\nall that matters is their product.\nthat players try to minimize expected cost, a reasonable\nWe can now plug in for the vectors q1 and q2 from\nalternative is to predict that each player i \xe2\x80\x99 s expected cost\nEq. 7 and simply write down a set of solutions for the\nwill be as small as possible, subject to that provided value\nfour three-dimensional vectors g z , j . For these ( g 2 } the\nof the entropy and the other players\xe2\x80\x99 strategies. The\nbounded ratinal equilibrium is given by the q of Eq. 7.\nassociated Lagrangians are o,[S(q,)- o,] E(g,), where\nIf desired, we can evaluate the associated expected values\no2 is the provided entropy value. This is equivalent to\nof the cost functions for the two players; our q is the\nthe maxent Lagrangian, and in particular has the same\nbounded ratinal equilibrium for thos eexpected costs.\nsolution, Eq. 4.\nNote that the variables in the first pair of equalities\nAnother alternative interpretation involves world\nin Eq. 9 are independent of those in the second pair. In\ncost functions, which are quantifications of the quality\nother words, whereas the Boltzmann equations giving q\nof a joint pure strategy x from the point of view of an exfor a specified set of g 2 are a set of coupled equations, the\nternal observer (e.g., a system designer, the government,\nequations giving the g2 for a specified q are not coupled.\nan auctioneer, etc.). A particular class of world cost\nNote also that our equations for the g2,Jare (extremely)\nfunctions are (negatives of) \xe2\x80\x98\xe2\x80\x98social welfare functions\xe2\x80\x9d,\nunderconstrained. This illustrates how compressive the\nwhich can be expressed in terms of the cost functions of\nmapping from the g2 to the associated equilibrium q is.\nthe individual players. Perhaps the simplest example is\nBear in mind though that that mapping is also multiG ( x ) = C,~2g,(x),\nwhere the P, serve to trade off how\nvalued in general; in general a single set of cost functions\nmuch we value one player\xe2\x80\x99s cost vs. anothers. If we know\n* can have more than one equilibrium, just like it can have\nthe value of this social welfare function, but nothing else,\nmore than one Nash equilibrium.\nthen maxent tells us to minimize the Lagrangian of Eq. 2.\nThe generalization of this example to arbitrary numOften our prior knowledge will not consist of exact\nbers of players with arbitrary move spaces is immediate.\nspecification of the expected costs of the players, even\nAs before, indicate the moves of every player by an assoif that knowledge arises from watching the players make\nciated set of integer numerals starting at 0. Recall that\ntheir moves. Such alternative kinds of prior knowledge\nthe subscript (2) on a vector indicate all components but\nare addressed in [4,61. In particular, in those referthe z\xe2\x80\x99th one. Also absorb the rationalities Pt into the\nences it is shown how one might define a \xe2\x80\x9crationality\nassociated gt .\noperator\xe2\x80\x9d that quantifies the rationality of any pair of\nNow specify q and the vectors g 2 ( x 2 = 0, .) (one veca player\xe2\x80\x99s mixed strategy and cost function, given the\ntor for each Z) to be anything whatsoever. Then for\nmixed strategy of all the other players. If one\xe2\x80\x99s prior\nall players i, the only associated constraint on the i\xe2\x80\x99th\nknowledge is the the values of the rationalities of the\ncost function concerns certain projections of the vectors\nplayers, then one again arises at solutions of the form in\n\n.\n\n5\n\nEq. 4, where the value of ,Ol reflects the rationality of that\nplayer.\nIn addition, in the real world the information we are\nprovided concerning the system often will not consist of\ne m c t values of functionals of q, be those values expected\ncosts, rationalities, or what have you. Rather that knowledge will be in the form of data, D , together with an\nassociated likelihood function over the space of q. For\nexample, that knowledge might consist of a bias toward\nparticular rationality values, rather than precisely specified values:\n\nmust reduce the common Lagrangian, in contrast to the\ncase W ~ L U\np,ztl&el ZiciiE-ei ~pki.tiig.\nThere are many versions of serial updating. In cyclic\nserial Brouwer updating, one cycles through the i in order. In r a n d o m serial Brouwer updating, one cycles\nthrough them in a random fashion.\nIn greedy serial Brouwer updating, instead of cycling through all i, at each iteration we choose what\nplayer to update based on how much that will reduce the common Lagrangian. Those reductions can\nbe evaluated without explicitly calculating the associated Boltzmann distributions. To see how, use Ni to\n4. Then deindicate the normalization constant of\nA.1\n\na.\n\nwhere CY sets the strength of the bias.\nAs mentioned in the introduction, these results can\nalso be extended in many ways (e.g., to allow multiple\ncost functions, variables numbers of players, etc.). Some\nsuch extensions are explored below.\n111.\n\nBOUNDED RATIONAL VERSIONS OF\nBEST RESPONSE\n\nOne crude way to try to find the q given by Eq. 4 would\nbe an iterative process akin to the best-response scheme\nof game theory [l6]. Given any current distribution q,\nin*tliis scheme all agents i simultaneously replace their\ncurrent distributions. In this replacement each agent i\nreplaces qi with the distribution given in Eq. 4 based on\nthe current q(l). This scheme is the basis of the use of\nBrouwer\xe2\x80\x99s k e d point theorem to prove that a solution to\nEq. 4 exists. Accordingly, it is d e d parallel Brouwer\nupdating. (This scheme goes by many names in the literature, from Boltzmann learning in the F U community\nto block relaxation in the optimization community.)\nSometimes the conditional expected cost for each agent\ncan be calculated explicitly at each iteration. More generally, it must be estimated. This can be done via MonteCarlo sampling, iterated across a block of time throughout which q is unchanging. During that block the agents\nall repeatedly and jointly IID sample their probability\ndistributions t o generate joint moves, and the associated\ncost values are recorded. These are then use to estimate\nall the conditional expected costs, which are then used\nto determine the p a r d e l Brouwer update[46].\nThis is exactly what is done in RLbased schemes in\nwhich each agent maintains a data-based estimate of its\ncost for each of its possible moves, and then chooses its\nactual move stochastically, by sampling a Boltzmann distribution of those estimates. (See [5] for ways to get accurate MC estimates more efficiently than in this simple\n. scheme, e.g., by exploiting the bias-variance tradeoff of\nstatistics.)\nOne alternative to parallel Brouwer updating is serial\nBrouwer updating, where we only update one q a at a\ntime. This is analogous to a Stackelberg game, in that\none agent makes its move and then the other(s) respond\n[17, 191. In a team game, any serial Brouwer updating\n\nhow much 2 is reduced if only qi undergoes the Brouwer\n\xe2\x80\x99\nupdate [47].\nAnother obvious variant of these schemes is mixed serial/parallel Brouwer updating, in which one subset of\nthe players moves in synchrony, followed by another s u b\nset, and so on. Such updating in a team game can be\nviewed as a simple model of the organization chart of the\nplayers. For example, this is the case when the players\nare a corporation, with G being a common cost function\nbased on the corporation\xe2\x80\x99s performance.\nSay we observe the functioning of such an organization\nover time, and view those observations as Monte Carlo\nsampling of its behavior. Then we use those samples\nto statistically estimate how best to do serial/parallel\nBrouwer updating, for the purpose of minimizing the\nshared cost function G. This can be viewed as a way\nto optimize the organization chart coupling the players.\n\nIV. PARALLEL BROUWER WITH\nDATA-AGING IS NEAREST NEWTON\n\nThis section considers a variant of best-response that\nis more realistic (more accurately modeling a b a s e d\ncomputational players that are actually used in machine\nlearning, and arguably more accurately modeling human\nplayers as well). In this variant the expected cost used\nby each player to update her strategy is a decaying average of recent expected utilities; this decay reflects a\nconservative preference for dampening large changes in\nstrategy.\nSuch a bias is used (implicitly or otherwise) in most\nmulti-player RL algorithms. For example, in the COIN\nframework each agent i collects a data set of pairs of what\nvalue its private cost function has at timestep t together\nwith the move it made then. It then estimates its cost\nfor move sa as a weighted average of all the cost values\nin its data set for that move. The weights are exponentially decaying functions of how long ago the associated\nobservation was made. This data-aging is crucial to reflect the non-stationarity of agent i\xe2\x80\x99s environment, Le.,\nthe fact that the other agents are changing their s t r a t e\ngies with time. Arguably similar modifications to best\n\nresponse are used by human players. Indeed, in idealized learning rules like ficticious play, such dampening is\ncrucial.\n\nA.\n\nThe dynamics of Brouwer updating\n\nB.\n\nThe continuum-time limit\n\nTo go to the continuum-time limit, let t be a real variable, and replace the temporal delay value of 1in Eq. 14\nwith 6 and cy with a6 (we'll eventually take 6 40). In\naddition differentiate Eq. 12 with respect to t to get\n&(xi,\n- - t) -\n\nConsider a multi-stage game where at the end of iteration t , each player i updates her distribution qi(.,t)\nto\n\nt)[\ndQiz(zi, ) t\n\ndt\n\ndt\n\nIn addition, in the 6 -+ 0 limit, assuming q is a continuous\nfunction o f t , Eq. 14 becomes\nThis is a generalization of parallel Brouwer updating,\nwhere the function being exponentiated can be Q values\n(as in Q-learning [30]),\nsingle-instant reward values, distorted versions of these (e.g., to incorporate data-aging),\netc.\nAs an example, for single-instant rewards (i.e., conventional parallel Brouwer), @,(xi,t ) is player i's estimate of\n(pi times) her conditional expected cost for taking move\nz, at time t - 1. If that estimate were exact, this would\nmean\n\nAs another example, for Q-learning, one player is Nature\nand her distribution is always a delta function. In this\ncase Qiz(z,, t ) is the Q-value for player i taking action z,,\nwhen the state of Nature is as specified by the associated\ndelta function in q ( . , t - 1).\nNote that there's no Monte Carlo sampling being done\nhere, as there is in most real-world RL;\nthis is a somewhat\nabstracted version of such RL. Alternatively, the analysis here becomes exact when Qi, is evaluated closed form,\nor (as when Qi, is an empirical expectation value) there's\nenough samples in a Monte Carlo block so that empirical averages effectively give us exact values of expected\nquantities.\nAt this point we have to say something about how Qi,\nevolves with time. Consider the case where Qi, is an estimate of some function 4 t , formed by exponential aging\nof the previous 4 values. In our case (since everything\nis evaluated closed form) assuming there have been an\ninfinite number of preceding timesteps, this is the same\nas geometric data-aging:\n\nfor some appropriate function 4, [48]. For example, in\nparallel Brouwer updating, 4,(zZ,= @(gZ I x,, q2)t ) ) ,\nt)\n(\nwhile Qi2(zz, is a geometric average of the previous valt)\nues of $ ( x l ) .\n\nwhere from now on the t variable is being suppressed for\nclarity.\nIf we knew the dynamics of $bi, we could solve Eq. 16\nvia integrating factors, in the usual way. Instead, here\nwe'll plug that equation for\ninto Eq. 15. Then use\nEq. 12 to write Qi,(zi,q)= constant -ln(q2(z2)). The\nresult is\n\n9\n\nC.\n\nRelation with Nearest Newton descent and\nreplicator dynamics\n\nAs mentioned previously, there are many ways to find\nequilibria, and in particular many distributed algorithms\nfor doing so. This is especially so in team games, where\nfinding such equilibria reduces to descending a single\nover-arching Lagrangian.\nOne natural idea for descent in such games is to use\nthe Newton-Raphson descent algorithm. However that\nalgorithm cannot be applied directly to search across q\nin a distributed fashion, due to the need to invert matrices coupling the agents. As an alternative, one can\nconsider what new distribution p the Newton algorithm\nwould step to if there was no restriction that p be a\nproduct distribution. One can then ask what product\ndistribution is closest to p , according to Kullback-Leibler\ndistance [I]. It turns out that one can solve for that optimal product distribution. The associated update rule\nis called the Nearest Newton algorithm [31].\nIt turns out that when one writes down the Nearest\nNewton update rule, it says to replace each component\nqi(zi) with the exact quantity appearing on the righthand side of Eq. 17, where cy is the stepsize of the update,\nand &(xi, t ) = PE(G I xi, ( i ) ( t ) ) as in parallel Brouwer\nq\n,\nupdating for a team game [49]. In other words, in team\ngames, the continuum limit of having each player using\n\n7\n\n(bounded rational) best response is identical to the continuum h u t ot the Newton-hphson a i g o r i c h lui &scending the Lagrangian, with the data-aging parameter\na giving the stepsize.\nEq. 17 arises in other yet other contexts as well. In\nparticular, say\nis conditional expected rewards (i.e.,\n4a(zz,- 1) = E(ga 1 q(., t - 1))).Then the p -+ cc limit\nt\nof Eq. 17 reduces to a simplified form of the replicator\ndynamics equation of evolutionary game theory [32, 331.\n(If the stepsize cr is an appropriately increasing function\nof E(G) other versions of that dynamics arise.) This is\nbecause in that limit the In term disappears, and the\nrighthand side of Eq. 17 involves only the difference between player i\xe2\x80\x99s expected cost and the average expected\ncost of all players. This 3-way connection suggests using\nsome of the techniques for solving replicator dynamics t o\nexpedite either p a r d e l Brouwer or Nearest Newton.\nD.\n\nConvergence and equilibria\n\n+\n\nBy Eq. 17, at equilibrium, for each i, qa(z2)[4a(za,\nq)\nln(qa(za))]\nmust be independent of 2 . One way this\ncan occur is if it equals 0. However qz(sa)\ncan never\nbe 0, by Eq. 12. This means we have an equilibrium\nat qa(za): e-4a(Za,q). Intuitively, this is exactly what\n0\nwe want, according to Eq. 12 and our interpretation of\n4a(zz,q) an estimate of 4a(za,q).\nas\nNote also that this\nsolution means that 4a(za,) = aa(za, so that (accordq\nq),\ning to Eq. 16) aa(za,q) also reached an equilibrium.\nhas\nWhen our equilibrium has qa(za)[4a(za,q)\nl n ( q a ( z a ) )= A # 0, we have\n]\n\nE.\n\nOther variants of Brouwer updating\n\nNote that data-aging can be viewed as moving only\npart-way from the current Q, to what it should be (Le.\nto p t ) . As an alternative, one can dispense with the GZ\nand q5z altogether, and instead step part-way from the\ncurrent q to what it should be. This is partial movement\nto the (bounded rational) best response mixed strategy.\nFormally, this means replacing Eq. 12 so that the u p\ndate is not implicit, in how @%(xa,t ) depends on the past\nvalue of q(t - 1) (Eq. 14), but explicit:\nqa(za,t) = qa(z2,t - 1)\n\n+\n\n~ [ h Z ( ~ l , Q ( l ) ( t - q a ( z a , t - 1)1 (21)\n- 1))\nwhere ha(za, a ) ( t ) ) the Boltzmann distribution of\nq(\nis\nwhat qa(za,) would be, under ideal circumstances, and\nt\nwe implicitly have small stepsize a.\nThe only fixed point of this updating rule is where qa =\nhaV i So just like with continuum-limit parallel Brouwer,\nwe have the correct equilibrium. To investigate how fast\nthe update rule of Eq. 21 arrives at that equilibrium,\nwrite its error at time t as the residual\n\nrft(zi,t) = qi(zi,t ) - hi(zi, q ( i ) ( t ) )\n= q i ( z i , t - 1)[1 - CY]\nahi(zi,q(i)(t- 1))\n\n+\n\n-\n\nhi(zi,\nq(i)(t))\n\n= Qi(Z1, t - 1)[1 - a]\n\n+ ahi(zi,Q(i)(t 1))\n-\n\n- hi[zi, q(i)(t - 1)\n\n+\n\n+\n\nQ[h(i) - 1)) - q(i)( t - 1 1 (22)\n(dt\n11\n\nwhere we have assumed that all all players other than i\nare updating themselves in the same that i does (Le., via\nIn light of Eq. 12, this means that @ t ( z a , q# 4a(za,q). Eq. 21), and h(,)(q(t- 1)) means the vector of the values\n)\nSo by Eq. 16, Qz(za,q)\nhasn\xe2\x80\x99t reached an equilibrium in\nof all h+(zj) evaluated for q(t - 1).\nthis case:\nWith obvious notation, rewrite Eq. 22 as\nq*(za) oc\n\ne-q*(Z.)4z(Z.dd.\n\n(18)\n\n(19)\n\nIf both qa(za)\nand $a(za,q)\nwere frozen at this point,\nthis solution for aa(za,q)\nwould not obey Eq. 14. So\neither qa(za)\nand/or 4r(za,cannot be hozen. In fact, if\nq)\n4 a ( ~ a , varies with time, then we know by Eq. 17 that\nq)\nqa(za)\nvaries as well. So in either case q z ( z a )must vary,\nLe., this equilibrium is not stable.\nAlthough the dynamics has the desired &xed point, it\nmay take a long time to converge there. There are several ways to analyze that: One is to examine the second\nderivatives (with respect to time) of the qz and/or the\nAnother is to examine the timedependence of the\nresidual error,\n\nThe next subsection includes a convergence analysis involving residual errors, but for a different variant of\nBrouwer from the ones considered so far.\n\nTPt(li,t ) = Qi(Zi, t\n\n1)[1 - a]\n\n+ ahi(zi,q(i)(t 1))\n- hi[zi, q ( i ) ( t- 1) - ar(i)(t- 1)](23)\n\nNow u e the fact that (Y is small to expand the last h\ns\ni\nterm on the righthand side to &st order in its second\n(vector-valued) argument, getting the fbal result\n\nrft(zi,t)\n\nz ri(zi,t)[l-\n\na]\n\n+ ~ V h iT(i)(t- 1)(24)\n.\n\nwhere the gradient of h is with respect to the vector comi\nponents of its second argument. Accordingly, if rft(zi)\nstarts much larger than the other residuals, it will be\npushed down to their values. Conversely, if it starts much\nsmaller than them, it will rise.\nThere are other ways one can reduce a stochastic game\nto a deterministic continuum-time process besides those\nconsidered here. In particular, this can be done in closed\nform for ficticious play games and some simple variants\nof it [16, 341.\n\nV.\n\nSTATISTICALLY COUPLING THE PLAYERS\nA.\n\nThe semicoordinate system of a game\n\nConsider a multi-stage game like chess, with the stages\n(i.e., the instants at which one of the players makes a\nmove) delineated by t. Now strategies are what are set\nby the players before play starts. So in such a multi-stage\ngame the strategy of player i, x2,must be the set of tindexed maps taking what that player has observed in\nthe stages t\xe2\x80\x99 < t into its move at stage t. Formally, this\nset of maps is called player 2\xe2\x80\x99s n o r m a l form strategy.\nThe joint strategy of the two players in chess sets their\njoint move-sequence, though in general the reverse need\nnot be true. In addition, one can always find a joint\nstrategy to result in any particular joint move-sequence.\nNow typically at any stage there is overlap in what the\nplayers have observed over the preceding stages. This\nmeans that even if the players\xe2\x80\x99 strategies are statistically\nindependent, their move sequences are statistically coupled. In such a situation, by parameterizing the space\n2 of joint-move-sequences z with joint-strategies x , we\nshift our focus from the coupled distribution P ( z ) to the\ndecoupled product distribution, q(x). This is the advantage of casting multi-stage games in terms of normal form\nstrategies.\nMore generally, any onto mapping : x -+ z , not necessarily invertible, is called a semicoordinate system.\nThe identity mapping z\nz is a trivial example of a\nsemicoordinate system. Another example is the mapping from joint-strategies in a multi-stage game to joint\nmove-sequences is an example of a semicoordinate system. In other words, changing the representation space\nof a multi-stage game from move-sequences z to strategies x i s a semicoordinate transformation of that game.\nWe can perform a semicoordinate transformation even\nin a single-stage game. Say we restrict attention to distributions over X that are product distributions. Then\nchanging C(.) from the identity map to some other function means that the players\xe2\x80\x99 moves are no longer independent. After the transformation their move choices the components of z - are statistically coupled, even\nthough we are considering a product distribution.\nFormally, this is expressed via the standard rule for\ntransforming probabilities,\n\n<\n\n+\n\n\xe2\x80\x99\n\nPZ(Z 2)E < ( P x )E\nE\n\nJ\n\ndxPx(~)d(z- <(x)), ( 2 5 )\n\nwhere Px and PZ are the distributions across X and 2 ,\nrespectively. To see what this rule means geometrically,\nlet P be the space of all distributions (product or otherwise) over 2. Recall that Q is the space of all product\ndistributions over X , and let <(&) be the image of Q in\nP . Then by changing <(.), we change that image; different choices of <(.) will result in different manifolds C ( Q ) .\nAs an example, say we have two players, with two\npossible moves each. So z consists of the possible joint\nmoves, labeled (1,l),(1,2), ( 2 , l ) and (2,2). Have X =\n\n2, and choose [(l,1) = (1,l ) , C ( 1 , 2 ) = (2, a), C ( 2 , l ) =\n(2,1), and ((2,2) = ( l , 2 ) . Say that q is given by\n~ ( z = 1) = q2(x2 = 1) = 2/3. Then the distribul\ntion over joint-moves z is P z ( l , l ) = P x ( l , l ) = 4/9,\nP z ( 2 , l ) = Pz(2,2) = 2/9, Pz(1,2) = 1/9. SO Pz(z)\n#\nP , ( Z ~ ) P Z ( Zthe ;moves of the players are statistically\n~)\ncoupled, even though their strategies x, are independent.\nSuch coupling of the players\xe2\x80\x99 moves can be viewed as a\nmanifestation of sets of potential binding contracts. TO\nillustrate this return to our two player example. Each\npossible value of a component x, determines a pair of\npossible joint moves. For example, setting x 1 = 1 means\nthe possible joint moves are (1,1)and (2,2). Accordingly\nsuch a value of x 2 can be viewed as a set of proffered binding contracts. The value of the other components of x determines which contract is accepted; it is the intersection\nof the proffered contracts offered by all the components of\nx that determines what single contract is selected. Continuing with our example, given that x1 = 1, whether the\njoint-move is (1, 1) or (2,2) (the two options offered by\nX I ) is determined by the value of 2 2 .\n\nB.\n\nRepresentational properties\n\nBinding contracts are a central component of cooperative game theory. In this sense, semicoordinate transformations can be viewed as a way to convert noncooperative game theory into a form of cooperative game theory.\nIndeed, any cooperative mixed strategy can be cast as a\nnon-cooperative game mixed strategy followed by an appropriate semicoordinate transformation. Formally, any\nPz, no matter what the coupling among its components,\ncan be expressed as < ( P x ) some product distribution\nfor\nPX for and associated <(.) [50]\nLess trivially, given any model class of distributions\n{ P z } , there is an X and associated C. such that { P z }\n()\nis identical to [ ( Q X ) . Formally this is expressed in a\nresult concerning Bayes nets. For simplicity, restrict attention to finite 2. Order the components of 2 from 1\nto N . For each index i E {1,2,.. . , N } , have the parent\nfunction P ( i ,z ) fix a subset of the components of z with\nindex greater than i, returning the value of those components for-the z in its second argument if that subset of\ncomponents is non-empty. So for example, with N > 5,\nwe could have P ( 1 , z ) = ( 2 2 , z 5 ) . Another possibility is\nthat P(1,z ) is the empty set, independent of z .\nLet A ( P ) be the set of all probability distributions PZ\nthat obey the conditional dependencies implied by P:\nVJ\xe2\x80\x98z E A ( P ) , zE 2,\n\n(By definition, if P ( i , z ) ) is empty, Pz(zi 1 P ( i , z ) ) is\njust the i\xe2\x80\x99th marginal of Pz, Pz(zi).) Note that any\ndistribution Pz is a member of A ( P ) for some P ) - in\n\n9\nthe worst case, just choose the exhaustive parent function\n\nP ( i , z ) = ( z j : j > ij.\nFor any choice of P there is an associated set of distributions <(e x ) that equals A(?) exactly:\nTheorem: Define the components of X using multiple\nindices: For all i E {1,2,. . . ,N } and possible associated\nvalues (as one varies over z E Z ) of the vector P ( i , z ) ;\nthere is a separate component of z, zi;p(i,z). comThis\nponent can take on any of the values that zi can. D e h e\n<(.) recursively, starting at i = N and working to lower\ni , by the following rule: V i E { 1,2,. .. ,N } ,\n\nThen A ( P ) = <(Qx).\n\nProof: First note that by definition of parent functions,\ndue to the fact that we\xe2\x80\x99re iteratively working down from\nhigher i\xe2\x80\x99s to lower ones, <(z) is properly defined. Next\nplug that definition into Q. 25. For any particular z and\nassociated z = <(s),those components of z that do not\n\xe2\x80\x9cmatch\xe2\x80\x9d z by having their second index equal P(i,z ) get\nintegrated out. After this the integral reduces to\nN\n\nPz(z) =\n\n~ P X ( l \xe2\x80\x9c 2 ; P ( , , z )= Z )\nl z,\n2=1\n\ni.e., is exactly of the form stipulated in Eq. 26. Accordingly, for any fixed z and associated z = <., ranging\n()\nover the set of all values between 0 and 1 for each of\nthe distributions P x ( [ ~ ~ ; p= ,zi) will result in rang( ,~)\ning over all values for the distribution P z ( z ) that are of\nthe form stipulated in l3q- 26. This must be true for\nall z. Accordingly, <(ex) 5 A(P). The proof that\nA(P) G\ngoes similarly: For any given PZ and\nz , simply set Px([z,;p(i,z)] z,) for all the indepen=\ndent components z,;p(,,z) z and evaluate the integral\nof\nin Eq. 25. QED.\n\n<(ex)\n\nIntuitively, each component of x in the lemma is the\nconditional distribution P z ( z i I P(i,z ) )for some particular instance of the vector P(i,z ) ) . The lemma means that\nin principle we never need consider coupled distributions.\nIt suffices t o restrict attention to product distributions,\nso long a we use an appropriate semicoordinate system.\ns\nIn particular, mixture models over Z can be represented\nthis way.\n\nwe can minimize a Lagrangian involving product distri\xe2\x80\x99 U b l U I W , ----- eLfi*--h t h o --------- r l d i q.r.. ~ ~ t , i n n the\nqccnpiitp\nin\nU\n__t. i h\nultimate space of interest is not a product distribution.\nThe Lagrangian we choose over X should depend on\nour prior information, as usual. If we want that Lagrangian to include an expected value over Z (e.g., of\na cost function); we can directly incorporate that expectation value into the Lagrangian over X , since expected values in X and Z are identical: J d z P ~ ( z ) A ( z )\n=\nJ dzPx(z)A(<(z)) any function A ( z ) . (Indeed, this\nfor\nis the standard justification of the rule for transforming\nprobabilities, Eq.25.)\nHowever other functionals of probability distributions\ncan differ between the two spaces. This is especially common when <(.) is not invertible, so X is larger than Z.\nIn particular, while the expected cost term is the same\nin the X and Z maxent Lagrangians, this is not true of\nthe two entropy terms in general; typically the entropy\nof a q E Q will differ hom that of its image, ( ( 4 ) E\nin such a case.\nMore concretely, the fully formal definition of entropy\nincludes a prior probability p: SX = s & p ( z ) l n ( % ) ,\nand similarly for S Z . So long as p ( z ) and p ( z ) are related\nby the normal laws for probability transformations, as\nare p ( z ) and p ( z ) , then if the cardinalities of X and Z\nare the same, Sz = SX [51]. When the cardinalities of\nthe spaces differ though (e.g., when X and Z are both\nfinite but with differing numbers of elements), this need\nno longer be the case. The following result bounds how\nmuch the entropies can differ in such a situation:\nGVGII\n\nMaxent Lagrangians over X rather than 2\n\nWhile the distribution over X uniquely sets the distribution over 2, the reverse is not true. However so long as\nour Lagrangian directly concerns the distribution over X\nrather than t h e distribution over Z, minimizing that\nby\nLagrangian we set a distribution over 2. In this way\n\nYll\xe2\x80\x9d\n\n<(e)\n\nTheorem: For all z E Z , take p ( z ) to be uniform over\nall z such that <(z) = z. Then for any distribution p(x)\nand its image p ( z ) ,\n-J\n\ndz\n\nP(Z)\n\nh ( K ( z ) ) I - sz\nsx\n\nI\n\n0,\n\nwhere K ( z ) = sdrcb(z - ((z)). (Note that for finite X\nand Z, ( z ) 2 1, and counts the number of z with the\nK\nsame image z.) If we ignore the p terms in the definition\nof entropy, then instead we have\n\n0 5\n\nsx - sz 5\n\n-\n\nJ\n\nd z p ( z ) ln(K(2)).\n\nProof: Write\n\n= -\n\nc.\n\n\xe2\x80\x9cll\xe2\x80\x9dU,yl\n\ndz\n\nJ dz S(z\n\n- <(z)) p(z) x\n\ns.\n\nwhere d,\ns d x dz - {z)\n(\n()\nDefine pz to be the\ncommon value of all p ( z ) such that ( ( 2 ) = z . So p ( z ) =\np \xe2\x80\x9d K ( z ) and p ( z ) = pzd(z). Accordingly, expand our\nexpression as\n\nThe 2-integral of the right-hand side of the last equation\ndeis just the entropy of normalized the distribution\nfined over those z such that <(x) = z. Its maximum and\nminimum are ln[K(z)] and 0, respectively. This proves\nthe first claim. The second claim, where we \xe2\x80\x9cignore the\np terms\xe2\x80\x9d, is proven similarly. QED.\n\n#\n\nIn such cases where the cardinalities of X and 2 differ,\nwe have to be careful about which space we use to formulate our Lagrangian. If we use the transformation <(.) as\na tool to allow us to analyze bargaining games with binding contracts, then the direct space of interest is actually\nthe x\xe2\x80\x98s (that is the place in which the players make their\nbargaining moves). In such cases it makes sense to apply\nall the analysis of the preceding sections exactly as it is\nwritten, concerning Lagrangians and distributions over\nx rather than z (so long as we redefine cost functions\nto implicitly pre-apply the mapping <(.) to their arguments). However if we instead use <(.) simply as a way\nof establishing statistical dependencies among the moves\n,of the players, it may make sense to include the entropy\ncorrection factor in our 2-space Lagrangian.\nAn important special case is where the following three\nconditions are met: Each point z is the image under\n<(.) of the same number of points in 2-space, n; p(x)\nis uniform (and therefore so is p ( z ) ) ; and the Lagrangian\nin x-space, 2, is a sum of expected costs and the en\xe2\x80\x99\n\xe2\x80\x98\ntropy. In this situation, consider a z-space Lagrangian,\nTZ,\nwhose functional dependence on P,, the distribution\nover z\xe2\x80\x99s, is identical to the dependence of -Z on Pz,ex!z\ncept that the entropy term is divided by n [52]. Now\nthe minimizer P*(z)of -Yz is a Boltzmann distribution\nin values of the cost function(s). Accordingly, for any\nz , P*(x) is uniform across all n points 2 E <-\xe2\x80\x98(z) (all\nsuch x have the same cost value(s)). This in turn means\nthat S(<(P,)) = nS(P,). So our two Lagrangians give\nthe same solution, i.e., the \xe2\x80\x9ccorrection factor\xe2\x80\x9d for the\nentropy term is just multiplication by n.\n\nD.\n\nSemicoordinate transformations in team games\n\nNow consider situations in which one wishes to find\nthe global minimum of the Lagrangian for a team game.\n\nTo illustrate the generality of the arguments, situations\nwhere one has to to use Monte Carlo estimates of conditional expectation values to descend the shared Lagrangian (rather than evaluate them closed-form) will be\nconsidered.\nSay we are currently at a local minimum q E Q of\n2 of the team game. Usually we can break out of that\n\xe2\x80\x99\nminimum by raising p and then resuming the updating;\ntypically changing p changes 2 so that the Lagrange\ngaps are nonzero. So if we want to anneal p anyway\n(e.g., to find a minimum of the shared cost function G),\nit makes sense to do so to break out of any local minima.\nThere are many other ways to break out of local minima without changing the Lagrangian (as we would if we\nchanged p, for example) 1311. Here we show how to use\nsemicoordinate transformations to do this. AS explicated\nbelow, they also provide a general way to lower the value\nof the Lagrangian, whether or not one has local minimum\nproblems.\nSay our original semicoordinate system is <\xe2\x80\x99(.). Switch\nto a different semicoordinate system <\xe2\x80\x99(.) for 2 and consider product distributions over the associated space X 2 .\nGeometrically, the semicoordinate transformation means\nwe change to a new submanifold C2(Q) c P without\nchanging the underlying mapping from p(z ) to 2 2 ( p ) .\n\xe2\x80\x99\nexcept\nAs a simple example, say C2 is identical to\nthat it joins two components of x into an aggregate semicoordinate. Since after that change we can have statistical dependencies between those two components, the\nproduct distributions over X 2 , C2(Qx2),\nmap to a SUperset of < \xe2\x80\x98 ( Q X l ) . Typically the local minima of that\nsuperset do not coincide with local minima of c\xe2\x80\x99(Qx1).\nSo this change to X 2 will indeed break out of the local\nminimum, in general.\nMore care is needed when working with more complicated semicoordinate transformations. Say before the\ntransformation we are at a point p* E <\xe2\x80\x98(Qy.i). Then in\ngeneral p* will not be in the new manifold <2(Qx2),\nLe.,\np* will not correspond to a product distribution in our\nnew semicoordinate system. (This reflects the fact that\nsemicoordinate transformations couple the players.) ACcordingly, we must change from p* to a new distribution\nwhen we change the semicoordinate system.\nTo illustrate this, say that the semicoordinate transformation is bijective. Formally, this means that X 2 =\nXI = X and C2(x) = < ( ( ) a bijective <(.). Have\n\xe2\x80\x98<z)\nfor\n<(.), the mapping from X 2 to X1, be the identity map\nfor all but a few of the M total components of X, indicated as indices 1 -+ n. Intuitively, for any fixed\nx:+~-M\n- x n + 1 + ~ ,the effect of the semicoordinate\ntransformation to <\xe2\x80\x99(.) from <I(.) is merely to \xe2\x80\x9cshuffle\xe2\x80\x9d\nthe associated mapping taking semicoordinates 1 -+ n to\n2, specified by <(.). Moreover, since E ( . ) is a bijection,\nas\nthe maxent Lagrangians over X\xe2\x80\x99 and X 2 are identical:\n~xl(\xe2\x82\xac(PX\xe2\x80\x99))2\xe2\x80\x99x.((pXZ)).\n=\nNow say we set qnxfl,M - qn+14M. This means we\n- x\ncan estimate the expectations of G conditioned on possible z\n:\n,\n~\nfrom the Monte Carlo samples conditioned\n\n11\non <(z?-J.\n\nIn particular, for any <(.I we can estimate\n\nE ( G )as Sd.:-dpXZ(~:,,)E(G I <(zf,...J) in the\n\n-\n\nway. Now entropy is the sum of the entropy of semicoordinates n 1 -+ M plus that of semicoordinates 1 R.\nX2\nSo for any choice of E(.) and ql-n, we can approximate\n9 \xe2\x80\x992 x 2 as (our associated estimate of) E ( G ) mi\xe2\x80\x99\n=\nnus the entropy of p f z n , minus a constant unaffected by\nchoice of <(.).\nSo for finite and small enough cardinality of the subspace ( X I - ~ I we can use our estimates E(G I C(Z;+~))\n,\nto search for the \xe2\x80\x9csh~f\xe2\x82\xaciing\xe2\x80\x99~ and distribution q\n<(.)\n:\n,\nthat minimizes -VX [53]. In particular, say we have descended 2~to a distribution qxl(z)= q*(z). Then\nwe can set qx z = q*, and consider a set of of \xe2\x80\x9cshuf\xe2\x82\xacling\nE(.)\xe2\x80\x9d. Each such <(.) will result in a different distribution qxl(z) = qX2(<-\xe2\x80\x99(z)) q*(<-\xe2\x80\x99(z)).\n=\nWhiie those\ndistributions will have the same entropy, typically they\nwill have different (estimates of) E(G) and accordingly\ndifferent local minima of the Lagrangian.\nAccordingly, searching across the <(.) can be used to\nbreak out of a local minimum. However since E(G)\nchanges under such transformations even if we are not at\na local minimum we can searching across E ( . ) , as a new\nway (in addition to those discussed above) for lowering\nthe value of the Lagrangian. Indeed, there is always a\nbijective semicoordinate transformation that reduces the\nLagrangian: simply choose E(.) to rearrange the G(s) so\nthat G(z) < G(z\xe2\x80\x99)\nq(z) < q(z\xe2\x80\x99). In addition one can\nsearch for that <(.) in a distributed fashion, where one after the other each agent i rearranges its semicoordinate to\nshrink E(G).\nFurthermore to search over semicoordinate\nsystems we don\xe2\x80\x99t need to take any additional samples of\nG. (The existing samples can be used to estimate the\nE ( G ) for each new system.) So the search can be done\noff-line.\nTo determine the semicoordinate transformation we\ncan consider other factors besides the change in the value\nof the Lagrangian that immediately arises under the\ntransformation. We can also estimate the the amount\nthat subsequent evolution under the new semicoordinate\nsystem will decrease the Lagrangian. We can estimate\nthat subsequent drop in a number of ways: the sum of\nthe Lagrangian gaps of all the agents, gradient of the\nLagrangian in the new semicoordinate system, etc.\n\n+\n\nE.\n\nDistributions over semicoordinate systems\n\nThe straightforward way to implement these kinds of\nschemes for finding a good semicoordinate systems is via\nexhaustive search, hill-climbing, simulated annealing, or\nthe like. Potentially it would be very useful to instead\nfind a new semicoordinate system using search techniques\ndesigned for continuous spaces. When there are a h i t e\nnumber of semicoordinate systems (Le., finite X and 2)\nthis would amount to using search techniques for continuous space to optimize a function of a variable having a\n\n-\n\nfinite number of values. However we now know how to do\n---- Dn +hn,,,, Ir, the r l z r e r ~ rcmteut, thin means\nbLlab.\n-c\nt\nplacing a product probability distribution over a set of\nvariables parameterizing the semicoordinate system, and\nthen evolving the probability distribution.\nMore concretely, write\n\xe2\x80\x99 i . - ~ .\n\nI\n\nuAAb\xe2\x80\x99vLJ.\n\nN\n\ne\n\nz\n\ni=l\n\nN\n0\n\nz\n\ni=l\n\nwhere 6 is a parameter on the semicoordinate system.\nWe can rewrite this using an additional semicoordinate\ntransformation, as\nN+1\n\n2*\n\ni=l\n\nwhere zt = x2 for al i up to N , and\nl\n= 8. (As usual,\ndepending on what space we cast our Lagrangian in, the\nentropy can either have the argument of the entropy term\nstarred - as here - or not.)\nIntuitively, this approach amounts to introducing a\nnew coordinate/agent, whose \xe2\x80\x9cjob\xe2\x80\x9d is to set the semicoordinate system governing the mapping &om the other\nagents to a z value. This provides an alternative to periodically (e.g., at a local minimum) picking a set of alternative semicoordinate systems and estimating which\ngives the biggest drop in the overall Lagrangian. We\ncan instead use Nearest Newton, Brouwer updating, or\nwhat have you, to continuously search for the optimal\ncoordinate system as we also search for the optimal z.\nThe tradeoff, of course, is that by introducing an extra\ncoordinate/agent, we raise the noise level all the original semicoordinates experience. (This raises the issue of\nwhat best parameterization of <(-) to use, an issue not\naddressed here.)\nVI.\n\nRELATED WORK AND EXTENSIONS\n\nThe core of this paper is the maxent Lagrangian and\nassociated Boltzmann distribution solution. These have\nbeen investigated for well over a century in the statistical\nphysics. The use o the Boltzmann distribution over posf\nsible moves a s has a long history in the R L literature.\nlo\nIn al of this l work though the Boltzmann distribul\nU\ntion is usually motivated either as an a priori reasonable\nway to trade off exploration and exploitation, as part of\nMarkov Chain Monte Carlo procedure, or by its aspp\ntotic convergence properties [30].\nIndependent of the work in [4]the maxent Lagrangian\n,\nand/or the Boltzmann distribution has previously been\nsuggested as a way to model human players [16, 34,351.\nSome of that work has explicitly noted the relation between the Boltzmann distribution and statistical physics\n\n,12\n[36]. However the motivation of the maxent Lagrangian\nand Boltzmann distribution in that work is ad hoc, based\non particular simple models of human decision-making\nand/or of player interactions. There is no use of information theory to derive the maxent Lagrangian from first\nprinciples, as is done in PD theory.\nSome of the benefits of such a first principles approach\nare presented in this paper. Others are reported in [4].\nThese include an explicit term in the analysis that, in\nlight of information theory, corresponds to cost of computation. Other benefits are natural ways to accommodate multiple cost functions per player. PD theory also\nhighlights the very close relationship betweeen bounded\nrational game theory and statistical physics. This relationship allows many of the tools of statistical physics\nto be applied to bounded rational games. For example,\nby exploiting the grand canonical ensemble of stastistical physics, they allow one to analyze bounded rational\ngames with variable numbers of players - in essence, a\n\n[l]T. Cover and J. Thomas, Elements of Information The-\n\nory (Wiley-Interscience, New York, 1991).\n[2] D. Mackay, Information theory, inference, and learning\nalgorithms (Cambridge University Press, 2003).\n[3] E. T. Jaynes and G. L. Bretthorst, Probability Theory :\nThe Logic of Science (Cambridge University Press, 2003).\n[4] D. H. Wolpert, in Complex Engineering Systems, edited\nby A. M. D. Braha and Y. Bar-Yam (2004).\n[5] D. H. Wolpert (2003), cond-mat/0307630.\n[6] D. H. Wolpert, Bounded rationality game theory and information t h e o q (2004), submitted.\n[7] W. Macready, S. Bieniawski, and D. Wolpert, Adaptive\nmulti-agent systems for constrained optimizatzon (2004),\ntechnical report IC-04-123.\n[8] C. F. Lee and D. H. Wolpert, in Proceedings of A A M A S\n04 (2004).\n[9] S . Bieniawski and D. H. Wolpert, in Proceedings of A A MAS04 (2004).\n[lo] S. Bieniawski, D. H. Wolpert, and I. Kroo, in Proceedings of 1 Uth AIAA/ISSMO Multidisciplinary Analysis\nand Optimization Conference, Albany, New York (2004),\nin press.\n[ll]W. B. Arthur, The American Economic Review 84(2),\n406 (1994).\n[12] R. S. Sutton and A. G. Barto, Reinforcement Learning:\nAn Introduction (MIT Press, Cambridge, MA, 1998).\n[13] L. P. Kaelbing, M. L. Littman, and A. W. Moore, Journal\nof Artificial Intelligence Research 4, 237 (1996).\n[14] R. H. Crites and A. G. Barto, in Advances i n Neural Information Processing Systems - 8, edited by D. s. Touretzky, M. C. Mozer, and M. E. Hasselmo (MIT Press,\n1996), pp. 1017-1023.\n(151 J . Hu and M. P. Wellman, in Proceedings of the Fifteenth\nInternational Conference o n Machine Learning (1998),\npp. 242-250.\n[16] D. F'udenberg and D. K. Levine, The Theory of Learning\nin Games (MIT Press, Cambridge, MA, 1998).\n[17] T. Basar and G. Olsder, Dynamic Noncooperative Game\nTheory (Siam, Philadelphia, PA, 1999), second Edition.\n\nbounded rational extension of evolutionary game theory\n[41.\nFinally, it's important to note that PD theory has\nmany applications beyond those considered in this paper. For example, see [8, 31, 37-40] for other work relating the maxent Lagrangian to distributed control and\nto distributed optimization. See [31] for algorithms for\nspeeding up convergence to bounded rational equilibria.\nSome of those algorithms are related to simulated and\ndeterministic annealing [41]. See also [42, 431 for work\nshowing, respectively, how t o use PD theory to improve\nMetropolis-Hastings sampling and how to extend it to\ncontinuous move spaces and time-extended strategies.\nAcknowledgements: I would like to thank Stefan Bieniawski, Bill Macready, Stephane Airiau, Chiu Fan Lee,\nGeorge Judge, Chris Henze, and Ilan Kroo for helpful\ndiscussion.\n\n[18] M. Osborne and A. Rubenstein, A Course in Game Theory (MIT Press, Cambridge, MA, 1994).\n[19] R. Aumann and S. Hart, Handbook of Game Theory with\nEconomic Applications (North-Holland Press, 1992).\n[20] D. F'udenberg and J. Tirole, Game Theory (MIT Press,\nCambridge, MA, 1991).\n[21] R. Axelrod, The Evolution of Cooperation (Basic Books,\nNY, 1984).\n[22] T . Sandholm and V. R. Lesser, Artificial Intelligence 94,\n99 (1997).\n[23] A. Neyman, Economics Letters 19, 227 (1985).\n[24] C. Boutilier, Y. Shoham, and M. P. Wellman, Artificial\nIntelligence Journal 94, 1 (1997).\n[25] N. I. Al-Najjar and R. Smorodinsky, Game and Economic\nBehavior 37(26-39) (2001).\n[26] A. Tversky and D. Kahneman, Journal of Risk and Uncertainty 5 , 297 (1992).\n[27] D. Kahneman, American Economic Review (Proceedings) 93:2, 162 (2003).\n[28] G. Judge, D. Miller, and W. Cho, in Ecological Inference:New methodological Strategies, edited by King,\nRosen, and Tanner (Cambridge University Press, 2004).\n[29] E. T. Jaynes, Physical Review 106, 620 (1957).\n[30] C. Watkins and P. Dayan, Machine Learning 8(3/4), 279\n(1992).\n[31] D. H. Wolpert and S. Bieniawski, in Proceedings of\nCOCO4 (2004).\n[32] K. Tuyls, D. Heytens, A. Nowe, and B. Manderick, in\nLecture Notes in Artificial Intelligence, L N A I , (ECML\n2003) (2003).\n(331 K. Verbeeck, A. Nowe, and K. Tuyls, in Proceedings of\nAAMAS-3. University of Wales, Aberystwyth (2003).\n[34] J. Sharnmit and G. Arslan, Dynamzc fictitious play, dynamic gradient play, and distributed convergence to nash\nequilibria (2004), submitted.\n[35] D. F'udenberg and D. Kreps, Game and Economic Behavior 5 , 320 (1993).\n[36] S. Durlauf, Proc. Natl. Acad. Sci. USA 96, 10582 (1999).\n[37] W . Macready and D. H. Wolpert, in Proceedings of ICCS\n\n*\n\n<\n\n13\n\n.\n\nU4 (2004).\n\n(38J Airiau and U. H. Woipert (%&ij, su'onliiiai LU AAs.\nMAS 04.\n[39] N. Antoine, S. Bieniawski, I. Kroo, and D. H. Wolpert, in\nProceedings of 42nd Aerospace Sciences Meeting (2004),\naIAA-2004-0622.\n[40] S. Bieniawski and D. H. Wolpert, in Proceedings of ICCS\n04 (2004).\n[41] R. 0. Duda, P. E. Hart, and D. G. Stork, Pattern Classification (2nd ed.) (Wiley and Sons, 2000).\n[42] D. H. Wolpert and C. F. Lee, Adaptive metropolis hastings sampling wing product distributions (2004), submitted to ICCS04.\n(431 D. H. Wolpert, in Proceedings of MSRASU4, edited by\nA. S. et al (Springer Verlag, 2004).\n[a]\nThroughout this paper, the integral sign is implicitly\ninterpreted as appropriate, eg., a Lebesgue integrals,\ns\npoint-sums, etc.\n[45] The subscript q(i) on the expetation value indicates that\nit is evaluated according the distribution\nqj.\n[46] Parallel Brouwer updating can be done with minimal\nmemory requirements on the agents. Say a particular\nmove has just been taken by agent i , and that the most\nrecent time it was taken before that was T iterations ago.\nFurthermore, say the cost recorded by i for that most\nrecent instance by was r . Then the new estimated cost\nfor that move, E', is related t o the previous one, E , by\nEI\n= r f k T E a , where k is a constant less than 1, and a is\ninitially set to 1, while itself a s being updated accordlo\ning to a+ = kT. So agent i only needs to keep a running\n\nnjfi\n\ntally of E , a, and T for each of its possible moves t o use\nu c l i u u Ub'Yb, *-I-_-_\n-3fh-T fh*n 3 tally nf a11 historical time-cost\npairs\n[47] Proof outline. Write the entropy after the update as a\nsum of non-i entropies (which are unchanged by the u p\ndate) plus i's new entropy. Then expand is new entropy.\n'\nThis gives the value of the new Lagrangian as -h[N,].\nThen do the subtraction.\n[48] To write this as exponential data-aging set the exponent\ny of such data-aging to -h(l- a)).\n[49] More generally, the Nearest Newton technique uses this\nupdate rule with +t(zz,t) &?3(gt I z.,q(,)(t)) where\n=\neach g l ( z ) = G ( z ) - D ( z ( , ) )for some function D.See [31].\n[50] In the worst case, one can simply choose X to have a single component, with <(.) a bijection between that component and the vector z - trivially, any distribution over\nsuch an X is a product distribution.\n[51] For example, if X = Z = R, then I [ ]\nn*\n=\nl n [Ppz() Z ,(2)Z=] l [ ] Jc(s) is the determinant\n( J ~( )\nns,\nwhere\n\n----\n\nof the Jacobian of <(.) evaluated at z. Accordingly, as far\nas transforming from X to Z is concerned, entropy is just\na conventional expectation value, and therefore has the\nsame value whichever of the two spaces it is evaluated in.\n[52] For example, if YZ(Pz) PEP, (G(C(.)))- S(P,), then\n=\nT,(P,) = PEp,(G(.)) - S ( P Z ) / n where P, and P, are\n,\nrelated as in Eq. 25.\n[53] penalizing by the bias' plus variance expression if we\nintend to do more Monte Carlo - see [5].\n\n"