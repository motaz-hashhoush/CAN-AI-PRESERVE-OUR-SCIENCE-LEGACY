b'NASA Technical\n\nMemorandum\n\n84604\n\nlNASA-TM-84604\n\nUse of CYBER 2 0 3 and CYBER 2 0 5\nComputers for Three-Dimensional\nTransonic Flow Calculations\n\nN. Duane\n\nMelson and James D. Keller\n\nAPRIL 1983\n\n25th Anniversary\n1958-1983\n\nN/A\n\n19830014273\n\nNASA Technical\n\nMemorandum\n\n84604\n\nUse of CYBER 2 03 and CYBER\nComputers for Three-Dimensional\nTransonic Flow Calculations\n\nN. Duane Melson and James D. Keller\nLangley Research Center\nHampton, Virginia\n\nNational Aeronautics\nand Space Administration\nScientific and Technical\nInformation Branch\n1983\n\n2 05\n\nThe use of trade names in this publication does not constitute endorsement,\neither expressed or implied, by the National Aeronautics and Space Administration.\n\nSUMMARY\nExperiences\nare discussed\nfor modifying\ntwo three-dimensional\ntransonic\nflow\ncomputer\nprograms\n(FLO 22 and FLO 27) for use on the CDC \xc2\xae CYBER\n203 computer\nsystem\nat the Langley\nResearch\nCenter.\nBoth programs\ndiscussed\nwere originally\nwritten\nfor\nuse on serial\nmachines.\nSeveral\nmethods\nwere attempted\nto optimize\nthe execution\nof\nthe two programs\non the vector\nmachine:\nleaving\nthe program\nin a scalar\nform\n(i.e.,\nserial\ncomputation)\nwith compiler\nsoftware\nused to optimize\nand vectorize\nthe program,\nvectorizing\nparts of the existing\nalgorithm\nin the program,\nand incorporating\na\nnew vectorizable\nalgorithm\n(ZEBRA\nI or ZEBRA II) in the program.\nComparison\nruns of\nthe programs\nwere made on CDC \xc2\xae CYBER 175, CYBER 203, and two-pipe\nCDC \xc2\xae CYBER\n205\ncomputer\nsystems.\n\nINTRODUCTION\nMost research\nin the computation\nof transonic\nflows in recent\nyears has been to\nimprove\nthe accuracy\nand geometric\ncapability\nof computational\ntools.\nMost threedimensional\ntransonic\ncodes,\nhowever,\nstill use large amounts\nof computer\nresources\nand are expensive\nto use extensively,\nsuch as in parametric\nand optimization\nstudies.\nThus,\nthere is a need for improving\nthe computational\nefficiency\n(and\nreducing\nthe cost)\nof these codes.\nTwo ways to reduce\nrun costs and run times are through\nimprovements\nin (I) convergence\nrate and (2) calculation\nrate.\nThe way to increase\nconvergence\nrate is by\nthe use of improved\niteration\nalgorithms\nto solve the governing\nequations.\nTo date,\nthe true workhorse\nof potential\nflow calculations\nfor transonic\nflow has been the\nsuccessive\nline overrelaxation\n(SLOR) algorithm.\nSLOR is a very robust\nalgorithm\nand\nis relatively\neasy to program.\nFor these reasons,\nit has found widespread\nacceptance\nand use.\nUnfortunately,\nSLOR requires\nmany iterations\nto obtain\na converged\nsolution;\nthis can make three-dimensional\ncalculations\nvery expensive.\nThe most straightforward\nway to improve\ncalculation\nrate is through\nthe use of\nlarger,\nfaster\ncomputers.\nThree examples\nof these machines\nare the CDC \xc2\xae STAR-100,\nthe CDC \xc2\xae CYBER\n203, and the CDC \xc2\xae CYBER\n205 computer\nsystems.\nThese machines\nare all\nknown\nas vector\nprocessors.\nThey obtain\nhigh calculation\nrates\nthrough\nunique\narchitecture\nwhich\nperforms\noperations\non groups\nof operands\n(vectors).\nUnfortunately,\nthe advantages\ngained\nby vector\nprocessing\nare not without\nsome penalty.\nTo use\nvector\ninstructions,\nall the operands\nmust be available\nbefore\nexecution\nof the\ninstruction\nis begun.\nIn iterative\nalgorithms\nwhere updated\nvalues\nat adjacent\npoints\nare used to calculate\nvalues\nat a given point,\nthis requirement\nis very\nrestrictive.\nQuite often\nthe direct\napplication\nof partially\nimplicit\nalgorithms,\nsuch as SLOR,\non a vector\nmachine\nresults\nin either\nthe inefficient\nuse of its\nvector-processing\nabilities\nor the necessity\nto resort\nto slower\nscalar\noperations.\nOn the other\nhand,\nan explicit\nalgorithm\nwhich is easily\nvectorized\nmay have a much\nslower\nconvergence\nrate.\nHence,\nthe efficient\nuse of a vector-processing\nmachine\nbecomes\na trade-off\nbetween\ncalculation\nrate, convergence\nrate, and ease of\nprogramming.\nIn this paper,\nexperiences\ntransonic\nflow computer\nprograms\n\nare discussed\n(FLO 22 and\n\nfor modifying\ntwo\nFLO 27) for use on\n\nthree-dimensional\nthe CYBER 203\n\ncomputer at the Langley Research Center.\nBoth programs discussed herein were\noriginally written for use on serial machines.\nSeveral methods were attempted to\noptimize the execution of these programs on the vector machine:\n(I) using the\nexisting compiler software to optimize and vectorize the scalar code, (2) vectorizing\nparts of the existing algorithm in the program, and (3) incorporating\na new vectorizable algorithm in the program (ZEBRA I or ZEBRA II).\nComparison runs of the programs\nwere made on CYBER _75, CYBER 203, and two-pipe CYBER 205 computers.\nAll calculations discussed in this paper were for the ONERA M6 wing as described in reference 1.\nThe free-stream conditions were a Mach number of 0.84 and an angle of attack\nof 3.06 \xc2\xb0 .\nJon Hall at Control Data Corporation\non the CYBER 205 computer system.\n\nVECTOR\nThe\n\nSTAR-100,\n\nthe\n\nCYBER\n\n203,\n\nand\n\nthe\n\nran the various\n\nversions\n\nof the FLO 22 code\n\nPROCESSORS\nCYBER\n\n205\n\ncomputers\n\nare\n\nall\n\nvector\n\nproces-\n\nsors.\nIn these machines,\neach operation\nis broken\ninto many steps which\nare performed\nin series\nto obtain\nthe result\nof the operation\n(ref. 2).\nThis is a\nproduction-line\ntype of process;\nonce the first result\nis produced,\nthe others\nfollow\nvery quickly.\nThe time required\nto produce\nthe first result\nis referred\nto as the\nstart-up\ntime.\nThese vector\nprocessors\ncontrast\nin operation\nto a scalar\nor serial\nprocessor\nwhere\nall the steps\ninvolved\nin a given operation\nare performed\non one set\nof operands\nbefore\nanything\nis done with the next set.\nThe crossover\nvector\nlength,\nwhere\na vector\ninstruction\nis faster\nthan a series\nof scalar\ninstructions,\nis a function of the ratio of vector\nspeed to scalar\nspeed.\nVector\nlength\nis an important\nfactor\nin the calculation\nrate for these\nprocessors.\nThe STAR-100,\nCYBER\n203, and CYBER 205 are pipeline\nprocessors\noperate\non vectors\nas long as 65 535.\nThey operate\nmore efficiently\nas the\nlength\nincreases\nand long vectors\nshould\nbe used whenever\npossible.\n\nvector\nwhich\nvector\n\ncan\n\nIt is important\nto limit the use of scalar\noperations\nsince such operations\nare\ngenerally\nslower\nthan vector\noperations.\nThe scalar\nspeed of the STAR-100\nwas about\none-seventh\nthat of the CYBER 203, but the vector\nspeeds\nwere about\nthe same.\nOn the\nCYBER\n203, the ratio of vector\nspeed to scalar\nspeed is not very high;\ntherefore,\nsome scalar\ncode may be used without\na large penalty.\nOn the CYBER 205, where\nthe\nvector\nspeed\nis significantly\nfaster\nand the scalar\nspeed is only slightly\nfaster\nthan the CYBER\n203, it is important\nto vectorize\nthe code rather\nthan leaving\nit in\nscalar\nform.\n\nIMPLEMENTATION OF CODES\nTo examine ways to make the most efficient use of the vector-processing abilities of the CYBER 203, two programs were studied to determine the trade-offs necessary to best implement the programs. The two codes investigated were FLO 22 and\nFLO 27. FLO 22 is the Jameson-Caughey transonic wing-alone program (refs. 3 and 4)\nwhich solves the nonconservative full-potential equation in finite-difference form.\nFLO 27 is another Jameson-Caughey transonic program (ref. 5), but it is for a wing\nalone or a wing on a cylindrical fuselage of infinite length. FLO 27 solves the\nconservative full-potential equation in finite-volume form. Both codes were\noriginally written for use on a conventional computer and use the SLOR iterative\nalgorithm. Of the two programs, FLO 22 was studied in the most detail.\n2\n\nFLO 22 Study\nFLO 22 solves the inviscid flow about swept or yawed wings by using a sheared\nparabolic coordinate system in chordwise planes. Over the past several years, it has\nbeen the most widely used program in the aircraft industry for transonic wing-alone\ncalculations and was therefore chosen for implementation on the CYBER 203.\nIn the present study and another study conducted at Langley, four different\napproaches were taken to implement FLO 22 on the CYBER 203. The simplest improvement\nwas to put the program on the machine in scalar form and to use the large central\nmemory and virtual memory architecture of the CYBER 203 to replace the buffered\ninput/output features in the original code. Various levels of optimization provided\nby the compiler were used, including a provision for automatic vectorization. The\nsecond approach was to explicitly vectorize as much of the program as possible without changing the original order of calculations or the iterative algorithm. The\noriginal algorithm and order of calculations in FLO 22 allowed only limited vectorization and short vector lengths. In the third approach, a highly vectorizable algorithm, ZEBRA II (ref. 6), was incorporated into the program. This incorporation was\ndone in conjunction with a reorganization of the storage for efficient execution and\nthe resulting program was then written with vector instructions. It was found that\nthis vectorized version of ZEBRA II obtained an excellent calculation rate but the\nconvergence rate suffered to such a degree that a net negative effect was obtained\nwith the incorporation of ZEBRA II. To improve the convergence rate, another algorithm, ZEBRA I (ref. 6), was incorporated into the program in the fourth part of the\nFLO 22 study. ZEBRA I gave a convergence rate comparable to the original SLOR algorithm but this was in conjunction with a penalty in calculation rate due to factors\ndiscussed subsequently.\nScalar FLO 22.- The scalar version of FLO 22 which was available at the beginning of this study was a serial code adapted for use on a standard CYBER 175 type\nmachine, that is, no extended core capability. To operate on a CYBER 175, it was\nnecessary to use special input/output commands to move the potential array in and out\nof central memory a plane at a time since the storage was not sufficient for the\nentire array in central memory. Only four planes at a time were kept in central\nmemory.\nExecution of the serial FLO 22 program on a grid with 192 points in the x or\ntangential direction, 24 points in the y or normal direction, and 32 points in\nthe z or span direction (192 by 24 by 32) on a CYBER 175 with the highest level of\ncompiler optimization (OPT = 2) gave a calculation rate of 7400 grid points per second (pps). (See table I.)\nThe scalar CYBER 175 version of FLO 22 was modified so that it would run in\ncentral memory on a CYBER 203. This modification involved the removal of all special\ninput/output statements and the correction of the z index for each reference to the\npotential function. The scalar version of FLO 22 was executed on the CYBER 203 on a\n192 by 32 by 32 grid and a calculation rate of 16 650 pps was obtained. The program\nwas then recompiled with the optimizing version of the FORTRAN compiler (OPT = BO)\nwhich eliminates redundant code, optimizes DO loops, and does instruction scheduling.\nA calculation rate of 48 800 pps was then obtained. Thus, use of the optimization\nfeature of the compiler can make a scalar program run significantly faster and is\nrecommended. With automatic vectorization also included in the compilation\n(OPT = BOY), the same calculation rate was obtained. This is the first of several\nexamples presented in this paper which show that the automatic vectorization option\nrarely is able to vectorize code in this type of program. Since the amount of work\n\nrequired to get FLO 22 running on the CYBER 203 in scalar form was small, the speedup\nfrom 7400 to 48 800 pps made the effort very worthwhile.\nVector FLO 22.- During the period of time when Langley had a STAR-100 computer,\nit was desirable to vectorize this code since the scalar performance of that computer\nwas poor relative to its vector performance. The initial vectorization effort\ndescribed herein occurred during that period. This initial vectorization of the code\nwas performed by a group of researchers at Langley and is reported in detail in reference 7. Since this work is so closely related to the present study, a brief\ndescription of the results is given in the following discussion.\nIn Jameson\'s original FLO 22 code, calculation of updated values of the potential are obtained by successive line overrelaxation (SLOR) for combinations of normal\nand tangential lines in chordwise planes, one plane at a time, starting at the root\nand going out the span. In each plane, line overrelaxation is performed by tangential lines in the region in front of the nose of the airfoil section to infinity and\nby normal lines from this region to infinity off the trailing edge for both the upper\nand lower surfaces. (See fig. I.) The extent of the tangential implicit lines in\nthe central strip off the leading edge can be varied by an input parameter from the\ncase where all relaxation is performed by using line overrelaxation along tangential\nlines to the case where all relaxation is performed by using normal lines.\nTo make vectorization easier and to maximize their vector length, Smith, Pitts,\nand Lambiotte (ref. 7) vectorized only the tangential overrelaxation routines. This\nchoice allowed them to use vector lengths equal to the number of grid points in the\nx-direction (192 for a 192 by 32 by 32 grid as used in the test case considered in\nthis paper). This vector length was used for all the calculations necessary to generate the residual at each point along a given tangential line. (The residual is\nherein defined as the result of the finite-difference operator operating on the values of the potential function.) It is very important to have a long vector length\nfor the residual calculation since this calculation is about 90 percent of the computational work in this potential flow code.\nSince the vector length used in a calculation greatly affects the calculation\nrate, it is obvious that the number of grid points in the tangential direction will\naffect the calculation rate of the vectorized FLO 22 code. There is a crossover\nlength below which scalar instructions are faster than vector instructions. Because\nof the superior scalar speed of the CYBER 203, the crossover is higher than on the\nSTAR-100 for which the vectorized code was originally developed. In fact, for some\nof the coarser grids the scalar CYBER 203 code was found to be faster than the vectorized version.\nThere is another consideration involving vector length for this code. FLO 22\nuses central differences at subsonic points and backward differences at supersonic\npoints. This causes some difficulty in vectorizing the code. The vectorized version\nof FLO 22 calculates the residual at all the points on a line using central differences. This residual is not correct for the supersonic points on that line and must\nbe recalculated. One alternative in this recalculation is to use vector instructions\nto calculate the residual at all points on the line by using backward differences (as\nif all the points were supersonic) and use the results of this calculation only at\nthe supersonic points. This is effective if there are a large number of supersonic\npoints in the line. If the number of supersonic points is below an experimentally\nobtained value, it is faster to calculate the supersonic residuals at only the supersonic points by using scalar instructions. Lambiotte found that for this code on the\n4\n\nCYBER 203, it was better to use scalar instructions if there were less than 40 supersonic points on a given line.\nThe vectorized FLO 22 code, working on a grid size of 192 by 32 by 32, operates\nat 50 100 pps with both the BO and BOV levels of optimization. Although this was a\nconsiderable improvement over the STAR-100 scalar code, it is only modestly better\nthan the scalar code which can now be run on the CYBER 203. Interestingly enough,\nthe improved vector performance of the CYBER 205 will undoubtedly reverse this comparison again. It is possible to predict the performance of the scalar and vector\nversions of the original FLO 22 code on the CYBER 205 by using performance estimates\ndiscussed in the following section. These results show that the scalar code runs\napproximately 20 percent faster on the CYBER 205 than on the CYBER 203. Hence, the\nscalar version of FLO 22 should run at about 58 600 pps on the CYBER 205. Vector\ncode tends to run 2 I/2 to 3 times faster on the CYBER 205. Therefore, the vectorized version of FLO 22 should run at 125 000 to 150 000 pps. Thus, the vector version of the original FLO 22 code should be significantly faster than the scalar\nversion on the CYBER 205.\nFLO 22 with ZEBRA II algorithm.- In an effort to improve the calculation rate\nof the vector FLO 22 code, an explicit vectorizable algorithm was incorporated. This\nalgorithm, called ZEBRA II, was developed by South, Keller, and Hafez (ref. 6).\nBasically, ZEBRA II is a two-color point relaxation scheme where each cross\nplane is a checkerboard pattern. (See fig. 2.) The cross planes are aligned in such\na way that each color traces out tangential lines. This alignment means that all the\npoints of one color in a cross plane can be updated before points of the other color\nare updated.\nThe incorporation of ZEBRA II in FLO 22 was initially done with scalar instructions. First, it was necessary to rewrite parts of the code to allow sweeps through\nplanes in the cross flow direction rather than the chordwise direction. The residual\ncalculation was also changed to the calculation of a steady-state residual with\nexplicit updates added. (The steady-state residual is the result of the finitedifference operator operating on the values of the potential function generated by\nthe previous global iteration.)\nThe calculation rate for the scalar version of FLO 22 with ZEBRA II was\n44 700 pps for both the BO and BOV levels of optimization on the CYBER 203. With no\noptimization, a calculation rate of only 13 400 pps was obtained. This speedup due\nto optimization is not uncommon for scalar codes such as the FLO 22 with ZEBRA II.\nThe FLO 22 code with the ZEBRA II algorithm was then rewritten with vector\ninstructions. It is possible to use vector instructions to calculate the steadystate residual for an entire cross plane at a time. Then, updates are performed on\nthe points of the first color, black, and then on points of the second color, white,\nof the cross-plane checkerboard. The residuals at the white points are updated by\nusing corrections at the adjacent black points. Calculation then proceeds to the\nnext downstream cross plane. Thus, the length of the vectors used in calculating\nthe residual is equal to the number of points in a cross plane. Because of the\nrequirement that vector elements be contiguous in storage, it was necessary for the\npotential to be stored in a two-dimensional array rather than in a three-dimensional\narray. In this two-dimensional array, the first index refers to the x-direction and\nthe second index points to all the elements in the given cross plane. (An option of\n\nthe compiler was used to invoke a nonstandard array-addressing algorithm so that the\nlast index of the array varied the quickest in contiguous storage to allow vector\ninstructions.)\nIn order to calculate the residual with full-plane vectors, it was necessary to\nmake all intermediate results temporary vectors the length of the cross plane; this\nadded considerable storage to the program. Again, there was a problem relating to\nthe fact that supersonic points are not treated in the same way as subsonic points.\nThe residual was first calculated at all points in a cross plane by using the subsonic formula. If there were any supersonic points in the plane, corrections to the\nresidual were calculated (again by using full-plane vectors) and applied at only the\nsupersonic points.\nWith ZEBRA II incorporated into FLO 22 in vector form, the CYBER 203 calculation\nrate was 59 000 pps with both the BO and BOV optimization levels. For the nonoptimized level of compilation, the computation rate was 57 500 pps. The small difference is because the program is highly vectorized and the optimization only works on\nscalar code.\nTo compare the relative speeds of the CYBER 203 and CYBER 205 for transonic flow\ncalculations, both the scalar and vectorized versions of FLO 22 with ZEBRA II were\nrun, unchanged, on a two-pipe CYBER 205. The CYBER 205 gave a calculation rate of\n54 100 pps on the scalar version - a 20-percent improvement in scalar calculation\nrate over the CYBER 203. For the vectorized FLO 22 with ZEBRA II, a rate of\n173 800 pps was obtained - a 195-percent improvement in vector calculation rate over\nthe CYBER 203.\nThe sustained calculation rate for the vectorized residual and update portions\nof FLO 22 with ZEBRA II was approximately 26 million floating-point operations per\nsecond on the CYBER 203 and 76 million floating-point operations per second on the\nCYBER 205.\nSince the developmental work on ZEBRA II in reference 6 was for a conservative\nformulation of the full-potential equation, it contained no cross derivatives.\nFLO 22 is nonconservative and, therefore, does contain cross-derivative terms.\nHence, it was necessary to experiment with these terms in FLO 22 with ZEBRA II to\ndetermine the best mix of old and new values to optimize convergence. This experimentation was done with the constraint of Jameson\'s rule of balanced coefficients for\nsupersonic points (ref. 8).\nThe convergence rate of ZEBRA II was not as good in FLO 22 as was anticipated\nfrom the work in reference 6, where it was found that ZEBRA II gave a convergence\nrate comparable to SLOR. In FLO 22, the ZEBRA II algorithm required more than twice\nas many iterations to obtain the same level of convergence as with the SLOR algorithm. These extra iterations more than canceled the 20-percent increase in calculation rate obtained with the ZEBRA II.\nFLO 22 with ZEBRA I algorithm.- To improve the poor convergence rate found with\nthe ZEBRA II algorithm, it was decided to implement an algorithm known as ZEBRA I\n(ref. 6). This algorithm uses the same arrangement of black and white points as the\nZEBRA II algorithm, but the tangential lines are solved implicitly using tridiagonal\nsystems of equations. It is still vectorizable because it solves many independent\ntridiagonal systems at the same time.\n\n6\n\nFor simplicity, the vector ZEBRA II version of the program was used as the\nstarting point for the ZEBRA I incorporation. The full-plane residual calculation\nwas retained but the whole update portion of the program was changed.\nFor ZEBRA I, the two back-substitution coefficients used to solve the tridiagonal system of equations generated along tangential lines are calculated for a cross\nplane and saved as three-dimensional arrays. Since only one color is updated at\na time, only one-half the coefficients for each plane are saved. The backsubstitution coefficients are calculated one plane at a time until the coefficients\nfor all the black points in the flow field are calculated. As mentioned, the storage\nof these coefficients requires two arrays, each one-half the size of the entire flow\nfield. The back substitution is then performed by using vector lengths equal to onehalf the length of the cross planes. Updates of the potential are performed as the\ncorrections are calculated. Once the black point backward sweep is completed, the\nresidual is recalculated using full cross-plane length instructions and the new values of the potential function at the black points. Then the forward and backward\nsubstitutions are done for the white points. An obvious problem with this scheme is\nthe calculation of the residual at each point twice for each iteration. The elimination of the extra calculation of the residual at each point would require reworking\nnearly all the storage in the program which was not done in this study.\nThe convergence rate of the ZEBRA I algorithm was found to be as good as the\noriginal SLOR algorithm. The calculation rate was only 38 800 pps on the CYBER 203,\na direct result of the extra work used to calculate the residual twice at each\npoint. On the CYBER 205, a calculation rate of 124 100 pps was obtained.\nIt is possible to predict the calculation rate obtainable if the extra residual\ncalculation at each point is eliminated in FLO 22 with ZEBRA I. This requires taking\ninto account the percentage of total work that the residual calculation involves and\nthe reduced vector lengths which would be used for the calculations. For the\nCYBER 203, a calculation rate of 61 000 pps is projected. If the same rate is used\nto predict the CYBER 205 performance, a conservative estimate of 195 000 pps is\nobtained. Thus, the efficient implementation of the ZEBRA I algorithm would result\nin a significant increase in calculation rate without any degradation in convergence\nrate.\n\nFLO 27 Study\nFLO 27 is a computer code written to analyze the transonic flow over a wing\nalone or a wing on a cylindrical fuselage. It uses a finite-volume formulation to\nsolve the full-potential equation in conservative form. In the construction of\nthe computational coordinate system, a Joukowski transformation is used to transform\nthe cylindrical fuselage to a vertical slit and then a sheared parabolic transformation is used in planes containing the airfoil sections.\nThe work performed on the FLO 27 code was limited to the area of programming\ntechniques. No changes were made to the iteration algorithm used in the program\n(SLOR).\nThe starting point for this study was a version of FLO 27 which was written for\nuse on the CYBER 175 computer. Its main three-dimensional array of potential functions was stored on disk, and special input/output statements were used to bring\nplanes of data in to central memory and to store updated planes of data back on the\ndisk. This buffering of data was eliminated and the code was modified so that a\n\nlarge array containing all the values of the potential function was used.\nfied code was able to fit in central memory on the CYBER 203.\n\nThis modi-\n\nOn the CYBER 175 (with OPT = 2), the original scalar code ran at a rate of\n3200 pps on a 160 by 16 by 32 grid. This same version of the code ran at about\n3560 pps on the CYBER 203 (with OPT = BO), a speedup of 11 percent.\nThe authors then investigated a suggestion from Raymond Blanc of Control Data\nCorporation. He obtained a surprising speedup in FLO 27 by changing two small loops\nfrom scalar code to vector code.\nA timing study was performed on the subroutine which accounts for the majority\nof the execution time for the program. This subroutine, YSWEEP, contains about 350\nlines of code. In the main portion of YSWEEP, many arithmetic operations are performed for each point in the computational grid. A breakdown of the number of\noperations is as follows:\n\nOperation\n\nOccurrences\n\nAdditions and subtractions\nMultiplications\nDivisions\nSquare roots\nSine or cosine\nATAN2 function\n\n249\n142\n7\n5\n4\n2\n\nAlthough there were only two evaluations Of the ATAN2 function, the timing study\ngave the surprising result that these two operations took 77 percent of the time used\nin this portion of the code! The two ATAN2 functions happen to be in a loop which\ncould be changed easily to allow the use of the vectorized version of the ATAN2 function (VATAN2). With a vector length of 160, VATAN2 is about 32 times as fast as\nATAN2. This minor coding change caused the calculation rate for the main iteration\nloop to increase from 3560 pps to 13 320 pps, an increase of 247 percent.\nBecause of the conservative, finite-volume formulation, the main portion of the\nFLO 27 code has some simple loops which can be recognized by the compiler as vectorizable. Use of the automatic vectorizing compiler option (OPT = BOV) further\nincreases the calculation rate to 19 460 pps. Thus, with only minor modifications,\nthe calculation rate of FLO 27 was increased from 3200 pps on the CYBER 175 to\n19 460 pps on the CYBER 203 - a net increase of over 500 percent.\n\nCONCLUDING REMARKS\nTwo three-dimensional transonic flow programs (FLO 22 and FLO 27) have been\nmodified for use on CYBER vector processing machines.\nFrom the study of the FLO 22 code, it was found that on the STAR-100 machine the\nmost efficient version of FLO 22 was the Smith-Pitts-Lambiotte code (NASA Technical\nMemorandum 78665) which used vector instructions with the original SLOR algorithm.\nOn the CYBER 203, it was found that the best version of FLO 22, in terms of the\n8\n\ntrade-off between work required to change the code, convergence rate, and calculation\nrate, was the scalar version of the original code. On the CYBER 205, the long vectors and good convergence rate of the efficiently implemented ZEBRA I version of\nFLO 22 make it the fastest of the versions of FLO 22 considered in this study.\nFrom the FLO 27 study, it was found that limited vectorization and the replacement of an inefficiently implemented scalar trigonometric function with a systemsupplied vectorized routine produced significant improvements in calculation rate\nover a serial machine. These changes required a minimum of work and were thus deemed\nto be quite effective.\n\nLangley Research Center\nNational Aeronautics and Space Administration\nHampton, VA 23665\nFebruary 22, 1983\n\nREFERENCES\n\nI. Monnerie,\nRange.\n2.\n\nKnight,\nvol.\n\nB.; and\nNASA TT\n\nCharpin,\nF-15803,\n\nF.:\nBuffeting\n1974.\n\nTests\n\nJohn C.:\nThe Current\nStatus\nof Super\n10, no. I/2, Apr. 1979, pp. 401-409.\n\n3. Jameson,\nAntony:\nNumerica_\nCalculation\nOver a Yawed Wing.\nProceedings\nAIAA\nJuly 1973, pp. 18-26.\n\nWith\n\na Swept\n\nComputers.\n\nof the Three\nComputational\n\nWing\n\nComput.\n\nin\n\nthe\n\nTransonic\n\n& Struct.,\n\nDimensional\nTransonic\nFlow\nFluid Dynamics\nConference,\n\n4.\n\nJameson,\nAntony;\nand Caughey,\nD. A.:\nNumerical\nCalculation\nof the Transonic\nFlow\nPast a Swept Wing.\nC00-3077-140\n(Contract\nEY-76-C-02-3077"000\nand NASA Grants\nNGR-33-016-167\nand NGR-33-016-201),\nCourant\nInst. Math.\nSci., New York Univ.,\nJune 1977.\n(Available\nas NASA CR-153297.)\n\n5.\n\nJameson,\nAntony;\nand Caughey,\nPotential\nFlow Calculations.\nComputational\nFluid Dynamics\nAIAA Paper\n77-635.)\n\n6.\n\nD.\n\nA.:\nA Finite\nVolume\nMethod\nfor\nA Collection\nof Technical\nPapers\nConference,\nJune 1977, pp. 35-54.\n\nSouth,\nJerry\nC., Jr.; Keller,\nJames D.; and Hafez,\nMohamed\nAlgorithms\nfor Transonic\nFlow Calculations.\nA Collection\nAIAA Computational\nFluid Dynamics\nConference,\nJuly 1979,\nable\n\nas\n\nAIAA\n\nPaper\n\nas\n\nVector\nProcessor\nTechnical\nPapers\n247-255.\n(Avail-\n\n79-1457.)\n\n7. Smith,\nRobert\nE.; Pitts,\nJoan I.; and Lambiotte,\nJules\nJ.:\nJameson-Caughey\nNYU Transonic\nSwept-Wing\nComputer\nProgram\nSTAR-100\nComputer.\nNASA TM-78665,\n1978.\n8. Jameson,\nAntony:\nIterative\nSolution\nIncluding\nFlows at Mach I.\nCommun.\n1974, pp. 283-309.\n\n10\n\nM.:\nof\npp.\n\nTransonic\n- AIAA 3rd\n(Available\n\nA Vectorization\nFLO-22-VI\nfor\n\nof Transonic\nFlows Over Airfoils\nPure & Appl. Math.,\nvol. XXVII,\n\nand\nno.\n\nof\nthe\n\nthe\n\nWings,\n3, May\n\nTABLE I.- FLO 22 TIMING RESULTS\n\nr\n\nProgram\nOriginal FLO 22\n(scalar)\n\nGrid\n\nMachine\n\n192 by 24 by 32 CYBER 175\n\nOptimization\nlevel\nOPT = 2\n\n192 by 32 by 32 CYBER 203 No optimization\n\nCaiculation\nrate, pps\n7 400\n16 700\n\nOPT = BO\nOPT = BOV\n\n48 800\n\nCYBER 205\n\nOPT = BO\n\na58 600\n\n192 by 32 by 32 CYBER 203\n\nOPT = BO\n\n50 100\n\nOPT = BOV\n\nOriginal FLO 22\n(vector)\n\n48 800\n\n50 100\n\nCYBER 205\nFLO 22 with ZEBRA II\n(scalar)\n\nOPT = BO\n\n192 by 32 by 32 CYBER 203 No optimization\n\na125 000 to 150 000\n13 400\n\nOPT = BO\nOPT = BOV\nCYBER 205\nFLO 22 with ZEBRA II\n(vector)\n\n44 700\n44 700\n\nOPT = BO\n\n54 100\n\n192 by 32 by 32 CYBER 203 No optimization\n,.,\nOPT = BO\n\n57 500\n59 000\n\nOPT = BOV\n\n59 000\n\nCYBER 205\n\nOPT = BO\n\n173 800\n\nFLO 22 with ZEBRA I\n192 by 32 by 32 CYBER 203\n(residual calculated\ntwice (vector))\nCYBER 205\n\nOPT = BO\n\n38 800\n\nOPT = BO\n\n124 100\n\nFLO 22 with ZEBRA I\n!192by 32 by 32 CYBER 203\n(residual calculated\nonce (vector))\nCYBER 205\n\nOPT = BO\n\na61 000\n\nOPT = BO\n\na195 000\n\naprojected value.\n\n11\n\nExtent of tangential\nimplicit lines\n\nExtent of normal implicit lines\n\nFigure I.- Schematic of chordwise plane iteration scheme in original FLO 22.\n\n12\n\nFigure 2.- Schematicof cross-planecheckerboard\npattern for ZEBRA algorithm.\n\n13\n\n1. Report No.\n\n2, Government Accession No.\n\n3. Recipient\'s Catalog No.\n\nNASA TM- 84604\n4. Title and Subtitle\n\n5. Report Date\n\nUSE OF CYBER 203 AND CYBER 205 COMPUTERS FOR THREEDIMENSIONAL TRANSONIC FLOW CALCULATIONS\n\nApril 1983\n6 Performing\nOrganization\nCode\n505-31-03-01\n\n7. Author(s)\n\n8. Performing Organization Report No.\n\nN. Duane Melson and James D. Keller\n\nL-15553\ni0. Work Unit No.\n\n9. Performing Organization Name and Address\n\n\'11.Contract GrantNo.\nor\n\nNASA Langley Research Center\nHampton, VA 23665\n\n13. Type of Report and Period Covered\n12. Sponsoring Agency Name and Address\n\nTechnical\n\nNational Aeronautics and Space Administration\nWashington, DC 20546\n\nMemorandum\n\n14. Sponsoring Agency Code\n\n15. Supplementary Notes\n\n16. Abstract\n\nExperiences are discussed for modifying two three-dimensional transonic flow computer\nprograms (FLO 22 and FLO 27) for use on the CDC CYBER 203 computer system at the\nLangley Research Center.\nBoth programs were originally written for use on serial\nmachines. Several methods were attemptedto\noptimize the execution of the two\nprograms on the vector machine:\nleaving the program in a scalar form (i.e., serial\ncomputation) with compiler software used to optimize and vectorize the program,\nvectorizing parts of the existing algorithm in the program, and incorporating a new\nvectorizable algorithm (ZEBRA I or ZEBRA II) in the program.\nComparison runs of the\nprograms were made on CDC CYBER 175, CYBER 203, and two-pipe CDC CYBER 205 computer\nsystems.\n\n17. Key Words (Suggested by Author(s))\n\nTransonic flow\nThree-dimensional flow\nVector processing\nFLO 22\nFLO 27\n19. Security Classif. (of this report)\n\nUnclassified\n\n18. Distribution Statemen\'t\n\nZEBRA I\nZEBRA II\n\n:\n\nUnclassified\n\n- Unlimited\n\nSubject Category 34, 61\n20. Security Classif. (of this page)\n\nUnclassified\n\n21. No. of Pages\n\n15\n\n22. Price\n\nA02\n\nForsale by the NationalTechnicalInformation\nService,Springfield,Virginia 22161\n\nNASA-Langley,\n\n1983\n\nNationalAeronauticsand\nSpaceAdministration\nWashington, D.C.\n\nTHIRD-CLASS\n\nBULK\n\nRATE\n\nPostage and Fees Paid\nNational Aeronautics and\nSpace Administration\n\nNASA-451\n\n20546\nOfficial\n\nBusiness\n\nPenalty for Private Use, $300\n\n__I_A\n\nPOSTMASTER:\n\nIf Undeliverable (Section 158\nPostal Manual) Do Not Return\n\n'