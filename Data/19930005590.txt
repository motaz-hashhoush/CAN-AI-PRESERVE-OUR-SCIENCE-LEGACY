b'N93-\n\nDATA\n\nMANAGEMENT\n\nWilliam\n\nJuly\n\nPRECEDING\n\nPPlGE BLANK\n\nNOT\n\nRLMED\n\n3-189\n\nIN\n\nM. Callicott\n\n25, 1991\n\nNOAA\n\nI_7\'_9\n\nABSTRACT\n\nNOAA has 11 terabytes of digital data stored on 240,000 computer tapes. There are\nan additional 100 terabytes (TB) of geostationary\nsatellite data stored in digital form on\nspecially configured\nSONY U-Matic video tapes at the University of Wisconsin.\nThere\nare over 90,000,000 non-digital form records in manuscript, film, printed and chart\nform which are not easily accessible.\nThe three NOAA Data Centers service 6,000\nrequests per year and publish 5,000 bulletins which are distributed to 40,000\nsubscribers.\nSeventeen CD-ROMs have been produced.\nThirty thousand computer\ntapes containing polar satellite data_r_be[ng\ncopied to i2 inch WORM optical disks\nfor research applications.\nThe present annual data accumulation\nrate of 10 TB will\ngrow to 30 TB in 1994 and to 100 _B by the year _000. Thepresent\nstorage and\ndistribution technologies\nwith their attendant support systems will be overwhelmed\nby\nthese increases if not improved.\nIncreased user sophistication\ncoupled with more\nprecise measurement\ntechnologies\nwill demand better quality control mechanisms,\nespecially for those data maintained in an indefinite archive. There is optimism that the\nfuture will offer improved media technologies\nto accommodate\nthe volumes of data.\nWith the advanced technologies,\nstorage and performance\nmonitoring tools will be\npivotal to the successful long-term management\nof data and information.\n\nz\n\n3-190\n\n:\n\n....\n\nTABLE OF CONTENTS\n\nData Management\n\nin NOAA\n\nNOAA Data Management\nClimate\n\nOperations\n\n2,1\xc2\xb0\n\nNational\n\n2.2.\n2.3.\n2.4.\n\nNational Geophysical Data Center\nNational Oceanographic\nData Center\nNOAA Centers of Data\n\nDigital Request\n\nData Center\n\nHistory and Projection\n\nMass Storage/File\n\nManagement\n\nRequirements\n\n4.1.\n\nHierarchical\n\n4.2.\n4.3.\n4.4.\n\nHierarchy of Storage Levels\nFile Management Software\nNetwork Access and Networking\n\n5.0,\n\nConsiderations\n\n6.0.\n\nAssumptions\n\n7.0.\n\nFile Director\n\nConclusion\n\nand Constraints\n\n3-191\n\nm-\n\nm\n\n1.0.\n\nData Management\n\nin NOAA\n\nManagement\nof environmental\ndata and information resources is becoming an\nincreasingly visible and important issue for the scientific community.\nThis is of\nparticular importance to the National Oceanic and Atmospheric\nAdministration\n(NOAA),\nwhich routinely measures and collects large amounts of environmental\ndata and\ninformation in its own work, and is also officially charged with maintaining\nenvironmental\nrecords for the Nation. Through its activities over time, NOAA has\nbecome the steward of a treasury of Earth systems data and information--the\nmost\ncomprehensive,\nlong-term, and up-to-date environmental\ndescription of the Earth that\nexists today. This treasury contains answers to urgent environmental\nquestions facing\nthe Nation. The success of all NOAA\'s scientific work, and the national priorities which\nit supports, depends on the accountability\nand accessibility of environmental\ndata and\ninformation.\nThese data and information must be accurate, complete, stable and fullyintegrated across the spectrum of NOAA\'s Organizational functions, and they must be\nmade easily accessible, in a timely and cost-efficient way. Throughout\nthis effort to\nprovide resources to meet the NOAA missions of managing data for global change\nresearch and for enhancing warning and forecast services, there will be a continual\ninherent process to migrate and protect data held in the NOAA archives.\n2.0.\n\nNOAA Data Management\n\nOperations:\n\nThe NOAA Centers respond to requests from a broad community of research, legal,\nengineering, individuals, insurance, business, consultants, and manufacturing.\nAbout\n6,000 requests per year are received for digital data. Within the next two years as\nglobal change research increases, this should grow to 9,000. The current data files\nstored on computer readable media have a volume of 11 terabytes stored primarily on\n240,000 computer tapes. There are I00 TB of GOES data at the University of\nWisconsin\ncopied in digital form on specially recorded SONY U-matic video tapes.\nThere are analog, i.e., non-digital, holdings having a volume equivalent to over 50 TB\nin a digital domain. With the conversion of some of the analog data to digital format,\nand with new sources of digital data, the digital holdings will grow by 1996 to about 35\nTB not counting source level data from GOES. The rapidly expanding quantity of data\nwill force a different approach to managing data and information within the centers.\nThe use of an integrated mass storage systems with appropriate file management will\nbe essential to manage the archive, storage hierarchy and data migration processes.\nNew mass storage hardware and software technologies\nwill have to be developed and\nadopted and the current archives copied.\nIf a mass storage system is not developed,\nthe data centers will require immense tape storage areas and reduce the access\nmechanism from data granules to physical volumes of data.\nIn accordance\nwith the NASA/NOAA\nMemorandum\nof Understanding\nfor remotely\nsensed earth observations,\ndata processing, distribution, archiving and related science\nsupport, EOS data is intended to be archived at the NOAA data centers. The EOS\n\nPRECEDING PAGE BLANK NOT RLMED\n\n3-193\n\ndata from prototype operational instruments used for NOAA operational purposes, will\nbe an inherent part of NOAA\'s data archives. As pa.rt of the EOS pathfinder activity,\nNOAA is migrating the environmental\nsatellite data from the operational polar orbiting\nsatellites from computer tapes to 12 inch SONY optical write once, read many\n(WORM) disks. Also, there are selected special sensor microwave data from the Air\nForce Defense Meteorological\nSatellite Program (DMSP) of interest to EOS scientists.\nInitially, 8 terabytes of data will be migrated to optical disk.\n.....\n\n:\n\n,_::\n\n_i\n\ni-::\n\n_\n\nThe computing capacity at the cen{ers:is provided by mainframes, workstations,\nand\npersonal computerS (Pc;S). In the aggregate, {he systems do not have the capacity to\nhandle the anticipated archive, the growth in data ingest and dissemination,\nand\nexpanded quality control, analysis and reprocessing requirements\nin the coming years.\nThe computational\ncapacity will need to grow from 10 MFLOPS at the beginning of\n1991 to over 300 MFLOPS by 1996, i.e., a factor of 30:1. On-line disk storage should\ngrow from 60 GB to over 500 GB during this period,\nThe_ehfigurations\nare evolving\nfrom a central processor surrounded\nby dedicatecftermTna[s\nto a fully distributed clientserver architecture which can expand in response to workload demands.\n2.0. NOAA Data Centers:\nThere are three National Data Centers and over a dozen\ncenters of data in NOAA. The data centers are structured as formal archive centers\nand serve at this Nation\'s world data centers for their respective disciplines.\nThe\nfollowing provides a description of the centers and their activities.\n2.1. The National Climate Data Center (NCDC)in Ashevilie, North Carolina was\nestablished in 1950. The National Archives and Records Service, in compliance with\nthe Federal Records Act of 1950, specified that NCDC\'s climatological\nrecords be\npermanently retained.\nIt has been designated as a World Data Center (WDC)-A for\nMeteorology.\nIt also operates the Satellite Data Service Division which manages the\nhigh volume satellite data. The Center is responsible for ingest; quaqity control,\narchiving, managing, providing user access, and performing analysis of data which\ndescribes the global climate_system.\nThe NCDC also supports major new programs\nsuch as the National Weather Service Modernization,\nClimate and Global Change, the\nCoast Watch Initiative, and the Level-0 Earth Observing Systems (EOS) path finder\neffort. In 1992, new data sources from foreign satellites will be introduced.\nThe center\nhas a staff of 290 full time employees (FTEs), 100 contract and 190 federal employees.\nEach working day results in 160 orders from 360 user contacts.\nAnnually, about 5,000\nbulletins are prepared and supplied to 40,000 subscribers.\nMuch of NCDC\'s data (by\nphysical volume) is in the form of paper records such as ship logs, and although\nmanually accessible, is not readily usable, and they appear to be deteriorating.\nThere\nare fifty thousand cubic feet of paper records at NCDC. There are also film and other\nnon-machine\nreadable information stored in the National Archives.\nNOAA has a contract with the University of Wisconsin to collect and archive data from\nthe NOAA geostationary\noperational satellites (GOES). The GOES data collected from\n\n3-194\n\n1978 to the present, is recorded and stored at the University of Wisconsin\'s\nSpace\nScience and Engineering Center. To date, the GOES data are stored on 25,000\nSony U-matic beta video (19mm commercial video standard) video cassettes in 4 GB\nincrements.\nAccess to the data is not highly efficient because some of the cartridges\nare offsite in the state records office, and because the data must be reingested as a\nsatellite readout.\nThe GOES archive represents the largest amount of data anywhere\nin the NOAA system to be rescued and be made more readily accessible.\nTo improve\naccess, the center is engaged in a pathfinder study on mechanisms to improve the\naccess to the data.\n2.2. The National Geophysical Data Center (NGDC) in Boulder, Colorado was\nestablished in 1972. its mission is to manage solid earth and marine geophysical\nas well as ionospheric,\nsolar and other space environmental\ndata; and to provide\nfacilities for World Data Center-A for Geophysics which encompasses\nSolid Earth\nGeophysics, Solar-Terrestrial\nPhysics, Marine Geology and Geophysics,\nand\nGlaciology.\nThe center has a staff of 60 FTEs.\n\ndata\n\nNGDC has over 300 databases including 54 million (M) lonograms,\n2.5M\nmagnetograms,\n12 million flight miles of aeromagnetic\ndata and 10 million miles of\nship track data. There are 25,000 magnetic tapes partly at NGDC in Boulder and\npartly in Asheville at NCDC. About 2000 tapes per year arrive from originators outside\nof NGDC. Their goal is to keep all of the ingest tapes and maintain two additional\ncopies for use in normal center activities (3 copies total). About 14,000 tapes have no\nbackup. There is a requirement to copy 12,000 tapes a year for routine migration.\nNGDC relies on the error checking provided by the tape copying system and\nsupplements\nthis with printouts of beginning/end\nof record data and record counts.\nNGDC began the NOAA CD-ROM program four years ago. Its first CD-ROM title was\n"Geophysics of North America".\nThe center now has a total of 12 CD-ROMs\ncompleted or underway.\nThis effort should change the distribution system for data\nfrom its tape orientation and probably deflect some use of the network to obtain data.\nDuring 1990, distribution of data by CD-ROMs has increased both the number of\nrequests for digital data and the annual amount of data distributed by the center. The\ndistribution of data by CD-ROM puts the data into a form highly convenient and useful\nto Pcs and workstations.\nThe National Snow and Ice Data Center (NSIDC) under contract to NGDC operates\nthe World Data Center-A for Glaciology. The role of NSIDC is to acquire, archive and\ndisseminate data relating to all forms of snow and ice. It provides data to about 500\nrequesters per year from a digital archive data base of about 15 GB (300 standard\ntapes); 7GB are from the NIMBUS-7 Scanning Multi-channel\nMicrowave Radiometer,\nand 3 GB are Special Sensor Microwave/Image\n(SSM/I) data. Many of the NSIDC\ndatasets are redundantly held at other NOAA data centers. A daily volume of 1 GB of\ndata from the Defense Meteorological\nSatellite Program (DMSP) Operational Unescan\n\n3-195\n\nSystem (managed by NGDC) will be forwarded from the DMSP readout site to NSDIC\non EXABYTE tape. A similar means is being developed to distribute a weekly data\nvolume of 800MB between the Joint Ice Center (JIC), in Suitland and the National\nSnow and Ice Data Center (NSIDC) in Boulder. The NSIDC has been designated\nas\nan EOSDIS Distributed Active Archive Center (DAAC). As such, it will build up its\ncomputational\nand archival ability to meet EOSDIS defined mission goals. As a DAAC,\nthe NSIDC will relocate to the University of Colorado campus.\n2.3. The National Oceanographic\nData Center (NODC) has been in operation since\n1961 as an interagency, facility under the U.S. Navy, and became a part of NOAA in\n1970. its mission is tO manage o-cean0graphic\ndata. It has Operated as a World Data\nCenter-A (WDC-A) for Oceanography\nsince i962/ NODC has astaff of 85 FTEs.\nNODC\'s files include data collected by U.S. federal agencies; state and local\ngovernment\nagencies; universities and research institutions; foreign government\nagencies and institutions; and private industry.\nCurrently, NODC maintains a digital\narchive of both in-situ and satellite-sensed\nocean data in excess of 30 GB. A potential\nequivalent of 10 GB of digital data are currently ma|ntained inanalog\nform such as\ndata reports, manuscripts,\nand analog instrument recordings.\nWith the establishment\nof NODC data management\nresponsibilities\nfor ocean observing satellites, including\nnon-NOAA geostationary\nand orbiting platforms; new global collection efforts, including\nthe Global Ocean Flux Study (GOFS), the World Ocean Climate Experiment (WOCE"),\nand the Climate and Global Change Project; and new U. S. coastal ocean studies,\nincluding CoastWatch\nand the Coastal Ocean Program, the archive is expected to\nincrease twenty fold between FY90 and FY95.\nNODC has a large amount of analog data. A tablet digitizer is used to annually\nprocess 10,000 expendable bathythermograph\ntraces (XBT) to 2 MB of data. NODC\nhas a contract with the Navy to annually process 100,000 similar traces. Mechanical\nbathythermographs\n(MBI) on glass slides (300,000) await conversion and are\nexpected to result in 1.5 GB of data. Acoustic Doppler Profiling, done from University\nand NOAA ships, is expected to be a new source of data. There are perhaps 20 ships\nthat may be equipped with these profilers. At the present, only a few are capturing the\ndata for archiving purposes.\nEach profiler should provide 10 MB per ship month.\nIn\nthe future, 100 ship months per year of these data could be archived.\nThe NOAA Coastwatc_Data\nManagement, Archive, and Access System (NCAAS) now\nunder development\nwill result in expansion of the archive, related quality control, and\nretrieval and distribution activities based on SONY 12 inch WORM Optical Disks.\nNODC also is responsible for the NOAA Library and there is interest in digitizing some\nof its data and metadata holdings.\nNODC is respons_bTe for the NOAA Earth System Data Directory, and interfacing\nthe larger NASA base-d master directory effort. This is part Of the catalog\ninteroperability\neffort which is underway among other government\nagencies and\n\nit with\n\nm\n_1,,=\n\n3-196\n\nforeign data centers.\nThe master\nversion 0 effort where the catalog\nManagement\nSystem) functions.\nspecially written software, and the\n\ndirectory is also needed to fit with the EOSDIS\ninteroperability\nwill perform relevant IMS (Information\nNOAA\'s master directory is VAX based, with\nORACLE Data Base Management\nSystem.\n\nA prototype database system has been developed to provide NODC users with direct\naccess to an on-line, interactive data archive, it maintains a data base of over 23\nmillion marine observations\nfrom 310,000 ocean stations. Access to the data is\nobtainable through spatial or temporal searches with arbitrary combinations\nof\ninstruments,\nplatforms, and parameters.\nBy FY 1993, NODC plans to add all of its\nvertical profile data (Nansen, Bathythermograph,\nC/STD, etc.) to the POSEIDON\nsystem.\n2.4. NOAA Centers of Data include those data collection and operations elements\nperforming\nobservations\nand monitoring services as part of NOAA\'s recurring mission\nresponsibilities.\nUsted below are the principal centers:\nDisoipline\nBathymetry,\nNautical charts,\nGeodesy\n\nTitle\n\nLooation\nRockville,\n\nCharting and Geodetic\nServices\n\nClimate\n\nClimate\n\nFisheries\n\nNational Marine\nFisheries Service\n\nSeattle, WA; Woods\nHole, MA; Miami,FL;\nBay St. Louis,MS;\nSan Diego, CA\n\nIce\n\nNavy Joint Ice Center\n\nSuitland,\n\nLake Data\n\nOceanography\n\nAnalysis\n\nGreat Lakes\nEnvironmental\n\nCenter\n\nCamp\n\nMaryland\n\nSprings,\n\nMaryland\n\nAnn Arbor,\nResearch\n\nCenter for Ocean\n\nMaryland\n\nMI\n\nLab\nMonterey, CA\n\nAnalysis and Prediction\nOceanography\n\nOcean\n\nProducts\n\nCenter\n\nPacific Ocean\nData\n\nEquatorial Pacific\nInformation Collection\n\n3-197\n\nSuitland,\n\nMaryland\n\nSeattle, WA\n\nParticle\nDeposition\n\nData\n\nSea Level\n\n....\n\nAir Resources\n\nUniversity\n\nSnow and Ice\n\nLab\n\nSilver Spring,\n\nof Hawaii\n\n_\n\n:_ Honolulu,\n\nNational Snow and Ice\nData Center\n\nBoulder,\n\nTides\n\nNational Tide and\nand Water :Level Data Base\n\nRockville,\n\nTrace Gases\n\nGlobal Monitoring\nClimate Change\n\nBoulder,\n\n3.0.\n\nDigital Data Rea.uest History\n\nfor\n\nMD\n\nHI\nCO\n\nMD\n\nCO\n\nand Projection\n\nThe support for global change by the NSF has increased about 35% per year since\n1987 and is expected to increase for FY 1992. NOAA\'s global change funding has\nroughly doubled each year since 1989, Other agencies are also increasing their global\nchange funding.\nThe overall funding for all agencies has increased seven times from\nFY89 to 1991. From this, one could expect a large increase in the number of data\nrequests at the centers.\nHowever, in the aggregate, there has only been a modest\nincrease in the number of requests for each of the last two years (\'89 and \'90), and\nthe projections are, therefore, based on this modest rate of increase.\nAnother view is\nthat the secondary distribution of data from scientist to scientist may be on the\nincrease because of the ease of transmission over networks, coupled with a desire to\nobtain a dataset that has had use in a familiar science project.\nPossibly this\nsecondary distribution masks the size of the actual data dissemination.\nThe biggest impact on the number of requests and volume of data distributed has\nbeen from the introduction\nof CD-ROMs.\nThis indicates that the increased use of\nCD-ROMs for data distribution provides a means for rapid deployment of the data\namong members of the research community.\nSecondary distribution of data from\nscientist to scientist may be on the increase because of the ease of transmission\nover\nnetworks, coupled with a desire to obtain a dataset that has had use in a familiar\nscience project.\n4.0.\n\nMass\n\nStorage/Fiie\n\nManagement\n\nRea.uir_ments\n\nAs the amount of data increases\'and\nthe NOAA mission focuses on improving\naccessibility of data for global change research, there is an urgent requirement to\ndevelop mass storage systems with file management\nsoftware at the centers to\n\n3-198\n\nimprove archive management,\nprovide vastly improved access mechanisms,\nand to\nreduce the amount of media and associated space requirements.\nMoreover, the mass\nstorage system is the heart of a file management\nsystem.\nFor the immediate purpose\nat hand, a mass storage system should include the following:\n4.1.\n\nHierarchical\n\nFile Director\n\nA hierarchical file directory is needed that permits, as a minimum, the acceptance\nof\nUNIX file names. The directory needs to maintain the access and update history\ninformation for the file. The directory should allow for handling mixed media within a\nsingle search, for cross indexing between devices, and for recording data set\nutilization records for future knowledge based system applications.\nThis directory\nmust interoperate with a number of different data base systems\ninformation through during interoperable data searches.\n4.2.\n\nA hierarchy\n\nof storage\n\npassing\n\nquery\n\nlevel__\n\nThe mass storage system should support a hierarchy of storage levels with increasing\nphysical capacity and decreasing performance\nat the bottom, and decreasing physical\ncapacity and high transfer rates at the top (as viewed from the user client processes).\nAt the top, this permits the evolution to direct access electronic storage, so that the\nmass storage becomes a truly integrated part of a computing environment.\n4.3. File M_nagement\n\nSoftware\n\nThe file management\nsoftware should offer options for data compression.\nIt should\npermit the use of checksums as an overall error control mechanism.\nData conversion\nsoftware should be available to migrate the data from one physical media to another,\nas generic files, without disturbing the data content.\nThe software would sample files\non a statistical basis reading them to verify that they were still intact and that the media\nhad not deteriorated beyond the point where only soft data checks were obtained.\nIn\nthe event that sufficient degradation was detected during this sampling process, the\nfiles would be migrated to new media with a corresponding\ndirectory update.\nMigration would also be triggered during normal accesses whenever too many soft\nerrors occur.\nMigration of files from archival or working media to a buffer storage area would occur\nfollowing the initial access to permit data to be more rapidly retrieved from the faster\ndevices in the storage hierarchy.\nThe migration and actual location of the data should\nbe presented to the user/application\nprograms in a transparent\nmanner including, as\nan option, presentation to client processes in a manner simulating direct retrieval from\nthe ingest media if desired. To accomplish this transparency,\nthe file management\nsoftware should provide for the ingest of data from existing media and distributed to:\nstandard half-inch magnetic tapes in all densities and formats, EXABYTE, DAT, CD-\n\n3-199\n\nROMs, optical disks, video cassette recording technologies,\ndigital optical media, etc.\nAn encapsulation\nof the ingested media\'s data should record the presence and\nlocation of permanent data errors, physical record lengths in bytes, the presence and\nlocation of ingest media specific flags, such as tape marks, end of tape flags, etc., so\nthat upon access, the data can be handed to processing programs that need to be\naware of the different media. The file management software should be able to handle\nany of the existing data formats and to invoke conversion routines to a standard if one\nis adopted.\nThe ability to move files from the mass storage to a requester\'s media in\nthe original format should be provided.\n4.4.\n\nNetwork\n\nAccess\n\nand Networking\n\nThe NOAA centers should be coupled to the internet, at internet backbone rates, and\neventually to the N_io-nal Research and EdSc_at_n Network\n(NREN). With 740\nuniversities, laboratories and industrial sites now on the network, and 75 more\nexpected in the 2nd half of 1991, the internet is widely available to the scientists\ninvolved in global change and EOS. There are 16 NSFnet backbone sites. Two of\nthese, the University of Maryland and NCAR, are in close proximity to NOAA data\ncenters.\nWhere large data volume data transfers are required, conventional\nconveyance services would probably suffice with economic considerations\ndetermining\nthe mode of conveyance.\n5.0.\n\nConsiderati0n8\n\nThe system life under the NOAA mandate to manage data for long-term global\nchange research\npurposes is open ended. The value of data increases with age for\nuse in performing long term environmental\nchange research.\nGlobal patterns are\nknown to be subtle over time, even when viewed in a rapidly changing environment.\nToday\'s collection of environmental\ndata is pitifully small and of too short a duration\ncompared to the amount required to filter out the statistical noise over an extended\ntime domain.\nThe operational requirements\nare influenced by incremental science requirements\nestablished as the knowledge of the relationships between instrument responses and\nconversion to physical units became better known from the results of research and\ndevelopment\nof more sophisticated\nprocessing algorithms.\nBecause the development\nof sufficient knowledge to fully understand the earth observation\nresponses is an\naccretive and repetitive process, the entire data set will require repeated reprocessing\nto adequately describe the data for long-term documentation\nand preservation.\nAggregation\nof similar but different and sometimes disparate data types is also an\nimportant feature to include. A well understood aggregation principle will allow for\ncompartmenting\nthe data across the media domain in a "most" convenient form for\nvastly improving the access mechanisms.\nThis will become increasingly important as\n\n3-200\n\ni\nB\n\nthe longevity of the archive increases.\nAggregation\nimplies some degree of\nredundancy,\nbut in a positive sense, in that redundancy of particularly critical data sets\nreduces the risk of data loss over time.\nVolume management\nmay require compression\nmechanisms to reduce the overvolume of data as it ages. Critical data and information will require the application of\nIossless data compression\nwhere data sets are reduced in volume. When permitted,\nother means can be used to reduce data volume with controlled data loss as achieved\nthrough sampling, or through small-loss data compression\ntechniques or a\ncombination\nof the techniques to yield much higher compression\nratios. This may\nparticularly attractive for managing very high volume image data sampled in the visible\nspectra.\nThe technological\ngallop of the last several years continues to accelerate and new\nstorage and processing technologies\nare obviating the need to consider destruction of\ncumbersome\ndata through full scene sampling, scan sampling or reduction to gross\ndescriptive parameters which describe the sum of the parts in abbreviated form.\nIn order to take advantage of improved and less costly storage technologies,\nthere is\na plan to migrate the data periodically as the volumes dictate and the technology\nallows and with each migration to yet developed capabilities, it becomes even more\nfeasible to consider placing all of the data in a near-line access environment.\nThe\nmigration process will require content processing to re-develop the cross reference\ninventory information to include additional content description information as part of\nthe inventory metadata file to increasingly document the data as it ages. Data\nmigration is an essential element in developing a never-ending data life for the sake of\noffering an extended time baseline data set essential for detecting global scale\nchanges.\nA wide variety of media will be used for distributing data and information to users. The\nlarge number of formats used by the NOAA data centers means that many conversion\nprocedures will be needed.\nIt would be better not to convert the data in the archives\nthemselves, but to write procedures which can be invoked in a demand fashion.\nIn\nthis way, the data can be left in its original form while confidence is gained in the\naccuracy of the conversion.\nIf any problems arise in the converted data, the original\ndata will not be contaminated.\nThe problem becomes one of reworking the\nconversion routine and alerting previous users of the defect rather than trying to fix a\npartially scrambled dataset. The downside is that there will be an additional processing\ncost when the data is requested.\nAnother problem with the standard data format\nconcept is that many researchers who submit data to the data centers will probably\nnever conform to a standard format. Insistence on a format may become an\nimpediment to releasing the data to a center for distribution.\n\n3-201\n\n6.0.\n\nAssumptions\n\nand Constraints\n\nFactoring today\'s technology advancement timescale for the purpose of being both\nrealistic and conservative, the period of migration to exploit new technologies\nand\navoid system obsolescence\nto extend the validity of the data and information is\nestablished at no less than every 10 years. A suggestion was recently made that the\nmigration rate be a function of the expected half life of the medium used. For the\n3480 tapes, the manufacturers\nagree that 10 years of full performance\nlife should be\nexpected, thus the half life for migration purp0ses would be fiveyearsl\nThe criteria for\naccepting a new technology\nas a candidate for data migration is; improved archival\nqualities, the per data byte storage cost must be one half the previous, the physical\nstorage requirements\nbe at least five times less, and the data transfer rate to move\nthe data from the media be no slower than that of the older media. And, finally, the\ndata migration step will include data processing to derive or extend content description\ndata to be used for the purpose of validating the preservation state of the data and for\nreinventoring\nthe data to add content information developed through the accretion of\nuser knowledge and experience.\nAnother assumption is that all of the data will be reprocessed three times during a 2.5\nyear cycle. This reprocessing\ncycle may coincide with a data migration step since all\nof the migration will include a content review during the passage of data from one\nmedium to another.\nThe development\nof reprocessing\nalgorithms will not be charged\nas a data management\nsystem task but will require that the data management\nsystem\nbe able to put significant quantities of data on-line or at-hand for "live" ingest mode\nprocessing.\n7.0.\n\nConclusion\n\nTo continue managing data as we do today would eventually require a facility to\naccommodate\nan enormous number of media units to hold the data volumes projected\nfor the future. As the new data continues, the added function of migrating data from\ndegenerating\nmedia (from a systems as well as physical viewpoint), will compound\nthe\nstorage requirements\nas the annual data volume accumulates bY the hundreds of\ntrillions of data samples each year. If acceptable mass store systems are not\ncontinually developed to match the data growth and data management\nrequirements,\nthe logistics would be overwhelmed\nand the system would fall apart never to be\nrecovered again because of the enormous cost to recover an inevitable backlog.\nz\n\nThe data only has value to the research community if it is conveniently and efficiently\naccessible.\nIf the data were placed in a warehouse environment,\nwhich would\nultimately have to happen if nothing was done, it would soon lose its value and\npossibly its identity because of the cost to acquire it and eventually would be lost\nbecause of the huge cost to locate, ship back, copy and return the data copies as the\ndata volume grows beyond manageable proportions.\nThis Is aside to the issue of data\n\n3-202\n\nw\nE\n\nE\n\nloss due to media deterioration.\nThe only acceptable solution would be through\ndevelopment\nof a system capability to provide highly efficient and sophisticated\ndata\nmanagement\ncapabilities which would accommodate\nonline data sources.\nIn order for\nthis to happen, advanced media technologies\nhave to be employed along with\nadvanced sophisticated\ndata management\nsoftware to eliminate the manual interfaces\nwhere possible to provide the data in a ready mode for user interaction at the\nsubsetting level. The data value increases dramatically when placed in this type of\nenvironment\nas the access to the system can provide instant gratification and\nencourages\nrepeated and expanded data query activities. This, in turn, accelerates\nthe research progress and enhances the research results thus broadening the value of\nthe data to the advancement\nof science and knowledge.\nTo physically compress the data through the implementation\nof high capacity media\ncoupled with the systems capability to control and index these data in an online or\nnear-line environment offers a significant reduction in the requirements\nto house the\narchives both in terms of physical space and recurring energy and labor expenses.\nThe closer on line the data are placed, the less labor is required to physically mount\ndata either in the appropriate archive slot or onto the processing system. As\nelectronic access become more fully integrated into the system, direct labor service\ncategories will be eliminated.\nA major cost avoidance to be reckoned with is the cost\nof adding increasing large physical facilities as the data volume grows at the projected\nrates. The pace of implementation\nof new technologies\nshould allow shrinking of the\nspace requirements\nto match the increase of data accumulating\nin the facilities.\nAn indirect benefit of space compression\nthrough improved storage technologies\nwould be realized form compressing\nthe facilities requirements sufficiently to consider\nreplicating the data in distributed locations as a risk reduction measure.\nThe broadest\n\nbenefits\n\nare in terms of what the value of the data is to the world\n\nof\n\nenvironmental\nchange. Without a responsibly managed data record of scientific\nmeasurements\nover time, there would be no baseline to objectively determine if the\nenvironment we live in is really changing, how much, and at what rate. Without these\ndata, economies would be based on subjective opinions and in some cases, hear say.\nPublic policy would more often than not be misguided and consequences\nof\nenormous proportions could occur to our physical well being either through economic\ncollapse or through direct physical changes,\nEven today, global change scenarios\nportend potential devastating effects to our coastal cities and this country\'s agroeconomies.\nBut, do we build dikes and seek alternate water sources if we are not\nreally sure what, if any, impacts there are? Without the data, no one knows, so any\ninvestment in mitigating a potential problem is an economic risk. The other question\nis; even if we know, can we afford to take avoidance action? Or better yet, is the\ncause due to environmental\ncauses or due to a much broader cyclic processes.\nOne\nthing is certain, there is a great potential for change based on the knowledge at hand\ntoday, and sound economic planning based on knowledge may be sufficient to avoid\neconomic collapse.\nIn a Nation with a trillion dollar economy, the risks are too great\n\n3-203\n\nnot to invest an insignificantly small percent of this economy to acquire the maximum\namount of knowledge\npossible and establish this knowledge base as soon as\npossible.\nIn the case of environmental\ndata, data is knowledge; there can never be\nenough data, and the data record can never be long enough.\nBut where there is\ndata, it must be systematically\nmanaged to be of any value at all.\n\n!\n\nz\n\nE\n\n__=\n\n3-204\n\nDATA\nMILLIAM\n\nHANAGEHENT\nH.\n\nCALLZCOTT\n\nOFFICE\nOF SYSTEMS\nDEVELOPMENT\nDATA MANAGFJ4ENT\nSYSTEMS\nDIVISION\nNATIONAL\nDATA AND\nNATIONAL\n\nNSSDC\n\nENVIRONMENTAL\nSATELLITEs\nINFORMATION\nSERVICE\nOCEANIC\nAND ATMOSPHERIC\n\nCONFERENCE\nFOR\n\nON MASS\n\nSPACE\n\nAND\n\nSTORAGE\n\nEARTH\n\nNASA/GODOARD\n\nSPACE\n\nGREENBELT.\nJULY\n\nDATA\n\n25,\n\nSYSTEMS\n\nSCIENCE\nFLIGHT\n\nCENTER\n\n1991\n\nHANAGENE_T PROCESSES\n\nINGEST\n\nO\n\n0UALITY\n\nO\n\nCATALOG\n\no\n\nAccEss\n\nO\n\nPRESERVATION\n\n3-205\n\nCONTROL\n\nAND\n\nAPPLICATIONS\n\nMARYLAND\n\nO\n\nADMINISTRATION\n\nTECHNOLOGIES\n\nINGEST,\nO\n\nTHREE\n\nDISCIPLINE\n\nNATIONAL\nNATIONAL\nNATIONAL\nSIXTEEN\n\nCENTERS\n\nU.S.\n\nWORLD\n\nDATA\n\nCENTERS:\n\nCLIMATE\nDATA CENTER\n- ASHEVZLLE,\nNORTH CAROLINA\nOCEANOGRAPHIC\nDATA CENTER\n- WASHINGTON,\nD.C.\nGEOPHYSICAL\nDATA CENTER\n- BOULDER,\nCOLORADO\n\nCENTERS\n\nSATELLITE\n\nOF\n\nDATA\n\nNATIONAL\nNATIONAL\nCHARTING\nCLIMATE\n\nAND\n\nDATA:\nPROCESSING\n\nAND\n\nDISTRIBUTION\n\nMEATHER\nSERVICE\n- SILVER\nMETEOROLOGICAL\nCENTER\nAND GEODETIC\nSERVICES\nANALYSIS\n\nNATIONAL_ARINE\n\nCENTER\n\n-\n\nFISHERIES\n\nCAMP\n\n-\n\nSUITLAND,\n\nMD\n\nSPRING,\nCAMP SPRINGS,\n_0\nROCKVILLE,\nMD\nSPRINGS,\n\nSERVICE\n\n-\n\nM0\n\nSEAI-rLE,\n\nWA;\n\nWOODS HOLE,\nST LouIs,\n\nMIAMI, FL; BAY\nSAN DIEGO, CA\nNAVY JOINT\nICE\nCENTER\nGREAT LAKES ENVIRONMENTAL\n\nSUITLAND,\n_O\nRESEARCH\nLAB\n\n-\n\nANN\n\nCENTER\nFOR OCEAN ANALYSIS\nAND PREDICTION\nOCEAN PRODUCTS\nCENTER\n- SUITLAND,\nMD\nEOUATORIAL\nPACIFIC\nINFORMATION\nCOLLECTION\n\n-\n\nMI\n\n_ONTEREY,\n-\n\nAIR\nRESOURCES\nLABORATORY\n- SILVER\nSPRING,\nUNIVERSITY\nOF HAWAII\n- HONOLULU,\nHI\nNATIONAL\nSNOW AND ICE DATA CENTER\n- BOULDER,\nNATIONAL\nTIDE\nAND\nGLOBAL MONITORING\n\nARBOR,\n\nMA;\nMS;\n\nCA\n\nSEATTLE,\n\nWA\n\nCO\n\nWATER LEVEL\nDATA BASE - ROCKVZLLE,\nFOR GLOBAL CHANGE - BOULDER,\nC0\n\nm)\n\nNOAA CENTERS\no\n\nDIGITAL\n\nHOLDINGS INCLUDE:\n\n11 TB ON 240,000\n\nCOHPUTERTAPES\n\no\n\nIH-SITU\n\nDATA INCLUDED IN ABOVE:\n\no\n\nGOES DATA HELD AT THE UNIVERSITY OF MISCONSIN:\nU-MATIC BETA TAPES\n\no\n\nSERVICE OVER 7,000\n\no\n\nANNUALLY PRODUCE 5,000\n\no\n\nOVER 90,000,000\n\n2 TB\n100 TB ON 25,000\n\nREOUESTS FOR DATA AND INFORHATION PER YEAR\nBULLETINS TO 40,000\n\nPAGES OF NON-DIGITAL\n\n3-206\n\nSUBSCRIBERS\n\nDATA AND INFORHATION\n\nSONY\n\nII4HED\'I;ATE DlrGlr\'J\'ALVOLUHE GROWTH\n(VOLUMES\n\nDATA\n\nOCEANOGRAPHIC\nGEOPHYSZCAL\nSATELLZTE\n\nDATA\nDATA\n\nDATA\n\nCENTER\n\nCENTER\n\nSERVZCES\n\nGOES DATA ARCHIVE\nACCIJI4ULATXVE\n\nTOTAL:\n\n1992\n\n1993\n\n520\n\nCENTER\n\nB\'rLLz\'ONS\n\n1991\nCLZI4ATE\n\nZN\n\nOF\n\nBYTES)\n\n570\n\n840\n\n1. 300\n\n30\n\n160\n\n200\n\n260\n\n280\n\n450\n\n540\n\n650\n\n780\n\n930\n\n1994\n\n1995_______199_\n2.130\n\n3,420\n370\n1,110\n\n10,400\n\n14,200\n\n18.000\n\n21.800\n\n25,600\n\n29,400\n\n107.000\n\n113J000\n\n120.000\n\n134.600\n\n149.200\n\n164.000\n\n128,470\n\n139,690\n\n158,740\n\n178,140\n\n198,300\n\n118,400\n\n3-207\n\n-=\n_5\n\n==_._Z=.@=\n\nn>-\n\n!o\n\n,,=,\nn\n\n\xc2\xa2,n\n\n8\n\n_\n\n0._\n\n_z\nm\nuJ\n\nP_\n\n.\xc2\xb0\n\nl\n\n,_ul.\n\nI\n\ndg_g6o_4\ni\n\nggNgddN\nw\n\nz\n\nz\n\n_\n\n_w\n\n_-_\n\n\xe2\x80\xa2-_\n_E_\n\n.._ _=\n=_,_..<\n\nz -_\n=\n\no\n\n_v__\n\n._\n\n._<_zO_m\n\n"_\n\n1\n\nE\n_o_\xc2\xb0_\n\n0\n\nZ\n\nt_l Z\n\no\xc2\xb0\xc2\xb0\xc2\xb0\n\nm. _\n\nt_ t_ Z\n\n\xc2\xb0<\xc2\xb0\xc2\xb0\xc2\xb0\nZ\n\n1_ ,< Z\n\nZ\n\n_(_\nI-\n\n_\xc2\xb0_-\n\nwwww\n\n\xc2\xb0\xc2\xb0\xc2\xb0_Q\xc2\xb0\xc2\xb0\xc2\xb0\xc2\xb0_)\xc2\xb0\n\n(_ ,_ UJ LU -m Z\n\nZ U.i Lg (.0 _\n\n_\n\n-\'.= Z\n\n3-208\n\nLU W Z\n\n_1\n\n_\n\n_\n\nI\n\n_ Z\n\niu\n\n_\n\n/\n\n_\n\ni\n\nZ\n\nm I\n\nI\n111111\n\n_=_\n\n_g_\n\n0\n\n_\n\nPRESERVATION\n\nO\n\nNOAA MISSION TO NANAGE THE ARCHIVES ON A PERMANENTBASIS AS A\nNATIONAL TRUST\n\nO\n\nALTERNATIVES FOR PRESERVING DATA ON AN INDEFINITE\n-\n\nFIND A MEDIA THAT IS\n\nINDELIBLE INDEFINITELY\n\n...\n...\n\nMANAGE\nENSURE\n\n...\n...\n...\n...\n\n-\n\nBASES:\n\nACCESS\nSYSTEM\nINDEFINITELY\nMEDIA\nLONGEVITY\n(ENTOMB\n\nRECURRING\nQUALITY\nCONTROL\nLIFETIME\nSYSTEM MAINTENANCE\nMIGRATE\nON DEMAND\nOPERATE\nARCHIVE\nCENTER(S)\nAS\n\nMEDIA\n\nDEEP\n\nAND\n\nSITE)\n\nARCHIVE\n\nASSUME A 10 YEAR SYSTEM AND TECHNOLOGYCYCLE\n...\n\nRECURRING\nMIGRATION\nOF 10 YEAR OLD MEDIA\nCONTINUALLY\nLOOKING\nAT DEVELOPING\nTECHNOLOGY\nADVANTAGES\n\n...\n\nKEEP TWO COPIES,\nONE ENTIRE\nDATA SET\nONE IN ACTIVE\nARCHIVES\nAT DISTRIBUTED\nARCHIVE\nCENTERS\n\nENTOMBED,\nDISCIPLINE\n\n...\n\nMAINTAIN\nPORTIONS\nOF\nTHE REST NEAR-LINE\nTHE MASTER\nCOPY KEPT\n\nACTIVE\n\n...\n\nCAPABILITY\nPROCESSING\n\nTO\n\nTHE\n\nDATA\n\nAT\n\nNEAR-LIE\n\nSERVICE\n\nACCESS\n\nTHE\n\nWITH\n\nCENTERS\n\nSUFFICIENT\n\nREOUESTS\n\nAND\n\nON-LINE,\n\nON-LINE\n\nFOR\n\nMIGRATION\n\nARCHIVE INTEGRITY\n\nMEDIA PERFORMANCE\nCONTINUALLY MONITORED THROUGH ERRORDETECTION\nAND CORRECTION INFORMATION PASS BACK\n-\n\no\n\nPROCESSING\n\n-\n\nSCHEDULED\n\nMIGRATION\n\nON-DEMAND\nMEDIA\n\nMAINTENANCE\n\nOFFERS OPPORTUNITIES TO:\n\n-\n\nUP-TO-DATA\nCATALOG\nFACILITIES\nINCLUDE\nLOW-LEVEL\nDATA INVENTORY\n\nDESCRIPTIONS\n\n-\n\nWITHIN\nIMPROVE\n\nCURRENT\n\nCATALOG\nINVENTORY\nDATA AGGREGATION\n\nFILE\nTO MEET\n\n-\n\nTRANSFER\n\nTHROUGH\n\n-\n\nREGENERATION\nOF SYSTEMS\nAND DATA\nFOR ITS\nHEALTH\nENABLES\nINCREASED\nON-LINE\nACCESS\n\n-\n\nOPENS\n\nSCIENCE\n\nEXERCISES\n\n-\n\no\n\nCOMPACT\nSTORAGE AND DATA\nADVANCED\nTECHNOLOGIES\n\nNEW DOORS\n\nFOR\n\nDATA\n\nACCESS\n\nTHE\nTHE\n\nREOUIREMENTS\n\nUSE\n\nOF\n\nDATA\n\nFACILITIES\nAND\n\nTRANSFER\n\nQUALITY CONTROL:\n-\n\nANALYZE\nMONITOR\nMAINTAIN\n\nDATA TO ENSURE\nCREDIBILITY\nDURING\nINGEST\nMEDIA\nPERFORMANCE\nTO ENSURE\nRELIABILITY\nOVER TIME\nLOG OF ACCESS\nACTIVITIES\nTO BUILD\nDECISION\nHISTORY\n\n3-209\n\nF\ni\n\n'