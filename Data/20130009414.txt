b'systems that are too large to be simulated with a discrete element approach,\nPowderSim incorporates a continuumbased SPH module, which when considering the addition of a calibrated, cohesive, constitutive model (Lunar Regolith\nConstitutive Model (LRCM)), is a novel\nuse of mesh-free methods. Because of\nthe discrete and continuum methods\nimplemented in the same framework,\nthe software can capture dynamic particulate material behavior at a variety of\n\nspatial scales from the coarse-grain scale\n(DEM) to the bulk scale (SPH). The\nDEM capability also supports clustering,\nwhich allows it to capture a rich variety\nof shape detail. Advanced contact models and charge spots capture many effects of contact plasticity and hysteresis,\nroughness, adhesion, and electrostatic\ninteraction of particles. The SPH capability for bulk material behavior uses the\nLRCM to capture the critical-state behavior of cohesive lunar regolith.\n\nThis work was done by Scott Johnson, Otis\nWalton, and Randolph Settgast of Grainflow Dynamics for Glenn Research Center.\nFurther information is contained in a TSP\n(see page 1).\nInquiries concerning rights for the commercial use of this invention should be addressed\nto NASA Glenn Research Center, Innovative\nPartnerships Office, Attn: Steven Fedor, Mail\nStop 4\xe2\x80\x938, 21000 Brookpark Road, Cleveland, Ohio 44135. Refer to LEW-18801-1.\n\nMultiple-Frame Detection of Subpixel Targets in Thermal\nImage Sequences\nThis technique has applicability in fire detection, and tracking ships, ground vehicles, and aircraft.\nNASA\xe2\x80\x99s Jet Propulsion Laboratory, Pasadena, California\nThe new technology in this approach\ncombines the subpixel detection information from multiple frames of a sequence to achieve a more sensitive detection result, using only the\ninformation found in the images themselves. It is taken as a constraint that the\nmethod is automated, robust, and computationally feasible for field networks\nwith constrained computation and data\nrates. This precludes simply downloading a video stream for pixel-wise co-registration on the ground. It is also important that this method not require precise\nknowledge of sensor position or direction, because such information is often\nnot available. It is also assumed that the\nscene in question is approximately planar, which is appropriate for a high-altitude airborne or orbital view.\nThis approach tracks scene content to\nestimate camera motion and finds geometric relationships between the images. An initial stage identifies stable\nimage features, or interest points, in\n\nconsecutive frames, and uses geometric\nrelationships to estimate a \xe2\x80\x9chomography\xe2\x80\x9d \xe2\x80\x94 a transformation mapping between frames. Interest points generally\ncorrespond to regions of high information or contrast. Previous work provides\na wide range of interest point detectors.\nIn this innovation, SIFT (Scale Invariant\nFeature Transform) keypoints recovered\nby a difference of Gaussians (DoG) operator applied at multiple scales are\nused. A nearest-neighbor matching procedure identifies candidate matches between frames. The end result of this first\nstep is a list of candidate interest points\nand descriptors in each frame.\nAn important benefit of SIFT detection is that the system permits absolute\ngeoreferencing based on image contents\nalone. The SIFT features alone provide\nsufficient information to geolocate a hot\npixel. This suggests an initial characterization phase where the remote observer\ntransmits high-contrast, SIFT descriptors\nalong with images of the (fire-free) sur-\n\nface. The ground system, with possible\nhuman assistance, would determine the\nSIFT features\xe2\x80\x99 geographic locations.\nDuring regular operations, the system\ncan query the database to find geographic locations of new observations.\nAny preferred single- or multiple-channel detection rule is applied independently in each frame with a very lenient\nthreshold. Then, the algorithm matches\nconsecutive detections across potentially\nlarge displacements, and associates\nthem into tracks, i.e., unique physical\nevents with a precise geographic location, that may appear in multiple\nframes. Finally, the system considers the\nentire sequence history of each track to\nmake the final detection decision.\nThis work was done by David R. Thompson of Caltech and Robert Kremens of\nRochester Institute of Technology for NASA\xe2\x80\x99s\nJet Propulsion Laboratory. Further information is contained in a TSP (see page 1).\nNPO-48129\n\nMetric Learning to Enhance Hyperspectral Image Segmentation\nNASA\xe2\x80\x99s Jet Propulsion Laboratory, Pasadena, California\nUnsupervised hyperspectral image\nsegmentation can reveal spatial trends\nthat show the physical structure of the\nscene to an analyst. They highlight borders and reveal areas of homogeneity\nand change. Segmentations are independently helpful for object recognition, and assist with automated production of symbolic maps. Additionally, a\ngood segmentation can dramatically re-\n\n36\n\nduce the number of effective spectra in\nan image, enabling analyses that would\notherwise be computationally prohibitive. Specifically, using an over-segmentation of the image instead of individual pixels can reduce noise and\npotentially improve the results of statistical post-analysis.\nIn this innovation, a metric learning\napproach is presented to improve the\n\nperformance of unsupervised hyperspectral image segmentation. The prototype demonstrations attempt a superpixel segmentation in which the\nimage is conservatively over-segmented; that is, the single surface features may be split into multiple segments, but each individual segment, or\nsuperpixel, is ensured to have homogenous mineralogy.\n\nNASA Tech Briefs, January 2013\n\nA segmentation strategy was tested\nbased on the \xe2\x80\x9cFelzenszwalb\xe2\x80\x9d algorithm\nfor its simplicity and computational efficiency. This approach represents the\nhyperspectral image as an 8-connected\ngrid of pixels that can begin as independent segments. Edges between\nnodes represent the distance between\nneighboring spectra, and each is\nweighted according to a measure of distance between pixels. The algorithm iteratively joins neighboring pixels together into larger segments, and\ndescribes each segment by the minimum spanning tree of edges that joins\nall segments in the cluster.\n\nHyperspectral segmentation algorithms partition images into spectrally\nhomogenous regions. However, the\nexact definition of homogeneity is dependent on the chosen similarity metric.\nThe segmentation algorithm is augmented with a task-specific distance metric. Here, a Mahalanobis distance metric\nis used, learned from training data. By\nleveraging a (small) set of labeled pixels\nwith known mineralogical interpretations, the metric suppresses uninformative spectral content. Multiclass linear\ndiscriminant analysis (LDA) is used to\nmaximize the ratio of between-class vs.\nwithin-class separation, defined by the\n\nRayleigh quotient computed over labeled training data. Other distance metrics and segmentation strategies are possible, and can be substituted for these\nchoices in modular fashion as different\napplications demand.\nThis work was done by David R. Thompson and Rebecca Castano of Caltech, Brian\nBue of Rice University, and Martha S.\nGilmore of Wesleyan University for NASA\xe2\x80\x99s Jet\nPropulsion Laboratory. For more information,\ncontact iaoffice@jpl.nasa.gov.\nThis software is available for commercial licensing. Please contact Daniel Broderick of\nthe California Institute of Technology at\ndanielb@caltech.edu. Refer to NPO-48092.\n\nBasic Operational Robotics Instructional System\nLyndon B. Johnson Space Center, Houston, Texas\nThe Basic Operational Robotics Instructional System (BORIS) is a six-degree-of-freedom rotational robotic manipulator system simulation used for\ntraining of fundamental robotics concepts, with in-line shoulder, offset\nelbow, and offset wrist. BORIS is used to\nprovide generic robotics training to\naerospace professionals including flight\ncrews, flight controllers, and robotics\ninstructors. It uses forward kinematic\nand inverse kinematic algorithms to\nsimulate joint and end-effector motion,\n\nNASA Tech Briefs, January 2013\n\ncombined with a multibody dynamics\nmodel, moving-object contact model,\nand X-Windows based graphical user interfaces, coordinated in the Trick Simulation modeling environment.\nThe motivation for development of\nBORIS was the need for a generic system\nfor basic robotics training. Before\nBORIS, introductory robotics training\nwas done with either the SRMS (Shuttle\nRemote Manipulator System) or SSRMS\n(Space Station Remote Manipulator System) simulations. The unique construc-\n\ntion of each of these systems required\nsome specialized training that distracted\nstudents from the ideas and goals of the\nbasic robotics instruction.\nThis work was done by Brian Keith Todd of\nJohnson Space Center, James Fischer of Titan\nSystems Corp., and Jane Falgout and John\nSchweers of L-3 Communications. For further\ninformation, contact the JSC Innovation Partnerships Office at (281) 483-3809. MSC24850-1\n\n37\n\n'