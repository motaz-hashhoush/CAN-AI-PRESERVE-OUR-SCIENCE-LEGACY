b'Technical\n\nMemorandum\n\nSoftware\n\n104799\n\nAnalysis\n\nHandbook:\n\nSoftware\nand\n\nComplexity\n\nSoftware\n\nReliability\n\nAnalysis\nEstimation\n\nand Prediction\n\nAlice T. Lee\nTodd\n\nGunn\n\nTuan\n\nPham\n\nRon\n\nRicaldi\n\n(NASA-TM-I04799)\nSOFTWARE\nANALYSIS\nHANDBOOK:\nSOFTWARE\nCOMPLEXITY\nANALYSIS\nAND\nSOFTWARE\nRELIABILITY\nESTIMATION\nAND\nPREDICTION\n(NASA.\nJohnson\n\nAugust\n\n1994\n\n..0\nNational Aeronautics and\nSpace Administration\n\nSpace\n\nCenter)\n\n96\n\nN95-I1914\n\nUnclas\n\np\n\nG3159\n\n0023056\n\nTechnical\n\nMemorandum\n\nSoftware\n\n104799\n\nAnalysis\n\nSoftware\nand\nSoftware\n\nHandbook:\n\nComplexity\nReliability\n\nAnalysis\nEstimation\n\nAlice T. Lee\nSafety, Reliability,\n& Quaflty Assurance\nL yndon B. Johnson Space Center\nHouston, Texas\n\nTodd Gunn,\n\nTuan\n\nPham,\n\nLoral Space Information\nHouston,\nTexas\n\nNational Aeronautics and\nSpace Administration\n\nand Ron Ricaldi\nSystems\n\nOffice\n\nand Prediction\n\nThispublication\n\nis available from the NASA Center for AeroSpace\nInformation,\n800 Elkridge\nRoad, Linthicum Heights, MD 21090-2934,\n(301) 621-0390.\n\nLanding\n\nSummary\n\nThe purpose of this handbook is to document the software analysis process as it is performed\nby the Analysis and Risk Assessment\nBranch of the Safety, Reliability, and Quality Assurance\nOffice at the Johnson Space Center. The handbook also provides a summary of the tools and\nmethodologies\nused to perform software analysis.\nThis handbook is comprised of two separate sections describing aspects of software complexity and software reliability estimation and\nprediction.\nThe body of this document will delineate the background,\ntheories, tools, and\nanalysis procedures\nof these approaches.\nSoftware complexity\nanalysis can provide quantitative\ninformation on code to the designing,\ntesting, and maintenance\norganizations\nthroughout\nthe software life cycle. Diverse information\non code structure, critical components,\nrisk areas, testing deficiencies,\nand opportunities\nfor\nimprovement\ncan be obtained using software complexity analysis.\nSoftware reliability estimation and prediction analysis is a quantitative\nmeans of assessing the\nreadiness of software before it is put into operation.\nIt provides a quantitative\napproach that\nwill assist the software acceptance process.\nIn addition, the results can aid in forecasting,\nmanaging,\n\nand scheduling\n\nThis handbook\nwill be revised\n\ntesting\n\nresources.\n\nis not intended to be the exclusive guide to software analysis.\nThis document\nas new information on models and their respective processes are gathered.\n\niii\n\nContents\n\nPage\n\nSECTION 1 - SOFTWARE\nCOMPLEXITY\nTHE DETERMINATION\nOF SOF\'I\'WARE\n1.1\n1.1.1\n1.1.2\n1.1.3\n\nANALYSIS AND\nTEST COVERAGE\n\nIntroduction ...............................................................................................\nStatic Analysis ...........................................................................................\nDynamic Analysis ......................................................................................\nSoftware Functional Criticality ...................................................................\n\n1.2\n1.2.1\n1.2.1.1\n1.2.1.1.1\n1.2.1.1.2\n1.2.1.1.3\n1.2.1.1.4\n1.2.1.2\n1.2.1.3\n1.2.1.4\n1.2.2\n1.2.2.1\n1.2.2.1.1\n1.2.2.1.2\n1.2.2.1.3\n1.2.2.1.4\n1.2.2.2\n1.2.2.3\n1.2.3\n1.2.4\n\nSoftware Complexity:\nTheories and Concepts .........................................\nIntroduction to Complexity Metrics .............................................................\nCommon Metric Definitions .......................................................................\nLines of Code ............................................................................................\n\n1.3\n1.3.1\n1.3.2\n1.3.3\n\nTools .........................................................................................................\n\n1.4\n1.4.1\n1.4.1\n1.4.1\n1.4.1\n1.4.1\n1.4.1\n1.4.1\n1.4.1\n1.4.1\n1.4.1\n\nDetailed Analysis Procedures ...................................................................\nCollection of Metrics ..................................................................................\nSource Code .............................................................................................\n\nHalstead\'s Textual Complexity Metrics ......................................................\nMcCabe\'s Cyclomatic Number ..................................................................\nThe Complexity Measure of Henry and Kafura .........................................\nUtilization of Complexity\nMetrics ................................................................\nApplicability of Metrics to the Software Life Cycle .....................................\nA Practical Example:\nConstruction\nof an Error Prediction Model ..............\nDynamic Analysis:\nDefinitions and Concepts ...... .....................................\nTest Coverage Monitoring Techniques ......................................................\nEntry Point Monitoring ...............................................................................\nSegment (Instruction Block) Monitoring .....................................................\nTransfer (Decision-to-Decision\nPath) Monitoring .......................................\nPath Monitoring .........................................................................................\nProfiling .....................................................................................................\nA Practical Example: Transfer Monitoring During Interface Test Execution\nSoftware Functional Criticality ...................................................................\nResults Consolidation ................................................................................\n\nLogiscope ..................................................................\n................................\nCompiler Tools ..........................................................................................\nOther Tools ...............................................................................................\n\n.1\n.2\n.2.1\n.2.2\n.2.3\n.2.3.1\n.2.3.2\n.2.4\n.2.5\n\nLogiscope Procedures\n..............................................................................\nStatic Analysis of Code .............................................................................\nArchiving Results Files ..............................................................................\nEditor Procedures for Generating\nOutput ..................................................\nMetrics Generation ....................................................................................\nControl Graph Procedures ........................................................................\nKiviat Graph Procedures ...........................................................................\nWorkspace Management\nProcedures .......................................................\nP_\n\nPA_\n\neLAr_\n\nL_3T FtL_f]E\'_\n\n3\n3\n4\n4\n4\n5\n6\n7\n7\n7\n9\n9\n9\n9\n10\n10\n10\n10\n12\n12\n16\n16\n17\n17\n17\n17\n17\n18\n18\n21\n21\n22\n24\n25\n25\n\nContents\n(continued)\nPage\n\n1.4.2\n1.4.3\n1.4.3.1\n1.4.3.1.1\n1.4.3.1.2\n1.4.3.1.3\n1.4.3.2\n1.4.3.2.1\n1.4.3.2.2\n1.4.3.2.3\n1.4.3.2.4\n1.4.4\n1.4.4.1\n1.4.4.2\n1.4.4.3\n1.4.4.3.1\n1.4.4.3.2\n1.4.4.4\n\nMetrics Database Format and Input ..........................................................\nDynamic Analysis for Determination\nof Test Case Coverage ....................\nDynamic Analysis Using Logiscope ...........................................................\nCode Instrumentation\n................................................................................\n\n1.5\n\nUNIX Basics and Commands\n\nSECTION\n\nCompilation\nof Instrumented\nCode ............................................................\nOther Procedures ......................................................................................\nDynamic Analysis Using Compiler Directives ............................................\nCode Instrumentation\nand Compilation .....................................................\nTest Case Execution .................................................................................\nCollection and Consolidation\nof Results ....................................................\nProfiling .....................................................................................................\nLogiscope Log On Procedures ..................................................................\nRemote Log On Procedures ......................................................................\nTerminal Configuration\n..............................................................................\nLogiscope Startup Procedures ..................................................................\nUsing the Menu System ............................................................................\nFrom the Command Line ...........................................................................\nA Quick Tour of Logiscope\n\n2 - SOFTWARE\n\nRELIABILITY\n\n........................................................................\n....................................................................\n\nESTIMATION\n\n26\n27\n27\n27\n28\n28\n28\n28\n29\n29\n34\n35\n35\n36\n36\n36\n37\n37\n38\n\nAND PREDICTION\n\n2.1\n2.1.1\n2.1.2\n2.1.3\n2.1.4\n\nIntroduction ...............................................................................................\nModels .......................................................................................................\nTools .........................................................................................................\n\n2.2\n2.2.1\n2.2.1.1\n2.2.1.2\n2.2.1.3\n\nBackground ...............................................................................................\nDescription of Models ................................................................................\nTime Domain Models .................................................................................\nData Domain Models .................................................................................\n\n2.3\n2.3.1\n2.3.2\n\nTools .........................................................................................................\nSMERFS Tool ...........................................................................................\nSRE Toolkit ...............................................................................................\n\n47\n47\n48\n\n2.4\n2.4.1\n2.4.1.1\n2.4.1.2\n2.4.1.3\n2.4.1.4\n2.4.1.5\n\nData Collection\nDescriptions of\nTest Time Data\nResource Data\n\n49\n49\n49\n49\n5O\n5O\n51\n\nData Collection and Analysis .....................................................................\nModeling Procedure and Analysis .............................................................\n\nFault Seeding\n\nModels ................................................................................\n\nand Analysis .....................................................................\nData Required ..................................................................\n.............................................................\n. ............................\n..........................................................................................\n\nTest Failures Reports ................................................................................\nTest Script .................................................................................................\nSoftware Source Code ..............................................................................\n\nvi\n\n4O\n4O\n4O\n41\n41\n41\n42\n43\n46\n46\n\nContents\n(continued)\nPage\n2.4.2\n2.4.3\n2.4.3.1\n2.4.3.2\n2.4.3.3\n2.4.3.4\n\nData Collection\n\n2.5\n2.5.1\n2.5.1.1\n2.5.1.2\n2.5.1.2.1\n2.5.1.2.2\n2.5.1.2.3\n2.5.1.3\n2.5.2\n2.5.2.1\n2.5.2.2\n2.5.2.2.1\n2.5.2.2.2\n2.5.2.2.3\n2.5.2.3\n\nModeling Procedure and Analysis .............................................................\nSMERFS ...................................................................................................\nData Conversion\n.......................................................................................\n\nAppendix\nAppendix\n\nProcedure\n\n........................................................................\n\nData Analysis Procedure ...........................................................................\nAnalysis of Test Plan and Test Procedure ................................................\nTest Data Analysis ....................................................................................\nFailure Reports Analysis ...........................................................................\nExample of Data Analysis .........................................................................\n\nSMERFS\n\nModeling\n\nProcedure\n\n..................................................................\n\nSMERFS Inputs .........................................................................................\nOutput .......................................................................................................\nExecution Procedure .................................................................................\nResults Interpretation\n................................................................................\nSRE ...........................................................................................................\nData Conversion .......................................................................................\nSRE Modeling Procedure ..........................................................................\nInput ..........................................................................................................\nOutput .......................................................................................................\nExecution Procedures ...............................................................................\nResults ......................................................................................................\nA - Definitions ...................................................................................................\nB - SMERFS Files ............................................................................................\n\nREFERENCES\n\n.................................................................................................................\n\nvii\n\n51\n52\n53\n53\n54\n54\n54\n55\n55\n57\n57\n59\n59\n62\n66\n66\n68\n68\n71\n73\n73\n77\n78\n88\n\nContents\n(concluded)\nPage\n\nFigures\n\nFigure 1\nFigure 2\nFigure 3\nFigure 4\nFigure 5\nFigure 6\nFigure 7\nFigure 8\nFigure 9\nFigure 10\nFigure 11\nFigure 12\nFigure 13\n\nDistribution of test case coverage ...................................................................\nDerivation of the risk index ..............................................................................\nControl graph symbols ....................................................................................\nTest coverage results consolidation ................................................................\n.tcov file structure ............................................................................................\nLittlewood & Verral failure estimation ..............................................................\nLittlewood & Verral failure estimation ..............................................................\nSchneidewind\nMethod 1 cumulative and predicted failures - 1 .......................\nSchneidewind\nMethod 1 cumulative and predicted failures - 2 .......................\nSample SRE Musa exponential time model result ..........................................\nSample SRE failure vs. execution time plot ....................................................\nSample SRE failure intensity vs. execution time .............................................\nFailure intensity vs. cumulative failures ...........................................................\n\n11\n14\n24\n30\n31\n62\n63\n64\n65\n72\n74\n75\n76\n\nTables\nTable\nTable\n\n1\n2\n\nTable\nTable\nTable\nTable\nTable\nTable\nTable\nTable\n\n3\n4\n5\n6\n7\n8\n9\n10\n\nDemonstration\n\nof the Error Prediction\n\nModel ..................................................\n\nSummary Parameters and the Risk Index .......................................................\nLogiscope Analyzer Executables ....................................................................\nMetrics File Format ..........................................................................................\nSample Statistic Table ....................................................................................\nWorkspace\nManagement\nCommands .............................................................\nBasic Parameters for Input Into Metrics Database ..........................................\nContrasts of Software and Hardware Reliability ..............................................\nManpower and Computer Resource Parameters ............................................\nList of Parameters for the ,fp File ....................................................................\n\nviii\n\n9\n15\n19\n22\n23\n26\n26\n42\n5O\n69\n\nSECTION\n\n1 : SOFTWARE\n\nDETERMINATION\n1.1\n\nCOMPLEXITY\nOF\n\nSOFTWARE\n\nANALYSIS\nTEST\n\nAND\n\nTHE\n\nCOVERAGE\n\nIntroduction\n\nThe purpose of this section of the manual is to provide a practical orientation to software complexity analysis.\nSoftware complexity analysis can provide meaningful information to the analyst, information\nwhich can be used to determine\n\xe2\x80\xa2Software\n\ninsight into the software structure\nidentification of critical software components\nassessments\nof relative risk areas within a software\nidentification\nof testing deficiencies\nrecommendations\nfor program improvement\ncomplexity\n\nanalysis\n\ncan make a proactive\n\nsystem\n\ncontribution\n\nto improving\n\nthe quality\n\nand\n\nreliability of a software system during all phases of the software life cycle, including the preliminary design, detailed design, coding, test, and maintenance\nphases. Although the impact\nof a complexity analysis is greater during the latter stages of the software life cycle, its contribution potential is maximized if it is begun early in the life cycle. The methodology\npresented\nin this report is applicable once code is available.\nThe concepts in this report can be applied to\nthe early design phases.\nConcepts and theories are presented for three distinct types of complexity analyses, as well as\nthe methodology\nfor performing each type of analysis.\nSome potential applications of these\nanalysis results are also presented.\nThis section will also describe the features of some of the\ntools used for performing these analyses, as well as various programs written to automate the\nprocess as much as possible.\nIt is the intent of this section to provide the reader with a rudimentary knowledge\nof the theories and concepts associated with software complexity, which\nwill allow the reader to develop some proficiency\nin performing this type of analysis.\nIt is not\nthe intent of this section to provide an in-depth theoretical discussion on the derivations and\nconcepts associated\nwith software complexity,\nrather it is to provide a guide for the practitioner.\nAfter introducing\nsome of the theories and concepts associated with software complexity,\nthis\nsection will then provide a specific hands-on approach to the methodology\nassociated with\nperforming a software complexity analysis, including detailed procedures\nand the applications\nof various commercial\nsoftware packages which have been developed to assist in performing\ncomplexity analysis.\nTwo different software projects were evaluated before writing this section of the handbook.\nThe projects were dissimilar, written in different languages, and run in different execution environments.\nThe analysis of each project had some unique characteristics\nwhich contrasted\neach other, and the methods for dealing with these types of differences will be addressed\nin\nthis section.\nThe experiences\nand difficulties encountered\nwith these pilot programs should\nprovide a benefit to those who will perform software complexity analysis in the future.\nThis section\n\nintroduces\n\nthree aspects\n\nof software complexity\n\nanalysis:\n\nstatic analysis\n\nsoftware system, a dynamic analysis of the software system, and a component-level\nof the functional criticality of a software system.\nEach are addressed\nin subsequent\nbelow.\n\nof the\nanalysis\nsections\n\n1.1.1\n\nStatic Analysis\n\nThe first aspect of complexity analysis presented is the evaluation\nof software complexity\nby\ncollecting and analyzing complexity metrics. This aspect of complexity analysis addresses\nquestions concerning the complexity of a software system and its impact on errors discovered\nduring development\nand use. Complexity\nmetrics, when applied correctly, can provide a prediction of errors based on the metric parameters;\na method of monitoring the modularity and\nmaintainability\nof software; a method of measuring the number and nature of a component\'s\ninterfaces; a method of measuring the ability of a specific component to be tested; and can\nprovide the ease with which a component\ncan be reused or re-integrated\nin a new environment. In summary, a static complexity analysis can provide valuable data to support software\nquality evaluations\nand reliability predictions.\nComplexity metrics can be defined as measurements\nrelating to a software\'s\nstructure, its size,\nand its interfaces.\nThese metrics range from relatively simple measures such as the number of\nlines of code, to much more complicated\nmetrics that measure very abstract characteristics\nof\nsoftware.\nAnother category of metric which can be collected relate to the development\naspects of the software program, such as number of requirements,\ndevelopment\ntime, and number of errors in requirements.\nA more detailed definition of some of these complexity\nmetrics\nwill be presented in section 1.2.1.1.\nIt should be emphasized\nthat a static analysis of software deals only with the structure and\ndevelopment\nof the software source code, and does not require running a piece of software to\nbegin to implement a metrics program.\nThe applicability of different metrics at various stages\nof the software life cycle will be discussed\nin section 1.2.1.3.\nIt is important to note that the interpretation\nand the evaluation of complexity metrics is still\nquite subjective at this time. The selection of the metric set and the interpretation\nof the data\nare all up to the judgment and expertise of the user. An example of an analytical technique\navailable for the evaluation of the metric set is presented in section 1.2.1.4.\n\n1.1.2\n\nDynamic\n\nAnalysis\n\nAnother aspect of software complexity analysis is dynamic analysis.\nA dynamic analysis\nmeasures the efficiency and effectiveness\nof software testing by monitoring the software system during its execution phase. Data collected during the execution is used to evaluate the\nthoroughness\nof the testing, to determine\nif there is adequate utilization of the testing resources, and to prioritize the test case distributions.\nThe efficiency of software testing is\ngauged by measuring the amount of actual code executed during a software test run. There\nare several methods of measuring the efficiency of testing, each with a varying degree in the\nlevel of detail and level of effort. These methods are described\nin more detail in section\n1.2.2.1.\nTo perform\n\na dynamic\n\nanalysis,\n\nit is necessary\n\nto have an operational\n\nversion\n\nof a software\n\nsystem. This restricts the ability to perform a dynamic analysis until later in the life cycle, at the\nunit test stage and beyond.\nA dynamic analysis is performed by actually making modifications\nto the code before beginning a test or operation.\nThe code is modified at selected points by\ninserting "traps" in the code. This "instrumented"\nversion of the source code is then re-\n\ncompiled, and the test cases are performed on the instrumented version of the code. At the\nconclusion of the test run, data may be extracted as to the frequency\nof execution and the\nidentification\nof unexecuted\nblocks of code.\n\n1.1.3\n\nSoftware\n\nFunctional\n\nCriticality\n\nSoftware functional criticality addresses the failure modes and effects of a specific software\ncomponent in relation to the system operation.\nThe first step in assessing the functional criticality of a component\nis to establish the failure effect criteria, including factors such as risk to\nhuman life, risk to national resources (vehicle, etc.), loss of the software functional capability,\nor reduction in software capability.\nThe next step is to approximate\nthe failure modes of each\ncomponent,\nand attempt to determine the overall effect of that individual component on the\nentire software system.\n\n1.2\n1.2.1\n\nSoftware\n\nComplexity:\n\nIntroduction\n\nTheories\n\nto Complexity\n\nand Concepts\n\nMetrics\n\nSoftware complexity\nmetrics can be defined as measurements\nused to characterize\nand\nquantify various properties of a software system.\nWhen they are applied correctly, software\ncomplexity\nmeasurements\ncan be used to make decisions which can reduce the development\ncost, increase the reliability, and improve the overall quality of the software system.\nThe science of software complexity was introduced\nin the mid 1970s when Halstead proposed\nmeasurements\ndesigned to determine various software quality characteristics.\nSince that time,\nresearchers\nhave proposed literally hundreds of measurements\nquantifying various aspects of\nsoftware, from relatively simple and comprehensible\nmeasurements\n(such as lines of code), to\nvery abstract measurements\n(such as entropy or hierarchical\ncomplexity).\nThere is much debate over the relative value of one metric over another, and there is no standard set of metrics\nwhich has widespread\nuse in industrial or research applications.\nIt is up to the user to make\nsubjective decisions as to what metrics to collect, what metrics are most applicable and useful\nat specific points in the software life cycle, and the proper and appropriate\nuse of these\nmetrics.\nSoftware complexity\narchitecture\nmetrics.\n\nmetrics can be divided into two general categories:\ntextual metrics and\nTextual metrics are measurements\ntaken on an individual software com-\n\nponent. Textual metrics stem directly from the original Halstead metrics, measuring attributes\nand functions which can be determined by looking at the module as a stand-alone\ncomponent.\nSome examples of textual metrics are the lines of code, the number of nests, and the number\nof operators.\nArchitecture\nmetrics are based on the design of a software system, and how individual components are connected together.\nArchitecture\nmetrics often measure such properties as the\nmodularity of a software system, the identification\nand use of shared (global) data structures,\nthe communication\nflow between individual components,\nand data coupling.\n\n3\n\nIn addition to complexity\nmetrics, other metrics can be collected to determine characteristics\nof\nthe software being analyzed.\nThese metrics usually measure programmatic\nfactors such as\nthe number of requirements,\nthe number of pages in a requirements\ndocument, and the number of changes in requirements.\nThis analysis limits its scope to complexity metrics, and does\nnot utilize the programmatic\nmetrics in the formulation of the results. The analysis example\ndescribed in section 1.2.1.4 can be easily adapted, however, to include such programmatic\nmetrics.\n\n1.2.1.1\n\nCommon\n\nMetric\n\nDefinitions\n\nThis section will define some of the metrics which can be used in a complexity analysis.\nThe\nmetrics defined here are not all the metrics used in current studies, but does provide a representative list of some metrics which one might consider to use. The fact that different metrics\nwere collected for these different projects illustrates one shortfall of complexity metric analysis:\nthere is a definite need for a standardized\nset of complexity metrics which can be used from\nproject to project and language to language.\n\n1.2.1.1.1\n\nLines of Code\n\nThe most familiar\n\nsoftware\n\nmeasure\n\nis the count of the lines of code.\n\noften abbreviated\nLOC, or for large programs\nsus to the exact definition of what constitutes\nveloped.\nOne common definition is :\n\nThe term lines of code is\n\nKLOC (1000 lines of code). There\na line of code, although a standard\n\nis no consenis being de-\n\nA line of code is counted as the line or lines between semicolons,\nwhere intrinsic\nsemicolons\nare assumed at both the beginning and the end of the source file.\nThis specifically\nincludes all lines containing program headers, declarations,\nexecutable and non-executable\nstatements.\nThis definition\n\nof the lines of code includes non-executable\n\ncode. This includes\ncount. Care should\n\nanalysis of a program.\nThis is especially\nlines of code in different fashions.\n\n1.2.1.1.2\n\nHalstead\'s\n\nstatements\n\nin the count of lines of\n\ncomments and header information,\nwhich some prefer to eliminate in their\nbe taken to ensure that one definition is used consistently\nthroughout\nan\n\nTextual\n\nComplexity\n\ntrue if using different\n\nanalyzers,\n\nwhich might calculate\n\nMetrics\n\nHalstead proposed several measures from which he was able to empirically derive characteristics. Four base measurements\nwere taken from the source code, and the rest of the quantities\nwere derived from the four base measurements.\nThe four base measurements\nnl\nn2\nN1\nN2\n\nare\n\nnumber of distinct operators\nnumber of distinct operands\ntotal number of operators\ntotal number of operands\n\n4\n\nFromthese four measurements, alsteadderivedthe followingquantities:\nH\nProgramLength\n\nN = N1 + N2\nThis is Halstead\'sdefinitionof the lengthof a program.\n\nProgramVolume\n\nV = (NI+N2)In(nl+n2)\nThis is an indicationof the numberof bits neededto\ncodea program\n\nProgramSize\n\nS = (nl) In(n1) + (n2)In(n2)\nThis is Halstead\'s indication\nprogram readability.\n\nProgram\n\nDifficulty\n\nD = [(nl )/2] (N2/n2)\nThis is an indication\nunderstanding\n\nMental\n\nEffort\n\nEstimated\n\nof "error aptitude"\n\nof the difficulty\n\na program\n\nin developing\n\nof Errors\n\nB= [E ** (2/3)] / 3000\nThis is an estimate of the amount\na program\n\nand\n\ncomponent.\n\nE = [(nl) (N2) (NI+N2)In(nl+n2)]/2(n2)\nThis is an indication of the effort required\nstand and develop a program.\n\nNumber\n\nand\n\nof errors\n\nto under-\n\nresident\n\nin\n\nmodule.\n\nLogiscope as well as most code analysis tools will calculate the majority of Halstead\'s metrics,\nboth derived and the base metrics. (For a complete list of the Logiscope standard metric set\nand the methods of defining new metrics, refer to section 1.4.1 .) All the tools researched to\ndate provide at least the four base metrics, from which the derived metrics can easily be calculated.\n\n1.2.1.1.3\n\nMcCabe\'s\n\nCyclomatic\n\nNumber\n\nMcCabe\'s\nprinciples\n\ncyclomatic\nnumber is one of the more popular complexity\nmetrics. Derived from\nadapted from graph theory, the cyclomatic\nnumber gives the minimum number of\n\nindependent\nnumber of paths through the logic flow of a program.\nThere are actually two\nversions of McCabe\'s cyclomatic number, so care should be used when comparing results.\n\nThe first version\n\nof the cyclomatic\n\nnumber\n\nis defined\n\nas\n\nV(G) = IEI - INI + 2p\nwhere\n\nV(G)\nE\nN\nP\n\n=\n=\n=\n=\n\n(Eq 1)\n\nMcCabe\'s\nnumber of\nnumber of\nnumber of\ncalculating\n\ncyclomatic number\nedges in the control graph\nnodes in the control graph\nconnected components\nor subprograms\nV(G) for single components\nor modules,\n\n(for\np = 1)\n\nThere is one problem with this version of the cyclomatic number: the numbers are not additive\nwhen stringing together program components.\nIn other words, the whole is not equal to the\nsum of its parts. For example, consider two software components,\nA and B, each with a cyclomatic number equal to 6. If A and B were tied together into one larger program (where the\nexit of A led straight into the beginning of B), one would expect the cyclomatic number of this\nnew component\nto be 12. That is not the case, as the new component would actually have a\ncyclomatic\nnumber 10. This can be a problem, especially when considering\ngroups of programs or evaluating\nan entire program.\nTo correct this problem, McCabe proposed a modification to the definition of the cyclomatic\nnumber, which is often referred to as the essential cyclomatic number. The essential cyclomatic number is calculated by\nV(G) = IEI- INI + p\nThe essential\n\ncyclomatic\n\nnumber\n\n(Eq 2)\nis additive.\n\nIt is not always clear which version of the cyclomatic number is being calculated, as the term\n"cyclomatic\ncomplexity"\nis often used to refer to either of the two versions, which may be represented by the same symbol V(G). One should try to use the essential cyclomatic number\nwhen possible due to the usefulness\nof its additive properties.\nLogiscope, for example, calculates the first version of the cyclomatic\nnumber.\nThis can be corrected by programming\na\nmodification\nequation into the reference file. The details of the procedures\nare found in sections 1.3.1, 1.3.3, and 1.4.4\n\n1.2.1.1.4\nAnother\n\nThe Complexity\nmetric was proposed\n\nMeasure\n\nof Henry and Kafura\n\nby Henry and Kafura:\n\nlength * (fan-in * fan-out)**2\nwhere\nlength\nfan-in\n\nis the number of lines of code in a program\nis the number of data objects passed into a called procedure plus the\nnumber of global data structures from which the procedure retrieves its\ninformation\n\nfan-out\n\nis the number of data objects received from a called procedure\nnumber of global data structures which the procedure updates\n\n6\n\nplus the\n\n1.2.1.2\n\nUtilization\n\nof Complexity\n\nMetrics\n\nComplexity\nmetrics are collected to measure various properties and aspects of a software program, either at a component\nlevel or a system level. Complexity metrics can be used to make\nquality evaluations\nof software, to monitor the development\ngrowth of software systems, and to\nestimate and predict the reliability of software.\nComplex software often leads to high development costs, high maintenance\ncosts, and decreased system reliability.\nBy monitoring the\ncomplexity\nof a software program during design and development,\nchanges can be made to\nreduce complexity and ensure the system remains modifiable and modular.\n\n1.2.1.3\n\nApplicability\n\nof Metrics\n\nto the Software\n\nLife Cycle\n\nSome metrics might be reflective of the software system (relative to error prediction) during the\nearly stages of the program development,\nwhile other metrics might characterize\nthe software\nsystem better during the later stages of development\nor during the maintenance\nphase of the\nprogram.\nOne example might be that during the early design phase (Preliminary\nDesign Review or prior), a simple metric such as the number of requirements\nmight be used to estimate\nthe error count in a software component.\nAfter coding and some testing, it might be found that\nthe specific metric might be replaced by metrics more representative\nof the software structure\nto give more accurate results. It is up to the analyst to select the appropriate\nmetric set which\nwill be most representative\nof the software system at a specific moment in the development\nand usage phase.\n\n1.2.1.4\n\nA Practical\n\nExample:\n\nConstruction\n\nof an Error Prediction\n\nModel\n\nThis example employed complexity metrics to predict the number of latent errors residing in a\nsoftware program.\nThis was accomplished\nby adapting the methodology\nproposed by Khoshgoftaar and Munson [1]. The concept behind this procedure is to develop a regression model\nwhich uses complexity\nmetrics to predict the number of errors in a software component.\nA similar method for predicting the failure rate of software was proposed by the Rome Air Development\nCenter (RADC)[2].\nHowever, the error prediction model presented here differs from\nthat presented by the RADC mainly in the selection of the model parameters.\nThe RADC\nmodel was developed to be applicable throughout the entire software life cycle, so the model\nemphasizes\nrequirements-oriented\nmetrics such as number of requirements,\nnumber of pages\nin the requirements\ndocuments,\nand requirements\ndevelopment\ntime. The error prediction\nmodel presented here does not include requirements-oriented\nmetrics as it is strictly based on\nthe code complexity and structural characteristics.\nThis allows one to investigate the detailed\ninteractions\nbetween the structural characteristics\nof a software program and its failure rate.\nThe first step in developing this regression model was to collect metric values\nmation from previous projects. The following metrics were collected:\n-Number\n\nof Statements\n\n-Total Number of Operands\n-Total Number of Operators\n-Henry and Kafura\'s Complexity\n-Comment\nFrequency\n-Larsen HC Metric\n\nMetric\n\nand error infor-\n\n-Essential Cyclomatic Complexity\n-Number of Inputs\n-Number of Outputs\n-Number of Direct Calls\n-Nesting Depth\n-Program Length\n\n7\n\n(Halstead)\n\nWith the exception of the complexity metric by Henry and Kafura, each of these metrics has\nthe properties of ratio-scaled\nmeasures as defined by Zuse [3]. The selection of ratio-scaled\nparameters\nis important, as it allows for such statistical operations\nas arithmetic mean and the\ncalculation of percentages.\nIt should be noted that the metric set was selected subjectively,\nand research is continuing to define a minimum standardized\ncomplexity\nmetric set. This\nminimum set would ideally be the minimum number of metrics required to fully characterize\nthe\nsoftware.\nFor samples of actual metric data, refer to tables 4 and 5 in section 1.4.1.2.4.\nA regression analysis was performed using the complexity metric values and associated errors\nfor each component.\nTo handle the multicollinearity\nbetween the metric values, we used the\nmethod of principle components.\nThe method of principle components\nreduces the effects of\nthis collinearity by taking only a limited subset of the principle components.\nFor a more complete description of the method of principle components,\nsee Montgomery\nand Peck [4].\nAfter the regression\nmodel is defined, the metrics can be grouped into categories according\ntheir relative effect in areas such as complexity,\ninterface criticality, testability, and maintainability. This method was proposed by McCabe [5].\n\nto\n\nWe developed the error prediction model based on error data collected from two dissimilar\nprojects.\nMetrics were collected using a commercial\ncode analyzer program and the regression analysis was performed using a PC-based statistical analysis package.\nThe R2 coefficient of determination\nfor the model using the method of principle components\nwas approximately 0.8. We believe that the accuracy of this model will be increased\nbeen collected over a larger portion of the software life cycle.\n\nonce more data has\n\nFour arbitrary levels were selected on the basis of the corresponding\nerror prediction model\noutput, each level corresponding\nto a higher predicted error count. These levels were referred\nto as error susceptibility\nlevels, and numbered from 1 to 4, with 1 corresponding\nto the level\nhaving the highest amount of predicted errors. The numbering of the levels was selected in\nthis fashion to be consistent with the failure effects analysis which will be described later. This\nzoning of the components\ninto levels reduces the granularity of the model; however, this reduction in precision is acceptable until the model can be solidly established\nwith a sufficient\namount of data.\nThe model was demonstrated\nby considering\nthe number of error reports encountered\nduring\ntesting of a large-scale test support software system developed for NASA. Metrics were collected for this system and inputted into the error prediction model. Components were assigned\na level of error susceptibility\nbased on the predicted number of errors. The number of error\nreports per component was found by dividing the total number of error reports generated for all\nthe components\nin that susceptibility\nlevel by the number of components\nin that level. The\nresult of this study is shown in table 1. Note that 6.5% of the components\n(identified as level 1\nand 2) contributed\nto 44% of the total failures.\n\nTable\nError\nSusceptibility\nLevel\n\n1 - Demonstration\nNumber\n\nof the Error Prediction Model\n\nof\n\nComponents\n\nNumber\n\nof\n\nError Reports\n\nNumber of Error\nReports per\nComponent\n\n1\n\n2.94\n\n3.60%\n\n14%\n\n1.16\n\n3\n\n14.75%\n\n37%\n\n0.76\n\n4\n\nDynamic\n\n30%\n\n2\n\n1.2.2\n\n2.93%\n\n78.72%\n\n19%\n\n0.08\n\nAnalysis:\n\nDefinitions\n\nand Concepts\n\nDynamic analysis focuses on the execution of software such as the portions of the actual code\nwhich get executed, the frequency of their execution, and the amount of time spent in each\napplication.\nThe frequency of execution and timing of a program are often referred to as\nprofiling.\nA typical application\nof a dynamic analysis is in the determination\nof test case coverage.\ndynamic analysis will track the execution path through the software code, and determine\nlogic paths are actually executed.\n\n1.2.2.1\n\nTest Coverage\n\nMonitoring\n\nA\nwhat\n\nTechniques\n\nTo evaluate the test case coverage, some method of monitoring the execution status of the\nsoftware component during the execution of the code must be used. Four types of monitoring\nmethods are typically used, listed below in increasing order of complexity and expense as to\nthe amount of execution time required.\nThe method selected for use will depend on the tool\nused for monitoring as well as the criticality of the component being monitored.\nEach method\nrequires a technique\nusually referred to as instrumentation.\nInstrumenting\na software program\nusually means adding additional code which records the execution status of various parts of\ncode. These status flags can then be post-processed\nto allow one to trace the execution sequence through the source code itself. The degree to which a software program is instrumented is determined\nby the monitoring technique which is to be used.\n\n1.2.2.1.1\n\nEntry Point Monitoring\n\nEntry point monitoring tests only to see that a given component is used by a test case but says\nnothing about which parts of the component were executed.\nThis is the least complicated\nof\nthe monitoring techniques,\nand requires the least amount of overhead in terms of computing\nresources.\n\n1.2.2.1.2\n\nSegment\n\n(Instruction\n\nBlock) Monitoring\n\nSegment monitoring tests to see that all the statements\ndoes not relate any information as the logic paths,\n\nin the code have been executed\n\nbut\n\n1.2.2,1.3 Transfer (Decision-to-Decision Path) Monitoring\nTransfermonitoring\n\nmeasures the logical branches from one segment to another.\nExecuting\nequates to the execution of all the statements as well as all the logic paths in\n\nall the transfers\nthe code.\n\n1.2.2.1.4\n\nPath Monitoring\n\nPath monitoring attempts to monitor the execution of all the possible paths through the code.\nPath monitoring is impractical\nexcept on very small routines due to the large number of possible paths that result in code with several control statements.\n\n1.2.2.2\n\nProfiling\n\nThere are also monitoring techniques for measuring the frequency and execution times of individual components.\nThis type of monitoring\ncan be very useful in determining\nthe actual\nexecution times of individual components,\nand such information can be used to estimate an\noperational\nprofile. Profiling information can be extremely useful, especially in future research\nand work. Most compilers have profiling options, which can provide useful information.\nProfiling, in this sense, can be referred to as monitoring a program\'s execution and outputting a\ndetailed procedure-by-procedure\nanalysis of the execution time, including\n-\n\nhow\nwhat\nhow\nwhat\n\nmany times a procedure was called\nprocedure called it\nmuch time was spent in the procedure\nother procedures did it call during its execution\n\nProfiling\n\ndata is typically\n\n1.2.2.3\n\nA Practical\n\nThis example\n\ncollected using a combination\n\nExample:\n\nevaluated\n\nTransfer\n\ntest coverage\n\nMonitoring\n\nof compiler\n\nDuring\n\ndirectives\n\nInterface\n\nand UNIX utilities.\n\nTest Execution\n\nand used these results to determine\n\nthe efficiency\n\nof\n\ntesting and the approximate\nusage over the operational\nprofile. To evaluate the test case coverage, one of the methods described in section 1.2.2.1 must be used to monitor the execution\nstatus of the software component.\nThe transfer monitoring technique (sometimes referred to\nas decision-to-decision\npath monitoring) was selected for use in this example.\nThe decision-todecision path monitoring technique requires instrumenting\neach decision node in the program,\nrecompiling\nthe program, and monitoring the execution status of each node. The data recorded was whether or not a specific path was executed during the test run, and the execution\nfrequency of that path.\nA histogram summarizing\nthe distribution of test case coverage for a system containing approximately\n140 components\nis presented in figure 1. The bars represent the number of components whose test coverage fell within the interval represented on the X-axis. The single line\nrepresents the cumulative\npercentage\nof components\nwhose coverage falls within a specified\ninterval or below. The left vertical axis is the scale for the frequency of components\nwithin a\ngiven interval, and the cumulative\npercentage\nscale is on the right vertical axis. For example,\nthe leftmost bar in figure 1 indicates that approximately\n28 components\nhad less than 10% of\n\n10\n\nthe code tested.\n\nThe cumulative\n\ncates that this represents\n\npercentage,\n\napproximately\n\nfound by referring to the right vertical\n\n20% of the total number\n\naxis, indi-\n\nof components.\n\nThe test coverage distribution after the completion of testing operations is shown in figure 1.\nNote that approximately\n20% of the components\nin this program were not executed at all (0%\ntest coverage) during the testing. The initial response to this situation is that the program was\nnot completely tested.\nHowever, after further investigation,\nit was found that the majority of\nthis untested code represented functions which were no longer used in this release of the\nsoftware, or represented\nredundant code. One recommendation\nmade from this study was to\neliminate redundant and unused code from the operational version of the software.\n\n30\n\n7\n\n100%\n\n/\n/\n/\n/\n/\n\n25\n\n/\n\n#\n\n80%\n\n/\n//\n//\n/\n/\n/\n\n2O\n\n__\n\n/\n60%\n\n/\n15\n//\n\nET\n\n=2\n\n40%\n\nLL\n/\n\nI0\n/\n/\n///\n\n1\n\n20%\n_J\n\ni\n\no\n\nI\n\nI\n\n0\n\n10\n\n20\n\n30\n\nPercentage\n\n40\n\nI\n\nI\n\n50\n\nI\n\n60\n\n70\n\nof Code\n\nFigure 1 - Distribution\n\nil\n\n80\n\nI\n\n90\n\n100\n\nExecuted\n\nof test case coverage.\n\nThe results of this study were used to determine the efficiency of testing in relation to the\namount of source code covered, and to provide an estimate of the relative amount of code\nexecuted per component for normal program operations.\nWhen combined with the results\nfrom the error prediction model, this result can be used to approximate the number of latent\nerrors remaining in a particular component.\nFor example, suppose a component was predicted\nto have 10 errors (from the output of the prediction model), and under a nominal operational\nprofile 40% of the entire code was expected to be executed.\nAssuming the faults are randomly distributed throughout\nthe entire code, six latent faults would be expected to reside in\nthe component at delivery, to be found during future operations of the software.\n\n11\n\n1.2.3\n\nSoftware\n\nFunctional\n\nCriticality\n\nThe first step in assessing the functional criticality of a component is to establish the failure\neffect criteria, including factors such as risk to human life, risk to national resources (vehicle,\netc.), loss of the software functional capability, or reduction in software capability.\nThe next\nstep is to approximate\nthe failure modes of each component,\nand attempt to determine the\noverall effect of that individual component on the entire software system. The propagation\nof\nthese failure modes across the interfaces can then be considered.\nThe weighting of each individual\nbeyond\n\nfailure mode by its associated\nthe current scope of the work.\n\nA Practical\n\nExample:\n\nFunctional\n\nprobability\n\nCriticality\n\nand failure criticality\n\nfor a Specific\n\nwas considered\n\nbut was\n\nSystem\n\nThis example discusses the determination of the functional criticality levels for components\nwhich made up a safety-critical ground software system. The failure criteria were established\nby creating four specific groups, similar to hardware FMEA classifications. Criticality one designates the most critical components, while criticality four components are the least critical.\nThe criteria for assigning a component to a specific criticality group were based on the software system under study being a safety critical item. For other particular systems, the criteria\nmay include other factors such as cost, schedule, and fault tolerance. In this case the following criteria were established:\n1)\n\nFailure of criticality 1 components would cause a complete\npossibly resulting in loss of vehicle and human life.\n\n2)\n\nLoss of criticality 2 components\nwould cause\ndegrade the overall system performance.\n\n3)\n\nLoss of criticality 3 components\nwould affect non-critical functions, such as some plotting and printing functions, but primary program objectives would remain unaffected.\n\n4)\n\nLoss of criticality\nof a non-essential\nfected.\n\nCost was not considered\nsearch\n\nis needed\n\na recoverable\n\nloss of program function,\n\nloss of function or severely\n\n4 components\nwould result in a recoverable loss or intermittent failure\nfunction, but the primary program objectives would remain unaf-\n\nas a factor\n\nto evaluate\n\nin the definition\n\nthe relation\n\nof the criteria for each level.\n\nof cost and development\n\nFurther\n\ntime to the criticality\n\nre-\n\nlevel.\n\nEach individual component was evaluated according to the above criteria. The results of this\nevaluation were then applied to the risk index derivation and described in section 1.2.4.1.\n\n1.2.4\n\nResults\n\nConsolidation\n\nThe results from these analyses\nfunctional criticality analysis--can\ndecision making process.\n\ndescribed above--a\nstatic analysis, a dynamic\nbe consolidated\nto form a single result which\n\n12\n\nanalysis, and a\ncan aid in the\n\nA Practical\n\nExample:\n\nDerivation\n\nof the Risk Index\n\nThe concept of a risk index for software has been developed to provide a quantitative risk assessment of software systems by integrating the results of the complexity analyses. This risk\nindex is a factor that takes into account the likelihood of a failure as well as the consequence\nof that failure. The underlying concept of associated risk has been adapted from safety hazard analyses performed on hardware systems. Risk is calculated as the product of the severity\nof a failure occurrence multiplied by the probability of such a failure occurrence. The risk index\nprovides a quantified measure for comparing cases such as a failure mode of a component\nthat is highly unlikely but catastrophic, versus a failure mode with less severe consequences\nbut with a greater chance of occurrence.\nPreviously, engineering judgment has been used to\nmake these comparisons.\nThe risk index factor is a quantitative\nmeasure of risk defined to be the\nity of a failure occurrence with the severity of that occurrence.\nResults\ntion model, the test coverage, and the failure effects analysis are used\nAn overview of the derivation of the risk index is shown below in figure\n\nproduct of the probabilfrom the error predicto calculate this factor.\n2.\n\nThe test coverage results can give profiling information for each component if the test cases\nconsidered approximate\nthe actual usage expected for the software.\nIdeally, a well-defined\noperational profile will be available early in the software life cycle. However, dynamic coverage\nresults can be used in the absence of an adequately defined operational profile. For a description of the development\nof the operational\nprofile, see Musa [6]. The coverage results\nprovide a measure of the amount of untested code. Multiplying this measure by the results\nfrom the error prediction\nmodel gives an estimate of the number of latent errors remaining in\nthe component.\nThis assumes that if a given statement or segment has been executed successfully during test, then the number of errors remaining in that statement or segment can be\nconsidered to be insignificant.\nThe test coverage\nresults can also provide the profiling information\nnecessary to determine the\napproximate\nprobability of execution of a particular component.\nIf the software is not yet mature enough for a dynamic test case analysis, the profiling information and associated probabilities can be approximated\nbased on the expected operational usage.\nWe wanted\n\nthe risk index to be normalized\n\nto be in the range [0,1], with higher values\n\nindicat-\n\ning greater risk. Because the functional criticality levels were chosen to be analogous to existing hardware criticality definitions, a linear transformation\nwas required before computing the\nrisk index. The transformation\nis:\nFC\'=\n\n(Eq 3)\n\n(1.25-0.25xFC)\n\nin which FC\' is the transformed\n\ncriticality\n\nindex and FC is the original value.\n\n13\n\nMetrics\nComplexity\n\nI\n\nI\n\nInstrumented\n\nI\n\nProgram\nExecutables\n\nRequirements\nFunctional\n\nl Designer\nInput\n\nI\nMaintainability\n\nI Testability I I ComplexityI I\n\nI\n\nSUSCEPTABILITY\nLEVEL\nFUNCTIONAL\nCRITICALITY\n1\n\n!2\nJ_\n\n3\n_4\n\nPERCENT JNTESTED\nCODE\n\nto error\nmoreprone\n\nt\n\nI\n\nmoreprone\n\npercentage\n\ntoerror\n\nI\nI\nRISK INDEX\n\nFigure 2 - Derivation of the risk index.\n\nA similar transformation was required for the error susceptibility measure,\ntion for the risk index as:\nRI\n\n= (1.25-0.25xFC)(1.25-0.25xES)(1-TC)\n\nRI\n\n= risk index\n\nFC\nES\nTC\n\nresulting in an equa-\n\n= functional criticality\n= error susceptibility\n= code coverage\n\nwhere\n\n(Eq 4)\n\nAn example of the results for some sample code is given in table 2.\nTable 2 - Summary\n\nParameters\n\nand the Risk Index\n\nCOMPONENT\n\nTOTALTEST\nCOVERAGE\nPERCENTAGE\n\nadtgud\n\n85.29\n\n1\n\n1\n\n0.14705882\n\nardinp\nardlin\n\n93.75\n62.50\n\n2\n2\n\n3\n\n0.0234375\n\n3\n\n0.140625\n\nb94med\n\n95.24\n\n4\n\n1\n\n0.0119\n\nchkbnd\n\n56.72\n\n3\n\n1\n\n0.2164\n\nconvrt\n\n97.14\n\n1\n\n2\n\n0.02145\n\nd2head\n\n81.25\n\n3\n\n1\n\n0.09375\n\nengdta\n\n93.65\n\n4\n\n1\n\n0.015875\n\nf02med\n\n91.30\n\n2\n\n1\n\n0.06525\n\nfstild\n\n97.67\n\n2\n\n1\n\n0.017475\n\n100.00\n\n2\n\n1\n\n0\n\nk96med\n\n89.47\n\n3\n\n1\n\n0.05265\n\nk85med\n\n90.91\n\n1\n\n1\n\n0.0909\n\nmain\n\n59.09\n\n4\n\n1\n\n0.102275\n\nmedout\n\n50.00\n\n2\n\n1\n\n0.375\n\nopnfil\n\n72.06\n\n2\n\n3\n\n0.104775\n\npoly\n\n85.71\n\n3\n\n2\n\n0.0535875\n\nrddasc\n\n57.14\n\n4\n\n4\n\n0.0267875\n\nrdhasc\n\n60.00\n\n4\n\n4\n\n0.025\n\nsvdlod\n\n83.05\n\n1\n\n3\n\n0.08475\n\n100.00\n\n2\n\n1\n\n0\n\ninitmd\n\nthrott\n\nERROR\nSUSCEPTIBILITY\n\n15\n\nFUNCTIONAL\nCRITICALITY\n\nRISK INDEX\n\nThe concept of associated\nrisk can be applied to software development\nand testing improvements. A component with high associated\nrisk should be allocated more development\nand\ntesting resources than low risk components.\nTherefore,\nmore test effort and review emphasis\nshould be placed of components\nwith a high risk index factor.\nThe risk index derivation as summarized\nin this handbook assumes the availability of source\ncode. One topic of further needed research is the expansion of the risk index concept, making\nit applicable during earlier phases of the life cycle before source code is available.\nTo evaluate\nthe risk without actual source code, one must first modify the parameters used in the error\nprediction model. A new model should be constructed\nusing available programmatic\nmetrics,\nsuch as the number of requirements\nfor a given component,\nthe number of interfaces for a\ngiven component,\nand the number of errors or changes against a specific component.\nFor\nmore information\non the use of programmatic\nmetrics in the construction\nof an error prediction\nmodel, including some metrics and their model parameters,\nsee Friedman [2]. If such metrics\nare not available, or if it is judged that these metrics will not give adequate results, the predicted number of errors can be based on past data from similar software components.\nAnother method would be to predict the number of errors using programmatic\nmetrics, and then\nupdate this prediction using Bayes\' Theorem with the historical data. The test coverage can\nbe predicted by a subjective evaluation of the preliminary test plans. The functional criticality,\nwhich is not dependent on source code, is evaluated the same as described above. This illustrates an important point: Since the functional criticality evaluation\nis independent\nof the life\ncycle, an evaluation\nof a component\'s\nfailure modes and effects should be begun as early as\npossible in the life cycle. The results of these analyses can then be combined into a factor\nrelating to overall risk.\n\n1.3\n1.3.1\n\nTools\nLogiscope\n\nLogiscope is a commercial software package developed to perform static and dynamic complexity analysis of software code. The version used for this study is owned by the JSC Software Technology\nBranch and was made available by special permission.\nLogiscope runs on a\nSun Sparq workstation\nand can be configured\nto analyze several different languages.\nPresently Logiscope\nis configured to analyze ADA, C, and five different language versions of\nFORTRAN.\nLogiscope has the capability to calculate over forty standard "built-in" complexity metrics. The\nuser is allowed to define the functions for new custom metrics as well. In addition, Logiscope\nhas the capability to perform three different types of dynamic monitoring.\nThis combination\nmakes Logiscope a very powerful tool for software quality assessment,\nsoftware complexity\nanalysis, and software metric collection.\nLogiscope performs its functions in a three step process:\nthe analysis of the raw code, archiving the analysis results, and editing the archived results. The analysis process looks at the raw\ncode and generates a results file (unintelligible\non its own). The archive sequence allows the\nuser to group various components\nand analyze them together.\nThe edit sequence allows the\nuser to view the results and manipulate\nthem into a more meaningful form.\n\n16\n\n1.3.2\n\nCompiler\n\nTools\n\nA variety of utilities and tools are also available and extremely helpful in the execution of a\nsoftware complexity analysis.\nOne of these includes a set of UNIX procedures and compiler\ndirectives useful for the determination\nof the usage profile and test case coverage.\nThese\ntools and directives are discussed here in section 3, but it should be noted that these tools and\nprocedures\nare concerned with dynamic operations which require a compiled and executable\nversion of the code.\n\n1.3.3\n\nOtherTools\n\nA wide variety of commercial\ntools have been developed to perform source code analysis and\nsoftware complexity analysis, including the McCabe Tool and the ASAP Tool. These are just\ntwo of the many programs available today, and it is up to the user to evaluate and select the\ntool most appropriate for his applications.\nThomas McCabe, of McCabe and Associates,\nis well known in the software complexity area as\nthe developer of the cyclomatic complexity\nmetric. The McCabe Tool, developed by Thomas\nMcCabe, provides capabilities very similar to Logiscope.\nThe McCabe Tool has a superior\ngraphical user interface, and superior graphical representations\non the software hierarchy;\nwhile Logiscope is superior in metric calculation and flexibility in selecting and defining the\nmetric set.\nAnother tool is the ASAP tool available through COSMIC.\nThis tool has been used\nsuccessfully\nby the JSC Engineering\ndirectorate to collect a limited set of metrics. The ASAP\ntool can collect some metrics, but the metric set is limited. Converting these metrics to another\nformat proved time-consuming\nand inefficient.\nDespite its disadvantages,\nASAP can provide a\nlow-cost alternative to the expensive code analyzers available today.\n\n1.4\n\nDetailed\n\n1.4.1\n1.4.1.1\n\nAnalysis\n\nCollection\nSource\n\nProcedures\n\nof Metrics\nCode\n\nThe first step of any static analysis is to collect the source code. The source code should be in\nan ASCII text file format, and can be loaded from tape or by using ftp, if available.\nTape\nloading and ftp procedures\nare discussed in more detail in the UNIX procedures section.\nIt is\noften more efficient to check with the UNIX systems administrator\nbefore loading any large\namount of files on a local machine.\nWhen transferring\na lot of files, it is often easier to create a compressed\ntar file. This file is a\nsingle compressed\nfile which contains all the files to transfer.\nThe file is also compressed,\nthus\nsaving transfer time, and, being a single file, is easier to handle. After the tar file is\ntransferred,\nit can be extracted, recreating the directory structure (including multiple\nsubdirectories)\nfrom the original.\n\n17\n\nTo create a tar file, first create a file which is a concatenation\nissuing the following UNIX command:\ntar cvf\nwhere\n\nThis creates\n\n<target filename>\n\n<source\n\nby\n\nfiles or directories>\n\nc - create\nv - verbose\n\nthe tar flags are\n\na single file which can then be compressed\ncompress\n\nof all the files or directories\n\nby issuing:\n\n<filename>\n\nThe compress command creates a compressed\nversion of the file which has the same name,\nbut with a .Z extension. After transferring, the files can be uncompressed and extracted by\nissuing the following sequence of commands:\nuncompress\ntar xvf\nThis will recreate\n\n<compressed\n\nfilename>\n\n<filename>\n\nthe directory\n\nstructure\n\nTo run Logiscope,\nit is necessary\nLogiscope,\nas well as an account\n\nand extract\n\nthe files on the new machine.\n\nto have all the files resident on the machine licensed to run\non that machine.\nAt this time that machine is galileo (ftp\n\naddress 134.118.101.24),\nwhich is owned by the JSC Information Systems Directorate.\nThe\ncreation of the tar file can be useful when transferring\nfile from the local machine to galileo,\nand vice versa.\n\n1.4.1.2\n1.4.1.2.1\n\nLogiscope\n\nProcedures\n\nStatic Analysis\n\nof Code\n\nSince Logiscope operates in an analyze - archive - edit sequence, the first step in performing\nstatic analysis is to run the appropriate\nLogiscope code analyzer with the raw source code.\nLogiscope\nhas a variety of analyzers corresponding\nto the programming\nlanguage, such as\nADA, many versions of FORTRAN, and several different versions of C. A listing of the\nexecutables\nassociated\nwith the analyzer for each of the languages is given in table 3.\nAfter initiating the appropriate analyzer, Logiscope will query the user.\nslightly depending on the language of the program being analyzed:\nSource_File\nBasic\n\nname\n\ncounts\n\nInstrumentation\nIgnore\n\nStatement\n\nThe queries\n\n?\n\n(l=screen,\n\n2:file,\n\n(y/n)\n\n?\n\n(y/n)\n\n?\n\n18\n\n3:not)\n\n?\n\nwill vary\n\na\n\nTable 3 - Logiscope Analyzer\n\nExecutables\n\nLANGUAGE\n\nEXECUTABLE\n\nFORTRAN\n\nIog_fort_std\n\nFORTRAN\n\n77 PLUS\n\nlog_fort_plus\n\nIBM FORTRAN\n\nlog_fort_ibm\n\nHP FORTRAN\n\nIog_fort_hp\n\nFORTRAN\n\n32\n\nlog_fort_f32\n\nVAX FORTRAN\n\nIog_fort_vax\n\nC\n\nlog c sun\n\nMicrosoft\n\nC\n\nlog. c msoft\n\nVAX C\n\nlog c vax\n\nADA\n\nIog_ada\n\nThe appropriate\nanswers to the last three questions are 3, n, n. When performing ADA\nanalysis, Logiscope\nwill also prompt for the ADA library directory.\nA simple script file can also\nbe written to answer all the questions.\nThe following script file (a copy of which resides on\nBilbo in the users/tgunn/utility\nsubdirectory)\ncan be used to perform a static analysis of all the\nADA files in the current subdirectory:\nset\n\n*.ada\n\nwhile\ndo\n\ntest\nlog_ada\n\n$i\n<<!\n\nSl\n3\nn\n/tmp/gal_home/alee/cmslib/testdir/ada_lib\n!\necho\nshift\n\n$i\n\ndone\n\nThis script file can be easily revised to accommodate\nwhatever parser you are using by\nrevising the file search string in line 1 and the analyzer executable (see table 3) in line 3.\nThe Logiscope\nanalyzers are particular to the order of code analysis as well as to the structure\nof the code itself. If there are any compiler dependencies\nassociated with the program,\nLogiscope requires that the program be analyzed in the same order as it is compiled.\nIf there\nis a make file (or its equivalent) associated with the program which is to be analyzed, it is\nnecessary to obtain that file to analyze the code properly.\n\n19\n\nA specificexample\n\nof this peculiarity occurred during the analysis of an ADA program.\nThe\nanalysis of the source code was failing since the Logiscope\ncode analyzer could not determine\nthat all the parent files had been analyzed for a specific file. We determined\nthat many of the\nprogram components\ncalled non-standard\nutility routines, or Bootch utilities, were not part of\nthe Logiscope standard ADA library files. When a program component\ncalled one of these\nutilities, Logiscope would detect that this utility had not been analyzed and was not part of the\nlibrary routines, so it would abort the process.\nA workaround\nwas developed which created\ndummy Bootch utility files; i.e., a procedure which used valid ADA structures but did not\naccomplish\nanything.\nThese empty files had the following structure:\nprocedure\nend\n\n<Bootch\n\nUtility\n\nName>\n\n;\n\nprocedure;\n\nThese empty files were analyzed, and then all the daughter files could be analyzed as well.\nThis type of workaround\ncould be used in other situations,\nbut keep in mind that this type of\n"quick fix" should only be used as a last resort. This type of fix will distort the end results, the\nextent of which is dependent\non the number of files being dummied and the number of\ncomponents which are daughters to the dummied component.\nAnother\n\nproblem\n\nwe encountered\n\nwas the lack of the compilation\n\ndependencies\n\nfor an ADA\n\nprogram.\nComponents\ncould not be analyzed until their parents were analyzed.\nWe decided\nto attempt to analyze this program in an iterative fashion, attempting to analyze the entire\nprogram but only succeeding\nin obtaining valid results for the first level of the software\nhierarchy.\nThe analyzer can then be re-run on the entire program, this time picking up the next\nlevel down as well. This cycle will be repeated until all levels of the program are able to be\nanalyzed.\nAlthough there is no reduction in the accuracy of the final result, this method can be\nvery time-consuming,\nboth in machine time and in user input time. Obtaining the compiler\ndirectives is a much more preferred and time-efficient\nprocess.\nAnother problem we discovered\nwith the Logiscope analyzer deals with incompatibilities\nwith\nthe parser and the source code. This problem occurred primarily with the FORTRAN parsers,\nand some could be rectified by trying another parser on the problem file. If that doesn\'t work,\nLogiscope has the ability to exclude certain lines from the file. This can be done by entering a\n"y" when Logiscope prompts "Ignore Statement\n(y/n) ?" and then entering the statement.\nWhen implementing\nworkarounds\nof any sort, keep in mind that the metric output obtained\nmight not be 100% accurate.\nIf accuracy is going to be sacrificed, one should make an\nestimate of the extent of the distortion of the data, compared to the amount of extra work\nrequired\n\nfor the desired\n\naccuracy.\n\nAfter attempting to analyze a file, Logiscope creates a results file with the identical name as\nthe source file with the extension .res. These .res files contain the information\nLogiscope will\nuse in its archive and edit sequences.\nTroubleshooting\nthe analysis can be accomplished\nby\nlooking at the size of the .res file. If the file size of the .res file is 85, this is a good indication\nthat the analysis has failed for this particular file. The Logiscope code analysis can fail for a\nnumber of reasons, such as the parent module has not been previously analyzed or there is an\nunrecognized\nsyntax in the program file. For more information\non Logiscope analyzer failure\ncodes, refer to the Logiscope manuals.\n\n20\n\n1.4.1.2.2\n\nArchiving\n\nResults\n\nFiles\n\nArchiving the .res files is probably the simplest of the Logiscope analyze - archive - edit\nsequence.\nArchiving groups the individual analysis results files (.res files) into a single .arc file\nrepresenting\nthe entire program.\nThe capability also exists to update individual program files\nas they are modified into an existing archive file. To archive a group of static analysis results\n(.res) files, simply issue the following command:\nlog_arcsta\n\nLogiscope will prompt you for the name of the archive file and the name(s) of the analysis\nresults files. Standard wild card characters\n(*, ?) are accepted.\nTo archive all the results files\nin the current subdirectory,\nsimply type in "*.res" when prompted for the analysis results file\nname.\nThe static archiver also has the capability to store several versions of a program, and note the\nmodifications\nof each different version.\nFor more information about this function and other\narchiving\n\n1.4.1.2.3\n\ncapabilities,\n\nrefer to the Logiscope\n\nEditor Procedures\n\nfor Generating\n\nmanuals.\n\nOutput\n\nWhen the program is analyzed and archived, users can run the static editor to create,\nmanipulate,\nand print results in tabular and graphical formats.\nThe editor is where most of the\nuser interface takes place, as this is where results and output are generated.\nExamples of\nsome of the editor functions are the generation of control graphs, graphical representations\nof\nthe distributions\nof metrics, textual representations\nof the code structure, distributions\nof the\nmetric functions, and metric statistics.\nThe editor can perform operations on both archive files or individual results files, and there are\nworkspace\nmanipulation\ncommands to look at subsets of archive files for large programs.\nSome of the basic editor functions are described in the sections below, and it is assumed that\nan archive file was loaded into the editor rather than a results file.\nThe static editor is invoked\n\nby issuing the following\n\ncommand\n\nfrom the command\n\nline:\n\nlog_edsta\n\nThe program will then prompt for the name of the archive or results file. After loading in the file\n(which can take several minutes for large programs), Logiscope will respond by displaying the\neditor prompt (>). Two commands to remember:\n"h" to display the help file and "end" to exit\nthe editor session.\nTo get acquainted\nwith the Logiscope editor, a quick tour of some of the capabilities\nand log on\nprocedures\nis given in section 1.4.4. This tour utilizes a user-generated\nmenu system and\nprovides a graphical interface to initiate the Logiscope editor.\n\n21\n\n1.4.1.2.3.1\n\nMetrics\n\nGeneration\n\nOne of the easiest tasks to accomplish\nin the editor is the generation of the metrics file. This is\nan ASCII text file which can be imported to a spreadsheet\nor database.\nIssuing the metfile\ncommand will generate a metrics file, which will be found in the current directory with the same\nfilename as the archive file, but with a .met extension.\nThis metric file will contain all the\nmetrics which have been selected for collection as defined in the reference file. For\ninformation\non the selection of metrics for collection,\nmetrics file for two components\nis shown in table 4.\nTable\n\nComponents:\nApplication:\nLanguage:\nVersion:\nMetric:\n\nComponents:\nApplication:\nLanguage:\nVersion:\nMetric:\n\n4 - Metrics\n\nsee section\n\n1.4.1.\n\nThe structure\n\nof this\n\nFile Format\n\nCHARACTER_MAPPING/MATCHES:STRING:NATURAL:PICTURE_MAP:\nBOOLEAN:INTEGER\nitve\nADA\n1\n\n17/Sp/92-11:57:24\n\nN_STMTS PR_LGTH N_ERRORS\nEFFORT VG MAX_LVLS N_PATHS\nN_JUMPS N_EXCEPT\nN_ACCEPT DRCT_CALLS\nN_RAISES COM_FREQ\nAVG_S 20 306\n0.48 54666.62 13 3 13 0 0 0 0 0\n0.00\n15.30\nPARSER_SERVICES_PKG_TT/FU\nTYPE:ECHO\nitve\nADA\n1\n\nNCTION_PARSE_TT:CU\n\nRRENT_RECORD_\n\n17/Sp/92-12:51:40\n\nN_STMTS PR_LGTH N_ERRORS\nEFFORT VG MAX_LVLS\nN_PATHS\nN_JUMPS N_EXCEPT\nN_ACCEPT\nDRCT_CALLS\nN_RAISES COM_FREQ\nAVG_S 34 231\n0.33 31349.49 11 4 66 0 1 0 9 8\n0.38\n6.79\n\nNotice that the actual values of the metric parameters are space-delimited\nin the seventh line\nof the section. When transferring this data to a spreadsheet or database, it is often easier to\neliminate all but the component name and the actual metric values, thus removing the filler\ninformation. This can be accomplished easily using the find and replace feature of any word\nprocessor, replacing each occurrence of "Application:\nitve" with nothing. Cleaning up the\nmetrics file makes it much easier to transfer to a database or spreadsheet.\nThe file can be\ntransferred into EXCEL as a space-delimited text file, where data analysis functions such as\ngeneration of the correlation coefficients, generation of the frequency\nhistograms, and\nregression against errors can be performed.\nStatistics can also be generated for each metric collected\nsample output of the stat command is shown in table 5.\n\n22\n\nby issuing\n\nthe stat command.\n\nA\n\nTable\n\nMetrics\n\n5 - Sample\n\nMnemonic\n\nStatistic\n\nTable\n\nStandard\nDeviation\n\nAverage\n\nMin\n\n%\nok\n\nMax\n\n%\nUndef\n\nNumber of statements\n\nN_STMTS\n\n27.61\n\n40.12\n\n1\n\n497\n\n96%\n\n0%\n\nNumber of comments\n\nN-COM\n\n14.28\n\n10.18\n\n0\n\n86\n\n99%\n\n0%\n\nTotal operand occurrences\n\nTOT-OPND\n\n87.52\n\n143.14\n\n0\n\n2070\n\n85%\n\n0%\n\nDifferent operands\n\nDIFF-OPND\n\n26.08\n\n23.84\n\n0\n\n139\n\n77%\n\n0%\n\nTotal operator occurrences\n\nTOT-OPTR\n\n126.12\n\n200.13\n\n2\n\n2949\n\n83%\n\n0%\n\nDifferent operators\n\nDIFF-OPTR\n\n15.17\n\n5.89\n\n2\n\n37\n\n76%\n\n0%\n\nProgram length\n\nPR-LGTH\n\n213.64\n\n342.29\n\n2\n\n5019\n\n83%\n\n0%\n\nEstimated error count\n\nN-ERRORS\n\n0.36\n\n0.77\n\n0.00\n\n12.38\n\n84%\n\n0%\n\nMental effort\n\nEFFORT\n\n66983.54\n\n3.91E+05\n\n36.18\n\n7.15E+06\n\n84%\n\n0%\n\nCoding time\n\nCODE-T\n\n3721.30\n\n21754.91\n\n2.01\n\n3.97E+05\n\n84%\n\n0%\n\nCyclomatic\n\nnumber\n\nVG\n\n7.30\n\n10.79\n\n1\n\n106\n\n91%\n\n0%\n\nMax number of levels\n\nMAX-LVLS\n\n2.62\n\n1.29\n\n1\n\n7\n\n97%\n\n0%\n\nComments frequency\n\nCOM-FREQ\n\n0.93\n\n0.94\n\n0.00\n\n13.00\n\n63%\n\n0%\n\nAverage size of statements\n\nAVG-S\n\n7.75\n\n3.28\n\n2.00\n\n25.73\n\n44%\n\n0%\n\nNumber of IN_OUT nodes\n\nN-IO\n\n2.07\n\n0.27\n\n2\n\n93%\n\n0%\n\n4\n\nStatistics Table\nApplication:\nVersion:\n\ndistribution\n\nissuing\n\nthe metbars\n\nof the first\n\nLogiscope\n\ntext\nissue\n\nwindow\n\n(.pos)\n\ngenerated\n\nby sending\n\nall graphical\n\nthe file size\n\nof the set.\n\nwill cycle\n\nfile containing\n\nhard\n\nthis file\nmanner,\n\nmetric\n\ncommand,\nthrough\n\nthe p command.\n\npostscript\n\ncopies,\nsimilar\n\n350\n\nalso has the capability\nto print out frequency\ngives the overall percentage\nof components\nAfter\n\nscreens,\n\nFORTRAN\n\nComponents:\n\nmetric.\n\nVERSION1\n\nLanguage:\n\nLogiscope\nhistogram\n\nsorter\n\ncopy\n\na graphical\n\nPressing\nthe\n\nmetrics\n\nall the graphical\n\nrequests\n\nfile to the\n\nto a single\n\nlist.\n\nkey\n\nwill appear\nwith the\n\nIf a hard\n\ncopy\n\nof the session,\nprint\n\nrequests.\n\nprinter.\n\nOne\n\nfile.\n\nIf the user\n\nmight be too large to send to the printer.\nbut in an ASCII file with a .al extension.\n\nis not really\n\nwindow\n\nthe enter\n\nAt the conclusion\n\nthe postscript\n\nhistograms\nfor each metric.\nThis\nwhich\nexhibit a specific\nvalue range\n\na problem.\n\n23\n\nword\n\nmouse\n\nhard\n\na large\n\nin the\n\nof any\n\nof the\n\nwill generate\n\ncopy\n\nof warning:\n\nrequests\n\npointer\n\nis desired\n\nLogiscope\nThe\n\nof a\n\nwith the\n\ncan\n\nLogiscope\nnumber\n\na\n\nbe\nsends\n\nof hard\n\nText hard copies are saved in a\nText files are usually\nmuch smaller,\n\nso\n\n1.4.1.2.3.2\n\nControl\n\nGraph\n\nProcedures\n\nA control graph is a graphical representation\nof a program\'s structure, illustrating graphically\nthe decision paths and logic structure.\nTo create a control graph for a given component,\ntype\n"control" at the editor prompt.\nLogiscope will bring up a graphical window which displays the\ncontrol graph. The control graphs of all components\nin the current workspace\ncan be cycled\nthrough by pressing the enter key in the text window. Typing "prv" at the prompt will return the\nuser to the previous graph. Figure 3 provides the definitions of some of the symbols\nrepresented\non the control graphs.\n\nA sequenceof statements\nEnd of an exception\nsequence\nEntry or Exit\n\n\xc2\xa9\n\nCondition_J\nSt=ement or deCision\nnode [_--\n\nBeginning an exceptionsequence\nof\n\nEnd of controlstructure\n\nADAAccept\nBranch\n\nEdge\nADARaise\nReducedStructure\n\nFigure 3 - Control graph symbols.\n\nControl graphs can be used to measure the level of structuredness\nof a program\'s individual\ncomponents.\nThis is accomplished\nby a technique known as control graph reduction.\nReducing a control graph entails representing\na nested structure with a single node, as long as\nthe structure can be represented\nwith a single entry and a single exit. After successive\nreductions, a well-structured\ncomponent will have an equivalent cyclomatic complexity equal to\none. This number is often referred to as the essential complexity.\nTo reduce a given control\nchart, type "red" at the control graph prompt. The graphical window will then show a modified\ncontrol graph with nested structures represented\nby the reduced structure symbol. Successive\niterations of the "red" command will show a complete reduction of the component.\n\n24\n\nPseudo code generation and representation\nof decision nodes by the appropriate\nline numbers\nis also possible when looking at control graphs. Typing "txt" will generate pseudo code for the\nselected component.\nTyping "nn" will give the line numbers on the control graph itself.\n\n1.4.1.2.4\n\nKiviat\n\nGraph\n\nProcedures\n\nA Kiviat graph is a method of graphically displaying a given set of metric values against a\npredetermined\nset of limits for these values. Kiviat graphs can be displayed by issuing the\nkiviat command at the Logiscope prompt.\n\n1.4.1.2.5\n\nWorkspace\n\nManagement\n\nProcedures\n\nWhen working with a large program, it is sometimes useful to reduce the size of the workspace\nto look at only portions of the program.\nThis can provide an increase in speed and efficiency\nof Logiscope, as well as allowing the user to focus in on a specific subset of programs.\nThis\ncan be accomplished\nby the wssel command.\nFor example,\nthe following\nwssel\n\nto consider\ncommand:\n\nonly the components\n\nwith a cyclomatic\n\nnumber\n\ngreater\n\nthan 20, issue\n\n(vg > 20)\n\nThis would limit the workspace to only those components\nwith cyclomatic number greater than\n20. The metric distribution histograms,\ncontrol graph displays, kiviat graphs, etc., would only\ninclude components\nwith a cyclomatic\nnumber greater than 20. The workspace\nmanagement\ncommands\ncan also be chained together.\nFor example, to look at only files with a cyclomatic\nnumber greater than 20 and with the number\ncommand sequence:\nwssel\nwssel\n\nof statements\n\nless than 70, issue the following\n\n(vg > 20)\n(n_stmts\n< 70)\n\nThis sequence\nof commands\nboth of the search conditions.\n\nwould limit the workspace\n\nto those components\n\nwhich satisfy\n\nOnce a workspace\nhas been defined, the workspace\n(actually the subset of components\nmatch a given criteria) can be saved by issuing the wssave command.\nThis workspace\nread from the file by issuing the wsrest\n\nwhich\ncan be\n\ncommand.\n\nAnother useful workspace management\ncommand is the wslistcont command.\nIssuing this\ncommand displays a list of all components in the current workspace. Table 6 provides a list of\nsome of the workspace management commands and their uses. For more information on\nmore command or details on how to use these commands, refer to the Logiscope manuals.\n\n25\n\nTable 6 - Workspace\n\nManagement\n\nCOMMAND\n\nDESCRIPTION\n\nwssel\n\nselects the workspace\n\nwssrest\n\nrestores\n\nwssave\n\nsaves the current workspace\n\nwslistcont\n\nlists the components\n\nwsinit\n\nreturns\n\nwsadd\n\nadds a selected\n\nwsinter\n\nfinds the intersection\n\nwsdiff\n\nfind the differences\n\nwscreate\n\n1.4.2\n\nCommands\n\ncreates\n\nMetrics\n\nDatabase\n\nFormat\n\na previously\n\ngiven a search\n\ncriteria\n\nsaved workspace\n\nin the workspace\n\nto the initial workspace\ncomponent\n\nto the workspace\n\nbetween\n\ntwo workspaces\n\nin two workspaces\n\nan empty workspace\n\nand Input\n\nA software failure history database can be created to improve the error prediction modeling\ncapabilities\nand accuracy.\nThis database should be defined to include metric data, software\nfailure data, and reliability growth information.\nSuch a database could be used to expand\nresearch in areas such as: reliability characteristics\nof reused or modified code, software\nfailure effects\nthe beginning\n\nanalysis, and the estimation\nof testing.\nSome proposed\n\nof the reliability growth of a software system\ninput parameters are given in table 7.\n\nTable 7 - Basic Parameters\nComponent\n\nfor Input Into Metrics\n\nprior to\n\nDatabase\n\nname\n\nLanguage\nFailure\n\nhistory\n\nStructural\n\ncomplexity\n\nmetrics\n\nRun time history\nReliability\n\ngrowth\n\ncurve parameters\n\nDevelopment\n\ntype metrics\n\nRequirement\n\nmetrics\n\nThis list is very rudimentary,\nbut can give an idea of what is required.\nA lot of work will be\nneeded to standardize the data collection process.\nThe specific metrics for collection need to\nbe defined so the information can be used from project to project.\n\n26\n\n1.4.3\n\nDynamic\n\n1.4.3.1\n\nAnalysis\n\nDynamic\n\nfor Determination\n\nAnalysis\n\nof Test Case Coverage\n\nUsing Logiscope\n\nLogiscope provides the capability to perform dynamic analysis of a software system. The\nprogram has instrumentation\noptions which correspond\nwith the above techniques.\nLogiscope\nautomatically\ninstruments the source code, which must then be recompiled along with a\nspecific file (provided by Logiscope).\nThe procedures for analyzing, archiving, and editing are\nvery similar to the procedures for performing static analysis.\nThe dynamic analysis capability of Logiscope can be obtained from the Logiscope\nmanuals\nand a report from Dennis Braley of the JSC Software Technology\nBranch entitled "Test Case\nEffectiveness\nMeasuring Using the Logiscope Dynamic Analysis Capability" [8]. Mr. Braley has\nsuccessfully\nperformed dynamic analysis on some of his code, and his report of the details of\nhis study is available in the STIC.\n\n1.4.3.1.1\n\nCode Instrumentation\n\nThe instrumentation\nof the source code is performed during the analysis of the source code\nmuch in the same fashion as the static analyzer.\nWhen first accessing the analyzer, the same\nfollowing questions are asked:\nSource_File\nBasic\n\nname\n\ncounts\n\n?\n\n(l:screen,\n\nInstrumentation\n\n2=file,\n\n(y/n)\n\nThe main difference will be noted by answering\nprompt for three more inputs:\nCreation\n\nof\n\na\n\nlisting\n\n)\n\ny to the third question.\n\n(y/n)\n\nInstrumentation\n\nof\n\ncontrol\n\nInstrumentation\n\nThe following\n\n3 :not\n\n?\n\n?\n\nof\n\ncall\n\n?\ndata\n\ndata\n\nThe program\n\n(y/n)\n(y/n)\n\n?\n\nscript can be used to analyze all the files in the subdirectory:\nset\n\n*.f\n\nwhile\ndo\n\ntest\n\n$i\n\n/case/Logiscope/bin/log_fort_vax\n\n<<!\n\n$i\n3\nY\nY\nY\nY\nY\nn\ni\n\necho\nshift\n\n$i\n\ndone\n\n27\n\nwill then\n\n1.4.3.1.2\n\nCompilation\n\nof Instrumented\n\nCode\n\nCompilation\nof the Logiscope instrumented code is accomplished\nin the same manner. To\nallow the SUN FORTRAN\ncompiler to compile the files, they must end in the .f extension.\nLogiscope gives all the files a .for extension.\nThe following shell script can be used to change\nthe filenames of the .for files to the .f extension:\nfor\n\ni\n\nin\n\n"is\n\n*.ftnlawk\n\n-F"\n\n"\n\n\'{print\n\n$I}\'"\n\ndo\n\nmv ${i}.ftn\ndone\n\n1.4.3.1.3\n\n${i}.f\n\nOther Procedures\n\nTo perform dynamic analysis using Logiscope, a user also will need to execute test cases, to\nrun editor procedures for generating output, and to archive results. Detailed information\nregarding these processes can be found in the Logiscope manual.\n\n1.4.3.2\n\nDynamic\n\nAnalysis\n\nUsing Compiler\n\nDirectives\n\nDynamic analysis can also be performed using compiler directives.\nMany compilers have this\noption, and the procedures\ndescribed in this section are relevant to the SUN FORTRAN\ncompiler.\nThis compiler is resident on the workstation\nmodeller.\nCompilation directives can be\nused to perform a decision-to-decision\npath test monitoring on a particular software component\nas well as identifying the frequency of execution of a particular decision path.\n\n1.4.3.2.1\n\nCode Instrumentation\n\nand Compilation\n\nInstrumenting\nthe code (or inserting the traps in the code) can be performed\ncommand line. For the FORTRAN code, this was done by:\nf77\n\n-c\n\n-a\n\n<source\n\nfilename>\n\nThis creates the instrumented\nversions of the object file(s).\nbe created after all objects have been created by :\nf77\n\nAn instrumented\n\n-a\n\nversion\n\n*\xc2\xb0o\n\n-o\n\nat the compiler\n\n<executable\n\nof the executable\n\nAn instrumented\n\nexecutable\n\ncan\n\nfilename>\n\nis now created.\n\nThere is a loss of performance\ndepending on the level of instrumentation.\nThe more\ncomponents\nwhich are instrumented\nthe slower the program will run. Instrumentation\nwill\ncompromise\nthe accuracy of profiling data (for more information on profiling see section 2.2.2).\nIt is recommended\nthat profiling be done separately from instrumentation.\n\n28\n\n1.4.3.2.2\n\nTest Case Execution\n\nNow that the instrumented\n\ncode is compiled\n\nand linked, the test cases of the program\n\ncan\n\nbegin to be executed normally. Test case execution procedures\nwill depend on the particular\nsystem being analyzed, and test execution procedures\nshould be followed as documented\nin\nthe test plan produced by the developer.\nFor the monitoring to work properly, the program\nmust terminate normally without any errors in the execution of the program.\nUpon conclusion\nof each individual test case, a file will be created in the executable\ndirectory with the\ncomponent\nname followed by a .d extension.\nThis .d file is what will be used by the tcov\nto determine the test coverage for that particular component.\nSince the .d files are created\n\nat the termination\n\nof a test case execution,\n\nwe recommend\n\nutility\n\nthat\n\nthese files be processed and archived for each individual test case before executing another\ntest case. This allows all the coverage information for one particular test case to be processed\nand collected before proceeding on to the next case. An alternative process is to move the *.d\nfiles into another archive directory and complete the execution of the next test case. At the\nconclusion of the testing, there should be a separate subdirectory\nfor each test case, each\nsubdirectory\ncontaining the raw test coverage files (the .d files).\n\n1.4.3.2.3\n\nCollection\n\nand Consolidation\n\nOnce the test case has been executed\n\nof Results\nand the .d files have been generated,\n\nyou are now\n\nready to generate the output files by running the UNIX tcov utility. The tcov will generate\nwith a .tcov extension corresponding\nto the .d file it was run on. The tcov utility is run by\ntyping "tcov" and then the component\ntcov\n\n<component\n\na file\n\nname:\nname>\n\nThis creates a text file with the name of the component\nthe consolidation\nof the results is shown in figure 4.\n\nand a .tcov extension.\n\nAn overview\n\nof\n\nThe command\nmore\n\n<component\n\nname>.tcov\n\nwill display the file on the screen. The file is a listing of the source code, with a couple of\nmodifications.\nListed in the left margin prior to each decision node is the number of times each\ndecision node has been executed.\nIf a particular node has not been executed, ##### will be\ndisplayed as the number of times. At the end of the source listing, additional information\nis\ngiven. This includes the top 10 blocks (relative to execution frequency), the number of blocks\nin the file, the number of blocks executed, and the percentage\nof the file executed.\nThe\nstructure of the .tcov file is shown in figure 5.\n\n29\n\ntcov\n\nCodaCoverage\n(*.tcov\nfiles)\n\nutility\nTest ase1\nC\n\nOva_l Test overage\nC\n\ntcov\nutility\n\nCoda\nCoverage\n(*.tcov\nfiles)\n\nTest ase\nC 2\n\netc.\n\netc.\n\nFigure\n\n4 - Test\n\netc.\n\ncoverage\n\nresults\n\n3O\n\nconsolidation.\n\nC\nC\nIMPLICIT\n\nDOUBLE\n\nPRECISION\n\nCHARACTER*(80)\n\n(A-H,\n\nO-Z)\n\nSCCSID\n\nC\nDATA\n\nSCCSID\n\n/\'@(#)amdval.f\n\n4.I\n\nC\nC.o.ORDER\n\nPOINTS\n\nA AND\n\nC\n\nC\n4004\n\n->\n\n4004\n\n->\n\nIF\n\n( A .LE.\nPTI\n= A\n\nPT2\n\n=\n\nC\n\n) THEN\n\nC\n\nELSE\n\n#####\n\n->\n\nPTI\nPT2\n\n=\n\n= C\n\nA\n\nENDIF\nC\nC...COMPUTE\n\nMIDDLE\n\nVALUE\n\nC\n4004\n\n->\n\n1954\n\n->\n\n1954\n\n->\n\nIF\n( B .GT.\nPTI\n) THEN\nIF\n( B .LE.\nPT2\n) THEN\nAMDVAL\n\n=\n\nB\n\nAMDVAL\n\n=\n\nPT2\n\nELSE\n#####\n\n->\nENDIF\n\n1954\n\n->\n\n2050\n\n->\n\nELSE\nAMDVAL\n\n= PTI\n\nENDIF\nC\n**************************************************************\n\nC\n4004\n\n->\n\nRETURN\nEND\nTop\n\nI0\n\nLine\n\nBlocks\nCount\n\n59\n\n4004\n\n60\n\n4004\n\n69\n\n4004\n\n81\n\n4004\n\n76\n\n2050\n\n70\n71\n\n1954\n1954\n\n75\n\n1954\n\n10\n\nBasic\n\nblocks\n\nin\n\n8\n\nBasic\n\nblocks\n\nexecuted\n\n80.00\n\nPercent\n\n23928\n\nTotal\n\n2392.80\n\nAverage\n\nof\nbasic\n\nthis\n\nthe\n\nfile\n\nblock\n\nexecutions\n\nFigure\n\nfile\nexecuted\n\nexecutions\nper\n\nbasic\n\nblock\n\n5 - .tcov file structure.\n\n31\n\n10:01:27\n\n90/09/27\'/\n\nA simple script can be createdto automaticallyprocessall the outputfiles in the current\ndirectory. The followingscriptwill runthe tcov utility on all the files in the currentsubdirectory:\nset *.d\nwhile\ntest\ndo tcov $I\necho $i\nshift\ndone\nWhen\n\nprocessing\n\nthe tcov\n\n$i\n\nutility on individual\n\ncomponents\n\nto determine\n\nthe coverage\n\nof\n\nindividual test cases, check the amount of available disk space. The tcov utility creates a file\nwhich is just slightly larger than the original source code listing. The output files for a single\ntest case (for full code monitoring) will take as much room as the original program, plus about\n10-15%.\nWhen executing multiple test cases, consumption\nof excess disk space can be a\nproblem.\nIt is less time-consuming\nto create a script file to execute each individual test case as well as\nmove the output to its own subdirectory\nand run the tcov utility output files. Each test case\ncan be run successively\nusing the batch mode, using the at feature of the batch mode. The at\nfeature allows one to execute a given command sequence\nmust be allowed for the previous execution to complete.\n\nat a specified\n\ntime.\n\nEnough\n\ntime\n\nThere was a problem associated with the dynamic analysis.\nDuring the execution of the test\ncases and the processing of the output data, the workstation would freeze up, requiring a\nreboot to restart. After the reboot, all the data files would be lost, forcing a rework of what was\nalready accomplished.\nAfter a period of troubleshooting,\nthe problem was rectified.\nThe\nprogram being run along with the monitoring overhead and tcov processing became very diskand memory-intensive.\nIf any other processes were initiated during the execution of the test\ncase and tcov processing, the system would crash. Therefore, a dedicated processor is\nneeded when performing the dynamic analysis.\nThis can be a problem in a multi-user\nenvironment\nduring normal working hours, so the analysis should be done during off hours.\nOne additional problem with performing this type of run during the off hours is that the machine\n(modeller) has an accounting program set to run daily. It might be necessary to have your\nsystems administrator\neither disable the accounting\nprogram or work around the expected\nhours of operation\n\nusing the at command.\n\nOnce the test coverage files have been generated for each test case, it is now necessary to\ncalculate the total amount of test coverage for the entire set of test cases. The output from the\ntcov utility provides the output from a single test case, but there is no real straightforward\nway\nto combine the outputs\ncombine outputs:\n\ninto one overall\n\nnumber.\n\nThe following\n\nis a summary\n\nof a method\n\nto\n\ncreate a file for each test case formatted with the name of the component\nand the\nunexecuted decision blocks in that component for that particular test case\ncompare the coverage outputs of two separate test cases on the same component\nto record the test case deviations between the cases on that component\nassume that the frequency of execution of a block is unimportant\n(executing a block\nonce is the same as executing a block 1000 times for this analysis)\n\n32\n\n-\n\ncontinuecomparingthe outputsof the two files, monitoringthe test case deviations\nuntilall the test casesare covered\n\nThe above methodis the methodusedto determinethe overalltest coverage,but it mightnot\nbe the mostefficientone. Furtherexperimentation\nmightyield better results.\nTo automatethis processas muchas possible,the usersmay usescript files. Forexample,\nthe actualscript files usedto performthis taskfor the DADSsystemare residentin tgunn on a\nSunworkstation. They are summarizedbelow. To createa file withthe file nameand the\nunexecuteddecisionblocks,use the grep utility:\ngrep\n\n\'#####\'\n\n*.tcov\n\n>\n\n$1_b.unc\n\nwhere the $1 is the variable representative\nof the test case name. Since each unexecuted\ndecision block was preceded by the ##### string, the .unc file contained all the untested\nblocks for that particular test case. After collecting the untested blocks for the test cases, the\ndiff utility could be used to compare the list of the untested blocks by:\ndiff\n\n<list\n\ni>\n\n<list2>\n\nThis listing was manually compared and the deltas in the test case coverages were noted. A\nsample script, which compares the untested decision nodes in test case PIA to case P1B, is\nshown below:\ncd\ngrep\ncd\n\nPiA_mod/a\n#####\n\n*.tcov\n\n>/users/tgunn/tmp/pla\n\n../../PiB_mod/a\n\ngrep\n\n#####\n\ndiff\n\n/users/tgunn/tmp/pla\n\n*.tcov\n\n>/users/tgunn/tmp/plb\n\n/users/tgunn/tmp/plb>/users/tgunn/tmp/pla_b\n\nThe total number of blocks for each file were extracted\ncd\ngrep\ncd\ngrep\ncd\ngrep\n\nand the percentage\ncd\ngrep\n\nagain by using the grep utility:\n\n$1/b\n\'Basic\n\nblocks\n\nin\n\nthis\n\nfile\'\n\n*.tcov\n\n>\n\n\'Basic\n\nblocks\n\nin\n\nthis\n\nfile\'\n\n*.tcov\n\n>ablocks.out\n\nblocks\n\nin\n\nthis\n\nfile\'\n\n*.tcov\n\n>sblocks.out\n\n../../$1/s\n\'Basic\n\nof total coverage\n\nper test case was exported\n\n$1/b\n\'Percent\n\nmv\n\n$1_b.out\n\ncd\n\nof\'\n\n*.tcov\n\n>\n\n$1_b.out\n\n*.tcov\n\n>\n\n$1_a.out\n\n*.tcov\n\n>\n\n$1_s.out\n\n../../$1/a\n\ngrep\n\n\'Percent\n\nmv\n\n$1_a.out\n\ncd\n\n$BIAS\nof\'\n\n../../$1/s\n\ngrep\nmv\n\nbblocks.out\n\n../../$1/a\n\n\'Percent\n$1_s.out\n\n$ARDG\nof\'\n$SHAP\n\n33\n\nby:\n\nWe then imported the results from these script files into an EXCEL spreadsheet\nby performing\nan ASCII import of the data. We calculated the total test coverage by dividing the actual\nnumber of blocks executed (found by reviewing the cliff outputs) by the total number of blocks\nin the file.\nOne problem did occur for several files in one portion of the FORTRAN project. The tcov\nutility could not process the information in these files, and NaN (not analyzed) was listed as the\ntest coverage and 0 was listed as the total blocks in that file. The number of components\nwith\nthis problem was found by the following script:\ngrep\n\n\'Basic\n\nblocks\n\nin\n\nthis\n\nfile\'\n\n/users/tgunn/DADS_TEST/bin/EXECUTABLES/$1_mod/b/*.tcov>/\nusers/tgunn/tmp/temp\ngrep\n\n\'\n\n0\'\n\n/users/tgunn/tmp/temp\n\ntwc\n\n-i\n\nTest coverage for calculation of the risk index for this particular program was estimated by the\namount of reused code resident in this piece of code. The reason for the inability to process\nthese files is still undetermined,\nbut we assume that it is either due to the iterative nature of the\nparticular program\'s execution or due to the inability of the tcov utility to process files beyond\nthe first parent/daughter\nlevel.\nfurther isolate this problem.\n\nExperimentation\n\ncan be done by altering\n\nthe calling script to\n\nAfter the total test coverage percentages\nhave been calculated,\nthe coverage distributions\ncan\nbe represented\nby frequency histograms.\nFrequency histograms\ncan be compiled either by\nusing EXCEL or by using Statgraphics.\nTo create a histogram under EXCEL, simply highlight the\ntotal test coverage column, select tools on the main menu, then select analysis tools, and then\nfrequency histograms.\nThis graph is usually more meaningful\nif the Pareto option is not\nselected.\nTo create a frequency histogram using Statgraphics,\nit is necessary to import the\ndata into Statgraphics.\n\n1.4.3.2.4\n\nProfiling\n\nProfiling data is collected using a combination\nof compiler directives and UNIX utilities. Profile\ndata was collected for the FORTRAN project, and the procedures\nin this section are applicable\nto profiling FORTRAN\ncode using the SUN FORTRAN\ncompiler.\nProfiling in other languages\ncan be accomplished\nby modifying the compiler directives, dependent upon the language and\nthe compiler used.\nThere are two methods\n\nof profiling\n\noptions\n\nusing the SUN\n\nFORTRAN\n\ncompiler:\n\nthe -p option\n\nand the -pg option. Each option must be used with its corresponding\nUNIX utility; prof for the\n-p option and gprof for the -pg option. The -pg/gprof\noption is what was used to provide a\nmuch more extensive profile information,\nbut both methods will be described in detail. We\nlater determined\nthat the added information obtained through the use of the -pg/gprof\noptions\ndidn\'t seem to be applicable for our purposes.\nThe -p/prof combination\nis recommended.\nTo profile a program\nf77\n\n(in FORTRAN),\n-p\n\n<program\n\nissue the following\nname>\n\n<executable\n\n84\n\ncommands\nname>\n\nwhen compiling:\n\nYou can then execute\n\nthe program.\n\nUpon conclusion\n\nof the program\n\ncreated which contains the profile information.\nThe prof\nthe results stored in this file by the following command:\nprof\n\nFive columns\n\n<executable\n\nfile is\n\nutility can then be used to interpret\n\nname>\n\nof data scroll across\n\nEach of the columns\n\nrun, a mon.out\n\nthe screen:\n\nhas the following\n\nname, % time, cumsecs,\n\n#call, and ms/call.\n\nmeanings:\n\nname\n\nthe name of the function\n\n%time\n\nthe percentage of the total execution\nthat particular function\n\ncumsecs\n\nthe cumulative\n\n#call\n\nthe number of times a specific\nexecution of the program\n\nms/call\n\nthe number\n\nseconds\n\nbeing\n\nmonitored\ntime of the program\n\nspent in the various\n\nof milliseconds\n\nfunction\n\nspent in\n\nparts of the program\n\nwas called during the\n\nper call of the specific\n\nfunction\n\nTo profile a program using the -pg/gprof\noption, simply replace the above -p with a -pg and\nprof with gprof in the above procedure.\nThis method provides similar data, as well as an\nabundance of data which is not really applicable for our use. More information on profiling can\nbe found in the SUN FORTRAN User\'s Guide and McGilton and Morgan\'s Introducing the\nUNIX System\n\n1.4.4\n1.4.4.1\n\n[9].\n\nLogiscope\nRemote\n\nLog On Procedures\nLog On Procedures\n\nLogiscope can be remotely logged on with proper setup. The ftp id\'s for some of the machines\nare listed below. Keep in mind that if the data is to be extracted data from galileo, any\nmachine can be used (as long as it is set up to access it), but if Logiscope is needed, an X\nWindows terminal is required.\nscl\n\nNed_j\nvenus\n\n141.205.3.61\n141.205.3.6\n\ngalileo\nmodeler\n\n134.118.101.24\n\nmercury\n\n134.118.124.10\n\n141.205.3.60\n\nTo remotely log on, simply use the rlogin or telnet commands to access the desired\nThe rlogin and telnet commands are described in more detail in section 1.5.\nTo use Logiscope,\ngalileo can display\nxhost\n\nfirst start up X Windows on the local machine\non the machine by typing:\n+galileo\n\n(or\n\n35\n\nhost\n\nmachine.\n\nand set up the machine\n\n+134.118.101.24)\n\nso\n\nThen use the rlogin or telnet command to remotely log on to galileo, and press return when the\nterminal asks for the display type. Next set the display variable on the machine by typing:\nsetenv\n\n1.4.4.2\n\nTerminal\n\nDISPLAY\n\n<local\n\nmachine\n\nftp\n\naddress>\n\nConfiguration\n\nTo run the Logiscope\nprogram, the working environment\nmust be configured properly.\nThis\nincludes allowing the local workstation to be controlled by galileo (using the xhost command)\nand making sure the account settings on galileo are properly configured and the environment\nis properly sey up. This is accomplished\nby making modifications\nto the .cshrc file on the\ngalileo machine (not the local machine).\nAdd the following lines for access to Logiscope to the\n.cshrc file:\nset\n\npath\n\n:\n\n($path\n\n/case/logiscope/bin)\n\nsetenv\n\nREF\n\n/gal_home/adp/logidir/reference\n\nalias\n\nfa\n\necho\n\nThese modifications\nused, the following\n\n\'32,\n\nhp,\n\nibm,\n\nnd,\n\nplus,\n\nvax\'\n\nstd,\n\nwill allow Logiscope to run properly.\nIf the REAP menu system\nline should be added to the .cshrc file:\n\nsource\n\nis to be\n\n-bmears/REAP/reap_setup\n\nThe REAP menu system\nLogiscope.\n\nis described\n\nin more detail in section\n\n1.4.4.4, A Quick Tour of\n\nThe .cshrc file should already be set up, but these commands\nare convenient in case\nsomething happens.\nTwo other additions to the .cshrc file which were useful were two alias\ncommands,\nused to set up the display environment:\nalias\n\nseethe\n\n\'setenv\n\nalias\n\nshowd\n\n\'echo\n\nThese commands\nallow the user to quickly\nmachine being used by typing in "sethome"\n\nDISPLAY\n\n<local\n\nmachine\n\nftp\n\naddress>\'\n\n$DISPLAY\'\n\nset up the display environment\nstring to the\nrather than something\nlike "setenv DISPLAY\n\n141.205.3.60".\nThe showd command is just a way to verify that the user is configured at the\nright machine.\nIf the display environment\nis not set properly, graphics images requested could\nbe sent to another machine.\n\n1.4.4.3\n1.4.4.3.1\n\nLogiscope\n\nStartup\n\nProcedures\n\nUsing the Menu System\n\nThe Information\nSystems Directorate has created a menu system to allow the Logiscope tool to\nbe accessed with more ease than the command line interface.\nTo initiate the menu system,\ntype "reap". This will bring up two menus, a controller menu, and a main menu. Logiscope\nis\nfound under the General Support section of the main menu. Double click on the General\nSupport\n\nwith the left mouse button,\n\nand double click on the Logiscope\n\n36\n\nTools selection.\n\nThe\n\nREAPmenusystemwill then prompt for what the desired operation is. Double\noperation and fill in the appropriate blocks as necessary.\n\nclick on the\n\nWhen accessing the static or dynamic editor, it is necessary to insert the path of the\nappropriate\nreference file for the analysis.\nSimply change the defaults to the file needed.\nsame is done for the archive filename when archiving results files.\n\nThe\n\nIn general, the REAP system provides a user friendly alternative to Logiscope\'s\ncommand line\ninterface.\nOne slight difficulty was noted with the menu system, which will be changed for\nfuture releases.\nOnly one FORTRAN analyzer exists on the menu, which defaults to the\nFORTRAN standard analyzer.\nIf you want to use other FORTRAN analyzers, it is necessary\nset an environment\nvariable before running REAP. This is accomplished\nby the following\ncommand (which must be issued before running REAP):\nsetenv\nLSCOPE_FORTRAN_ANALYZER\nexecutable>\n\n<Logiscope\n\nFORTRAN\n\nanalyzer\n\nThe Logiscope FORTRAN executables\nwere listed in table 1. An example of this command\nwould be to type "setenv LSCOPE_FORTRAN_ANALYZER\nlog_fort_plus"\nto run the F77+\nanalyzer.\nAn alias has been set up to list the FORTRAN\nanalyzers on the fly. Typing "fa" will\nlist the FORTRAN analyzers available.\n\n1.4.4.3.2\n\nFrom the Command\n\nLine\n\nThere are a variety of run commands and execution switches which can maximize the\nexecution efficiency of the program for a specific application.\nFor more information on these\ntypes of command line options, refer to the Logiscope\nmanuals.\n\n1.4.4.4\n\nA Quick Tour of Logiscope\nLogon to local terminal (with X Windows\nstartx - runs the X Windows program\nxhost + <your\n\nresident\n\ntelnet <your resident\nusername:\npassword:\nTerminal Type:\nsethome\n\nshowd\n\nterminal\n\nterminal\n\ncapability)\n\nname>\nname>\n\n<xterm>\n\n(should default to this, press return)\n\ndirects the graphical display to your resident terminal name;\nwhen on a different machine, type "setenv DISPLAY <node id>";\nexample:\nwhen accessing from a different terminal, type "setenv\nDISPLAY 130.40.10.86:0"\n- verify display\n\nis set to node id\n\n37\n\nto\n\nLogiscopecan be accessedin two ways, usingthe menusystemor accessingfromthe\ncommandline. The followingcommandwill start the menusystem. To accessfromthe\ncommandline, the Logiscopetool is residentin the/case/Iogiscopesubdirectory.\nreap\n\n- starts the menu system\n\nThe Logiscope tool is under the general support section of the main menu. Logiscope\ncontains three main components,\nan analyzer, an archiver, and an editor. The analyzer is\nused to analyze/instrument\nraw source code. It will create a .res file for each component\nanalyzed.\nThe archiver is used to group .res files together into a .arc file, which can be read\nby the editor. The editor is where we will do the majority of our work. Some sample editor\ncommands\nare:\ncontrol\n\n- displays the control graph of the selected\nthrough the components,\nq quits)\n\ncall\ncrit\n\n- displays\n- displays\n\nkiviat\nmetbars\n\n- displays the kiviat graph of the selected component\n- displays the metric distribution charts (enter cycles through\nq quits)\n\nh\nh <cmd>\n\n- displays\n- displays\n\nP\nend\n\n- outputs screen to a print file\n- ends session\n\ncomponent\n\n(enter cycles\n\nthe call graph of the system (usually takes a while)\nthe criteria graph of the selected component\nthe metrics,\n\nthe general help screen\nhelp on specific command\n\nThe editor can be toured using the output data from one of our programs (or some of the\nsample files in the/case/Iogiscope\ndirectory).\nThe REAP menu should default to the data\ngenerated for our program.\nIf this fails, the filename is cmslib/cmsout/itve.arc\nand the\nreference file is reference in the home directory alee.\n\n1.5\n\nUNIX\n\nBasics\n\nand\n\nCommands\n\nThese are several simple commands which enabled an idiot like me to get around.\nmight be handy to have a "cheat sheet" of some nifty commands.\ncp\n\n- copies files\n\nrm\n\nI thought it\n\n- removes files\n\nIs\n\n- lists files in directory\n-I\n-a\n\nman <cmd>\n\n- displays a more detailed file listing (date, size, etc)\n- displays all files, including hidden\n- displays an on-line manual for the selected command.\nsample usage:\n\nmv\n\nman Is - displays the manual for the list command\n\n- moves files from one directory to another, or renames files\nsample usage: mv nulan geek - renames file nulan to geek\n\n38\n\nftp <hostname>\nget <file>\nmget <files>\nput <file>\n\nEnters the file transfer procedure.\n- get a file from <hostname>\n\nand copy to current\n\n-a\n\nvi <file>\n\nare:\ndirectory\n\n- get multiple files (use with wildcard) from <hostname>\nand copy to\ncurrent directory\nputs file on hostname\nHint for use: If you have a lot of files to transfer, you can eliminate that\nannoying need for answering "Y" prior to each file by typing "ftp -i."\ndisplays\n\nps\n\nSome options\n\nprocess\n\ndisplays\n\nall users processes\n\nstatus (processes\n\n- invokes the vi test editor.\n:w - writes the file to disk\n\nSome\n\nactive\n\non the machine)\n\nhelpful commands\n\nare:\n\n:q - quits vi\ndd - deletes entire line\nx - deletes character\ni - inserts characters\n\n(>esc< to quit inserting)\n\nrlogin <name>\n\n- remote\n\nIogin to another\n\nmachine\n\n(use when accounts\n\nare the same)\n\ntelnet <name>\n\n- remote\n\nIogin to another\n\nmachine\n\n(use when\n\nare the different)\n\nalias\n\n- allows substitution of specified keystrokes\nlonger) commands.\nFor example,\nlist, type "alias II \'Is -I\'."\n\naccounts\n\nfor commonly\n\nif you wanted\n\nUnalias\n\n- removes\n\nmore <file>\n\n- lists file, one screen at a time (v to directly\n\ncat <file>\n\n- compare\n\ngrep\n\n- invokes the UNIX batch command\n\nat\n\n- similar\n\na long\n\n- searches files for specific string\n\nbatch\n\nII to display\n\n- lists file\n\ndiff <file1 ><file2>\n\nused (usually\n\nthe aliases on a command.\n\nFor example,\n\nII\n\nedit screen)\n\ntwo files and prints out the differences\n\non the screen\n\nprocessor\n\nto batch, allows one to execute commands\n\n39\n\nunalias\n\nat specific\n\ntimes\n\nSECTION\n2.1\n\n2:\n\nSOFTWARE\n\nRELIABILITY\n\nESTIMATION\n\nAND\n\nPREDICTION\n\nIntroduction\n\nSince the early 1970\'s tremendous growth has been seen in the development\nof software.\nSoftware has increased\nin complexity and size at a rapid rate. Part of this change is a result of\nthe development\nof faster microprocessors,\ndistributed\nprocessing systems, and networking.\nSoftware today performs a myriad of complex tasks that require accuracy and reliability.\nThe\ndevelopment\nof failure-free\nsoftware has always been a major goal in any organization.\nTesting until the software is totally failure free, however, may be an impossible task for large programs (it would require checking all the combinations\nof logic paths and inputs). Software\nreliability estimation and prediction, or software reliability, can provide a practical insight\nmethod into the failure state of the code during the testing process.\nSoftware\nspecified\n\nreliability is the probability of failure-free\noperation of a computer program for a\ntime in a specified environment.\nThe probability of failure-free\noperation can be ex-\n\npressed using failure intensity.\nFailure intensity is defined as the rate of change of the number\nof failures per unit time (usually expressed as failures per 1000 CPU hours). Therefore, a\nspecific failure intensity objective (or goal) can be set for a piece of software depending on the\nrequirements\nand the application of such software (usually defined to be 0 to 10 failures per\n1000 CPU hours for flight software, and 10 to 20 failures per 1000 CPU hours for ground support software as used in industry, academia, and the Department\nof Defense).\nOther expressions of software reliability are described in terms of the mean time to the next failure (when\nthe next failure will occur), the maximum number of failures (total number of failures expected\nfrom the software), and the hazard rate (the conditional\nfailure density).\nThe current status of software reliability can forecast the projected testing termination date. It\ncan also aid in determining\nhow to best allocate testing resources (manpower,\ncomputer time)\namong the various program modules.\nSoftware reliability can also be used in making decisions regarding design tradeoffs between reliability, costs, performance,\nand schedule.\nThe\nmain thrust behind software reliability, however, is to provide a quantitative\nmeans to measure\nthe probability of failure-free\noperation of a computer program.\n\n2.1.1\n\nModels\n\nThere are currently over 40 software reliability models.\nThese models can be categorized\ninto\ntime domain, data domain, and fault seeding approaches.\nThe focus of this section will be on\nthe time domain approach.\nDetailed description of the models can be found in section 2.2.1.\n\n2.1.2\n\nTools\n\nTwo tools used by JSC SM&A are the Software Reliability Engineering\n(SRE) written at the\nAT&T Bell Laboratory,\nand the Statistical Modeling and Estimation of Reliability Function for\nSoftware (SMERFS Version 4.0) tool developed by the Naval Surface Warfare Center. Concepts behind the SRE Toolkit can be found in Software Reliability Measurement,\nPrediction,\nApplication\nby Musa, lannino, and Okumoto [12]. Detailed information on the SMERFS Tool\ncan be found in Statistical Modelinq and Estimation of Reliability Function for Software\n(SMERFS) Users Guide by Farr [14]. These tools accept test data such as computer\n\n40\n\nexecutiontime for software,failure occurrencetime, and resourceutilizationandrelatedcalendar time informationas inputdata (seesection2.3). The toolsthen estimatethe reliabilityof\nthe softwareusingthe approachesof the differentsoftwarereliabilitymodels.\n\n2.1.3\n\nData Collection\n\nSoftware\nphase.\ntime the\nsource,\ntion and\n\n2.1.4\n\nand Analysis\n\nreliability uses the software testing data at and beyond the software integration\nThis data includes total execution time, failure description comments, and the exact\nfailure occurred.\nSome models require additional data that include computer recomputer utilization, and manpower resources.\nDetailed description of the data collecanalysis can be found in section 2.4.1.\n\nModeling\n\nThe SMERFS\n\nProcedure\n\nand Analysis\n\nTool and the SRE Toolkit\n\ncontain\n\nmathematical\n\nmodels\n\nthat are used in estimat-\n\ning and predicting failure occurrences.\nThe input failure data is collected and analyzed for\napplicability\nto each model\'s assumptions.\nFailure data that matches a particular model\'s assumptions is used to estimate and predict the present and future failure occurrences.\nThe\nmodeling results are presented in graphical or tabular form, depending on the particular\nmodel\'s interface.\nA detailed description of the procedures for each tool can be found in\nsection 2.5.\nThe model results, in tabular or graphical form, show the estimated and predicted failure occurrences. In all cases, the models will report the approximate\ntime to the next future failure.\nModels can report total predicted number of failures that will be encountered,\ntime to reach a\nfuture failure, number of failures left to correct, and the maximum number of failures predicted\n(results\n\n2.2\n\nare model dependent\n\nand may not be reported\n\nthe same from model to model).\n\nBackground\n\nSoftware reliability estimation and prediction is usually referred to as software reliability.\nThis\nsection will define software reliability, and will provide a table that outlines the differences\nbetween software and hardware reliability.\nIn addition, a brief description\nof some of the software\nreliability\n\nmodels will be included.\n\nSoftware\nas:\n\nreliability\n\nis defined\n\nby the Institute\n\nof Electrical\n\nand Electromechanical\n\nEngineers\n\n[9]\n\n1) "The probability that software will not cause the failure of a system for a specified time under\nspecified conditions.\nThe probability is a function of the inputs and the use of the system,\nand is also a function of the existence of faults in the software.\nThe inputs to the system\ndetermine\n\nwhether\n\nexisting\n\nfaults,\n\nif any, are encountered."\n\nand\n2) "The ability of a program\nperiod of time."\n\nto perform\n\na required\n\n41\n\nfunction\n\nunder stated\n\nconditions\n\nfor a stated\n\nAnother definition by Shooman [10] is "the probability\naccording to specifications,\nfor a given time period."\nA more precise definition, as stated\nfailure-free\noperation of a computer\n\nthat the program\n\nperforms\n\nsuccessfully,\n\nby Musa, lannino, and Okumoto [11], is "the probability of\nprogram for a specified time in a specified environment."\n\nA failure is defined as the departure of the external results of a program operation from the\nrequirements.\nA fault is a defect in the program that, when executed under particular conditions, causes a failure.\nFor example, a fault is created when a programmer\nmakes a coding\nerror. A failure is a manifestation\nof that fault. The reverse argument, however, is not true.\nTo understand\n\nthe software\n\napproach\n\ntween hardware and software\nthose differences.\n\nto reliability,\n\nreliability\n\nTable 8 - Contrasts\nHARDWARE\n\nan understanding\n\nneeds to be established.\n\nof Software\n\nFAILURES\n\nand Hardware\n\nSOFTWARE\n\nof the differences\n\nThe following\n\nbe-\n\ntable outlines\n\nReliability\n\nFAILURES\n\nFailures can be caused by deficiencies\nin\ndesign, production,\nuse and maintenance\n\nFaults are caused\n\nFailures\n\ncan be due to wear or other\n\nThere is no wearout\n\nenergy-\n\nrelated\n\nby design\n\nerrors, with\n\nproduction (copying), use, and maintenance (excluding corrections)\nhaving negligible effect\nphenomenon\n\nphenomena\n\nFailures of components\nof a system is\npredictable\nfrom the stresses on the and\ncomponents,\nother factors\n\nFaults are not predictable from analysis of\nseparate statements.\nErrors are likely to\n\nReliability can depend upon operating\ntime (i.e. burn-in, wearout)\n\nReliability does not depend on operational\ntime. It is a function of the effort put into\n\nReliability\nfactors\n\nis related to environmental\n\nexist randomly\n\nthroughout\n\nthe program\n\ndetecting and correcting errors\nExternal environment\ndoes not affect reliability, except\n\nit might affect program\n\ninputs\nRepairs\n\ncan be made to increase\n\nity of the equipment\n\n2.2.1\n\nDescription\n\nreliabil-\n\nThe only repair is by redesign\n(reprogramming)\n\nof Models\n\nOver 40 models that estimate and predict software reliability exist. A comprehensive\ntheoretical discussion of many of the models may be found within A Survey of Software Reliability,\nModeling, and Estimation, Naval Surface Warfare Center, NSWC TR 82-171 [12]. Another\nsource of theoretical\nand practical approaches\nmay be found in Software Reliability Measurement, Prediction, Application\n[11].\n\n42\n\nThe models describedin this\n\nsection generally assume one of three approaches\n\nin estimating\n\nand predicting reliability:\n-\n\nTime Domain:\nData Domain:\n\n-\n\nFault Seeding:\n\n2.2.1.1\n\nTime Domain\n\nuses specific failure time data to estimate and predict\nuses the ratio of successful runs vs. total runs\ndepends\n\non artificially\n\nreliability\n\nplacing faults within a piece of code\n\nModels\n\nTime domain models estimate program reliability based on the number of failures experienced\nduring a certain period of testing time. This can be either the computer execution CPU time, or\nthe calendar time that was spent in testing.\nSpecifically, time domain models predict the number of failures remaining, the time to next failure, and the test time required to achieve a reliability objective (goal). A brief description of the models is supplied in this section. The majority of the described models use the time domain approach.\nBrooks and Motley\'s\n\nModels---This\n\nmodel assumes that in a given testing\n\nperiod not all of the\n\nprogram is tested equally, and in the development of a program, only some portion of the code\nor modules may be available for testing. In addition, through the correction of discovered failures, additional failures may be introduced.\nDuane\'s Model--This\nmodel employs a non-homogeneous\ncounts. This model was originally proposed as a hardware\nGeneralized\n\nPoisson\n\nModel--This\n\nmodel assumes\n\nPoisson process for the failure\nreliability growth model.\n\nthat failures\n\noccur purely at random and\n\nthat all faults contribute equally to unreliability. In addition, all fixes are perfect and the failure\nrate increases by the same amount except within the failure count framework.\nGeometric\n\nModel_This\n\nmodel assumes\n\n1) that a number\n\nof program\n\nfailures that will be\n\nfound will not be fixed; and 2) that failures are not equally likely to occur, and as the failure\ncorrection process progresses, the failures become harder to detect.\nGoeI-Okumoto_This\nmodel accounts for the fact that failures occur purely at random and that\nall failures contribute equally to unreliability. In addition, fixes are perfect and the failure rate\nimproves continuously over time.\nGeometric Poisson Model---This\nmodel assumes that the reporting of software failures occurs at a periodic basis. Only the numbers of failure occurrences per testing interval are\nneeded. The testing intervals, however, are all assumed to be the same length with respect to\ntime (a testing period is composed of a day, week, etc.). Additionally, since the model assumes a constant rate of failure occurrence during a time period, the model is best applied to\nsituations in which the length of the reporting period is small in relationship to the overall length\nof the testing time.\n\n43\n\nJelinski and Moranda "De-Eutrophication"\n\nModel---This\nmodel assumes that failures occur\npurely at random and that all failures contribute equally to unreliability. In addition, fixes are\nperfect; thus, a program\'s failure rate improves by the same amount at each fix. This model\nshould only be applied to complete programs.\nJelinksi\n\nand Moranda\'s\n\nModel 1 and Model 2--The\n\nbasic Jelinski and Moranda\'s\n\n"De-\n\nEutrophication" model described above cannot be applied to software programs that are not\ncomplete. These models (model 1 and model 2) deal with software under development.\nIf at\nany point in time a failure is discovered, an estimate of the reliability can be obtained based\nupon the percentage of the program completed.\nBayesian Jelinski-Moranda--This\nmodel assumes that failures occur purely at random and\nthat all failures contribute equally to unreliability. In addition, fixes are perfect; thus, a program\'s failure rate improves by the same amount at each fix.\nLittlewood\'s\nBayesian Debugging Model_This\nmodel reformulates the Jelinski-Moranda\nModel into a Bayesian framework.\nIt assumes that each failure does not contribute equally\nsince the correction of failures in the beginning of the testing phase has more of an effect on\nthe program than ones corrected later (a program with two failures in a rarely executed piece\nof code is more reliable than a program with one failure in a critical section of code).\nLittlewood and Verrall\'s Bayesian Reliability Growth Model--This\nmodel assumes that the\nsize of the improvement in the failure rate at a fix varies randomly. This represents the uncertainty about the fault fix and the efficiency of that fix. The model accounts for failure generation in the corrective process by allowing for the probability that the program could be worsened by correcting the failure. The tester\'s intention is to make a program more "reliable"\nwhen a failure is discovered and corrected, but there is no assurance that this goal is\nachieved. With each failure correction, a sequence of programs is actually generated (code is\nrecompiled with each failure corrected).\nEach is obtained from its predecessor by attempting\nto correct a failure. Because of the uncertainty involved in this correction process, the relationship that one program has with its predecessor cannot be determined with certainty.\nLittlewood--This\nmodel assumes that failures occur purely at random, have different sizes,\nand contribute unequally to unreliability. In addition, fixes are perfect; thus, a program\'s failure\nrate improves by the same amount at each fix. Larger failures, however, tend to be removed\nearly, so there is a law of diminishing returns in debugging.\nLittlewood\'s\nSemi-Markov\nModel--This\nmodel incorporates the structure of the program in\ndeveloping its reliability. Littlewood adopts a modular approach to the software and describes\nthe structure via the program\'s dynamic behavior using a Markov assumption. The program\ncomprises a finite number of modules with exchanges of control between them that follow a\nsemi-Markov law.\nLittlewood Non-Homogeneous\nPoisson Process--This\nmodel assumes that failures occur\npurely at random, have different sizes, and contribute unequally to unreliability. In addition,\nfixes are perfect; thus, a program\'s failure rate improves by the same amount at each fix, and\nassumes a continuous change in failure rate (instead of discrete jumps) when fixes take place.\n\n44\n\nKeiller-Littlewood_This modelassumesthatfailures occurpurelyat random,havedifferent\nsizes, and contributeunequallyto unreliability. Inaddition,fixes are perfect; thus, a program\'s\nfailure rate improves by the same amount at each fix. Larger failures, however, tend to be\nremoved early, so there is a law of diminishing returns in debugging. This model is similar to\nthe Littlewood model, but it uses a different mathematical form for reliability growth estimation\nand prediction.\nModified\n\nGeometric\n\n"De-Eutrophication"\n\nModel---This\n\nmodel assumes that the number of\n\nprogram failures that will be found will not be fixed. In addition, failures are not equally likely to\noccur, and as the failure correction process progresses, the failures become harder to detect\nand do not have the same chance of detection.\nMusa-Okumoto_This\n\nmodel accounts for the fact that failures occur purely at random and\n\nthat all failures contribute equally to unreliability.\nrate improves continuously over time.\ngram\'s reliability than earlier ones.\nMusa Execution\n\nTime Model--The\n\nIn addition, fixes are perfect and the failure\n\nLater fixes, however, have a smaller effect on a pro-\n\nmodel is based upon the amount of central processing\n\nunit (CPU) execution time involved in testing rather than on calendar time; however, the model\nattempts to relate the two. By doing this, Musa is able to model the amount of limiting resources (failure identification personnel, failure correction personnel, and computer time) that\nmay come into play during various time segments of testing. In addition, this model eliminates\nthe need for developing a failure correction model since the failure correction rate is directly\nrelated to the instantaneous failure rate during testing. It assumes that the expected number\nof failures exponentially approaches an asymptote with time. This model is also known as the\nMusa Exponential Time Model (the reliability growth function increases exponentially with\ntime).\nMusa Logarithmic Time Model---This\nmodel is similar to the Musa Execution Time Model\ndescribed above, except it assumes that the reliability growth function is a logarithmic function\n(increases\n\nlogarithmically\n\nwith time) instead of an exponential function.\n\nNon-Homogeneous\nPoisson Process---This\nmodel assumes that the failure counts over nonoverlapping time intervals follow a Poisson distribution. The expected number of failures for\nthe Poisson process in an interval of time is assumed to be proportional to the remaining number of failures in the program at that time.\nRushforth,\n\nStaffanson,\n\nand Crawford\'s\n\nModel---This\n\nmodel assumes that failures occur\n\npurely at random and that all failures contribute equally to unreliability.\nnot perfect and allow introduction of new faults into the program.\nSchick-Wolverton\n\nModel--This\n\nmodel assumes\n\nthat the hazard\n\nIn addition, fixes are\n\nrate function is not only pro-\n\nportional to the number of failures found in the program, but proportional to the amount of\ntesting time as well. As testing progresses on a program, the chance of detecting failures\nincreases because of convergence on those sections of code in which the faults lie.\n\n45\n\nShooman Model_This modelassumesthatthe numberof faults\n\nin the code are fixed, and\n\nthat no new faults are introduced into the code through the correction process. The failures\ndetected are independent of each other and the rate is proportional to the number of failures\nremaining in the code.\nS-Shaped Reliability Growth Model--This\nmodel accounts for the fact that failures occur\npurely randomly and all faults contribute equally to total unreliability. Fixes are perfect, and\nfailure rate improves continuously in time. It also accounts for the learning period that testers\ngo through as they become familiar with the software.\nSchneidewind\'s\nModel--The\nbasic philosophy of this model is that the failure detection process changes as testing progresses, and that recent failure counts are of more consequence\nthan earlier counts in predicting the future. This means that more weight should be given to\nthe most recent intervals, where the failure rate has dropped, when estimating and predicting\nreliability.\nThompson\nand Chelson\'s Bayesian\nsoftware program might be failure-free\n\nReliability Model_This\nmodel assumes that a given\n(an infinite mean time between failures, or M\'I\'BF). It\n\nalso assumes that the software redesign and repair after failures in a given test phase will be\nconducted at the end of the testing intervals in which the failures occurred.\nTrivedi and Shooman\'s\nMany State Markov Models-This\nmodel is used in providing estimates of the reliability and availability of a software program based upon a failure detection\nand correction process. Availability is defined as the probability that the program is operational\nat a specified time.\n\n2.2.1.2\n\nData Domain\n\nModels\n\nData domain models estimate reliability by using the number of successful runs vs. the total\nnumber of testing runs completed during specified intervals. These models estimate the average reliability and the number of remaining faults.\nNelson Model--This\nmodel combines the results of a given set of runs and tries to take into\naccount the input values needed to execute the program.\nLaPadula\'s\n\nReliability\n\nGrowth\n\ncurve through the success/failure\n\n2.2.1.3\n\nFault Seeding\n\nModel---The\n\napproach\n\nis to fit, using least squares, a reliability\n\ncounts observed at various stages of the software testing.\n\nModels\n\nThis approach requires programs to be seeded with faults and released to a group for testing.\nIt assumes that the seeded fault distribution is the same as the inherent fault distribution. The\nestimates of the remaining failures are made based on the number of faults discovered by the\ntest group. It is the opinion of this group that fault seeding models should not be employed\nbecause information on the algorithms and their effectiveness are not readily available or\nproven.\n\n46\n\n2.3\n\nTools\n\nThere are currently various tools available that address software reliability.\nThe Analysis &\nRisk Assessment\nBranch has acquired and used the SMERFS Tools and the SRE Toolkit.\nSMERFS is used because it contains more models than any other tools mentioned above, and\nis a public domain utility. The SRE Toolkit is used because it takes CPU time as input and\ngives a precise display on the plotting results. It is commercially\navailable through AT&T Bell\nLaboratories.\nThis section will briefly summarize the workings of the SMERFS and SRE tools.\n\n2.3.1\n\nSMERFS\n\nTool\n\nThe Statistical Modeling and Estimation of Reliability Functions for Software (SMERFS) version\n4.0 includes 10 models described below. This tool is a public domain software application\nthat\ncan run on an IBM PC and compatible platform.\nThe tool was developed by the Naval Surface\nWarfare Center (Dr. William Farr [13]).\nSMERFS calculates the reliability of software by using execution time or interval counts and\nlengths data as input. SMERFS models that use execution time data require software CPU\nfailure times or failure counts as input. Interval counts and lengths data models, on the other\nhand, require the number of failures that occurred for a particular interval. An interval is\ndefined as an equal time partition of the total testing time (CPU time or calendar time).\nThe models\n\nincluded\n\nExecution\n\nin SMERFS\n\nare:\n\nTime Data Models\n\nLittlewood and Verrall\'s\nMusa\'s Basic Execution\n\nBayesian Model\nTime Model\n\nMusa\'s Logarithmic\nGeometric Model\n\nExecution\n\nNon-homogeneous\nInterval\n\nPoisson\nPoisson\n\nModel for execution\n\nCounts\n\nTime\n\nModel\ntime data\n\nand Lengths Data Models\n\nGeneralized\n\nPoisson\n\nModel\n\nNon-Homogeneous\nPoisson Model for Interval Data\nBrooks and Motley\'s Discrete Model\nSchneidewind\'s Model\nS-Shaped\n\nReliability Growth Model\n\nRefer to the SMERFS\n\nUser\'s Guide [13] for assumptions\n\nSpecifically,\n\nis divided\n\nAccess\n\nSMERFS\n\nand Activation\n\ninto the following\n\nOption_An\n\nscreen output for later review.\navailable.\n\nto each model.\n\noptions:\n\noption exists for an optional\n\nhistory file that saves all\n\nA plot file that stores all graphical output data as text is also\n\n47\n\nInput Option--SMERFS allowsthe usertwo types of inputmethods\nentered manually by keyboard or automatically\ncapability to load a maximum of 1000 entries.\nEditing\n\nOption_The\n\nthrough file insertion.\n\nfor data. Data may be\nSMERFS has the\n\nedit option permits altering of the software failure\n\ndata entries in case\n\nediting is needed.\nTransformations\n\nOption--This\n\noption\n\npermits\n\nthe scaling of a software failure\n\ndata entries.\n\nFive types of transformations are allowed along with the options of restoring the data entries to\nit\'s original state. The basic purpose of this module is to transform the data by multiplication or\naddition into a refined and transformed set of numbers.\nGeneral Statistics Option--This\noption provides general summary statistics of the software\nfailure data. Statistics such as the median of the data, the upper and lower bounds, the maximum and minimum values, the average, standard deviation, sample variance, skewness,\nkurtosis, number of failures found, and total testing time are reported.\nRaw Data Plots Option--This\noption generates\nplots of the software failure data. The raw\ndata is the original input of the software failure data. This module will also provide smoothing\nthe sample size contains more than six failure entries.\nExecution\n\nTime Data Option--This\n\nInterval Count and Lengths\nlengths data models.\nAnalysis\n\noption executes\n\nData Option--This\n\nof Model Fit Option---This\n\nthe five execution time data models.\n\noption executes the five interval count and\n\noption allows the plot of the raw and predicted values for\n\nthe last model executed to be displayed and saved. The prediction must be run, and the future values stored in memory so that a plot can be created. Residual plots and goodness-of-fit\nstatistics are also provided within this subroutine.\nTermination and Follow-Up\nrecently created files.\n\n2.3.2\n\nOption--Normal\n\ntermination\n\nwill write and close any open or\n\nSRE Toolkit\n\nThe SRE Toolkit, developed by AT&T Bell Laboratory, consists of two models: the Musa\nExponential Time model and the Musa Logarithmic\nTime model. It is written in "C" and can be\nrun on IBM PC and compatibles.\nThe current version is 3.10A, published on April 5, 1991.\nThe SRE Toolkit uses only CPU execution data as input; therefore, the failure occurrence time\nhas to be precisely recorded by the computer during testing.\n\n48\n\nif\n\nThe general\n\nassumptions\n\n-\n\nThe software\n\n-\n\nThe CPU execution\ndistributed.\n\n-\n\nThe hazard\n\n-\n\nFailure\n\nof the Musa models\n\nis operated\n\nin a similar\n\ntimes\n\nbetween\n\nrate is proportional\n\nintensity\n\ndecreases\n\nare:\n\nmanner\nfailures\n\nas anticipated\nare piece-wise\n\nto the number\n\nwith failures\n\nof failures\n\nexperienced\n\noperational\nexponentially\n\nremaining\n\nusage.\nor logarithmically\n\nin the program.\n\nand corrected.\n\nThe SRE Toolkit is not a menu-driven\nsoftware.\nWhen the toolkit is run, all options have to be\nentered into an ASCII file, the .fp file. The .fp file contains user\'s choice of model type, failure\nintensity objective, and confidence level. Section 2.5.2.2 will describe in detail the .fp file.\n\n2.4\n\nData\n\nCollection\n\nand Analysis\n\nThe input data to software reliability modeling is software test data software failure occurrences, test time, and resources.\nThe required data has to be defined, and a collection procedure needs to be established\nprior to the start of the testing.\nFor any analysis efforts conducted on projects already started, concluded, or missing a data\ncollection procedure, a collection of failure reports of the test can be used for modeling using\nthe interval and counts lengths models (see section 2.3.1).\n\n2.4.1\n\nDescriptions\n\n2.4.1.1\n\nof Data Required\n\nTest Time Data\n\nThe most accurate test time data is recorded by the computer.\nconsists of time that a test run starts and stops. Any messages\nfailures should usually accompany the execution time data.\n\n2.4.1.2\n\nResource\n\nThe test time data required\nassociated with the test\n\nData\n\nThis data contains the manpower and computer resources allocated for the software test.\nManpower data consists of the number of hours that testers and developers\nspent to correct\nsoftware failures.\nComputer resource data consists of total computer user wall clock time reported. Both manpower and computer resource data will be used as parameters\ninput to the\nCalendar Time Model which is embedded in the SRE tool. This model will convert the estimation and prediction\nremaining\n\nresults\n\nfrom CPU execution\n\nto reach the reliability\n\nTable 9 is a list of manpower\nin an ASCII file (the .fp file).\nthe table.\n\nobjective\n\ntime to calendar\n\nand completion\n\ntime, such as testing\n\ndays\n\ndate.\n\nand computer resource parameters.\nThe parameters are stored\nData types and ranges of the parameters are also described in\n\n49\n\nTable\n\n9 - Manpower\n\nParameter\nName\n\nand Computer\n\nResource\n\nType\n\nDescription\ncomputing\n\nmuf\n\ncorrection\n\nmui\n\nidentification\n\nwork expended\n\npi\n\nidentification\n\npersonnel\n\npc\n\navailable\n\nRange\n\navailable\n\ntime in prescribed\n\nwork\n\n0 - 999\n\nreal\n\nfailure\n\n0 - 999\n\nreal\n\nfailure\n\n0 - 999\n\nreal\n\nper failure\n\nreal\n\n1 - 999\n\nreal\n\n0.01 - 9999\n\nreal\n\nexpended\n\nwork expended\n\ncomputer\n\nValue\n\n1 - 999\n\nreal\n\n0.001 -1\n\nreal\n\nmuc\n\nresources\n\nParameters\n\n0.001 -1\n\nreal\n\n0.001\n\nperiods)\npf\n\ncorrection\n\npersonnel\n\nrhoi\n\nutilization\n\nof identification\n\nrhoc\n\ncomputer\n\nutilization\n\nrhof\n\nutilization\n\nof correction\n\nthetai\n\nidentification\n\nthetac\n\ncomputer\n\nthetaf\n\ncorrection\n\nworkda\n\naverage\n\navailable\npersonnel\n\nfactor\npersonnel\n\nwork expended\n\ntime expended\n\nper CPU hour\n\nper CPU hour\n\nreal\n\n0 - 99999\n\nreal\n\n0 - 9999\n0 - 99999\n\nwork expended\n\nper CPU hour\n\nreal\n\nhours per workday\n\ndefault = 8\n\nreal\n\n0.1 - 24\ndefault\n\nworkwk\n\naverage\n\nworkdays\n\nper week default = 5\n\nTest Failures\n\n= 8 hrs\n0.1 -7\n\nreal\ndefault\n\n2.4.1.3\n\n-1\n\n- 5 days\n\nReports\n\nTrouble Report (TR) or Discrepancy\nReport (DR) contains a description of the failure, failure\noccurrence\ndate, severity level of the failure, and corrective action. TRs or DRs related to a\nfailure during a software test are generated\nby testers, and the corrective actions can be provided by either designers or testers. The information found in the TRs or DRs could be very\nhelpful in the event that computer-automated\nfailure information\nis not available or complete.\n\n2.4.1.4\n\nTest Script\n\nTest script is a computer file written by tester in a specific language that specifies the software\ntest sequence\nof a specific test case. Test script can help an analyst validate a software failure, specifically when a test case is designed to intentionally fail the software.\n\n50\n\n2.4.1.5\n\nSoftware\n\nSource\n\nThis is the computer\n\nCode\n\nprogram\n\nwhich is usually stored in ASCII format.\n\nIn addition\n\nto the source\n\ncode analysis which is discussed in Section 1, Software Complexity Analysis and the Determination of Software Test Coverage, source code can be used to identify which part of the code\ncontains the software fault that caused a software failure during a test run. This, in turn, will be\nused to approximate\n\n2.4.2\n\nthe time that the software\n\nData Collection\n\nfailure\n\noccurs.\n\nProcedure\n\nBefore gathering and evaluating failure data, the software test process needs to be considered\nfor appropriateness\nto the modeling assumptions.\nSoftware reliability works best on the software code which is ready to be integrated.\nThe reliability measured during testing should\nclosely resemble the actual reliability of the software during operation.\nFailures obtained\nduring unit testing are not recommended\nfor use in modeling.\nThe most appropriate\n\nlevel of testing\n\nto apply to software\n\nreliability\n\nincludes\n\nthe software\n\ngration test for multiple computer software configuration\nitems (CSCIs), the software\nacceptance\ntest, and a system test which combines software and hardware.\n\ninte-\n\nuser\n\nThis section provides the step-by-step\nprocedure of how to obtain data for the modeling process. Before testing, the analyst should coordinate with the testing group to set a routine time\nfor data collection to ensure that data are promptly collected.\nThe frequency of data collection\ndepends on the project, number of tests, number of testers, and the amount of data accumulated. The data obtained may include the automated test logs, tester\'s resource data, trouble\nreports, discrepancy\nreports, software source code, and test scripts. The software source\ncode and test scripts should be available before testing starts. Tester\'s resource data should\nbe available when testing starts.\nStep 1 : Transfer\n\nraw data from testing\n\ngroup to the analysis\n\nThis step describes the procedures of downloading\nsite tester to the JSC Engineering\nVAX Laboratory,\n\ngroup.\n\nfiles from VAX system located at the offthen transferring to the SR&QA facilities.\n\nCurrently, there is no direct transfer from the off-site tester to SR&QA facilities.\nThe downloading procedures will transfer the computer-automated\ntest files. However, there is information\nthat is provided by the testers manually, such as the failure reports (TRs, DRs) and the testing\nresource data. Therefore, this section will also discuss the hard copy data acquisition.\n(1)\n\nDownload files from tester directory\nware Development\n(ASD) VAX.\n\non the VAX to your directory\n\non the Avionics\n\nSoft-\n\na)\nb)\nc)\nd)\n\nCopy all files into the designated\ntester\'s specified VAX account\nMake a subdirectory\nusing the current day\'s date\nMove these files into the above subdirectory\nRename all files before moving them over to the ASD VAX (ASD VAX will save only\nfive versions of a given file)\n\ne)\nf)\n\nCopy all files to the analyst\'s ASD VAX account\nLog on to the analyst\'s ASD VAX account and verify that all files were transferred\ncorrectly (you should get a message about which files were not transferred;\njust copy\nthose files again)\n\n51\n\ng)\nh)\ni)\nj)\n(2)\n\nCreatea newsubdirectoryusingthe current day\'s date\nMovethesefiles intothe new subdirectory\nDeletefiles that wereon the rootdirectory\nGo backto the taster\'sdisk and deletethe .log files, and oldfiles fromthe directory.\nFirst purge,then deleteany remainingfiles, exceptthe .comfile andthe .datfile.\n\nDownload files from ASD VAX to UNIX system.\na) Log on to the UNIX system\nb) Remote log on to NASA VAX system\nc) Create a directory in UNIX (directory name should\nd) Change to current directory\ne) Transfer files\n\nbe the current\n\ndate)\n\nf)\nEnter VAX ID and password\ng) Quit\nh) Verify that all files were transferred\nI) Rename all files\n(3)\n\nDownload\na)\nb)\nc)\nd)\ne)\nf)\ng)\n\n(4)\n\nfiles from UNIX system\n\nto IBM floppy\n\ndisk.\n\nInsert IBM diskette to disk drive\nLog on to UNIX\nChange to the directory that you want to copy the files from\nCopy all files to a directory\nVerify that all files are copied\nEject the diskette\nLog off\n\nHard copy data.\n\nIf the data is logged by testers manually on paper, check the data for any entry errors, missing\ndata, or questionable\nentries. The tester should be contacted for any missing or questionable\ndata.\nStep 2: Input raw data into the database.\nNote: In the case of JSC SR&QA\'s practice, a database named SWREL_DB\nwith instructions\nis set up on the LAN in Shared Directory using R:BASE for DOS. The SMERFS, SRE, and\nfailure reports are already built in to this database.\n\n2.4.3\n\nData Analysis\n\nThe main purposes\n\nProcedure\nof evaluating the raw data are to:\n\nDetermine\n\nthe CPU time that the computer\n\nDetermine\ncase.\n\nthe exact CPU time when a failure\n\nEliminate\nproblems\n\nspends\n\nexecuting\n\nis triggered\n\ntest cases.\n\nduring the execution\n\nof a test\n\nall duplicate executions of a particular test case due to non-software\nfailures\nsuch as hardware failures, communication\nproblems, or test script re-writes.\n\n52\n\nIdentify and eliminate failures that are dependent on previous\nent failures are applicable for software reliability models.\n\nfailures.\n\nOnly independ-\n\nIdentify any differences in the testing machine CPU speeds and normalize the execution time with respect to the slowest CPU. Normalization\ncan be done by multiplying\nexecution time of a test run by a conversion factor that is determined by comparison\nof\nthe speeds of the different processors.\nThe data analysis process\nware functions, structures,\nThe following\navailable.\n\n2.4.3.1\n\nprocedures\n\nAnalysis\n\ncould be very time-consuming.\nA good understanding\nand test cases will help to ease the process.\nare based on the assumption\n\nthat all the recommended\n\nof the soft-\n\ndata are\n\nof Test Plan and Test Procedure\n\nReview the test plan to ensure that the data collection\ncollection procedure will allow access to all necessary\n\nprocedure\ndata.\n\nis included,\n\nand that the\n\nReview the test procedure to identify the objective of each individual test case. Examine test\nscripts to identify the expected output. Description of the test cases and test scripts will help\nthe analyst identify any failure that occurs during the test, and to verify if any test data is\nmissing.\n\n2.4.3.2\n\nTest Data Analysis\n\nStep 1: Examine the computer automated test log (i.e. Error History Logs) to determine\nstart time, end time, and failure time of the software test run.\nStep 2: Determine\nStep 3: Identify\nfailures.\n\nany duplicate\n\nsoftware\n\nfailures\n\nrun in the computer\nin the computer\n\nthe\n\nlog and discard.\n\nlogs (if possible).\n\nDiscard\n\nhardware-related\n\nStep 4: Determine and discard any duplicate or dependent\nsoftware failures.\nKeep a list of\nthe assumptions\nused for interpreting\na dependent failure to be consistent throughout\nthe\nanalysis\nStep 5: Examine tester\'s log to identify any test cases that don\'t appear in the computer log,\nand to identify any software failure that was logged by testers but were not identified in the\ncomputer logs.\nStep 6: Update the database.\n\n53\n\n2.4.3.3\n\nFailure\n\nReports\n\nAnalysis\n\nThe TRs or DRs are usually submitted by testers or developers\nto report significant problems\nafter the test run. We recommend cross referencing\nbetween the TRs or DRs and the automated computer logs to enhance the data analysis process.\nStep 1 : Set up criteria\n\nto categorize\n\nfailure\n\ncriticality.\n\nStep 2: Determine\n\nif the failure\n\nis a software-related\n\nStep 3: Determine\n\nif the failure\n\nis independent\n\nStep 4: Categorize\n\nfailure\n\nbased on criticality\n\nfailure.\n\nof previous\n\nfailures.\n\nlevel.\n\nStep 5: Cross reference with failures identified in computer log and tester\'s log and determine\nany discrepancy.\nCommunicate\nwith testers or developer to clarify any discrepancies\nfound.\n\n2.4.3.4\n\nExample\n\nof Data Analysis\n\nThe following data was collected from a software test project.\nThe data was automatically\nlogged by computer in an ACII file every time a software test run executed.\nThe data includes\na tag which indicates the time when a certain event occurred, such as test start time, failure\noccurrence time, and test end time.\n07:39:12.29\nINTEGRATED\n\nINFORMATIVE\n\n07:44:52.60\nWARNING\nAddress_Conversion.\nConstraint_Error!\n\nMessage\n\nfrom\n\nSES\n\nParser\n\n::>\n\nMessage\nfrom\nDkmc. BUFFER_SERVICES\nProcess_Not_Located\nexception\n\n07:44:52.63\nWARNING\nMessage\nfrom\n07:44:52.63\nWARNING\nMessage\nfrom\nCommand\nSddu#=\n5 Log_Id=\n1\n07:44:52.64\nINFORMATIVE\nMessage\n=> END_BLOCK\n07:44:52.64\nINFORMATIVE\nMessage\nmand\ncompleted.\n\nDKMC_CP/IF\nDKMC_CP/IF\n\n==>\n:=>\n\nTest\n\nConfiguration\n\n==>\nPATCH\noccurred!\n\nException\nException\n\nfrom\n\nDKMC_CP/IF\n\n==>\n\nfrom\n\nDkmc.TEST_CMD_EXEC\n\nCommand\nRaising\n\nOccurred!!\noccurred\n\nReceived\n==>\n\nCMPR\n\nis\n\non\n\nPATCH\n\nCommand\n\nEND_BLOCK\n\nCom-\n\nPreliminary analysis of the data above indicates that the software run starts at 07:39:12.29.\nAt\n07:44:52.60\na software failure is observed.\nAll other failures reported afterward appeared to\nbe dependent on the first reported failure. Thus we have only one software failure in this case.\nThe software test run was terminated at 07:44:52.64.\n\n2,5\n\nModeling\n\nProcedure\n\nand Analysis\n\nOnce all the data has been collected and analyzed for applicability,\nspecific models need to be\nchosen to estimate and predict reliability.\nThe selection criteria used to choose a model are\nthe data input, the nature of the software, and the test level needed to comply with the various\nassumptions\nfound within each model. The current tools available are the SMERFS tool and\nthe SRE Toolkit.\nBoth tools have specific models that may be used. The SRE model assumptions are listed in section 2.3.2, while the various models assumptions\nin SMERFS are described\n\nin the SMERFS\n\nUser\'s Guide [13].\n\nThe SMERFS\n\n54\n\ntool also lists the model assumptions\n\non screen and in the output ASCII file. Once the model assumptions\nare satisfied and the tool\nchosen, proceed to convert the data stored in the database to input files that the tools will\naccept. The following sections describe the procedure of database conversion and model\nplotting.\n\n2.5.1\nThe\nThe\ndata\nbase\n\nSMERFS\nSMERFS tool runs on an IBM PC or compatible systems.\nDOS 3.0 or above is required.\nprocedures\noutlined below assume that R:BASE was used as the repository of the failure\ndescribed in section 2.4.0. The procedures can easily be adapted for use with any datadesired.\n\n2.5.1.1\n\nData Conversion\n\nSMERFS has two data input options.\nManual input should be used for small amounts of failure\ndata (20 lines or less). If the failure data collected is greater than 20 lines, however, then the\nfile insertion method should be used. Data file editing is necessary before SMERFS can read\nthe data produced by the database.\nTo transfer the data into the SMERFS\nfollowing:\nStep 1: R:BASE\n\nprogram\n\nwithout resorting to manual\n\ninput, do the\n\nOutput\n\nThere exists a report type within the R:BASE file SWRDB.DBF\n(this database can be obtained\nfrom the Analysis & Risk Assessment\ngroup) that is called smerep. While in the command line\nmode within R:BASE, direct the output from the screen to a file using the following set of\ncommands:\nR>out filename.dat\nR>print smerep sorted by option1\nR>out screen\n\n(redirect output to file)\n(output sorted report)\n(redirect output to screen)\n\noption2\n\nwhere:\nfilename.dat is the output file name that R:BASE will write the sorted failure data to. The output file contains headings and data pertaining to time between failures, accumulated\nnonfailure time, and start and stop times.\nSmerep\n\nis the report used to create the sorted failure data file (filename.dat).\n\noption1\n\nand option2\n\ntimes\n\nare R:BASE\n\n(database)\n\ndefined\n\ncolumns\n\nthat contain the dates and CPU\n\nof all the test jobs.\n\nThe following\nautomatically\n\nexample shows the contents\nmade by smerep).\n\nof the R:BASE\n\n55\n\nsmerep report output (this file is\n\nSMERFS\n\nREPORT\n\nFOR XXXXXX\n\nDate OX/27/9X\nPage\nF#\n\n-0-0-0-0-0-0-\n\nTBE2 TBE3TBE4TBE5 TBE6TBE7TBE8\n\nTBE1\n\n-0-0-0-0-0-0-\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\nET_ST\n\n1\n\nTBm TBD2TBD3TBD4TBD5TBD6TBD7TBD8\n\n252.695847\n169.289017\n32.4508667\n31.5410614\n31.6234589\n31.1565399\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\nWhere:\nF#: The number\nTBEx:\n\nof failures\n\nThe time between\n\naccumulated\nthe previous\n\nfor one job.\nfailure\n\nor start of job, and the current\n\nfailure.\n\n(Note:\n\nThe SMERFS program uses the term TBE to represent time between failures, also\ncommonly\nreferred to as TBF. When referring to the SMERFS program only, these are\ninterchangeable.\nIn other cases, TBE and TBF have different definitions.)\nET_ST:\nTBDx:\n\nThe time between\nThe time between\n\nStep 2:\n\nthe end and the start of the job.\nthe end of the job and the current failure.\n\nEditing\n\nOnce the R:BASE output data file (filename.dat)\nhas been created, delete any heading and\nblank lines within this file (this is done because the conversion program discussed in step 3 will\nhalt if any of these are present).\nIn addition, any special characters or printer commands\nneed\nto be removed.\nThe next example shows the R:BASE smerep Report once the editing has\nbeen accomplished.\n-0-0-0-0-0-0-\n\n-0-0-0-0-0-0-\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\nStep 3: Basic Conversion\nThis program\n\nwill convert\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n252.695847\n169.289017\n32.4508667\n31.5410614\n31.6234589\n31.1565399\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\n0.\n0.\n0.\n0.\n0.\n0.\n\nProgram\nan R:BASE\n\nsorted failure\n\nfile into a SMERFS\n\ninput file.\n\nNote:\n\nthe file\n\ncontains only data (no headings or blank lines are acceptable--see\nstep 2). The conversion\nprogram (SMERFS.BAS)\nis written in GWBASIC format.\nIts input is the edited file output of\nstep 2 of this section.\nComments and instructions are included in the file SMERFS.BAS.\nRun SMERFS.BAS\nDOS 5.O).\n\nfrom any machine\n\nthat has GWBASIC\n\n56\n\nloaded\n\n(generally\n\nall machines\n\nwith\n\nSMERFS.BASwill ask for the editedfile output namefrom step2 of this section. The program\nwill echothe inputfile. It will thenask for the output file namesfor the time betweenfailures\nand intervalfiles (they maynot bethe same). The last input askedwill be the intervaltime in\nhours. This intervaltime will be usedto partitionthe total executiontime into equivalent\nlengthsof the desiredtime (as usedby the intervaland count lengthsmodels).\nExecutionis automaticafterthe intervaltime in hoursis enteredby the user.\nThe SMERFS.BASprogramwill outputtwo user-specified\nfiles. One will be usedwith the CPU\nTBF models,andthe otherwith the intervalmodels. Theseoutputfiles will be usedas the\ninputfiles to the SMERFSmodelingapplication.\n\n2.5.1.2\n\nSMERFS\n\nModeling\n\nProcedure\n\nThis section will describe the actual input, execution, and output procedures for the SMERFS\napplication.\nFurther information may be obtained from the SMERFS manual (see reference\n[13]).\n\n2.5.1.2.1\nSMERFS\n\nSMERFS\n\nInputs\n\nallows a maximum of 1000 data point entries for each of the following\n\n1) Wall Clock\nThis data type\n13.39583),\nthe\noccurred within\n\ndata types:\n\nData\nconsists of the starting and ending times of testing (24-hour units - 13:23:45 =\nnumber of failures for that session, and the time of any software failures that\nthat session (24-hour units).\n\n2) CPU Data\nThis data type consists of the expended CPU time (in seconds only) and a flag of 0 or 1. A\nflag of 1 indicates that the testing session was terminated with a failure; a 0 indicates that a\nfailure was not detected during the testing session.\nThe time between failures input file, as shown in the example\nentries that will contain time between failures data, the failure\nfailures data entry, and the time between failures (reported in\nthat will be included is reflected as the first number of the first\n\nbelow, contains the number of\nstate of the last time between\nseconds).\nThe number of entries\nline in the table. The failure\n\nstate flag is reflected as the second number of the first line in the table. This flag indicates\nwhether the last entry in the table is a failure (1) or additional testing time that had no failure\n(0). The entries found within the rest of the input file reflects the CPU failure execution time\nbetween failures collected (in line 2 of the example below - 15 seconds elapsed before the first\nfailure occurred).\nSMERFS requires that all CPU data input files contain 1000 data entries.\nTherefore,\nit is necessary to fill the unused entries with zeros. The example shows only the\nfirst 15 lines of the SMERFS CPU data file (total of 1001 lines exist in the actual file).\n\n57\n\n15\n1\n.15000000E+02\n.23000000E+02\n.20000000E+02\n.28000000E+02\n.35000000E+02\n.37000000E+02\n.43000000E+02\n.52000000E+02\n.49000000E+02\n.58000000E+02\n.67000000E+02\n.62000000E+02\n.76000000E+02\n.82000000E+02\n.10100000E+03\n\n3) Wall Clock and CPU Data\nThis data type consists of the starting and ending times of testing (in 24-hour units) and the\nnumber of failures detected in that testing session.\nIf any software failures were detected in\nthat session, SMERFS prompts for the failure time (in seconds) for each failure.\nIf the last\nfailure occurred before the end of the testing session, SMERFS prompts for the expended\nCPU time between the last failure occurrence\nand the end of the testing session.\nIf no software failures were detected in the session, SMERFS prompts for the entire CPU time\nexpenditure for the session.\n4) Interval Data\nThis data type consists of the number of failures detected in a testing interval and the associated testing length of that interval. An interval is defined as a user-selected\nlength of time that\nequally divides the total testing time of the software.\nThe use of interval data is beneficial if\nexact failure times are not available (total testing time is still critical).\nThe interval input file to SMERFS, as shown in the next example, contains the number of entries that will be used as intervals for modeling, the interval data, and the interval length. The\nfirst number on the first line depicts the number of entries in the file that will contain data. The\nsecond number of the first line is not used. The next 1000 entries (only the first 15 entries\nbefore the dashed line are shown on the example) will contain failure data as found within\neach respective interval (the example below shows that there are 15 intervals that correspond\nto the first 15 entries that contain failure data). The data in each entry represents the number\nof failures found within that particular interval. The next 1000 entries (only the first 15 entries\nafter the dashed line are shown on the example) contain the length of each interval (this is by\ndefault 1 for SMERFS).\nThe total length of this file should be 2001 lines (1 for the first two\nnumbers, 1000 for entry data, and 1000 for interval lengths of the first 1000 entries).\n\n58\n\n15\n0\n\xe2\x80\xa235000000E+02\n\xe2\x80\xa238000000E+02\n\xe2\x80\xa227000000E+02\n.21000000E+02\n\xe2\x80\xa218000000E+02\n\xe2\x80\xa219000000E+02\n\xe2\x80\xa214000000\nE+02\n.20000000E+02\n.90000000E+01\n.40000000E+01\n.00000000E+00\n.20000000E+01\n\xe2\x80\xa210000000E+01\n.00000000E+00\n.20000000E+01\n\xe2\x80\xa210000000E+01\n.10000000E+01\n.10000000E+01\n\xe2\x80\xa210000000E+01\n\xe2\x80\xa210000000E+01\n.10000000E+01\n\xe2\x80\xa210000000E+01\n\xe2\x80\xa210000000E+01\n\xe2\x80\xa210000000 E+01\n\xe2\x80\xa210000000E+01\n.10000000E+01\n.10000000E+01\n\xe2\x80\xa210000000E+01\n.10000000E+01\n\xe2\x80\xa210000000E+01\nThis handbook\n\n2.5.1.2.2\n\nwill only cover CPU and interval\n\ndata procedures.\n\nOutput\n\nAn optional user-selected\nhistory file (see Appendix B for an example) that saves all screen\noutput for later review contains all the user interactions\nwith the SMERFS application as seen\non the screen. A plot file (see Appendix B for an example) that stores all graphical output data\nas text is also available if selected by the user\xe2\x80\xa2 This file contains all of the graphical output\nprovided by SMERFS in text form. This file may be used to create plots of the data using other\napplications\n(such as Lotus 1-2-3, Harvard Graphics, and Microsoft EXCEL)\xe2\x80\xa2\n\n2.5.1.2.3\n\nExecution\n\nProcedure\n\nThis section contains a stepwise description of a typical SMERFS modeling session\xe2\x80\xa2 There is\nan optional history file that saves all screen output for later review. A plot file is also optional\xe2\x80\xa2\n\n59\n\nThe plot file will save allthe SMERFS-createdplotdataonto a textfile. An examplesession\ncan be found withinthe SMERFSUser\'sGuide[13].\nStep 1: Checkthat all of the SMERFSexecutablesare within the same directoryas the data\nfiles are.\nStep2: Preparethe SMERFSinputfile.\nStep3: Run SMERFSby typing"SMERFSIV"at the prompt.\nStep4:\n\nProvide file names for the history and plot files as applicable.\n\nAn option exists for an optional history file that saves all screen output of the SMERFS\nfor later review. A plot file that stores all graphical output data as text is also available.\nStep 5: Enter "<smf4rdc.lis>"\nwhen prompted\nsumptions and data requirements\nfile.\n\nwith the file name of the SMERFS\n\nsession\n\nmodel as-\n\nNote: The file <smf4rdc.lis> contains all the model assumptions\nin text form. These are exclusively used by SMERFS and should not be altered. This file is accessed by SMERFS\nwhenever there is a user request for a listing of assumptions.\nStep 6:\n\nEnter the appropriate\n\nStep 7: Choose\n\ndata type selection\n\nfile or keyboard\n\nat the prompt.\n\ninput at the next prompt.\n\nSMERFS allows for two methods of data input. Data may be entered manually by keyboard or\nautomatically\nthrough file insertion.\nThe input files may be created by the SMERFS editor\nwhen the keyboard option is chosen.\nInput files created with another utility can be used if such\nfiles mimic the structure of the SMERFS created files (see 2.5.1.2).\nStep 8: Enter a "1" or a "0" to list or skip the listing at the list prompt.\nStep 9: Enter a "1" at the next prompt\nenter a "0."\n\nonly if a keyboard\n\nStep 10:\n\nEnter a "0" to view a list of the main module\n\nStep 11:\n\nEnter the desired\n\noption\n\n1 - data needs\n\noptions,\n\nfrom the main module\n\nat step 7, else,\n\nor skip to step 11.\n\noptions.\n\nto be input\n\n2 - data editing\n\ninput was chosen\n\nis needed\n\nThe edit subroutine permits altering of the software failure data. All data updates\nshould be performed using the same units of measurements.\nEditing choices are:\n- Change specified element\n- Delete specified element(s)\n- Insert up to ten elements\n\n60\n\n- Combine two or more adjacent elements\n- Change the Time Between Failure (TBE) flag\n- List the current data\n3 - data transformations\n\nare needed\n\nThis option permits the scaling of software failure data entries.\nFive types of transformations are allowed along with the options of restoring the data entries to their original\nstate. This transformation\noperates on one data entry at a time. Two time between error entries cannot be transformed\ntogether, nor can the interval counts and lengths.\nThe transformed\ndata is held locally. If any change is desired, it must be done in the\nedit module.\nThe purpose of this subroutine is to transform the data by multiplication\nor addition into\na refined and transformed\nset of numbers.\nThe data will still contain the basic characteristics of the original data. The difference,\nhowever, will be in the presentation of the\ndata (in graphical form). The output data may appear smoothed, expanded, multiplied,\nand scaled. See the SMERFS Users Guide [13] for further information on execution\nflow.\n4 - data statistics\n\nare required\n\nThis option provides general summary statistics of the software\nSMERFS User\'s Guide [13] for explanation\nof the terms.\n\nfailure\n\ndata.\n\nRefer to\n\n5 - create plots of the raw data\nThis option generates plots of the software failure data. The raw data are the original\ninputs of the software failure data. This module will also provide smoothing if the sample size is greater than 6 failure data entries.\n6 - module\n\nexecution\n\nThis option\n7 - analyses\n\nallows the execution\n\nof the models.\n\nof the model fit\n\nThis option allows the plotting of the raw and predicted plots for the last model executed. The model must be run, and the future values from the prediction stored in\nmemory so that they may be plotted.\nPlots may be directed to an outside file when\nquitting this module. This module also provides residual plots. A residual plot is the\ndifference between the predicted and "raw" data. The plot may be used to determine\nthe validity of the data. A goodness-of-fit\noption is also available.\nThis option provides\nthe chi-square\ngoodness-of-fit\nstatistic for the predicted\ninterval failure counts only.\n8 - termination\nStep 12: Repeat\n\nof SMERFS\nstep 11 as necessary.\n\nStep 13: End session\n\nby choosing\n\noption 8 from the main SMERFS\n\n61\n\nmenu.\n\n2.5.1.3\n\nResults\n\nInterpretation\n\nThis section will discuss the estimation plot results of the Littlewood & Verral\nand the Schneidewind\nModel. Other models will be added in the future.\nLittlewood\n\nand Verral\n\nBayesian\n\nBayesian\n\nModel\n\nModel\n\nFigure 6 shows the estimation failure results obtained from properly developed and debugged\nsoftware code (this code is referred to as "mature").\nThe chart shows the actual and fitted\nestimation failure data. The actual and estimated data should approximate\nan asymptote\nparallel to the Y axis on some X axis point (failure). As the estimated curve reaches the asymptote, the time between failures (as read from the Y axis) is increasing with each consecutive failure.\nThe degree to which this convergence\noccurs also points to code that shows a\ndecrease in the failure rate (defined as the inverse of the time between failures).\n\n0\n\n................................................................................................................................................................................................................................................\n\ni\n\ni\ni\n\nO\nO\nO\n\ni\n\n40 .......................................\n\nv\n\n._\n/\n\n(f)\n./\n0\nc,) 30\n(I)\nO3\n\nE\n\n20\n\n...............................................\n\n\'_//J_" .... ,\n\n.\n\n/_"/\n\n................................................\n\n0\n\n./_\'///\n\n10 ...............................\n\n..;_\'_\'_"\n.............\n.._,\'B_\n\nx\nI.U\n\n,\n\nI/1\n\n0 _..,,.._: ..............\n.\n_ ..................................................................................................\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\nFailures\n" Actual\n\n\'_} Estimated\n\nFigure 6 - Littlewood & Verral failure\n\n62\n\nestimation.\n\n13\n\n14\n\n15\n\nIf the estimated failure data curve shows a linear or does not converge on an asymptote (as\nshown in figure 7) then there exist problems with the test data gathered, a violation of the\nmodel assumptions,\nor the software under test is not mature. If the estimation failure curve is\nlinear (as shown as Estimated-1\non figure 7), the failure rate is constant with time (the time\nbetween failures is relatively equal from failure to failure).\nThis case shows that more testing\nand debugging\n\nare needed\n\nbefore this code shows a decreasing\n\nfailure\n\nrate.\n\nIn the case of non-convergence\n(shown as Estimated-2\non figure 7), the failure rate is increasing with time (the time between failures is decreasing with each new failure).\nIn this case, new\nfailures are being introduced\ninto the code. This can occur if new software is added, the testing is not truly random, or the debugging is introducing failures.\n\n700\nE)\nO\nO\n\n"EP\nr\nO\nO\n\n6oo;\n\n..............................\n\n_:_-_f_\n\n5oo ..........................\n\n./_._._-_\'_\n\n4UU I ........................\nE\nE\n\n; " _ _"_\n\n_oo_ .......... .,\n\n" .......\n\n" Jl\n\ni-;_ --=\n.........\n\n_;:::;.\na-_\'-_........\n.\n\nL\n\np-\n\n.o\n\n2oo\n\nt_\n\n_ool ,,S \'\'_\n....\n.....................................\n\nx\nLU\n\n..........\n\n,_-\n\n5\n\n............................\n\n10\n\n15\n\n20\n\n25\n\nFailures\n\nl\n\n" Actua_\n\n_\n\nEstimated-1\n\nFigure 7 - Littlewood\n\n-m- Estimated-2\n\n& Verral failure\n\n63\n\nestimation.\n\ni\n\n30\n\ni\n35\n\nSchneidewind\n\nModel\n\nFigure 8 shows the estimation failure results obtained from properly developed and debugged\nsoftware code (this code is referred to as "mature").\nThe chart shows the actual and fitted\nestimation failure data. The actual and estimated data should converge on an asymptote\nparallel to the X axis on some Y axis point (failure number).\nAs the estimated curve reaches\nthe asymptote, the time between failures (as read from the X axis) is increasing with each\nconsecutive failure. The degree to which this convergence\noccurs also points to code that\nshows a decrease in the failure rate (defined as the inverse of the time between failures).\n\n6\n\n.................................................................................................................................................................................................................................\n\n12 ..........\n\ni\n\n;. \xe2\x80\xa2....\n\n10 .......\n\n.._\n--\n\n.................................\n\n/\n\nLL\n\n8\n\n"-._/\n/\n\ni\n\n/\n\nE\ni/\n\no\n\ni /\n\n4_\n\n.......................\n\n1\n\n2\n\n3\n\n_\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10 11 12 13 14 15 16 17 18 19 20 21\nlntervaas\n\n" Actual\n\nFigure 8 - Schneidewind\n\nMethod\n\n_\n\nEstimated\n\n1 cumulative\n\n64\n\nand predicted\n\nfailures\n\n- 1.\n\nIf the data is linear or does not converge\n\non an asymptote\n\ncould be problems with the test data gathered,\nware under test is not mature.\nIf the estimation\n\n(as shown\n\nin figure 9) then there\n\nviolation of the model assumptions,\nor the softfailure curve is linear (as shown as Estimated-\n\n1 on figure 9), the failure rate is constant with time (the time between failures is constant).\nThis\ncase shows that more testing and debugging are needed before this code shows a decreasing\nfailure rate.\n\n25\n\n2O\n\xc2\xa2t)\n\n,m\n\n15\n\nLL\n>\n\xc2\xb0_\n\nE\ni\n5\n\n...................................\n\n10\n\ni\n\n20\n\n30\n\n40\n\n50\n\nIntervals\n\nActual\n\nFigure\n\n9 - Schneidewind\n\nX\n\nEstimated-1\n\nMethod\n\n_\n\nEstimated-2\n\n1 cumulative\n\nand predicted\n\nfailures\n\n- 2.\n\nIn the case of non-convergence\n(shown as Estimated-2\non figure 9), the failure rate is increasing with time (the time between failures is decreasing).\nIn this case, new failures are being\nintroduced into the code. This can occur if new software is added, the testing is not truly random, or the debugging\n\nis introducing\n\nfailures.\n\n65\n\n2,5.2\n\nSRE\n\n2.5.2.1\n\nData Conversion\n\nTo transfer\n\nthe data into the SRE Toolkit\n\nStep 1: R:BASE\n\nwithout\n\nresorting\n\nto manual input:\n\nOutput\n\nThere exists a report type within the R:BASE file SWRDB.DBF\n(this database can be obtained\nfrom the Analysis & Risk Assessment\ngroup) that is called srerep. While in the command line\nmode within R:BASE, direct the output from the screen to a file using the following set of\ncommands:\nR>out filename.dat\nR>print srerep\nR>out screen\n\n(redirect output to file)\n(output sorted report)\n(redirect output to screen)\n\nsorted by option I option2\n\nwhere:\nfilename.dat\n\nis the output file name that R:BASE\n\nput file contains headings and data pertaining\nfailure time, start, and stop times.\nSmerep\n\nwill write the sorted failure\n\nto time-between-failures,\n\nis the report used to create the sorted failure\n\noption1 and option2 are R:BASE\ntimes of all the test jobs.\nThe following\nautomatically\n\n(database)\n\nexample shows the contents\nmade by smerep).\n\ndefined\n\nnon-\n\ndata file (filename.dat).\ncolumns\n\nof the R:BASE\n\nSRE REPORT\n\ndata to. The out-\n\naccumulated\n\nthat contain\n\nsmerep\n\nthe dates and CPU\n\nreport output (this file is\n\nFOR XXXXX\n\nDate XX/XX/XX\nPage\nTestdate\n\nStarttime\n\nEndtime\n\nFail#\n\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n\n14.62694\n14.73194\n14.82111\n14.84306\n14.86667\n14.88722\n14.91361\n14.94028\n14.96528\n16.01583\n16.1525\n16.2625\n16.28611\n\n14.68184\n14.77539\n14.82953\n14.85128\n14.87463\n14.89568\n14.92176\n14.94863\n14.96546\n16.07115\n16.19469\n16.27051\n16.29364\n\n-0-0-0-0-0-0-0-01\n-0-0-0-0-\n\nFT1\n-0-0-0-0-0-0-0-014.96546\n-0-0-0-0-\n\n66\n\n1\n\nFT2 FT3 FT4 FT5 FT6 FT7 FT8 FT9 F10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n930802\n\n16.30611\n\n930802\n930802\n930802\n930802\n930803\n930803\n930803\n\n16.32667\n16.35\n16.37306\n16.63139\n8.890833\n9.225\n9.261666\n\n16.3138\n16.33449\n16.35788\n16.38095\n16.67976\n8.894263\n9.288181\n9.314458\n\n-0-0-0-0-0-0-0-0-\n\n-0-0-0-0-0-0-0-0-\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\nWhere:\nTestdate:\n\nThe date tag of the job specified\n\nstartime:\n\nThe start time of the job in hours.\n\nEndtime:\n\nThe end time of the job in hours.\n\nFail#:\n\nThe number\n\nof failures\n\nas year, month,\n\ndate.\n\nin the job.\n\nFT1 -FTIO: The actual failure times for each failure.\nStep 2: Editing\nOnce the R:BASE\n\noutput data file (filename.dat)\n\nhas been created,\n\ndelete any heading\n\nand\n\nblank lines within this file (this is done because the conversion\nprogram discussed in step 3 will\nhalt if any of these are present).\nIn addition, any special characters or printer commands\nneed\nto be removed.\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930802\n930803\n930803\n930803\n\n14.62694\n14.73194\n14.82111\n14.84306\n14.86667\n14.88722\n14.91361\n14.94028\n14.96528\n16.01583\n16.1525\n16.2625\n16.28611\n16.30611\n16.32667\n16.35\n16.37306\n16.63139\n8.890833\n9.225\n9.261666\n\n14.68184\n14.77539\n14.82953\n14.85128\n14.87463\n14.89568\n14.92176\n14.94863\n14.96546\n16.07115\n16.19469\n16.27051\n16.29364\n16.3138\n16.33449\n16.35788\n16.38095\n16.67976\n8.894263\n9.288181\n9.314458\n\n-0-0-0-0-0-0-0-01\n-0-0-0-0-0-0-0-0-0-0-0-0-\n\n67\n\n-0-0-0-0-0-0-0-014.96546\n-0-0-0-0-0-0-0-0-0-0-0-0-\n\n0 0 0 0\n0 0 0 0\n0 0 0 0\n0 0 0 0\n0 0 0 0\n0 0 0 0\n0 0 0 0\n0 0 0 0\n0 0 0 0\n000000000\n000000000\n000000000\n000000000\n000000000\n000000000\n000000000\n000000000\n000000000\n000000000\n000000000\n000000000\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\nStep 3: Basic ConversionProgram\nThis program\n\nwill convert\n\nan R:BASE\n\nsorted failure\n\nfile into an SRE Toolkit\n\ninput file.\n\nNote:\n\nthe file contains only data (no headings or blank lines are acceptable--see\nstep 2). The conversion program (SRE.BAS) is written in GWBASIC format.\nIts input is the edited file output of\nstep 2 of this section. Comments and instructions are included in the file SRE.BAS.\nRun SRE.BAS\n\nfrom any machine that has GWBASIC\n\nloaded (generally\n\nall machines\n\nwith DOS\n\n5.0).\nInput to the SRE.BAS\n\nis the edited output file. The program will echo the file.\n\nIt will then ask\n\nfor the output file name to direct the output.\nExecution\n\nis automatic after the output file name is entered by the user.\n\nThe SRE.BAS program will output one user-specified\nthe SRE Toolkit.\n\n2.5.2.2\n\nSRE Modeling\n\nfile. This output file is the input file for\n\nProcedure\n\nSRE Toolkit runs on an IBM PC or compatible.\nDOS 3.0 or above is needed when SRE Toolkit\nruns in IBM PC or compatible.\nModify the autoexec.bat\nfile to have a direct path toward the\nSRETOOLS\ndirectory.\n\n2.5.2.2.1\n\nInput\n\nTo run the SRE tool, two input files are required: the .ft file which contains the execution time\nand the failure time, and the .fp file which contains the parameters which specify reliability\nobjective, type of model used, plot options, and labor and computer resource parameters.\nThe\ntwo files are described separately in the following sections.\n\n.ft file\nFrom the database,\nfollowing format:\nA dd.dddd\n\ngenerate\n\na text file with extension\n\n.ft, using any text editor,\n\nYYMMDD\n\nwhere:\nA is an alphabet\n\nwhich represents\n\neither "S", "F", or "E" as follows:\n\nS: time the test case execution starts\nF: time a software failure occurs\nE: time the test case execution ends\n\n68\n\nwith the\n\ndd.ddd\n\nis the military\n\nYYMMDD\n\ntime of the day when the event (Start, Fail, or End) occurs.\n\nis year, month,\n\nHere is an example\n\nand date when the test case is run.\n\nof execution\n\ntime data as input file to the SRE:\nS\nF\nF\nF\nF\nF\nE\n\n12.13000\n12.16433\n12.18968\n12.19858\n12.36725\n12.36825\n12.36925\n\n920827\n920827\n920827\n920827\n920827\n920827\n920827\n\nS 12.58194\nE 12.82595\n\n920828\n920828\n\nIS 9.88194 920829\nF 9.89000 920829\nE 10.15405 920829\nS 10.28222\nE 10.52841\n\n920829\n920829\n\n.fp file\nThis file is used in conjunction with the .ft file. Both files should be stored in the same directory. The .fp file is used to indicate what option the user wishes to select for the modeling,\nsuch as model type (logarithmic,\nexponential),\nfailure intensity objective, computer resource,\nlabor resource, and type of output plots to be displayed on screen.\nDescriptions\nand ranges,\n\nof the parameters\n\nused for the .fp file, parameter\n\nare listed in table 10.\n\n69\n\nnames,\n\ndescriptions,\n\ndata types,\n\nTable10 Parameter\nName\nmodel\n\nDescription\n\nfor the .fp File\n\nType\n\n0 - exponential\n\nValue\nRange\n\ninteger\n\n0- 1\n\ninteger\n\n1 -logarithmic\ndefault value\ngrp data\n\nList of Parameters\n\n0-1\n\nstring\n\nnone\n\n= 0\n\ngroup data\n0 - failure time analysis\n1 - group data analysis\ndefault value = 0\n\ntU\ncompf\n\ntitle for output\n\nreport\nfactor\n\nreal\n\nobjective\n\nreal\n\ntest compressions\n\n0.1-99999\n\ndefault value = 1\nlambf\n\nfailure\n\nadjflg\n\nadjustment\n\nintensity\n\n0 - 999\n\ninteger\n\n0-1\n\ninteger\n\n0-1\n\ninteger\n\n0-4\n\nstring\n\nflat\n\nnone\n\n0 - no adjustment\n1 - failure time adjustment\ndefault value = 0\ngenplt\n\ngenerate\n\nplot commands\n\n0 - no plot\n1 - yes\ndefault value = 0\nconlvl\n\nconfidence\n\nlevel\n\n0 - no confidence\n1 - 95% conf. limit\n2 - 95% conf. limit\n3 - 75% conf. limit\n4 - 50% conf. limit\ndefault value\nplot\n\n"y versus\n\n= 3\n\nx"\n\nSRE has the capability to generate many different plots, depending on what plots the user\nwishes to obtain. To obtain a plot from an SRE run, the user needs to insert a line in the .fp\nfile which has the following format:\nplot="__Y_Y versus\nX "\nwhere Y and X are defined\n\nas follows:\n\n70\n\nY\n\nX\nfailures\ncalendartime\nexecutiontime\npresentdata\n\nfailures\ncalendartime\nexecutiontime\ncompletiondate\ninitialintensity\npresentintensity\ntotal failures\nfailure decay rate\nadditionalfailures\nadditionalexecutiontime\nadditionalcalendartime\n\nThe formatand descriptionof all parametersusedin this file are describedbelow:\nExample:\n\nparameters\n\ndata\n\n# model type and objective parameters\nmodel = 0\nlambf = 20\n# calendar time parameters\nmuc = 4.09\nmuf = 10.0\n\nmui =5.0\n\npc =10.00\nrhoc = 1.0\nthetac =8.37\n\npi=4\nrhoi = 1.\nthetai =10.46\n\n# plot\ngenplt\nplot =\nplot =\nplot =\n\noption parameters\n= 1\n"failures vs execution time"\n"present intensity versus execution\n"present intensity vs failures"\n\n2.5.2.2.2\nOutput\n\npf = 15\nrhof = .250\nthetaf =20.93\n\nworkday = 10\nworkwk = 6\n\ntime"\n\nOutput\ndata for the SRE Toolkit\n\nThe Tabular\n\nconsist\n\nof a tabular chart and plots that are user-specified.\n\nChart Output\n\nThe tabular chart appearing on the screen can be printed on hard copy by simply applying the\nprint screen key or printing it to an ASCII file.\nAn example of the tabular chart output data is shown in figure 10:\n\n71\n\nSOFTWARERELIABILITYESTIMATION\nEXPONENTIAL\n(BASIC)MODEL\nTEST DATASET\nBASEDON SAMPLEOF\nTEST EXECUTION\nTIME IS\nFAILUREINTENSITYOBJECTIVEIS\nCURRENT DATEIN TEST\nTIME FROMSTARTOF TEST IS\n\n15TEST FAILURES\n20.9872CPU-HR\n20 FAILURES/1000-CPU-HR\n920910\n15 DAYS\n\nCONF.LIMITS\n\nMOST\n\nCONF.LIMITS\n\n95%\n\n............\n\n75%\n\n50%\n\nLIKELY\n\n50%\n\n75%\n\n90%\n\n95%\n\n15\n\nTOTAL FAILURES\n\n90%\n15\n\n15\n\n15\n\n15\n\n15\n\n15\n\n16\n\n16\n\nFAILUREINTENSITIES(FAILURES/1000-CPU-HR)\n............\n\nINITIAL\n\n2148\n\n2475\n\n3011\n\n3543\n\n4312\n\n5089\n\n5641\n\n6216\n\n6582\n\nPRESENT\n\n0.660\n\n1.04\n\n2.11\n\n4.14\n\n10.50\n\n25.84\n\n47.65\n\n87.67\n\n127.1\n\n*** ADDITIONALREQUIREMENTS MEET FAILUREINTENSITYOBJECTIVE***\nTO\nFAILURES\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\nTEST EXEC. TIME\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\nWORK DAYS\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.616\n\n2.49\n\n5.30\n\n7.87\n\nCOMPLETIONDATE\n\n920910\n\n920910\n\n920910\n\n920910\n\n920910\n\n920911\n\n920915\n\n920918\n\n920921\n\nFigure\n\nSRE\n\n.pc Output\n\nThis\n\nfile is generated\n\nrameter\n\nappropriate\nis described\n\nGenerate\n\nby the\n\nSRE\n\nset to 1 (plot=l)\n\nand\n\nSRE\n\nMusa\n\nexponential\n\ntime\n\nthe associated\n\nautomatically\n\nin the\nstatistical\n\napplication\nsoftware\nsuch\nin the following\nsection.\n\nwhen\n\n.fp file.\n\nThe\n\ndata.\n\nThis\n\nthe\n\nuser\n\n- Use\n\nresult.\n\nthe SRE\n\nwill\n\nallow\n\nas EXCEL or LOTUS\n\nusers\n\nto make\n\n1-2-3.\n\ntool\n\nnumerical\n\nThe\n\nwith the\nresults\n\nplot files,\n\nmethod\n\npa-\n\nof the\nusing\n\nof using\n\nEXCEL\n\nPlots\n\nPlots appear\n\n- Save\n\nruns\n\n.pc file will provide\n\non the screen\n\nusing\n\nthe\n\nPLOT\n\ncommand\n\nfrom\n\nthe\n\nnot generate\nplot files to print plots.\nTo get the hard copies\ndures are recommended\nusing Microsoft\nWord and Microsoft\n- Using\n\nmodel\n\nFile\n\nPLOT\n\nprediction\n\n10 - Sample\n\nMicrosoft\n\nWord\n\nthe file and\nMicrosoft\n\n- Generate\n\nplots\n\nquit\n\nto replace\nMicrosoft\n\nEXCEL to load\nusing\n\ngraphic\n\nblanks\n\nwith\n\nthe commas\n\nSRE\n\ntool.\n\nHowever,\n\nSRE\n\nin the\n\n.pc file.\n\nWord.\nup the modified\nfeatures\n\n.pc file with\n\nin EXCEL or using\n\n72\n\ncan\n\nof the plot, the following\nproceEXCEL for IBM PC or compatible.\n\na comma\nappropriate\n\nas a delimiter\nuser-created\n\nmacros.\n\n2.5.2.2.3\n\nExecution\n\nProcedures\n\nStep 1 : Create two input files in ASCII format, the filename.ft\n\nand the filename.fp,\n\nin the same\n\ndirectory.\nStep 2: Type "EST filename." The computer will prompt the tabular chart on screen which\nincludes the summary of the input data and the estimation/prediction\n\nresults.\n\nStep 3: Capture the tabular chart onto hard copy.\nStep 4: Capture the plots onto hard copies.\n\n2.5.2.3\n\nResults\n\nSRE Toolkit\nels available\nTabular\n\nproduces two types of results:\nin the Toolkit.\n\na tabular\n\nchart and plots for each of the two mod-\n\nChart\n\nMusa Exponential\n\nTime Model\n\nThis model provides the estimation of the failure intensity and the total failure of the software. In addition, the model predicts the number of additional failures needed to be detected to reach the objective failure intensity. If calendar time model parameters are\nprovided to the model, a prediction of how many calendar days needed to complete the\ntest is returned. The results are reported with confidence bounds. See figure 10 for a\nsample output.\nMusa Logarithmic\n\nTime Model\n\nThis model provides the estimation of the failure intensity and the failure intensity decay\nparameter, which indicates how fast the failure intensity will drop as the software test progresses. Similar to the Musa exponential model, the logarithmic model predicts the number\nof additional failures needed to be detected to reach the objective failure intensity and calendar time model prediction. The results are also reported with confidence bounds.\nTabular chart for this model is similar to that of the Musa exponential time model. See figure 10 for a sample output.\nSRE Plots\nTypically, three plots are generated along with the tabular chart for each model. The exponential time model and logarithmic time model results are similar.\nCumulative\n\nFailures\n\nvs, Execution\n\nTime Plot\n\nThis plot provides a quick comparison\nbetween the predicted curve and the actual curve in\nterms of cumulative\nfailures occurred during the execution period. The raw data and predicted curves should converge on an asymptote.\nThe degree of convergence\ngives a\nmeasure\n\nof maturity.\n\nSee figure\n\n11 for a sample\n\n73\n\nresult of the exponential\n\nmodel.\n\n16 i\n\n...L_\n\ni\n2\n\n_-r x\'\'--_-\n\ni\n\ni\n\ni\n\n.... _"=\'_"?t...............\n.\n\ni\n\nI0 .............. .......................\n._::"\n\xc3\xb7/."\n2\n\n8 _"\n\nLL\n\n_\n\ni\n\n.,_\n\n6 i-\n\n-J ......................................\n\n.L\ni\n\n/: ...............................\n\n/\n/\n\n4[/_ ......................................................\ni\n\n1,46\n\n2.92\n\n4,39\n\n5.85\n\n7.31\n\nExecution\n\n_\n\nFigure 11 - Sample\n\nFailure\n\nIntensity\n\nvs. Execution\n\nEstimetion\n\n8.78\n\n10,2\n\n11.7\n\n13.1\n\nTime\n\n_- Actual\n\nSRE failure\n\nvs. execution\n\ntime plot.\n\nTime Plot\n\nThis plot provides a visual aid in determining whether the software has reached the failure\nintensity objective in terms of execution time spent, See figure 12 for a sample result,\nThe sample result shows that failure intensity of the software was initially very high (10,000\nfailures per 1000 CPU hours).\nIt then gradually declined as testing progressed.\nFinally,\nafter 15 hours of CPU execution times, the software reached the failure intensity objective\nof 10 failures per thousand CPU hours. In the case of inadequate software testing, figure\n13 will show that the actual failure intensity line never reaches the failure intensity objective\n(10 failures per 1000 CPU hours), and, sometimes,\nthe line even diverges upward at the\nend of testing.\n\n74\n\nE\n\n......................................\n\n:\n\n,\n\n..............\n\n!\n\n100,000__\n\xe2\x80\xa2\n.-_.\n\n_\n\n,\'.--\n\n\', ,,_-"\n\n-\n\n-.........\n\n" -\n\ni\n\n\xe2\x80\xa2 ..........\n\n"_\n\n...........\n\n_\n\n......\n\n_ ............... = .....\n\ni\n\n._-....:..,...,._\n\ni ........................................\n_!:-_!!!!!!:\n\n....\n\n!!!::__!:!\n\n!__!-:\n\nExecution\n\nFigure\n\n\\\n\n"\n\n\\ ",,i\n::!:::!\n\n:!!:!:!::"i_\n\nI\n\n......Lower\n\n\\\n\n{ Actual\n\n_\n\n12 - Sample SRE failure\n\n75\n\nUpper\n\nTime\n\n=-Objective\n\nintensity vs. execution\n\ntime.\n\nFailure\n\nIntensity\n\nvs. Cumulative\n\nFailure\n\nPlot\n\nThis plot provides visual aid in determining\nwhether the software reaches the failure intensity objective in terms of number of cumulative failure occurrences.\nSee figure 13 for a\nsample result.\n\n"4\n::::::::::::::::::::::::::::::::::::::\n\ni\n\n/\n\ni\n\n=======================\n\ni_: Y::\\:V\',iY?_!\n-lt.J_t,lL,li>Ji:!:\ni:x-::\n\n....\n\n:\n\n......\n\n.....\n\n::\n\n"\n\n-\n\n......\n\n.........\n\n::::::\n\n" .:-:-::::--i\n\n\\_- ..... _\'. X--"-:--:_:-:-_-_:_:_::_;\n:\',-\n\n- - .-"- -X\n\n-\n\n-\n\n=======================\n\n:-\n\nI\n\n:_:-i-::_:\n"::::::::::\n\nE\nE\n\n.\n\nI .......\ni\n\nLL\n\n100\n\n\'\n\n- X. _.-.\n\n................\n\n_-----.._.\n...........\n<.:.....\n\n...........\n\n_"--+_\xc3\xb7--\n\n......\n_\',, i\n\n7"_-_.-_-,----,---,\n....................... ", i\n,,,,\n\n.................................\n10 =-\n\n=\n::!!---;;._\n\n=\n\n=\n=\n- - .\n\n, \',j\n=\n! ....\n\n=\n=\n=\n!:.:.:!:!-!!::!:!!:.!.::\n\n_\n\n=\n\n=\n\n1\nCumulative\n\n-_ Lower\n\nFigure\n\n_\n\nActual\n\n_\n\nUpper\n\n13 - Failure intensity\n\n76\n\nFailures\n\n-_ Objective\n\nvs. cumulative\n\nfailures.\n\n.\n\n=\n\n- =\n\n",_\n,,\n\nAppendix\n\nCalendar\n\ntime:\n\nClock time:\n\nA - Definitions\n\nnormal 24 hour clock time\n\nthe elapsed\n\nCPU execution:\n\ntime from the start to the end of program\n\nsee execution\n\nexecution\n\ntime\n\nCriticality: the degree of complication\nof a system or system component,\ndetermined by such\nfactors as the number and intricacy of interfaces, the number and intricacy of conditional\nbranches, the degree of nesting, the types of data structures,\nand other system characteristics\nCriticality:\n\nthe relative measure\n\nof the consequences\n\nCyclomatic\ncomplexity: measurement\nto determine\nule; the measure is a count of the basic executable\n\nof a failure\n\nmode\n\nthe structural complexity of a coded modpaths in the module expressed in terms of\n\nnodes and edges\nExecution\n\ntime: the actual time spent by a processor\n\ngram; also known as CPU execution\nEnvironment:\n\ndescribed\n\nFailure: the departure\n\nby the operational\nof the external\n\nFailure intensity function:\nures per unit time\n\nin executing\n\nthe instruction\n\nresults\n\nthe rate of change\n\nprofile\nof program\n\noperation\n\nfrom requirements\n\nof the mean value function\n\nFault: a defect in the program that, when executed under particular\nure; created when a programmer\nmakes a coding error\nModel: a mathematical\ntion of software\n\nequation\n\nusing failure\n\nNon-homogeneous:\n\na random\n\nused to model one software\n\nprocess\n\nruns that are identical\n\nwhose probability\n\nrepetitions\n\nconditions,\n\nreliability\n\ncauses\n\nestimation\n\nof fail-\n\na fail-\n\nand predic-\n\nSoftware availability:\nthe expected\ntem is functioning\nacceptably\n\nfraction\n\ndistribution\ncan execute\n\nvaries with time\nalong with the probabilities\n\nof each other\n\nSoftware reliability: the probability of failure-free\nfied time in a specified environment\n\nTool: a compilation\n\nor the number\n\ndata\n\nOperational\nprofile: a set of run types that the program\nwith which they will occur\nRun type:\n\nof that pro-\n\ntime\n\noperation\n\nof a computer\n\nof time during which a software\n\nof models\n\n77\n\nprogram\n\nfor a speci-\n\ncomponent\n\nor sys-\n\nAppendix\n\nB - SMERFS\n\nFiles\n\nSMERFSHistoryFile:\n\nSample Execution Time Analysis\nSSSSSSS\nS\n\nM\nMMMM\nS\n\nSSSSSSS\n\nEEEEEEE\nE\n\nRRRRRRR\nR\nR\n\nFFFFFFF\nF\n\nSSSSSSS\nS\n\nM\nM\n\nSSSSSSS\n\nM\nM\nM\n\nEEEE\nE\n\nRRRRRRR\nR\nR\n\nFFFF\nF\n\nSSSSSSS\n\nM\n\nEEEEEEE\n\nR\n\nF\n\nSSSSSSS\n\nM\n\nM\n\nSOFTWARE\nSOFTWARE\n\nBASELINE\nBASELINE\n\nSOFTWARE\n\nREVISION\nREVISION\n\nENTER\n\nIS NOT\n\nDESIRED,\n\nOUTPUT\n\nV V\nVV\n\nIII\n\nV\n\n1990\n\n19\n\nFILE\n\nOR A ONE\n\nTHE HISTORY\nFILE\nFILE CAN BE USED\n\n4\n21 JUNE\n\nV\nV\n\nI\nI\n\nS\n\nDATE\n\nPLEASE\n\nNUMBER\nDATE\n\nV\nV\n\nLETTER\n\nSOFTWARE\n\nR\n\nIII\nI\n\nNAME\n\nFOR\n\nFOR HISTORY\n\nDETAILS\n\nFILE;\n\nON THE\n\nA ZERO\n\nIF THE\n\nFILE\n\nFILE.\n\nIS A COPY OF THE ENTIRE INTERACTIVE\nSESSION.\nFOR LATER ANALYSIS\nAND/OR\nDOCUMENTATION.\n\nTHE\n\nt_one,hst\nPLEASE ENTER OUTPUT\nFILE NAME FOR THE PLOT FILE;\nIS NOT DESIRED,\nOR A ONE FOR DETAILS\nON THE FILE.\nTHE\n\nPLOT\n\nFILE IS A COPY\n\nOF THE\n\nDATA\n\nUSED\n\nFILE CAN BE USED (BY A USER-SUPPLIED)\nHIGH QUALITY\nPLOTS. THIS PROCESSING\nTERNAL\nRATHER\n\nLINE PRINTER\nCRUDE.\n\nPLOTTER\n\nA ZERO\n\nTO PRODUCE\n\nGRAPHICS\nIS HIGHLY\n\nIS BEING\n\nUSED;\n\nFOR\n\nSMERFS\n\nTHE\n\nPROGRAM\nSUGGESTED\n\nTHE\n\nIF THE\n\nFILE\n\nPLOTS.\n\nTHE\n\nTO GENERATE\nIF THE IN-\n\nGENERATED\n\nPLOTS\n\nARE\n\ntryone.plt\nPLEASE\nDATA\n\nENTER\n\nINPUT\n\nFILE\n\nREQUIREMENTS\n\nNAME\n\nTHE\n\nMODEL\n\nASSUMPTIONS\n\nAND\n\nFILE.\n\nsmf4rdc.lis\nPLEASE\n\nENTER\n\nTHE\n\nDESIRED\n\nDATA\n\nTYPE,\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n0\nTHE\n\nAVAILABLE\n\nDATA\n\nTYPES\n\nARE\n\n1 WALL CLOCK\nTIME-BE\'FWEEN-ERROR\n2 CENTRAL\nPROCESSING\nUNITS (CPU)\n3 WC TBE AND CPU TBE\n4 INTERVAL\nCOUNTS\nAND LENGTHS\nPLEASE ENTER THE DESIRED\nDATA\n\n(WC\nTBE\n\nTBE)\n\nTYPE.\n\n2\nPLEASE\n\nENTER\n\n1 FOR\n\nFILE\n\nINPUT;\n\nENTER\n\n1 FOR\n\nKEYBOARD\n\nELSE\n\nZERO.\n\n0\nPLEASE\n\nINPUT;\n\nELSE\n\nZERO.\n\n1\nA RESPONSE\nOF NEGATIVE\nVALUES\nAND 0 FOR ERROR-FREE\nOR 1 FOR\nPLEASE\n\nENTER\n\nCPU;\n\nAND\n\n0 FOR\n\n15.000000000000000\nPLEASE\n\nENTER\n\nCPU; AND\n\n23.000000000000000\n\nFOR THE\nERRORED"\n\nERROR-FREE\n\nPROMPT\n"PLEASE\nENTER\nCPU;\nWILL END THE PROCESSING.\nOR 1 FOR\n\nERRORED.\n\nOR 1 FOR\n\nERRORED.\n\n1\n0 FOR\n\nERROR-FREE\n\n1\n\n78\n\nPLEASE\n\nENTER\n\nCPU; AND\n\n0 FOR\n\nERROR-FREE\n\n20.000000000000000\nPLEASE\n\nENTER\n\nCPU;\n\nAND\n\n0 FOR\n\nERROR-FREE\n\n28.000000000000000\nPLEASE\n\nENTER\n\nENTER\n\nCPU; AND\n\n0 FOR\n\nENTER\n\nENTER\n\nCPU; AND\n\n0 FOR\n\nENTER\n\nENTER\n\nCPU; AND\n\n0 FOR\n\nENTER\n\n0 FOR\n\nENTER\n\nCPU;\n\n0 FOR\n\nENTER\n\nCPU;\n\nENTER\n\nCPU;\n\nAND\n\n0 FOR\n\nENTER\n\nCPU;\n\nENTER\n\nCPU;\n\nAND\n\nENTER\n\nERRORED.\n\nOR\n\n1 FOR\n\nERRORED.\n\nOR\n\n1 FOR\n\nERRORED.\n\nOR\n\n1 FOR\n\nERRORED.\n\nOR\n\n1 FOR\n\nERRORED.\n\nOR 1 FOR\n\n0 FOR ERROR-FREE\n\nERRORED.\n\n1\n\nAND\n\n0 FOR ERROR-FREE\n\n1\n\nAND\n\n0 FOR\n\nERROR-FREE\n\n1\n\nAND\n\n1 TO LIST\n\n1 FOR\n\n1\n\n0 FOR\n\nERROR-FREE\n\n1\n0 FOR\n\nERROR-FREE\n\n-1.000000000000000\nPLEASE\n\nERRORED.\n\nOR\n\n0 FOR ERROR-FREE\n\nAND\n\nAND\n\n1 FOR\n\n1\n\n101.000000000000000\nPLEASE\n\nERRORED,\n\nOR\n\nERROR-FREE\n\n82.000000000000000\nPLEASE\n\n1 FOR\n\n1\n\n76.000000000000000\nPLEASE\n\nERRORED,\n\nOR\n\nERROR-FREE\n\n62.000000000000000\nPLEASE\n\nOR 1 FOR\n\nERROR-FREE\n\n67.000000000000000\nPLEASE\n\nERRORED.\n\n1\n\nCPU; AND\n\nCPU;\n\nERRORED.\n\nOR 1 FOR\n\nERROR-FREE\n\n58.000000000000000\nPLEASE\n\nOR 1 FOR\n\n1\n\nCPU; AND\n\nCPU;\n\nERRORED.\n\n1\n\n49.000000000000000\nPLEASE\n\nOR 1 FOR\n\nERROR-FREE\n\n52.000000000000000\nPLEASE\n\nERRORED.\n\n1\n\n43.000000000000000\nPLEASE\n\nOR 1 FOR\n\nERROR-FREE\n\n37.000000000000000\nPLEASE\n\nERRORED.\n\n1\n\n35,000000000000000\nPLEASE\n\nOR 1 FOR\n\n1\n\n-1\nTHE\n\nCURRENT\n\nDATA;\n\nELSE\n\nZERO.\n\nNEW\n\nDATA\n\n1\nERROR\n\nNO\n\nTIME-BETWEEN\n\n1\n\n.23000000E+02\n.20000000E+02\n.28000000E+02\n\n5\n6\n7\n\n.35000000E+02\n.37000000E+02\n.43000000E+02\n\n8\n9\n10\n\n.52000000E+02\n.49000000E+02\n.58000000E+02\n\n11\n12\n\n.67000000E+02\n.62000000E+02\n\n13\n14\n15\nPLEASE\n\n.15000000E+02\n\n2\n3\n4\n\n.76000000E+02\n.82000000E+02\n.10100000E+03\n\nENTER\n\n1 FOR THE\n\nPROGRAM\n\nTO MAKE\n\nFILES;\n\nELSE\n\n(YOUR RESPONSE\nWILL BE USED THROUGHOUT\nTHE EXECUTION;\nAND\nWILL VOID THE DATA RESTORE\nOPTION\nIN DATA TRANSFORMATIONS.)\n\n1\nPLEASE\n\nENTER\n\nOUTPUT\n\nNAME\n\nFOR CPU\n\nTBE\n\nDATA.\n\ntryone.dat\nTHE\n\nOUTPUT\n\nOF 15 CPU\n\nTBE\n\nELEMENTS\n\nWAS\n\nPERFORMED.\n\n1\nPLEASE\n\nENTER\n\nTHE\n\nMAIN\n\nMODULE\n\nOPTION,\n\nOR ZERO\n\nFOR A LIST.\n\n0\n\n79\n\nZERO.\nA ZERO\n\nTHE\n\nAVAILABLE\n\nMAIN\n\nMODULE\n\nOPTIONS\n\nARE\n\n1 DATA\n2 DATA\n\nINPUT\nEDIT\n\n5 PLOT(S)\nOF THE RAW DATA\n6 EXECUTIONS\nOF THE MODELS\n\n3 DATA\n4 DATA\nPLEASE\n\nTRANSFORMATIONS\nSTATISTICS\nENTER THE MAIN\n\n7 ANALYSES\nOF MODEL\nFIT\n8 STOP EXECUTION\nOF SMERFS\nOPTION.\n\nMODULE\n\n4\nCPU\n\nTIME-BETWEEN-ERROR\n\nWITH TOTAL TESTING\nAND TOTAL ERRORED\n\nTIME\nTIME\n\nOF\nOF\n\n.74800000E+03\n.74800000E+03\n\n*_ww***_w_w****************************_***_*\n\nMEDIAN\nOF THE DATA\nLOWER & UPPER\nHINGES\nMINIMUM\nAND MAXIMUM\n\n*\n*\n\n.49000000E+02\n.31500000E+02\n.64500000E+02\n\n*\n*\n\n*\n*\n*\n*\n\n.15000000E+02\n\nNUMBER\nOF ENTRIES\nAVERAGE\nOF THE DATA\nSTD. DEV. & VARIANCE\n\n.10100000E+03\n15\n.49866667E+02\n.24761337E+02\n.61312381 E+03\n\n*\n*\n*\n*\n\nSKEWNESS\n\n*\n\n.42596916E+00\n\n*\n\nPLEASE\n\n& KURTOSIS\n\nENTER\n\nTHE\n\nMAIN\n\nMODULE\n\nTIME\n\nMODEL\n\n-.64196038E+00\n\nOPTION,\n\nOR ZERO\n\nFOR A LIST.\n\n6\nPLEASE\n\nENTER\n\nTHE\n\nOPTION,\n\nOR ZERO\n\nFOR\n\nMODELS\n\nA LIST.\n\nARE\n\n0\nTHE\n\nAVAILABLE\n\n1 THE\n2 THE\n3 THE\n\nWALL\n\nLITTLEWOOD\nMUSA BASIC\nGEOMETRIC\n\nCLOCK\n\nOR CPU\n\nTIME\n\nAND VERRALL\nBAYESIAN\nEXECUTION\nTIME MODEL\nMODEL\n\nMODEL\n\n4 THE NHPP MODEL\nFOR TIME-BETWEEN-ERROR\n5 THE MUSA LOG POISSON\nEXECUTION\nTIME\n6 RETURN\nTO THE MAIN PROGRAM\nPLEASE\n\nENTER\n\nTHE\n\nMODEL\n\nOCC.\nMODEL\n\nOPTION.\n\n1\nPLEASE\n\nENTER\n\n1 FOR\n\nMODEL\n\nDESCRIPTION;\n\nELSE\n\nZERO.\n\n0\nPLEASE\nENTER\n3 TO TERMINATE\n\n1 FOR MAXIMUM\nLIKELIHOOD,\nMODEL\nEXECUTION.\n\n2 FOR\n\nLEAST\n\nSQUARES,\n\nOR\n\n1\nWHICH\n\nOF THE\n\nPHI(I)\n\nIN THE\n\nPRIOR\n\nWITH\n\nFOLLOWING\n\nGAMMA\n\nFUNCTIONS\n\nDO\n\nDISTRIBUTION?\n\nPARAMETERS\n\nTHE\n\nALPHA\n\nAND\n\nYOU DESIRE\nGAMMA\n\nTO USE AS THE\n\nIS USED\n\nAS THE\n\nPHI(I)\n\n1. PHI(I)\nOR\n\n= BETA(0)\n\n+ BETA(l)\n\n* I\n\n(LINEAR)\n\n2. PHI(I)\n\n= BETA(0)\n\n+ BETA(l)\n\n* 1"\'2\n\n(QUADRATIC).\n\n1\nTHE INITIAL ESTIMATES\nFOR\nBETA(0)\n: .65523810E+01\nBETA(l)\n\nBETA(0)\n\nAND\n\nBETA(l)\n\nARE:\n\n: .54142857E+01\n\nPLEASE\n\nENTER\n\nBETA(l),\n\n1 TO USE THESE\n\nOR 2 TO INPUT\n\nINITIAL\n\nINITIAL\n\nESTIMATES\n\nFOR\n\nBETA(0)\n\nESTIMATES.\n\n1\nPLEASE\n\nENTER\n\nTHE\n\nMAXIMUM\n\nNUMBER\n\nOF ITERATIONS.\n\n100\nML MODEL\n\nESTIMATES\n\nAFTER\n\nALPHA\nBETA(0)\n\nARE:\n\n: .13173016E+06\n\nBETA(l)\n\n25 ITERATIONS\n\n: .13544290E+05\n: .67176336E+05\n\n80\n\nAND\n\nTHE\n\nFUNCTION\n\nPLEASE\n\nEVALUATED\n\nENTER\n\nERROR;\n\nAT THESE\n\n1 FOR AN ESTIMATE\n\nELSE\n\nPOINTS\n\nOF THE\n\nIS\n\n.71757203E+02\n\nMEAN\n\nTIME\n\nBEFORE\n\nTHE\n\nNEXT\n\nZERO.\n\n1\nTHE\n\nEXPECTED\n\nTIME\n\nPLEASE\nENTER\n3 TO TERMINATE\n\nIS\n\n.89088509E+02\n\n1 FOR MAXIMUM\nLIKELIHOOD,\nMODEL\nEXECUTION.\n\n2 FOR\n\nLEAST\n\nSQUARES,\n\nOR\n\n3\nPLEASE\n\nENTER\n\n1 TO COMPUTE\n\nTHE\n\nPREDICTED\n\nTBES;\n\nELSE ZERO.\n\n1\nPLEASE\n\nENTER\n\nTHE\n\nTIME\n\nMODEL\n\nOPTION,\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n0\nTHE AVAILABLE\nWALL CLOCK\nOR CPU TIME MODELS\nARE\n1 THE LI\'I-FLEWOOD\nAND VERRALL\nBAYESIAN\nMODEL\n2 THE\n3 THE\n4 THE\n\nMUSA BASIC EXECUTION\nTIME MODEL\nGEOMETRIC\nMODEL\nNHPP MODEL\nFOR TIME-BETWEEN-ERROR\n\n5 THE MUSA LOG POISSON\nEXECUTION\n6 RETURN\nTO THE MAIN PROGRAM\nPLEASE\nENTER THE MODEL OPTION.\n\nOCC.\n\nTIME\n\nMODEL\n\n6\n1\nPLEASE\n\nENTER\n\nTHE\n\nMAIN\n\nMODULE\n\nOPTION,\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n0\nTHE\n\nAVAILABLE\n\nMAIN\n\nMODULE\n\nOPTIONS\n\nARE\n\n1 DATA\n2 DATA\n\nINPUT\nEDIT\n\n5 PLOT(S)\nOF THE RAW DATA\n6 EXECUTIONS\nOF THE MODELS\n\n3 DATA\n4 DATA\nPLEASE\n\nTRANSFORMATIONS\nSTATISTICS\nENTER\nTHE MAIN\n\n7 ANALYSES\nOF MODEL\nFIT\n8 STOP EXECUTION\nOF SMERFS\nOPTION.\n\nMODULE\n\n7\nTHE\n\nANALYSES\n\nPORTION\n\nWILL\n\nOPERATE\n\nON YOUR\n\nLAST\n\nSELECTED\n\nPREDIC-\n\nTIONS FROM THE LITTLEWOOD\nAND VERRALL\nBAYESIAN\nMODEL.\nPLEASE\nENTER A ONE TO CONTINUE\nWITH THE ANALYSES;\nOTHERWISE\nA ZERO TO RETURN\n\nTO THE\n\nMAIN\n\nMODULE\n\nMENU.\n\n1\nPLEASE\n\nENTER\n\nTHE\n\nANALYSES\n\nOPTION;\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n0\nTHE AVAILABLE\nANALYSES\nOPTIONS\nARE\n1 GOODNESS-OF-FIT\nTESTS AND DATA LISTINGS\n2 PLOT OF ORIGINAL\nAND PREDICTED\nDATA\n3 PLOT OF THE RESIDUAL\nDATA\n4 RETURN\nTO THE MAIN PROGRAM\nPLEASE\nENTER THE ANALYSES\nOPTION.\nPLEASE\n\nENTER\n\n1 FOR THE\n\nENTER\n\nTHE\n\nDATA\n\nLISTING;\n\nELSE\n\nZERO.\n\n0\nPLEASE\n\nANALYSES\n\nOPTION;\n\nOR ZERO\n\nFOR A LIST.\n\n0\nTHE AVAILABLE\nANALYSES\nOPTIONS\nARE\n1 GOODNESS-OF-FIT\nTESTS\nAND DATA LISTINGS\n2 PLOT OF ORIGINAL\nAND PREDICTED\nDATA\n3 PLOT OF THE RESIDUAL\nDATA\n4 RETURN\nTO THE MAIN PROGRAM\nPLEASE\nENTER\nTHE ANALYSES\nOPTION.\n\n2\nPLEASE\n\nENTER\n\nTHE\n\nPLOT\n\nTITLE\n\n(UP TO 30 CHARACTERS).\n\n(type your title)\n\n81\n\n.10100000E+03\n\nP\n\n$\n$\nC\nP\n\n$P\n*p\n\nU\n\np*\nP$\n\n$*\n$\n*$\n.14686719E+02\n\nI$P\n........\n\n+ .........\n\n1\n\n10\n\n+ .........\n\n+ .........\n\n20\n\n+ .........\n\n30\n\n40\n\n+\n\n50\n\nERROR\nPLEASE\n\nENTER\n\nTHE\n\nANALYSES\n\nENTER\n\nTHE\n\nPLOT\n\nOPTION;\n\nOR ZERO\n\nFOR A LIST.\n\n3\nPLEASE\n\nTITLE\n\n(UP TO 30 CHARACTERS).\n\n(type your title)\nPLEASE\n\nENTER\n\n1 TO SMOOTH\n\nTHE\n\nRESIDUALS;\n\nELSE\n\nZERO.\n\n0\n\n.16871610E+02\n\nC\nP\nU\n\nI ....\nI\n\n*\n\n_*\n\n*\n\xe2\x80\xa2\n\n**\n\n!*\n!\n!\n\n-.72480318E+01\n\n*\n\n*\n*\n\n+ .........\n\n-I-- ........\n\n1\n\n10\n\n+ .........\n\n20\n\n+ .........\n\n+ .........\n\n30\n\n40\n\n+\n\n50\n\nERROR\nPLEASE\n\nENTER\n\nTHE\n\nANALYSES\n\nOPTION;\n\nOR ZERO\n\nFOR A LIST.\n\n0\nTHE AVAILABLE\nANALYSES\nOPTIONS\nARE\n1 GOODNESS-OF-FIT\nTESTS AND DATA LISTINGS\n2 PLOT OF ORIGINAL\nAND PREDICTED\n3 PLOT OF THE RESIDUAL\nDATA\n4 RETURN\nTO THE MAIN PROGRAM\nPLEASE\n\nENTER\n\nTHE\n\nANALYSES\n\nENTER\n\nONE TO PLACE\n\nDATA\n\nOPTION,\n\n4\nPLEASE\nPLOT\n\nFILE;\n\nTHE\n\nPLOT\n\nDATA\n\nON THE\n\nOPTIONAL\n\nELSE ZERO.\n\n1\nPLEASE\n\nENTER\n\nLittlewood\n\nA PLOT TITLE\n\nFOR ALL DATA\n\n(UP TO 40 CHARACTERS).\n\n& Verral Maximum Likelihood L\n\n1\n\n82\n\nSMERFS\n\nPLEASE\n\nENTER\n\nTHE\n\nMAIN\n\nMODULE\n\nOPTION,\n\nOR\n\nZERO\n\nFOR A LIST.\n\n0\nTHE\n\nAVAILABLE\n\nMAIN\n\nMODULE\n\n1 DATA\n2 DATA\n3 DATA\n4 DATA\nPLEASE\n\nSTATISTICS\nENTER THE\n\nOPTIONS\n\nINPUT\nEDIT\nTRANSFORMATIONS\nMAIN\n\nARE\n\n5 PLOT(S)\nOF THE RAW\n6 EXECUTIONS\nOF THE\n7 ANALYSES\nOF MODEL\n8 STOP EXECUTION\nMODULE\nOPTION.\n\nDATA\nMODELS\nFIT\n\nOF SMERFS\n\n8\nTHE\n\nSMERFS\n\nSMERFS\n\nEXECUTION\n\nPLOT\n\nHAS ENDED.\n\nFile:\n\nSample Interval Data Analysis\nM\nMMMM\n\nSSSSSSS\nS\nSSSSSSS\n\nM\n\nEEEEEEE\nE\n\nRRRRRRR\nR\nR\n\nFFFFFFF\nF\n\nSSSSSSS\nS\n\nM\nM\nM\n\nSSSSSSS\nS\n\nM\nM\nM\n\nEEEE\nE\nEEEEEEE\n\nRRRRRRR\nR\nR\nR\nR\n\nFFFF\nF\n\nSSSSSSS\n\nF\n\nSSSSSSS\n\nM\n\nSOFTWARE\nSOFTWARE\n\nBASELINE\nBASELINE\n\nNUMBER\nDATE\n\nSOF-I\'WARE\n\nREVISION\n\nLETTER\n\n:\n\nSOFTWARE\n\nREVISION\n\nDATE\n\n: _\n\nPLEASE\n\nENTER\n\nIS NOT\n\nDESIRED,\n\nOUTPUT\n\n: 4\n: 21 JUNE\n\nIII\nI\n\nV\nV\n\nI\nI\n\nS\n\nIII\n\nV\nV\n\nV V\nV V\nV\n\n1990\n\n0\n__\n\nFILE NAME\n\nOR A ONE FOR\n\n19\n\nFOR\n\nHISTORY\n\nDETAILS\n\nFILE;\n\nON THE\n\nA ZERO\n\nIF THE\n\nFILE\n\nFILE.\n\ntrytwo.hst\nPLEASE\n\nENTER\n\nIS NOT\n\nDESIRED,\n\nOUTPUT\n\nFILE\n\nNAME\n\nOR A ONE FOR\n\nFOR\n\nTHE\n\nDETAILS\n\nPLOT\n\nON THE\n\nFILE;\n\nA ZERO\n\nIF THE\n\nFILE\n\nFILE.\n\ntrytwo.plt\nPLEASE\nENTER INPUT\nDATA REQUIREMENTS\n\nFILE NAME\nFILE.\n\nFOR THE\n\nSMERFS\n\nMODEL\n\nASSUMPTIONS\n\nAND\n\nsmf4rdc.lis\nPLEASE\n\nENTER\n\nTHE\n\nDESIRED\n\nENTER\n\n1 FOR FILE\n\nENTER\n\n1 FOR\n\nDATA\n\nTYPE,\n\nOR ZERO\n\nFOR A LIST.\n\n4\nPLEASE\n\nINPUT;\n\nELSE ZERO.\n\n0\nPLEASE\n\nKEYBOARD\n\nINPUT;\n\nELSE\n\nZERO.\n\n1\nA RESPONSE\nERROR\nPLEASE\n\nOF NEGATIVE\n\nCOUNT\nENTER\n\nAND\n\nTESTING\n\nTHE\n\nERROR\n\n35.000000000000000\nPLEASE\n\nENTER\n\nTHE\n\nERROR\n\n38.000000000000000\nPLEASE\n\nENTER\n\nTHE\n\nERROR\n\n27.000000000000000\nPLEASE\n\nENTER\n\nTHE\n\nERROR\n\n21.000000000000000\nPLEASE\n\nENTER\n\nTHE\n\nERROR\n\n18.000000000000000\nPLEASE\n\nENTER\n\nTHE\n\nERROR\n\n19,000000000000000\n\nVALUES\n\nFOR THE\n\nLENGTH"\nCOUNT\n\nWILL\n\nPROMPT\nEND\n\nAND TESTING\n\nTHE\n\n"PLEASE\n\nLENGTH.\n\n1.000000000000000\nCOUNT\n\nAND\n\nTESTING\n\nLENGTH.\n\n1.000000000000000\nCOUNT\n\nAND\n\nTESTING\n\nLENGTH.\n\n1,000000000000000\nCOUNT\n\nAND\n\nTESTING\n\nLENGTH.\n\n1.000000000000000\nCOUNT\n\nAND\n\nTESTING\n\nLENGTH.\n\n1.000000000000000\nCOUNT\n\nAND\n\nTESTING\n\nENTER\n\nPROCESSING.\n\nLENGTH.\n\n1.000000000000000\n\n83\n\nTHE\n\nPLEASE\n\nENTER\n\nTHE\n\nERROR\n\nCOUNT\n\n14.000000000000000\nPLEASE\n\nENTER\n\nTHE\n\nERROR\n\nCOUNT\n\n20.000000000000000\nPLEASE\n\nENTER\n\nTHE\n\nENTER\n\nTHE\n\nERROR\n\nENTER\n\nTHE\n\nENTER\n\nTHE\n\nERROR\n\nENTER\n\nTHE\n\nENTER\n\nTHE\n\nERROR\n\nENTER\n\nTHE\n\nENTER\n\nTHE\n\nERROR\n\nENTER\n\nTESTING\n\nAND\n\nCOUNT\n\nLENGTH,\n\nTESTING\n\nLENGTH.\n\nAND\n\nTESTING\n\nLENGTH.\n\n1.000000000000000\n\nERROR\n\nCOUNT\n\nAND\n\nTESTING\n\nLENGTH.\n\n1.000000000000000\n\nERROR\n\nCOUNT\n\nERROR\n\nAND\n\nTESTING\n\nLENGTH.\n\n1.000000000000000\nCOUNT\n\nAND\n\nTESTING\n\nLENGTH.\n\n1,000000000000000\n\nERROR\n\nCOUNT\n\n-1.000000000000000\nPLEASE\n\nLENGTH.\n\n1,000000000000000\n\n2.000000000000000\nPLEASE\n\nTESTING\n\nAND\n\nCOUNT\n\n0.000000000000000E+000\nPLEASE\n\nLENGTH.\n\n1.000000000000000\n\n1.000000000000000\nPLEASE\n\nTESTING\n\nAND\n\nCOUNT\n\n2.000000000000000\nPLEASE\n\nLENGTH.\n\n1.000000000000000\n\n0,000000000000000E+000\nPLEASE\n\nAND\n\nCOUNT\n\n4,000000000000000\nPLEASE\n\nTESTING\n\n1.000000000000000\n\n9.000000000000000\nPLEASE\n\nAND\n\n1.000000000000000\n\nAND\n\nTESTING\n\nLENGTH.\n\n-1.000000000000000\n\n1 TO LIST THE\n\nCURRENT\n\nDATA;\n\nELSE\n\nZERO.\n\n1\nINTERVAL\n\nNO. OF ERRORS\n\nTESTING\n\nLENGTH\n\n1\n2\n\n,35000000E+02\n.38000000E+02\n\n.10000000E+01\n.10000000E+01\n\n3\n4\n5\n\n.27000000E+02\n.21000000E+02\n.18000000E+02\n\n.10000000E+01\n.10000000E+01\n.10000000E+01\n\n6\n7\n8\n\n.19000000E+02\n.14000000E+02\n.20000000E+02\n\n.10000000E+01\n.10000000E+01\n.10000000E+01\n\n9\n10\n11\n\n,90000000E+02\n.40000000E+02\n.00000000E+00\n\n,10000000E+01\n,10000000E+01\n,10000000E+01\n\n12\n13\n14\n\n.20000000E+01\n,10000000E+01\n,00000000E+00\n\n.10000000E+01\n.10000000E+01\n.10000000E+01\n\n15\n\n,20000000E+01\n\n,10000000E+01\n\nPLEASE\n\nENTER\n\n1 FOR THE\n\nPROGRAM\n\nTO MAKE\n\n(YOUR RESPONSE\nWILL BE USED THROUGHOUT\nWILL VOID THE DATA RESTORE\nOPTION\nIN DATA\n\nNEW\n\nDATA\n\nFILES;\n\nTHE EXECUTION;\nTRANSFORMATIONS.)\n\n1\nPLEASE\n\nENTER\n\nOUTPUT\n\nNAME\n\nFOR\n\nINTERVAL\n\nDATA.\n\ntrytwo.dat\nTHE\n\nOUTPUT\n\nOF 15 INTERVAL\n\nELEMENTS\n\nWAS\n\nPERFORMED.\n\n1\nPLEASE\n\nENTER\n\nTHE\n\nMAIN\n\nMODULE\n\nOPTION,\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n0\nTHE\n\nAVAILABLE\n\nMAIN\n\nMODULE\n\n1 DATA\n2 DATA\n\nINPUT\nEDIT\n\n3 DATA\n4 DATA\nPLEASE\n\nTRANSFORMATIONS\nSTATISTICS\nENTER THE MAIN\n\nOPTIONS\n\nARE\n\n5 PLOT(S)\nOF THE RAW\n6 EXECUTIONS\nOF THE\n\nDATA\nMODELS\n\n7 ANALYSES\nOF MODEL\nFIT\n8 STOP EXECUTION\nOF SMERFS\nMODULE\nOPTION.\n\n4\n\n84\n\nELSE\nAND\n\nZERO.\nA ZERO\n\nINTERVAL\nDATA WITH EQUAL LENGTHS\nWITH ERROR\nCOUNTS\nTOTALING\nTO\n\n210\n\n*************_*************_***_***************\n\nMEDIAN\nLOWER\n\nOF THE DATA\n& UPPER\nHINGES*\n\n*\n\n.14000000E+02\n.20000000E+01\n.20500000E+02\n\n*\n*\n\nMINIMUMAND\nMAXIMUM\nNUMBER\nOF ENTRIES\nAVERAGE\nOF THE DATA\n\n*\n*\n*\n\n.00000000E+00\n\n*\n*\n*\n\nSTD. DEV.\nSKEWNESS\n\n*\n*\n\n.12778330E+02\n.48772605E+00\n\nPLEASE\n\n& VARIANCE\n& KURTOSIS\n\nENTER\n\nTHE\n\nMAIN\n\n.38000000E+02\n15\n.14000000E+02\n\nMODULE\n\n.16328571E+03\n-.94226755E+00\n\nOPTION,\n\nOR ZERO\n\n*\n*\n\nFOR A LIST.\n\n0\nTHE\n1\n2\n3\n4\n\nAVAILABLE\n\nDATA\nDATA\nDATA\nDATA\n\nMAIN\n\nMODULE\n\nOPTIONS\n\nINPUT\nEDIT\nTRANSFORMATIONS\nSTATISTICS\n\nPLEASE\n\nENTER\n\nTHE\n\n5\n6\n7\n8\n\nMAIN\n\nARE\n\nPLOT(S)\nOF THE RAW DATA\nEXECUTIONS\nOF THE MODELS\nANALYSES\nOF MODEL\nFIT\nSTOP EXECUTION\nOF SMERFS\n\nMODULE\n\nOPTION.\n\n6\nPLEASE\n\nENTER\n\nTHE\n\nCOUNT\n\nMODEL\n\nOPTION,\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n0\nTHE AVAILABLE\nERROR\nCOUNT\n1 THE GENERALIZED\nPOISSON\n2 THE\n3 THE\n4 THE\n\nMODELS\nMODEL\n\nNON-HOMOGENEOUS\nPOISSON\nBROOKS\nAND MOTLEY\nMODEL\nSCHNEIDEWIND\nMODEL\n\nARE\nMODEL\n\n5 THE S-SHAPED\nRELIABILITY\nGROWTH\n6 RETURN\nTO THE MAIN PROGRAM\nPLEASE\nENTER THE MODEL OPTION.\n\nMODEL\n\n4\nPLEASE\n\nENTER\n\n1 FOR\n\nMODEL\n\nENTER\n\n1 FOR\n\nDESCRIPTION\n\nDESCRIPTION;\n\nELSE\n\nZERO.\n\nO\nPLEASE\n\nOF TREATMENT\n\nTYPES;\n\nELSE\n\nZERO.\n\n1\nTREATMENT\n\n1 -\n\nUTILIZE\nALL THE ERROR\nTHE TESTING\nPERIODS.\n\nTREATMENT\n\n2 -\n\nIGNORE\n\nTHE\n\nERROR\n\nCOUNTS\n\nCOUNTS\n\nFROM\n\nFROM\n\nEACH\n\nTHE\n\nFIRST\n\nING PERIODS,\nAND USE ONLY THE ERROR\nPERIOD\nS THROUGH\nTHE TOTAL\nNUMBER\nTREATMENT\n\n3 -\n\nUSE THE\n\nCUMULATIVE\n\nNUMBER\n\nOF THE\n\nS-1 TEST-\n\nCOUNTS\nFROM\nOF PERIODS.\n\nOF ERRORS\n\nFROM\n\nPERIODS\n\n1 THROUGH\nS-l,\nAND THE INDIVIDUAL\nCOUNTS\nFROM\nPERIOD S THROUGH\nTHE TOTAL NUMBER\nOF PERIODS.\nPLEASE\nMINATE\n\nENTER\nMODEL\n\nTHE DESIRED\nEXECUTION.\n\nMODEL\n\nTREATMENT\n\nNUMBER,\n\nOR A 4 TO TER-\n\n1\nTREATMENT\n1 MODEL\nBETA\n.22695165E+00\nALPHA\nAND THE\nSERVED\nPLEASE\nIN THE\n\nESTIMATES\n\nARE:\n\n.49298066E+02\nWEIGHTED\nSUMS-OF-SQUARES\nBETWEEN\nERROR COUNTS\nIS .15366929E+04\nENTER\n\n1 FOR\n\nAN ESTIMATE\n\nNEXT TESTING\n\nPERIOD;\n\nELSE\n\nOF THE\n\nTHE\n\nNUMBER\n\nPREDICTED\n\nOF ERRORS\n\nZERO.\n\n1\nTHE\n\nEXPECTED\n\nNUMBER\n\nOF ERRORS\n\nIS\n\n.14656216E+01\n\n85\n\nAND\n\nOB-\n\nEXPECTED\n\nPLEASE\n\nENTER\n\nNEEDED\n\n1 FOR AN ESTIMATE\n\nTO DISCOVER\n\nTHE\n\nOF THE\n\nNEXT\n\nNUMBER\n\nM ERRORS;\n\nOF TESTING\n\nELSE\n\nPERIODS\n\nZERO.\n\n1\nPLEASE\n\nENTER\n\nTHE\n\nVALUE\n\nFOR\n\nM.\n\n1.000000000000000\nTHE\n\nEXPECTED\n\nPLEASE\n\nENTER\n\nNUMBER\n\nOF PERIODS\n\nIS\n\n1 TO TRY\n\nA DIFFERENT\n\n.65706276E+00\n\nVALUE\n\nFOR\n\nM; ELSE\n\nZERO.\n\n0\nPLEASE\nMINATE\n\nENTER\nMODEL\n\nTHE DESIRED\nEXECUTION.\n\nMODEL\n\nTREATMENT\n\nNUMBER,\n\nOR A 4 TO TER-\n\n4\nPLEASE ENTER\nELSE ZERO.\n\n1 TO COMPUTE\n\nTHE\n\nPREDICTED\n\nINTERVAL\n\nERROR\n\nCOUNTS;\n\n1\nPLEASE\n\nENTER\n\nTHE\n\nCOUNT\n\nMODEl\n\nOPTION,\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n6\n\nPLEASE\n\nENTER\n\nTHE\n\nMAIN\n\nMODULE\n\nOPTION,\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n7\nTHE\n\nANALYSES\n\nPORTION\n\nWILL\n\nOPERATE\n\nON YOUR\n\nTIONS FROM THE SCHNEIDEWlND\n(FOR ERROR\nTER A ONE TO CONTINUE\nWITH THE ANALYSES;\nTURN TO THE MAIN MODULE\nMENU.\n\nLAST\n\nSELECTED\n\n0\nPLEASE\n\nENTER\n\nTHE\n\nANALYSES\n\nOPTION;\n\nOR ZERO\n\nFOR\n\nA LIST.\n\n0\nTHE AVAILABLE\nANALYSES\nOPTIONS\nARE\n1 GOODNESS-OF-FIT\nTESTS AND DATA LISTINGS\n2 PLOT OF ORIGINAL\nAND PREDICTED\n3 PLOT OF THE RESIDUAL\nDATA\n4 RETURN\nTO THE MAIN PROGRAM\nPLEASE\n\nENTER\n\nTHE\n\nANALYSES\n\nENTER\n\nTHE\n\nPLOT\n\nDATA\n\nOPTION.\n\n2\nPLEASE\n\nTITLE\n\n(UP TO 30 CHARACTERS).\n\n(type your title)\n\n.44104166E+02\n\nIP\nI\nI *\nI*P\n\nC\nO\nU\nN\nT\n\nP\nP\n\n$\np*\npp*\nPP\n*PPPP\n.00000000E+00\n+.........\n1\n\nPREDIC-\n\nCOUNT)\nMODEL.\nPLEASE\nENOTHERWISE\nA ZERO TO RE-\n\n+ ......... + ......... + ......... + ......... +\n10\n20\n30\n40\n50\n\nINTERVAL\n\n86\n\nPLEASE\n\nENTER\n\nTHE\n\nANALYSES\n\nOPTION;\n\nOR ZERO\n\nFOR A LIST.\n\n4\nPLEASE\n\nENTER\n\nON THE\n\nOPTIONAL\n\nTHE\n\nAVAILABLE\n\nONE\n\nTO PLACE\n\nSMERFS\n\nMAIN\n\nMODULE\n\n1 DATA\n2 DATA\n\nINPUT\nEDIT\n\n3 DATA\n4 DATA\nPLEASE\n\nTRANSFORMATIONS\nSTATISTICS\nENTER THE MAIN\n\nTHE\n\nPLOT\n\nPLOT\n\nDATA\n\nFILE;\n\nELSE\n\nOPTIONS\n\nAND\n\nCHI-SQUARE\n\nZERO.\n\nARE\n\n5 PLOT(S)\nOF THE RAW DATA\n6 EXECUTIONS\nOF THE MODELS\n7 ANALYSES\nOF MODEL FIT\n8 STOP EXECUTION\nOF SMERFS\nMODULE\nOPTION.\n\n8\n\nTHE\n\nSMERFS\n\nEXECUTION\n\nHAS\n\nENDED.\n\n87\n\nSTATISTIC\n\nREFERENCES\n\n[1]\n\nASQC\n\nSoftware\n\nMetrics\n\nTutorial\n\nNotes.\n\n[2]\nKhoshgoftaar,\nTaghi M. and Munson, John C. "Predicting\nSoftware Development Errors Using Software Complexity\nMetrics"\nIEEE Journal on Selected Areas in Communications.\nVol. 8, No. 2.\n\nFebruary\n\n1990.\n\n[3]\nFriedman, M. A. et. al. "Reliability Techniques for Combined Hardware\nSystems."\nFinal Report, Contract No. F30602-89-C-0111.\nRome Laboratory.\n\nand Software\n1991.\n\n[4]\nZuse, Horst. "Measuring\nFactors Contributing\nto Software Maintenance\nComplexity."\nMinutes from the ASQC Second International Conference\nOn Software Quality.\nResearch\nTriangle Park, NC. October 1992.\n[5]\nMontgomery,\nDouglas C. and Peck, Elizabeth\nAnalysis.\nWiley and Sons. 1992.\n\nA. Introduction\n\nTo Linear\n\nRegression\n\n[6]\nMcCabe, Thomas J. Structured Testing: A Software Testing Methodology\nCyclomatic Complexity Metric. NBS Special Publication\n500-99. U. S. Department\nmerce. December 1982.\n[7]\nware.\n\nMusa, John D. "Operational\nMarch 1993. pp. 14-32.\n\nProfiles in Software-Reliability\n\n[8]\nBraley, Dennis "Test Case Effectiveness\nAnalysis Capability."\nJSC Software Technology\n[9]\n1983.\n\nMcGilton\n\nand Morgan,\n\n[10]\nIEEE Standard\n1983, p. 32.\n\nGlossary\n\nIntroducing\n\nof Software\n\n[11]\nMartin L. Shooman, Software\nMcGraw Hill. 1983. p. 304.\n\nMeasuring\nBranch.\n\nthe UNIX System.\n\nEnqineering\n\nEngineering\n\nDesign,\n\nUsing the\nof Com-\n\nEngineering."\n\nIEEE Soft-\n\nUsing the Logiscope\n\nMcGraw\n\nTerminology.\n\nReliability,\n\n[12]\nMusa, John D.; lannino, Anthony; and Okumoto, Kazuhira.\nurement, Prediction, Application.\nMcGraw Hill. New York.\n\nDynamic\n\nHill Book Company.\n\nANSI/IEEE\n\nStd 729-\n\nand Management.\n\nSoftware\n\nReliability\n\n[13]\nFarr, William H. A Survey of Software Reliability, Modeling, and Estimation.\nSurface Warfare Center. NAVSWC TR 82-171. September\n1983.\n[14]\nFarr, William H., and Smith, Oliver D. Statistical Modeling and Estimation\nFunction for Software (SMERFS)\nUsers Guide. Naval Surface Warfare Center.\n84-373 Rev. 2. March 1991.\n\n88\n\nMeas-\n\nNaval\n\nof Reliability\nNAVSWC TR\n\nREPORT\nPublic\n\nreporting\n\nmaintaining\nincluding\nVA\n\nburden\n\nthe\n\ndata\n\nsuggestions\n\n22202-4302,\n\nfor this\n\nneeded,\n\ncollection\nand\n\nof information\n\ncompleting\n\nfor reducing\n\nand to the\n\nDOCUMENTATION\n\nthis\n\nOffice\n\nand\n\nburden,\n\nis estimated\n\nreviewing\n\nthe\n\nto Washington\n\nof Management\n\nand\n\n1 hour\n\nper response,\n\nof Information.\n\nHeadquarters\n\nBudget,\n\n1. AGENCY USE ONLY (Leave Blank)\n\nto average\n\ncollection\n\nSend\n\nServices,\n\nPaperwork\n\nForm Approved\nOMB NO. 0704-0188\n\nPAGE\n\nDirectorate\n\nReduction\n\nProject\n\nincluding\n\ncomments\n\nthe\n\nfor information\n\n(0704-0188),\n\ntime\n\nregarding\n\nfor reviewing\n\nthis\n\nburden\n\nOperations\n\nWashington,\n\nDC\n\ninstructions,\n\nestimate\n\nand\n\nRe )orts,\n\nAnalysis\n\n1215\n\nexisting\n\naspect\n\nJefferson\n\ndata\n\nof this\n\nDavis\n\nsources,\n\ncollection\n\nHighway,\n\ngathering\n\nand\n\nof information,\n\nSuite\n\n1204,\n\nArlington,\n\n20503.\n\nTechnical\n\nMemorandum\n5. FUNDING NUMBERS\n\n4. TITLE AND SUBTITLE\n\nReliability\n\nother\n\n3. REPORT TYPE AND DATES COVERED\n\ni 2. REPORT DATE\nAug/94\n\nSoftware\n\nseamhing\n\nor any\n\nHandbook:\n\nEstimation\n\nSoftware\n\nComplexity\n\nAnalysis\n\nand\n\nSoftware\n\nand Prediction\n\n6. AUTHOR(S)\nAlice\n\nT. Lee;\n\nTodd\n\n7. PERFORMING\nSafety,\n\nGunn*;\n\nORGANIZATION\n\nReliability,\n\nSpace\n\nStation\n\nLyndon\n\nand\n\nSafety\n\nTexas\n\nMission\n\nSpace\n\nAssurance\n\nAGENCY\nSpace\n\nfrom\n\n(301)\n\nTM-104799\n\nHeights,\n\nHouston,\n\nTexas\n\n12b. DISTRIBUTION\n\nSTATEMENT\n\nCenter\n\nfor AeroSpace\n\nInformation\n\nMD\n\n21090-2934\nSubject\n\nStation\nresults\n\n(Maximum 200 words)\ndocuments\nthe three\n\nsoftware,\nare also\n\nfirst\n\nareas,\n\nstructure;\n\ndescribes\n\npositively\n\nThe\n\nsecond\n\nquantitative\n\nsoftware\n\nanalysis\n\nbackgrounds,\n\nhow\n\nthroughout\n\nthe\n\ncritical\n\nprogram\n\ncan\n\ntheir\n\nprocesses\n\ntheories,\n\ncategory:\n\nthe Space\n\ntools,\n\nand\n\n59\n\nStation\n\nanalysis\n\nSoftware\n\nprocedures.\n\nAnalysis\n\nPotential\n\nteam\n\nuses\n\nto assess\n\napplications\n\nof these\n\nSpace\nanalysis\n\npresented\xc2\xb0\n\nidentify\n\nrecommend\n\nby JSC\n\nincluding\n\nsection\n\nand risk\n\nCODE\n\nRoad\n\n621-0390\n\n13. ABSTRACT\nThis handbook\n\nThe\n\n10. SPONSORING/MONITORING\nAGENCY REPORT NUMBER\n\nNAME(S) AND ADDRESS(ES)\n\nAdministration\n\nSystems,\n\nthe NASA\n\nLanding\n\nLinthicum\n\nS-774\n\nNOTES\n\nInformation\n\n800 Elkridge\n\nDivision\n\n20546-0001\n\n12a. DISTRIBUTION/AVAILABILITY\nUnclas sifted/Unlimited\nAvailable\n\n8. PERFORMING ORGANIZATION\nREPORT NUMBERS\n\nOffice\n\nCenter\n\nand\n\nD.C.\n\n11. SUPPLEMENTARY\n*Loral\n\nRicaldi*\n\n77058\n\nAeronautics\n\nWashington,\n\nRon\n\nAssurance\n\nand\n\ni 9. SPONSORING/MONITORING\nNational\n\nPham*;\n\nNAME(S) AND ADDRESS(ES)\n\nQuality\n\nSpace\n\nB. Johnson\n\nHouston,\n\nTuan\n\nsoftware\nsoftware\n\nsoftware\n\ncomponents;\n\nimprovements.\n\naffect\n\nthe\n\ndescribes\n\nmeans\n\nto measure\n\nto determine\n\nand\n\nhow\n\nfailure\n\nthe\nrates\n\nmay\n\nsoftware\n\nprovides\n\nlater,\n\nreliability\n\nquantitative\n\ncomplexity\n\nrisk\n\nthis type\n\nprevent\n\nprobability\nand\n\nanalysis\nSoftware\n\nassess\n\nPerforming\n\nprocess,\n\nsection\n\ncomplexity\nlife cycle.\n\nanalysis\n\nareas\n\ntradeoffs\n\na software\n\nof analysis\n\nmuch\n\nlarger,\n\nestimation\n\nof failure-free\n\ndesign\n\nwithin\n\nduring\n\nthe\n\non code,\n\nan analyst\n\nsystem;\nearly\n\nidentify\n\ndesign\n\nsystems\n\ndesign,\n\ncomputer\n\nsystems\n\nperformance,\n\nsoftware\n\nengineering,\ndesign\n\nanalysis,\n\nprogram\nreliability\n\nand prediction\nreliability,\n\nverification,\n\nanalysis,\n\nof a computer\ncosts,\n\nor software\n\nprogram,\n\nperformance,\n\ncomputer\n\nprograms,\n\nanalysis\n\nas code\nthe\n\nstructure\nsoftware\n\ndeficiencies;\n\nand\n\nof software\n\ndevelopment\n\nreliability,\n\nprovides\n\nand describes\n\nthe\n\ntwo\n\ntools\n\na\nused\n\nand schedule.\n\n15. NUMBER OF PAGES\n91\n16. PRICE CODE\n\n17. SECURITY CLASSIFICATION\nOF REPORT\n\n18. SECURITY CLASSIFICATION\nOF THIS PAGE\n\n19. SECURITY CLASSIFICATION\nOF ABSTRACT\n\nUnclassified\n\nUnclassified\n\nUnclassified\n\nNSN 7540-01-280-5500\n\ntesting\n\nphases\n\n14. SUBJECT TERMS\ncomputer\n\nsuch\n\nto understand\n\ndifficulties.\n\noperation\n\nbetween\n\ninformation\nallows\n\n20. LIMITATION OF ABSTRACT\nUnlimited\nStandard Form 298 (Rev 2-89)\nPrescribed\n298-102\n\nby ANSI\n\nStd.\n\n239-18\n\n'