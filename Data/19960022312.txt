b'J\n221\n\nCenter for Turbulence Research\nAnnual Research Briefs 1995\n\nA preliminary\nnetworks\nfor\n\nattempt\nto\nturbulent\neddy\nBy\n\n1.\n\nMotivation\nThis\n\nand\n\nnote\n\ndescribes\n\nan\n\nof detecting\nwas motivated\n\nrameters\n\nthat\n\na simple\nactuators\n\nformulation\nembedded\n\nflush\n\nwith\n\nattempt\n\neddy\nby\n\nto use\n\non\n\nthe\n\nwall\n\ncan be obtained,\nin the wall.\nSuch\n\nthe\n\nBlackwelder\n\nBlackwelder\nthe common\n\nwall\n\nstandard\n\nneural\n\nnetwork\n\nwhen\n\nto describe\n\nit could\nactuators\n\nthe\n\npassing\n\nconceivably\nhave been\n\nnot deployed.\n\nis only a nascent\nand the eddies\n\nsucceed,\n\nmethods\n\nto couple\n\nGeneral\n\nmethods\n\nmust\n\nof the\nwill\n\ndesired\n\narrive\n\nat the\n\nstress\nwhen\ntion.\n\nIn\n\nthis\n\nsignature\n2.\n\nWhen\n\nthey\n\nare activated,\n\nactuators\nthat\n\nof the\n\nturbulent\nor reduce\n\nto the\nwill\n\noncoming\n\ndetect\n\nthe\n\nflow\n\nspace\n\nthe\n\neddy\n\nIf\n\nit is assumed\neddies near\nthe mixing,\n\nit will be necessary\n\nactuator.\n\nThis\n\npattern\n\nto be\n\ndetected\n\nwas\n\nbeto\n\nlocation\n\nwhen\n\nattempted\n\ntheir\netc.\n\nbe obtained.\n\ntemporal\n\nto know\n\nresearch\n\ndynamics\ninteraction\n\nmust\n\nand\n\non the wall in the vicinity\nof an actuator\neddy pattern\nwould\narrive\nand/or\noccur\nat\n\nwork\n\nstructure.\n\nthe\n\neddies\n\nto use the\n\nshear\n\nlocation\nto predict\nthe designated\nloc_-\n\nidentified\n\nby\n\nits\n\nvelocity\n\nonly.\n\nTechniques\n\nArtificial\ntheory\nfor\n\nNeural\na variety\n\nconfigured,\net\n\nthe\n\nIn particular,\n\nlocation\n\nmeasurements\na particular\n\neddy\n\nbe utilized\nto control\ndeveloped\nby Jacobson\n\nunderstanding\nof the interaction\nin the flow. Nevertheless,\nfor such\n\nbe found\n\nstructure.\n\nto fashion\n\nand Liu (1994),\nTung\net al. (1995),\nand others.\ncharacteristics\nthat\nthey are small and are typi-\n\nthat they will be able to interact\nconstructively\nwith the\nlocation\nto either decrease\nthe wail shear stress,\nenhance\nAt present,\nthere\ntween the actuators\n\ntools\n\npatterns\nin the wall region\nof a turbulent\nflow.\nThe\nthe desire\nto formulate\na means\nto use only flow pa-\n\nbe sensed\n\nand Reynolds(1993a),\nThese\nactuators\nhave\ncally\n\nF.\n\nobjectives\n\na means\nresearch\n\ncan\n\nRon\n\nuse neural\nclassification\n\nal.\n\nhave\n\n(1994)\n\ntraining\n\nit\n\nNetworks(ANN)\nof purposes.\n\nthe\n\nability\n\nutilized\n\nwith\n\nof 8%.\n\nto reduce\n\nthe\n\nThe\n\nused\n\nHowever\n\net al.\n\nto predict\n\n(1993)\n\nof the\nin this\n\ninstead\n\nnumerical\n\nsimulation\n\n(DNS)\n\nANN\n\nconfigured\n\nsimilar\n\nwas\n\na desired\n\nnote\ndata\nto the\n\nextensively\nin control\nthat,\nwhen properly\n\nIn fluid\npressure\n\nobtained\n\nat\n\ntwo different\nANN\nboundary\nlayer and\n\nutilized\n\nANN\n\nis similar\n\nto\n\non\n\nan\n\ndifferent\n\ncontrollers\ndeduced\n\nFaller\n\nairfoil\npitch\n\nafter\nrates.\n\nto alter\nthe\na skin friction\nboundary\n\nlayer\n\nlayer.\nthat\n\nof\n\nto generate\n\na turbulent\n\nfeed-forward\n\nmechanics,\n\nin a transitional\n\nin the\n\na model\nfrom\n\nresponse.\n\ndata\n\ndisturbances\n\nof using\n\nused rather\nof algorithms\n\nseparation\n\nairfoil\n\n(1993b)\nused\nof a modeled\n\nmagnitude\n\napproach\n\n(1993b).\n\nFan,\n\nANN\nunsteady\n\nJacobson\nand Reynolds\nshear\nstress\non the wall\nreduction\n\nto "learn"\n\nan\n\nexisting\n\nhave been\nThey consist\n\nJacobson\ndata,\n\nchannel\nnetwork\n\nand\n\nReynolds\n\nwell-resolved\nflow\n\nshown\n\nwas\nin Fig.\n\ndirect\n\nused.\n\nThe\n\n1 adapted\n\nRon F. Blackwelder\n\n222\n\n_z\n\nLAYER\n\nFIGURE 1.\n\nSchematic\n\nof a two layer\n\nONE\n\nLAYER\n\nANN with five inputs\n\nTWO\n\nand one output.\n\nfrom Jacobson and Reynolds (1993b). This two-layer network consists of five inputs,\ntwo internal nodes, and a single output.\nIt is designated\nas a 5-2-1 network which\nrepresents\nthe number\nof inputs,\nnodes, and outputs.\nIn a practical\ndevice, the\ninputs would correspond\nto signals obtained from a series of sensors located on the\nwall of the flow. Thus only data obtained in the wall region was used as input into\nthe ANN. It was further assumed that for practical\napplication\nthe output from an\nANN would be utilized to operate an actuator\nlocated at a point, p, on the wall.\nFor the work presented\nhere, the input to the ANN utilized imax inputs obtained\nfrom the two velocity components\nparallel to the wall. Typically\nthis data was\nobtained at the first resolved calculation\npoint lying above the wall and hence represented the wall shear, Ou/Oy and Ow/Oy, at the various data points. The choice of\nthese variables and their physical location with respect to the point, p, are crucial\nbecause this is one of the primary\nmeans by which the physics enter the problem.\nThe number of inputs,/max,\nvaried during the course\n50. The inputs included data obtained\nfrom locations\nAz, from the position p. Usually Ou/Oy and Ow/Oy\n\nof the investigation\nfrom 5 to\nupstream,\nAx, and spanwise,\nwere both used from a single\n\nspatial location; hence, the number of spatial locations providing\ndata was always\nless than or equal to the number of inputs,/max.\nNeural networks as shown in Fig. 1 are quite flexible and can consist of a large\nnumber of inputs and layers. Few rules exist for their\nuser to develop a network best suited to his application.\navailable is that more than one layer must be utilized\nlinearities\ninputs,\n\nin a problem.\nnodes,\n\nand layers\n\nIn addition,\nto a minimum\n\nit behooves\nto reduce\n\ndesign and it is left to the\nOne of the few guidelines\nto adequately\nmodel non-\n\nthe user\n\nto keep the number\n\nthe computational\n\neffort.\n\nof\n\nNeural\n\nnetworks\n\nand eddy classification\n\n223\n\nThe ANN used in this investigation\nconsisted of two layers with two to five nodes\nin the second layer and a single output.\nThe weights were designated\nas Wiyk, where\nthe first subscript\ndenotes the node in the previous layer, the second subscript\nis\nthe output node for the present layer, and the third subscript\nis the layer number.\nA bias input is included in each layer and thus i varies from zero to/max.\nLikewise\nj and k have values between unity and jmax and kmax respectively.\nThus a total of\n(/max + 1)jm_x + (jmax + 1) coefficients,\nvalues were chosen as random numbers\nLetting\n\nIi be the\n\ni th\n\ninput,\n\nWijk, were used in the ANN.\nand adjusted later by training.\n\nthen the linear\n\nsum of the outputs\n\nHjk = Iiwijk, was scaled to lie between +1 by the sigmoid\ntaken to be the hyperbolic\ntangent function;\n\nTheir\n\ninitial\n\nfrom the first layer,\n\nfunction,\n\nF, which\n\nwas\n\nZjk = tanh(Hjk)\nZjl are the outputs\nfrom the first layer and the input into the second layer. By\nconvention,\nthe output of the last layer, O, is not passed through the sigmoid; hence,\na two layer ANN with a single output is simply O = H12.\nThe value of the weights were found by training which used a back propagation\nalgorithm\ndescribed\nby Hertz et aL (1991).\nThis requires a priori knowledge\nof\na target vector, \xc2\xa2, which the ANN attempts\nto predict.\nChoi et al. (1994) have\nshown that a 25% drag reduction\ncan be accomplished\nby using the normal velocity\ncomponent\nat y+ = 10 to prescribe\nsuction and blowing at the wall directly below\nits location.\nUsing this result, the target chosen for the present study was the scalar\nvalue of the normal velocity component\nlocated at y+ = 10 above the point p. The\nDNS data were used to extract rrtmax training sets; each consisted of the pattern of\nthe u and w data near the wall in the neighborhood\nof p and the value of the target,\n\xc2\xa2 = v(p,, y+ = 10,pz).\nAs each training set was presented\nto the ANN algorithm,\nthe standard\ndeviation\nwas computed\nfrom the difference between the target and\nthe ANN output over the m sets of data as\nmmax\n\n=\n\nO\n\n(w ik)]\n\nm=l\n\nTo minimize the standard\ndeviation,\nthe gradient descent algorithm\nsuggests\ning wijk by an amount zkwijk proportional\nto the gradient\nof e2 given by\n\nchang-\n\n_2\n\nAwijk\nwhere\n\nit is an arbitrary\n\nim_x-2-1 network,\n\nconstant\n\nthe changes\n\n= -it c3wijk\n\nof order\n\nunity.\n\nfor the weight\n\n= it\n\nThus\n\nfor the output\n\ncoefficients\n\nc30m .\xc2\xa2m\n\n-\n\n0m\n\nare given\n\n)\n\nby\n\nstage\n\nof an\n\n224\n\nRon F. Blackwelder\n0.6\n\n0.4\n\n0.2\n0\n\n,_\n\n0.0\n\n-0.2\n\n-0.4\n\n!\n\n-0.6\n\n\xe2\x80\xa2\n\n!\n\n-0.4\n\n\xe2\x80\xa2\n\n-0.2\n\ni\n\n\xe2\x80\xa2\n\n0.0\n\n|\n\n-\n\n|\n\n0.2\n\n0.4\n\n0.6\n\nTarget Velocity\nFIGURE 2.\nPredicted\nvelocity\ntions of the test patterns.\nwhere the\n0 m = H_\n\nsum over\n= wk12Z_\n\nversus\n\nm is implied.\nso that\n\nthe target\n\nFor such\n\nvelocity\n\na network,\n\nm C_Wkl2\n\nAwi]2\n\nafter\n\nthe\n\none hundred\n\noutput\n\nitera-\n\nvariable\n\nis\n\ncm\n\n= pZka 0---_i12o\n\nor\n/_Wil2\n\nm\n= p_ m Zil\n\nwhere sm = Cm _ Om. This specifies the weights in the second layer. In a similar\nmanner, the back propagation\nalgorithm can determine the weights in the first layer.\n3.\n\nResults\nThe main\n\nresults\n\nof this study\n\nwere obtained\n\nby examining\n\nthe predicted\n\noutput\n\nvelocity as a function of the target velocity for the mmax patterns\nafter training.\nExcept where noted, the results are for a 10-4-1 neural network.\nTypically,\n1024\ntest patterns\nwere taken from one temporal\nset of data and used in the training.\nFigure 2 illustrates\nthe output for ten values of cgu/Oy and Ow/cgy taken at Ax = 0\nand at five spanwise locations,\nAz = 0, +13 and +26, with respect to p. The best\nresults as determined\nby the standard\ndeviation\nwere obtained\nwhen Ax = 0; e.g.\ne = 0.062 for the data in Fig. 2 with Ax = 0. At Ax = +20, e increased to 0.10\nand at Ax = -4-40, e = 0.14.\nThe training\nalgorithm\nattempted\nto calculate\nvalues of the weights that minimize\n\nthe\n\ndifference\n\nbetween\n\nthe output\n\nand\n\nthe\n\ndesired\n\ntarget.\n\nHence\n\nit is not\n\nNeural\nunreasonable\n\nto assume\n\nindication\ncould\n\nof the\n\nbe used\n\na valid\n\nwhich\n\nthan\n\nwas\n\nwere\n\npresented\n\neven\n\nthough\n\nthrough\n\nthe\n\nhaving\nthe\n\nOn\n\nthe\n\nwas\n\ndue\n\nlayer.\n\nused\n\nIt\n\nfor\n\nwhen\n\na random\n\nconsistent;\n\nthe\n\nof correlation\nof the weights\n\non that\n\nThe\nwas\n\ntaken\n\nvalue\n\nof 0.06\n\nprovided\nweights\n\nfor the\n\nwere\n\nnot\n\nconstant\n\niterations\nthrough\nnot constant\nafter\nfound\nconstant\nThe\n\nat\n\nall but\n\nthat\n\nconvergence\nweights\n\nof the\n\nroot\n\nthe\n\nfinal\nof the\n\nat each\nmean\n\nand\n\nsquare\n\nweights\n\nvalue\n\nhelp\n\nthan\n\nvalue\n\nthrough\noften\n\nweights\n\niteration.\nvalue\n\nwas studied\nof the\n\nof the\n\nfrom\n\nthe\n\nother\n\ninputs,\n\nthe\nthe\n\nit often\n\ntook\n\nthan\n\n200\n\nwas\n\na much\n\nmore\n\nprediction\n\nWhen\n\nsignificantly\n\nthe\n\nby adding\n\nof the\nto\n\nthe\nafter\n\none\n\ndid\n\nrandom\n\nvalues\n\nthe\n\niterations\nof the\nhundred\n\nof the weights\nno convergence\n\n(i.e.\n\nwere\nwas\n\nconverge\n\noscillatory\n\nto a\n\nof weights.\n\nrandom\n\nof approximately\n\neliminated\n\ntarget\n\nvalues\n\nweights\n\ndither\n\nand\n\na nominal\n\nFurther\n\nHowever\n\ncould\n\nweights.\n\nrapidly\n\npatterns.\n\nthe initial\n\namplitude\n\nweights\n\nguide\n\nto a good\nof e decreased\n\nvery\nthat\n\nand\n\na better\n\nof the\n\ntest\n\nof the\nhelp in\n\nthe target\nwas\n\nmagnitude\n\nthe\n\nindicator\n\nzero and remained\na good indication\n\ninsight\n\nthe\n\nbetter\n\nan examination\nwas of little\n\nin predicting\n\nchanging\n\nupon\n\nA dither\n\nweights\nfor\n\nwith\n\ndeviation.\n\noscillated.\n\ndepended\n\nthe\n\nassociated\n\nIn general,\nof iterations\n\nphysical\n\nstandard\n\nwere\n\nas one\n\nalong\n\nis, when\n\nthan\n\na\n\nANN\n\nweights\n\nof the\n\nlarger\n\nof\n\nwas\n\nin any\n\nThat\n\nset of patterns.\nIn some cases, the values\nthousand\niterations.\nIn a couple of cases\n\nrather\n\nvalue,\n\nto the\n\nthe\nten\n\nthat\n\nthe\n\nin the\n\nrelationship\n\nof the\n\nweights\napproached\nthis was considered\n\nof little\n\nto six passes\n\ndecrease\n\ntrue\n\nweights\n\nchannel\n\nto converge\n\nthat\n\nwas\n\nanalyzed\n\nsmall,\nthe values\nwas not propagated\n\nproduct\n\nHowever\n\npropagation\n\nalgorithm\n\nThis\n\nbe found\n\nused\n\ntargets\n\nalready\n\ninputs.\n\nof the\n\nthe\n\nmore\n\nbefore this result\nwas achieved,\nwhich was\nabove,\nthe products\nof the weights\nfrom\n\nvariables\n\nfound\n\nthree\n\nlittle\n\nwas\n\nvalue\n\nthe\n\ncontaining\n\nand\n\nThis\n\nmuch\n\nto be\nwith\n\nthe value of the\nwijl,\nthat\nwere\n\ninput\n\nproduct\n\nthe\n\nwas\n\nzero.\n\nfound\n\ninput\n\nIt was\nafter\n\nvery\n\nchannel,\n\ndata\nstated\n\nwas indeed\nit was\n\nof appropriate\ntime\n\nthe\n\nwith the target\nvalue.\nafter\na fixed number\n\nchannel\n\nIn general,\n\nof concern.\n\nhave\nlayer,\n\ntargets\n\nof correlated\n\nthe\n\napproached\n\nthrough\n\nthat\n\nappeared\n\nto change.\n\nthere.\n\nbe an\nweights\n\ncorrelated\n\nwill probably\n\npath\n\nchoosing\nappropriate\ninputs.\nBut if the\nsmall for a large\nnumber\nof iterations,\n\nindicator\n\nThis\n\nto the\n\nof those\n\nvaluable\n\nnamely\n\nultimately\n\nlayers\n\none\n\ndata\n\nwould\nof the\n\ncontinued\n\nweight\n\nfound\n\non\n\nthat\n\nthrough\nthe set of pattern\nto be excessively\nlong. As\n\nbe pruned.\n\nrelated\n\nand\n\nindicator\n\nvariable\n\nthe input\n\nuseful.\n\ncontinued\n\nsmaller\n\nwas\n\niterations\ndeemed\n\nof the lack\nmagnitude\n\nweights\n\nwas strongly\n\na repeat\n\nnetwork\n\nas an input\nlayer\n\nhand,\n\nmore\n\ndifferent\n\nleast\n\nas training\n\nweights\n\nto the\n\nrandom\n\nthe\n\nthe\n\nweight\n\nwere\n\nwas a better\n\nsecond\n\nother\n\nthe\n\nin the\n\none\npath\n\nwere\n\nare\n\nHowever,\n\npresented\n\nlayer\n\nthan\n\nand\n\nresults\n\ncalculated\n\nIf so, an examination\n\nparameter\n\nfor the\n\nalgorithm,\n\nnon-linearity\n\nvalue\n\n225\n\nIf the weight\nin the second\nlayer became\nfirst layer were often large since their effect\n\nsecond\n\nmore\n\nthe first\nchannels.\n\nthat\n\ninput\n\nunity.\n\ntargets\n\npropagation\n\ntarget\n\nof the\n\ninput.\n\nweights\n\n0.1 except\n\nto the\nthe\n\nof the\n\nmagnitude\n\nas the\n\nof order\n\nby the algorithm.\nthe weights\nin the\nresult\n\nclassification\n\nThis was tested\nby letting\none of the inputs\nThe algorithm\nproduced\nweights\nin the first\n\nsmaller\n\ntarget\n\neddy\n\nparticular\n\nthose\n\nas long\n\nand\n\nthe\n\nof that\n\nto prune\n\nvariable.\nvelocity.\n\ntypically\n\nthat\n\nvalue\n\nassumption\n\ntarget\ntarget\n\nnetworks\n\none\n\nnoise)\nper\n\nnature\n\ncent\nof\n\n226\n\nRon\n\nthe weights,\nbut did not seem\nvalue of the standard\ndeviation\nDuring\nvalues\n\nthe\n\nwas\n\nthat\n\npresenting\n\nthe\n\nweights\n\nstandard\n4.\n\niterations,\n\nusually\n\nF. Blackwelder\n\nto speed their convergence.\nHowever,\nwas found\nwith the dither.\n\nthe\n\nset\n\nof approximately\n\n1024\n\npresented\n\nsequentially;\n\ni.e.\n\npatterns\n\nin a random\n\nfashion\n\nthe\ndid not\n\nget\n\ndeviation\n\ncaught\n\ndecreased\n\ninto\n\nm\n\na cyclical\n\nslightly\n\n=\n\npattern\n\n1,2,3\n\nhad\n\ndata\n\n....\n\nIt\n\nseveral\n\npattern\n\na slightly\n\nand\n\nsmall\n\nand\n\nwas\n\ntarget\n\ndiscovered\n\nadvantages.\n\noscillate.\n\nFirst,\n\nSecondly,\n\nthe\n\nto e < 0.05.\n\nConclusions\nThe\n\ntion\n\nartificial\n\nthe\n\nof the\noutput\n\nnumber\n\nneural\n\ndesired\n\nnetworks\n\nresults.\n\nvalue\n\nwas\n\nof iterations\n\ndata\n\n6%\n\nto converge,\n\nuse\n\nin this\n\nstandard\n\ntypically\n\nimproving\ntheir speed.\nprovide\nimprovements\nthe spatial\n\nused\n\nThe\n\nexercise\n\nprovided\n\ndeviation\n\nor less.\n\nHowever,\n\nthe\n\nthe\n\nthat\n\nsuggesting\n\na reasonable\n\nbetween\nwork\n\nmore\n\nstudy,\n\nmay\n\nalso\n\nspeed\n\npredicvalues\n\nand\n\nalgorithms\n\ntook\n\nneeds\n\nto be devoted\n\nPossible\nuses of the conjugate\ngradient\nin the algorithms.\nThe use of temporal\n\nin this\n\ntarget\n\na large\nto\n\nor other tools could\ndata,\nin addition\nto\n\nconvergence.\n\nAcknowledgments\nThe\n\nauthor\n\nwished\n\nto thank\n\nTom Bewley\nfor his helpful\nthe course of this research.\n\nStu\n\nJacobson\n\ndiscussions\n\nfor the\n\nof control\n\nuse\n\nof his ANN\n\ntheory\n\nand\n\nalgorithm\n\nhis comments\n\nand\nduring\n\nREFERENCES\nBLACKWELDER,\nembedded\n193,\nCHOI,\n\nR.\n\nF. & LIU,\n\nin a boundary\n\nTurbulence\nH.,\n\nControl,\n\nMOIN,\n\nP.,\n\nin wall-bounded\nFALLER,\nand\n\nX.,\n\n1994\n\nJ. Fluid\n\nAerospace\n\nHOFMANN,\n\nJ.,\n\nNeural\n\nL.,\n\n_\n\nAIAA\n\nShear\n\nANDERS,\n\nK.,\n\nComputation.\n\nwall\n\nS. A.\n\nMech.\n\nshear\n\nference,\nTUNG,\nS.,\nControl\n\nstress\n\nJuly\n\nusing\n1993,\n\nShear\n\nFED-Vol.\n\nfor\n\ndrag\n\nreduction\n\n75-110.\n\nJan\n\nT.\n\nActive\n\n1993\n\nConference,\n\n& PALMER,\n\nW.\n\ncontrol\n\nMeeting,\n\nHERBERT,\nFlow\n\nturbulence\n\nR.\n\nJuly\nG.\n\n1991\n\nPub.\n\n10-13,\n\n6-9,\n\n1994,\n\nflow\n\n1993,\n\nAIAA-94-0532.\n\ncontrol\n\nwith\n\nRedwood\n\nto the\nCity,\n\nTheory\n\nself-learning\n\n1993b\n\nneural\n\nActive\nnetworks.\n\ncontrol\nAIAA\n\nof\n\nCA.\n\nC. 1993a Active boundary\nlayer control\nBull. Am. Phy. Society.\n38, 12, 2197.\nC.\n\nneural\n\nAIAA-93-3273.\n\nIntroduction\n\nCo.,\n\nof boundary\nShear\n\nFlow\n\nusing\n\nlayer\nCon-\n\nAIAA-93-3272.\n\nHONG, W.,\nHUANG,\nJ., Ho,\nC.-H.,\nof a streamwise\nvortex\nby a mechanical\n\non Turbulent\n\nvortices\n\nDivision.\n\n_: LUTTGES,\nM. W.\nReal-time\nprediction\nunsteady\nseparated\nflow fields using\nneural\n\nSciences\n\nREYNOLDS,\n\n6-9,\n\nof streamwise\n\nEngineering\n\n262_\n\nAddison-Wesley\n\n_:\n\nof break-down\n\nFluids\n\nActive\n\nJACOBSON,\nS. A. & REYNOLDS,\nW.\nflush-mounted\nsurface\nactuators.\nJACOBSON,\n\nDelay\n\nASME\n\n9, ASME.\n\nKIM.\n\nflows.\n\nAIAA\n\nnetworks.\nHERTZ,\n\n_\n\n1994\n\nW.\nE.,\nSCHRECK,\nS. J.,\ncontrol\nof three-dimensional\n\nnetworks.\nFAN,\n\nD.\nlayer.\n\nFlows,\n\nPenn.\n\nState\n\nUniversity,\n\nLuI,\nC.,\nactuator.\nAugust\n\n&\n\nTAI,\nTenth\n14-16.\n\nY.-C.\n1995\nSymposium\n\n'