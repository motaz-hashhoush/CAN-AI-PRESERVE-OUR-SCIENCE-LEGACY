b'Transition-Independent Decentralized Markov Decision\nProcesses\nRaphen Becke;\n\n, Shlomo Zilberstein, Victor Lesser, Claudia V. Goldman\nDepartment of Computer Science\nUniversity of Massachusetts\nAmherst, MA 01003\n\n{ raphen,shlomo,lesser,clag) @ cs.umass.edu\n\nABSTRACT\nThere has been substantial progress with formal models for\nsequential decision making by individual agents using the\nMarkov decision process (MDP). However, similar treatment\nof multi-agent systems is lacking. A recent complexity result, showing that solving decentralized MDPs is NEXPhard, provides a partial explanation. To overcome this complexity barrier, we identify a general class o transitionf\nindependent decentralized MDPs that is widely applicable.\nThe class consists of independent collaborating agents that\nare tied up by a global reward function that depends on both\nof their histories. We present a novel algorithm for solving\nthis class of problems and examine its properties. The result\nis the first effective technique t o solve optimally a class of\ndecentralized MDPs. This lays t h e foundation for further\nwork in this area on both exact and approximate solutions.\n\n1. INTRODUCTION\nThere has been a growing interest in recent years in formal\nmodels to control collaborative multi-agent systems. Some\nof these efforts have focused on extensions of the Markov decision process (MDP) to multiple agents, following substantial progress with the application of such models t o problems involving single agents. Examples of these attempts\ninclude the multi-agent Markov decision process (MMDP)\nproposed by Boutilier [2], the Communicative Multiagent\nTeam Decision Problem (COM-MTDP) proposed by Pynadath and Tambe [ll], and the decentralized Markov decision\nprocess (DEC-POMDP and DEC-MDP) proposed by Bernstein, Givan, Immerman and Zilberstein [l]. The MMDP\nmodel is based on full observability of the global state by\neach agent. Similar assumptions have been made in recent\nwork on multi-agent reinforcement learning (31. We are interested in situations in which each agent might have a different partial view of the global state. This is captured by\n*Student author\n\nS u b m i t t e d to the Second I n t e r n a t i o n a l j o i n t Confere n c e on A u t o n o m o u s A g e n t s and Multi-Agent Systems.\n\nthe DEC-POMDP model, in which a single global Markov\nprocess is controlled by multiple agents, each of which receives partial information about the global state after each\naction is taken. A DEC-MDP is a DEC-POMDP with the\nrestriction that at each time step the agents\xe2\x80\x99 observations\ntogether uniquely determine t h e global state.\nA recent complexity study of decentralized control shows\nthat solving such problems is extremely difficult. The complexity of both DEC-POMDP and DEC-MDP is NEXPhard, even when only two agents are involved [l]. This is in\ncontrast to the best known bounds for MDPs (P-hard) and\n6.\nPOMDPs (PSPACEhard) [9, 1 The few recent studies of\ndecentralized control problems (with or without communication between the agents) confirm that solving even simple\nproblem instances is extremely hard [ll, 14).\nOne way t o overcome this complexity barrier is to exploit\nthe structure of the domain offered by some special classes of\nDEC-MDPs. We study one such class of problems, in which\ntwo agents operate independently but are tied together by\nsome reward structure t h a t depends on both of their execution histories. The model is motivated by the problem\nof controlling the operation o multiple space exploration\nf\nrovers, such as the ones used by NASA t o explore the surface of Mars [13].Periodically, such rovers are in communication with a ground control center. During that time, the\nrovers transmit the scientific data they had collected and\nreceive a new mission for the next period. The mission involves visiting several sites at which the rovers could take\npictures, conduct experiments, and collect data. Each rover\nmust operate autonomously (including no communication\nbetween them) until communication with the control center\nis feasible. Because the rovers have limited resources (computing power, electricity, memory, and time) and because\nthere is uncertainty about the amount of resources that are\nconsumed a t each site, a rover may not be able to complete\nits mission entirely. In previous work, we have shown how to\nmodel and solve the single rover control problem by creating\na corresponding MDP [15].\nWhen two rovers are deployed, each with its own mission, there is important interaction between the activities\nthey perform. Two activities may be complementary (e.g.,\ntaking pictures of the two sides of a rock), or they may\nbe redundant (e.g., taking two spectrometer readings of the\nsame rock). Both complementary and redundant activities\npresent a problem: the global utility function is no longer\nadditive over the two agents. When experiments provide\nredundant information, there is little additional value to\n\ncompleting both so the global value is subadditive. When\nexperiments are complementary, completing just one may\nhave little value so the global value is superadditive. The\nproblem is t o find a pair of policies, one for each rover, that\nmaximizes the value of the information received by ground\ncontrol.\nWe develop in this paper a general framework t o handle\nsuch problems. Pynadath and Tambe [ll]studied the performance of some existing teamwork theories from a decisiontheoretic perspective, similar t o the one we develop here.\nHowever, our technique exploits the structure of the problem to find the optimal solution more efficiently.\nA simplified version of our problem has been studied by\nOoi and Wornell [7], under t h e assumption that all agents\nshare state information every K time steps. A dynamic programming algorithm has been developed t o derive optimal\npolicies for this case. A downside of this approach is that the\nstate space for the dynamic programming algorithm grows\ndoubly exponentially with K . The only known tractable algorithms for these types of problems rely on even more assumptions. One such algorithm was developed by Hsu and\nMarcus [SI and works under the assumption that the agents\nshare state information every time step (although it can take\none time step for the information to propagate). Approximation algorithms have also been developed for these problems, although they can at best give guarantees of local o p\ntimality. For instance, Peshkin et al. [IO]studied algorithms\nthat perform gradient descent in a space of parameterized\npolicies.\nThe rest of the paper is organized as follows. Section 2\nprovides a formal description of the class of problems we\nconsider. Section 3 presents the coverage set algorithm,\nwhich optimally solves this class of problems, and proves\nthat the algorithm is both complete and optimal. Section\n4 illustrates how the algorithm performs on a simple scenario modeled after the planetary rover. We conclude with\na summary of the contributions of this work.\n\n2. FORMAL PROBLEM DESCRIPTION\nIn this section we formalize the rovers control problem\n\nas a transition-independent, cooperative, decentralized decision problem. The domain involves two agents operating\nin a decentralized manner, choosing actions based upon their\nown local and incomplete view of the world. The agents are\ncooperative in the sense t h a t there is one value function for\nthe system as a whole that is being maximized\'.\n1.\nDEFINITION A factored, 2-agent, DEC-MDP i s defined\nby a tuple < S , A , P, R >, where\n0\n\n0\n\nS = SIxSz is afinite set of states, with a distinguished\ninitial state (sh,sa). S indicates the set of states of\ni\nagent i .\n\nA = A1 x A2 i s a f i n i t e set of actions. A , indicates the\naction taken by agent i .\nP is a transition function. P ( ( s i ,si)/(sI, ( a l , a z ) )\nSZ),\ni s the probability of the outcome state (si,si) when the\naction pair ( a l , az) i s tuken in state (SI,\nsz).\n\n\'In this paper, we assume that the agents cannot communicate between execution episodes, so they need to derive\nan optimal joint policy that involves no exchange of information. We are also studying decentralized MDPs in which\nthe agents can communicate at some cost[4].\n\n0\n\nR is a reward function. R((s1,SZ), ( a \' , a ~ )(si,si)) i s\n,\nthe reward obtained from t&ng actions ( a 1 , a z ) from\nstate (SI, Z ) and transitioning to state (si,si).\nS\n\nWe assume that agent i observes si, hence the two agents\nhave joint observability. We call components of the factored\nDEC-MDP that apply t o just one agent local, for example\nsi and a, are local states and local actions for agent i. A\npolicy for each agent, denoted by ni,is a mapping from local\nstates t o local actions. ai(si) denotes the action taken by\nagent i in state s i . A j o i n t policy is a pair of policies, one\nfor each agent.\n\n2.\nDEFINITION A factored, 2-agent DEC-MDP is said to\nbe transition independent i f there exist PI and Pz such\nthat\nP((s~,s;)l(sl,s2),(ar,az))\n\n= Pl(s;lsl,al). PZ(s;lsz,az)\n\nThat is, the new local state of each agent depends only on\nthe previous local state and the action taken by that agent.\n\n3.\nDEFINITION A factored, 2-agent D E C - M D P i s said to\nbe r e w a r d i n d e p e n d e n t af there exist R1 and R such that\nz\nR ( ( s i , s z ) , ( a i , a z ) , ( s ~ , ~ R i)()s i , a i , s ; )\n=~\n\n+ Rz(sz,az,s;)\n\nThat is, the overall reward is composed of the sum of two\nlocal reward functions, each of which depends only on the\nlocal state and action of one of the agents.\nObviously, if a DEC-MDP is both transition independent\nand reward independent, then it can be formulated and\nsolved as two separate MDPs. However, if it satisfies just\none of the independence properties it remains a non-trivial\nclass of DEC-MDPs. We are interested in the general class\nthat exhibits transition independence without reward independence. The Mars rovers application is an example of\nsuch a domain. The local states capture the positions of\nthe rovers and the available resources. The actions involve\nvarious data collecting actions at the current site or the decision to skip t o the next site. The overall reward, however,\ndepends on the value of the d a t a collected by the two rovers\nin a non-additive way.\n\n2.1 Joint Reward Structures\nWe now introduce further structure into the global reward\nfunction. To define it, we need t o introduce t h e notion of a n\noccurrence of an event during the execution of a local policy.\n4.\nao,\nDEFINITION A history, @ = [so, SI,a l , ...I i s a sequence that records all the local states and actions f o r one of\nthe agents, starting wiuI the local initial state of that agent.\n\nDEFINITION A p r i m i t i v e e v e n t , e = ( s , a , s \' ) 2s a\n5.\ntnplet that zncludes a state, a n actzon, and an outcome state.\nA n event E = { e l , e z , ..., e,,} i s a set of pnmztzve events.\nDEFINITION A pnmztzve event e = ( s ,Q , s\') occurs a\n6\nn\nhzstory @, denoted @ b e t f the tnplet ( s ,a , s\') appears as a\nsubsequence of @. A n event E = { e l , e z , e\n,}\n,\noccurs a\nn\nhistory @, denoted @ E zfl\n\n+\n\n3eEE: @+e\nEvents are used t o capture the fact that an agent accomplished some task. In some cases a single local state may be\n\n.\nsufficient t o signify the completion of a task. But because of\nthe uncertainty in the domain and because tasks could be\naccomplished in many different ways, we generally need a\nset of primitive events to capture the completion of a task.\nTo illustrate this in the rover example, we will define the\nstate to be < t , l > where t is the time left in the day and\n1 is the location of the current data collection site. The actions are skip and collect. The event took picture of site 4\nwould be described by the following set of primitive events:\n\n{(< t , 1 > , a , < t \' , t >)I < t , l >=< * , 4 >,\n.\na-= collect, < t\', I\' >=< *, 5 >}..\nDEFINITION A primitive event is said to be p r o p e r if\n7.\ni t can occur at most once in any possible history of a given\nMDP. That is:\n\nV@ = @lea2 :\n\n-(@I\n\ne) A\n\nk e)\n\nDEFINITION An event E = { e l , e2, ...,e,} is said to be\n8.\nproper if it consists of mutually exclusive proper primitive\nevents with respect to some given MDP. That is:\n\nV@ -3i # j : ( e , E E A e j E E A @\n\ne , A @ k e,)\n\nWe limit the discussion in this paper t o proper events.\nThey are sufficiently expressive for the rover domain and\nfor the other applications we consider, while simplifying the\ndiscussion. Later we show how some non-proper events can\nbe modeled in this framework.\nDEFINITION Let @ 1 andQ.1 be histories of two tmnsition9.\nindependent MDPs. A j o i n t r e w a r d s t r u c t u r e\nP=\n\n[ ( E : ,E?, ci), .-., (EA,\nE:,\n\n&)I,\n\nspecifies the reward (or penalty) ck that is added to the global\nE: and Cpz E;.\nvalue function if @ I\nWe limit the discussion in this paper t o factored DECMDPs that are transition-independent, and whose global\nreward function R is composed of two local reward functions R1 and Rz for each agent plus a joint reward structure p. This allows us t o refer to the underlying MDP,\n< Si,Ai, P,, > even though the problem is not reward\nR,\nindependent.\nGiven a local policy, n, the probability that a primitive\nevent e = ( s , a , s \' ) will occur during any execution of I F ,\ndenoted P ( e J n )\ncan be expressed as follows.\n\n--.\n\nP(el.rr) =\n\nC\n\nP ~ ( S I I F ) P ( U I SF)P(S\'IS, a )\nI,\n\n(1)\n\nt =O\n\nwhere P ( l r is the probability of being in state s at time\nts7)\nstep t . Pf(sln)can be easily computed for a given MDP\nfrom its transition model; P ( a l s , x ) is simply 1 if x ( s ) =\na and 0 otherwise; and P ( s \' l s , a ) is simply the transition\nprobability. Similarly, the probability that a proper event\nE = { e l , ez, ..., e,} will occur is:\n\nWe can sum the probabilities over the primitive events in E\nbecause they are mutually exclusive.\n\nDEFINITION Given a joint policy (n1, and a joznt\n10.\n7r2)\nreward structure p, the j o i n t v a l u e is:\nn\n\nJV(PIR1,Az) =\n\nP(Ei\'Ini)P(E?lR?)G\ni=l\n\nDEFINITION A global value f u n c t i o n of a transition11.\nindependent decentmlized M D P with respect to a joint policy\n( I F ~ , I F Z ) , local reward functions R1, R2 and a joint reward\nstructure p is:\n\nwhere V,,fsb) is the standard value of the underlying M D P\nf o r agent i at\ngiven polzcy I F , .\n\nSA\n\nWhile Vn(s) is generally interpreted t o be the value of\nstate s given policy n, we will sometimes refer t o it as the\nvalue of IF because we are only interested in the value of the\ninitial state given n.\nThe goal is to find a joint policy that maximizes the global\nvalue function.\nDEFINITION An o p t i m a l j o i n t policy, denoted\n12.\n(al,\nnz)*,is a pair of policies that muximize the global value\nfunction, that is:\n\n( m IFZ)\'\n,\n2.2\n\n= argmax,;.,;GV(n;,\n\n4)\n\nExpressiveness of the Model\n\nTkansition-independent DEC-MDPs with a joint reward\nstructure may seem to represent a small set of domains, but\nit turns out that the class of problems we address is quite\ngeneral. Many problems that do not seem to be in this class\ncan actually be represented by adding extra information to\nthe global state. In particular, some problems that do not\nnaturally adhere t o the mutual exclusion among primitive\nevents can be handled in this manner. The mutual exclusion\nproperty guarantees that exactly one primitive event within\nan event set can occur. We discuss below some cases violating this assumption and how they can be treated within the\nframework we developed.\nA t least one primitive event - Suppose that multiple\nprimitive events within the set can occur and that an additional reward is added when at least one of them does occur.\nIn this case the state can be augmented with one bit per\nevent that is initially 0. When a primitive event in t h e set\noccurs, the bit is set t o 1. If we redefine each primitive event\nso that the corresponding bit switches from 0 to 1 when it\noccurs, we make the event set proper because the bit can\nswitch from 0 t o 1 only once in every possible history.\nA L primitive events - Suppose that an event is comL\nposed of n primitive events, all of which must occur to trigger the extra reward. In this case, each local state must\nbe augmented with n bits, one bit for each primitive event\n(ordering constraints among the primitive events could be\nexploited to reduce the number of bits necessary). The new\nset of events that describe the activity are those that flip\nthe last bit from 0 t o 1.\nCounting occurrences - Suppose that an event is based\non a primitive event (or another event set) repeating a t least\nn times. Here, the local state can be augmented with logn\nbits to be used as a counter. The extra reward is triggered\nwhen the desired number of occurrences is reached, at which\npoint the counting stops.\n\nTemporal constraints - So far we have focused on global\nreward structures that d o not impose any temporal constraints on the agents. Some temporal constraints can also\nbe represented in this framework if time is enumerated as\npart of the local states. For example, suppose that event\nE1 in agent 1 facilitates event E2 in agent 2, that is, the\noccurrence of E1 before E2 leads t o some extra reward when\nE2 occurs. If we include time in local states, then we could\ninclude in the joint reward structure a triplet for every possible combination of primitive events that satisfy the temporal\nconstraint. Obviously, this leads to creating large numbers\nof event sets. A more compact representation of temporal\nconstraints remains t h e subject of future research.\nTo summarize, there is a wide range of practical problems that can be represented within our framework. Nontemporal constraints tend to have a more natural, compact\nrepresentation, but some temporal constraints can also be\ncaptured.\n\n3. COVERAGE SET ALGORITHM\nIn this section, we develop a general algorithm for solving\ntransition-independent, factored, 2-agent DEC-MDPs with\na global value function shown in DEFINITION The algo11.\nrithm returns the optimal joint policy.\nSo far, no optimal algorithm has been presented for solving the general class of DEC-MDPs, short of complete enumeration and evaluation of policies. Some search algorithms\nin policy space and gradient decent techniques have been\nused to find approximate solutions, with no guarantee of\nconvergence in the limit on the optimal joint policy (e.g.,\n[IO]). Here we present a novel algorithm that utilizes the\nstructure of the problem to find the optimal joint policy.\n(Throughout the discussion we use i t o refer to one agent\nand j to refer t o the other agent.)\nThe algorithm is divided up into three major parts:\n1. Create augmented MDPs. An augmented MDP represents one agent\xe2\x80\x99s underlying MDP with an augmented\nreward function.\n2. Find the optimal coverage set for the augmented MDPs,\nwhich is the set of all optimal policies for one agent\nthat correspond t o any possible policy of the other\nagent. As we show below, this set can be represented\ncompactly.\n3. Find for each policy in the optimal coverage set the\ncorresponding best policy for the other agent. Return\nthe best among this set of joint policies, which is the\noptimal joint policy.\n\nPseudo-code for the coverage set algorithm is shown in\nFigure 1 . The main function, CSA, takes a factored DECMDP and a joint reward structure as described in Section\n2, and returns a n optimal joint policy.\n\n3.1\n\nCreating Augmented MDPs\n\nThe first step in the algorithm is t o create the augmented\nMDPs, which are derived from the underlying MDPs for\neach agent with an augmented reward function. The new reward is calculated from the original reward, the joint reward\nstructure and the policy of the other agent. The influence\nof the other agent\xe2\x80\x99s policy on the augmented MDP can be\ncaptured by a vector of probabilities, which is a point in the\nfollowing parameter space.\n\nfunction CSA(MDP1, MDPz, p )\nreturns the optimal joint policy\ninputs: MDPI, underlying MDP for agent 1\nMDP2, underlying MDP for agent 2\np, joint reward structure\noptset +- COVERAGESET( MDPl, p )\nvalue + -cu\njointpolicy\n{}\n/* find best joint optimal policy */\nfor each policy1 i n optset\npolicy2 +- SOLVE(AUGMENT( MDPz,policyl, p ) )\nv\nGV({policyl,policyz}, MDP1, MDPz, P )\nif (v > value)\nthen value +- v\nelse jointpolicy +- {policyl,policyz}\nr e t u r n jointpolicy\n+\n-\n\n+\n\nfunction COVERAGESET( MDP, p)\nr e t u r n s set of all optimal policies with respect t o p\ninputs: MDP, arbitrary MDP\np, joint reward structure\nplanes +- { /* planes are equivalent to policies * /\npoints +/* initialize boundaries of parameter space */\nfori +- 1 to IpI\nboundaries +- boundaries u {xi = 0, I, = I}\n/* loop until no new optimal policies found */\ndo\nnewplanes\n{}\npoants + INTERSECT(planes u boundaries)\n/* get optimal plane at each point */\nfor e a c h poznt in points\nplane\nSOLVE(AUGMENT(MDP, point, p )\nif plane not i n planes\nthen newplanes\nnewplanes u {plane}\nplanes\nplanes u newplanes\nwhile lnewplanesl > 0\nr e t u r n planes\n+\n\n+\n\n+\n-\n\n+\n-\n\nF i g u r e 1: Coverage Set A l g o r i t h m\n13.\nDEFINITION The parameter space is an n dzmenszonal space where each dimension has a range of[O,1 Each\n1\npokcy x, has a corresponding point in the parameter space,\nZ m I , which measures the probabilities that each one of the n\nevents wall occur when agent follows policy T , :\n\nGiven a point in the parameter space, Zn,, agent i can define a decision problem that accurately represents the global\nvalue instead of its local value. I t can do this because both\nthe joint reward structure and agent j \xe2\x80\x99 s policy are fixed. The\nnew decision problem is defined by an augmented hlDP.\nDEFINITION An a u g m e n t e d MDP, denoted M D P : \xe2\x80\x9c I ,\n14.\n< Si, A , , Pi, R : , s ~ , Z * ,p >, where Fr, is a\npoint in parameter space computed fmm the policy for agent\nj , p i s the joint reward structure and R: is:\nis defined as\n\nIPI\n\n% ( e ) = %(e)\n\n+\n\n6kP(Eilrj)ck,\nk=l\n\nwhere 6k =\n\n1 eEE;\n0 otherwise\n\nNote that for e = ( s ,a , s\'), R ( e ) is the same as R(s,a, s\').\nTHEOREM 1. The value of\n\na p o k y xi over MDP:*\' is:\n\nV ? (sh) = VT,(s6)\nX\'\n\n+ JV(pln,,\n\nTJ).\n\nPROOF. value of an MDP given a policy can be calThe\nculated by summing over all time steps t and all events e,\nthe probability of seeing e after exactly t steps, times the\nreward gained from e:\n30\n\nt=o\n\nt=O\n\nt=O\n\ne\n\nIPI\n\ne\n\nk=l\n(PI\n\nbc\n\nIPI\n\n+ P(E;IxJ)P(E;lni)ck\nk=l\nV~,(S:) + JV(plxi,Tj).\n\n= Vz.(s6)\n=\n\nWhile the hill-climbing algorithm reaches a Nash equilibrium, and the optimal joint policy is a Nash equilibrium,\nthere is no guarantee that they are the same equilibrium.\nIn many problems there are many local optima, with the\noptimal joint policy being the best of them. However, the\nhill-climbing algorithm, even with random restarts, does not\nguarantee finding the global maxima.\n\n3.2 Finding the Optimal Coverage Set\n\ne\n\n3\n0\n\nThis contradicts ( x ~ , x z ) * By symmetry, we can show the\n.\nsame for ~ 2 Therefore, the optimal joint policy is a Nash\n.\nequilibrium over augmented hlDPs. 0\n\n0\nThe function AUGMENT in Figure 1 takes an MDP, a\npolicy and a joint reward structure and returns an augmented MDP according to DEFINITION\n14.\nThe global value is equal t o the local value of a policy for\none agent plus the value of the corresponding augmented\nMDP for the other agent. This allows an augmented MDP\nto be used in an iterative, hill-climbing algorithm. In such\nan algorithm, the policy for agent j , x j , is held fixed and the\noptimal corresponding policy for agent i is found by finding\nthe optimal policy for MOP:"\' . Then the roles are reversed,\nand agent i\'s new policy is held fixed while the optimal corresponding policy for agent j is found. This process repeats\nuntil neither policy changes, i.e., until a Nash equilibrium is\nreached.\n\nPROPOSITION optimal joint policy (n1,sz)\' is a\n1. An\nNash equilibrium over the augmented MDPs:\n\nAn augmented MDP is defined over the parameter space,\nwhich is a continuous space. This means that for both\nagents, there is an infinite number of augmented MDPs,\nhowever, only a finite number of them are potentially useful: the ones where the point in parameter space corresponds\nto a policy of the other agent. This set is still quite large\nsince the number of policies is exponential in the number\nof states. It turns out that in practice, only a very small\nnumber of these augmented MDPs are really interesting.\nGiven any particular augmented MDP, the only policy\nthat is useful is the policy that maximizes the expected\nvalue of that MDP. Fortunately, most of the augmented\nMDPs have the same optimal policy, and the set of interesting augmented MDPs can be reduced to one per optimal\npolicy. This set of optimal policies, the optimal coverage\nset, is what we are really after and the augmented MDPs\nprovide a way t o find them.\n15.\nis\nDEFINITION The optimal coverage set, O;, the\nset of optimal policies for MDP: given any point in parameter space, f:\n\noi = {xi I 32,xi = argmax,,V:;(s6)).\nAnother way t o look at the optimal coverage set is to\nexamine the geometric representation of a policy over the\nparameter space. The value of a policy x , , given in THEOREM 1, is a linear equation. If (f,,I = 1, then the value\nf,\nfunction is a line in two dimensions. When I, 1 = 2, the\nvalue function is a plane in three dimensions. In general,\nI&,[ = n and the value function is a hyperplane in n 1\ndimensions.\nThe optimal coverage set, then, is the set of hyperplanes\nthat are highest in the n + 1 dimension for all points in\nthe parameter space (first n dimensions). Figure 2 gives an\nexample of planes over a 2-dimensional parameter space.\n\n+\n\nTHEOREhl 2 . If two points f and\nin n-dimensional parameter space have the same corresponding optimal policy n,\nthen all points on f ( a ) = f a(Q - f), 0 5 o 5 1, the line\nsegment between f and 8, have optimal policy x .\n\n+\n\nPROOF.\nAssume 3 6 # x i : V,f;n2\n11 and THEOREM\n1:\n\n(SA)\n\nGV(xl,sz) =\nGV(xi, 8 2 )\n\n> VzT2(SA).\n\nPROOF.\nFrom DEFINITION\n\nv z 7 w + Vz,(Si)\n\n= Vzp\n\n(SA) + Vz,(s;)\n\nTherefore,\nG V ( d , n z ) > GV(x1,nz)\n\nLet x = argmax, V," (so) = argmax, V (so),\n:\nZ = f ( a o ) , < a < 1, and\n0\n0\nx\' = argmax,,, V (so).\n$\nAssume V:(so) < V;:(SO). We know V,\'(SO) 2 V:(SO), and\nbecause V(,) and f(.) are linear functions, we can compute\ntheir value at f (1) = Q by computing the unit slope.\n\nJ\n\nP\n\nF i g u r e 2: I n t e r s e c t i n g Planes. ( a ) The first i t e r a t i o n checks the c o r n e r s of the parameter space: (O,O), (0, l ) ,\n(1,0), (1, l), which yields three planes. In the second i t e r a t i o n four n e w i n t e r e s t i n g p o i n t s are found. The top\nthree all have the same o p t i m a l plane, which is added i n (b). The f o u r t h p o i n t yields the p l a n e added i n (c).\nThe next i t e r a t i o n produces eight n e w interesting points, t w o of which r e s u l t in the s i x t h plane, added i n\n(d). The next i t e r a t i o n finds no n e w o p t i m a l planes and t e r m i n a t e s , r e t u r n i n g t h e set of six policies.\n\nThese are the hyperplanes that enclose the parameter space.\nSince each dimension is a probability, it can range from 0\nt o 1, so in this case there are 4 boundary lines: 2 1 = 0,\n( s o ) < v; ( s o )\n11 = 1, 1 2 = 0, 1 2 = 1. The algorithm then loops until no\nThis contradicts that A is optimal at g, therefore\nnew planes are found.\nIn each loop, INTERSECT is called on the set of known\nV,f(SO) = v()\n:s.\nto\npolicy hyperplanes and boundary hyperplanes. INTERSECT\ntakes a set of hyperplanes and returns a set of points that\n0\nrepresent the intersections of those hyperplanes. Linear p r e\nA bounded polyhedron in n dimensions is composed of a\ngramming would be an efficient way to compute these interset of faces, which are bounded polyhedra in n - 1 dimensections because the majority of them are not interesttng:\nsions. The corners of a bounded polyhedron are the points\nthey lie outside the parameter space or lie below another\n(polyhedra in 0 dimensions) that the polyhedron recursively\nplane. For example, Figure 2(d) h a s six policy planes and\nreduces to.\nthe four boundaries of the parameter space. The total number of points is approximately 120, but only 14 points are\nTHEOREM Given a bounded polyhedron in n dimen3.\nvisible in the top view, and they are the only interesting\nsions whose comers all have the same corresponding optimal\npoints because they form the corners of the polyhedra (or\npolicy K , any point on the surface or in the interior of that\npolygons in this example) over the parameter space whose\npolyhedron also has optimal policy A.\ninterior all have the same optimal policy (THEOREM 3).\nPROOF. induction on the number of dimensions\nBy\nAfter computing the set of points, the augmented MDP\nBase case: n = l . A polyhedron in 1 dimension is a line\nfor each of those points is created and the optimal policy\nsegment with corners being the endpoints. From THEOREM\nfor each of those augmented MDPs is computed by SOLVE,\n2, all points on the line have optimal policy A.\nwhich can use standard dynamic programming algorithms.\nI n d u c t i v e case: Assume true for n - 1, show true for n.\nThe value of a policy and a point in parameter space is\nFrom the inductive assumption, all points in the faces of the\npolyhedron have optimal policy K. For any point within the\npolyhedron, any line passing through that point intersects\ntwo faces, with the interior point between the intersection\npoints. From THEOREMthis point has optimal policy\n2,\nFor a given R I , the value function is a plane over the paramK. 0\neter space. The plane for the new optimal policies will either\nbe equivalent (different policy but same value) or equal t o\nThe part of the algorithm discussed in this section is hana plane already in the coverage set, or it will be better than\ndled by the function COVERAGESET in Figure 1. I t takes\nevery other plane in the coverage set a t this point in paraman hlDP and a joint reward structure and returns the optieter space. If it is the latter case. this new plane is added\n3.\nmal coverage set, based on THEOREMTo illustrate how\nto the coverage set. If a complete iteration does not find\nthis works, we will step through a small example.\nany new planes, then the loop terminates and the current\nConsider an instance of the Mars rover problem, with just\ncoverage set i returned.\ns\ntwo elements in the joint reward structure: (E:, E;, c1) and\n( E ; ,E:, CZ). The function CSA starts by calling COVERAGE\n3.3 Selecting the Optimal Joint Policy\nSET on MDPl and p. The first thing that COVERAGE\nSET does is to create the boundaries of the parameter space.\nGiven the optimal coverage set for agent i: finding the o p\n\nv\n!\n\ntimal joint policy is easy. From PROPOSITION optimal\n1, an\njoint policy includes an optimal policy for agent i given some\naugmented MDP. Since the optimal coverage set includes all\nof the optimal policies i t includes one that is a part of an\noptimal joint policy. To find the optimal joint policy the algorithm finds the corresponding optimal policy for agent j\nfor every policy in the optimal coverage set. It does this by\ncreating an augmented MDP for agent j for each policy in\nthe optimal coverage set and then finding the optimal policy\nof that MDP. The optimal joint policy is the resulting joint\npolicy with the highest global value.\nThe function GV returns the global value as defined in\n11.\nDEFINITION\n\n(I\n\n12 1\n\nI\n\n10\n\nI\n\n6 11 16 21 26 31 36 41 46 51 56 61 66 71 7\n\nSire of Optimal Coverage Set\n\nTHEOREM The coverage set algorithm is complete and\n4.\noptimal.\nPROOF.(sketch) To prove that the coverage set algorithm\nis complete and optimal, we must prove three claims: the\nalgorithm terminates, it finds the optimal coverage set, and\nit returns the optimal joint policy. We avoid discussion of\nmultiple equivalent policies for clarity.\nT e r m i n a t i o n - There are four loops in this algorithm. The\nfirst is over a set of policies, which is finite. The second\nis over the number of dimensions in the parameter space,\nwhich is finite. The third is looping until no new policies\nare found. The new policies are added t o the coverage set\neach iteration, so a particular policy could never be a new\npolicy twice. Since the set of policies is finite, this loop will\nterminate. The fourth loop is over a set of points, which are\na subset of intersection points of a finite set of planes, which\nis finite. All of the loops in this algorithm halt, therefore\nthe algorithm halts.\nO p t i m a l coverage set i s f o u n d - All the planes/policies\nin the returned set are derived by solving the corresponding\nMDP using dynamic programming and are therefore optimal. All the relevant point intersections between the hyperplanes are found. This set of points divides the parameter\nspace into a set of polyhedra. From THEOREM no new\n3 if\noptimal policies are found from those points, then the set of\noptimal policies is the optimal coverage set.\nThe o p t i m a l j o i n t policy is r e t u r n e d - The set of joint\npolicies created by taking an optimal coverage set and finding the corresponding optimal policy for the other agent\nincludes all Nash equilibria. Fkom PROPOSITION best\n1, the\nof those equilibria is the joint optimal policy. 0\n\n4. EXPERIMENTAL RESULTS\nWe have implemented a version of this algorithm that\nworks on problems with a 2-dimensional parameter space,\nand we have tested it on a number of randomly generated\nproblems. The test instances were modeled after the Mars\nrover example used throughout this paper. There are two\nrovers, each with an ordered set of sites t o visit and collect\ndata. The state for each rover is composed of their current\ntask, and the current time left in the day. The action for\neach state is t o skip the current site or t o collect data a t the\ncurrent site. If the rover chooses skip, then it moves on to the\nnext site without wasting any time. There is a distribution\nover time t o collect data, but it always succeeds unless the\ntime in the day runs out.\nThe problem instances generated had a total time of 15\nhours and 6 sites for a total of 90 states for each rover. The\n\nF i g u r e 3: D i s t r i b u t i o n over the size of the o p t i m a l\ncoverage set. The r i g h t m o s t bar i s 2 76.\n\n60000\nm\n\n.- 50000\n6\n\nE\n2\n\n40000\n30000\n\nB 20000\n*\n10000\n\n0\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n81\n\nSize of Optimal Coverage Set\nF i g u r e 4: N u m b e r o f value i t e r a t i o n s b y the size of\nthe o p t i m a l coverage set.\n\nreward for skipping a task was 0, and for collecting a t a site\nwas randomly chosen from a uniform distribution between\n0.1 and 1.0. The time to collect was a Gaussian distribution\nwith a mean between 4.0 and 6.0 and a variance 40% of the\nmean. Two of the sites (3 and 4) had a reward that was\nsuperadditive in the global value. CI and c2 were randomly\nchosen from a uniform distribution between 0.05 and 0.5.\nData was collected from 4208 random problems. About\n11% of the underlying MDPs were trivial, that is, the optimal coverage set only included one policy. At the other\nextreme, about 7% of the underlying MDPs were hard, and\nhad a large number of policies in the optimal coverage set.\nA problem is composed of two underlying MDPs, and the\ncomplexity of a problem is really only as hard as the easiest\nof the two, which means that about 21% of the problems\nwere trivial and less than 1% were hard. In our tests, we\nconsidered an optimal coverage set of size 2 76 to be hard.\nThe average size (not including the hard problems) was 13.3.\nThe full distribution can been seen in Figure 3.\nFigure 4 demonstrates how this algorithm scales with the\nsize of the optimal coverage set. The average number of\nvalue iterations was 1963. The number of reachable states\nin this problem ranged from 70 to 75. To solve this using\nbrute force policy search would result in a t least 2\xe2\x80\x99\xe2\x80\x99 policy\nevaluations. While policy evaluation is usually cheaper than\n\nvalue iteration, it is still clearly infeasible for these problems.\nTo collect this data, we used a brute force intersection algorithm that found all of the intersection points and weeded\nout only those that were out of bounds. It did not check t o\nsee whether the points were below another already known\nplane. We also cached the results of each run of value iteration and never ran it twice on the same point. We are\nworking on a more efficient implementation of the function\nINTERSECT that will not need t o intersect all planes t o\nfind the interesting points. This will lead to both a faster\nINTERSECT function, and fewer runs of value iteration.\n\n5. CONCLUSION\nThe framework of decentralized MDPs has been proposed\nt o model cooperative multi-agent systems in which agents\nreceive only partial information about the world. Computing the optimal solution t o the general class is NEXP-hard,\nand the only known algorithm is brute force policy search.\nWe have identified an interesting subset of problems called\ntransition-independent DEC-MDPs, and designed and implemented an algorithm that returns the optimal joint policy\nfor these problems.\nBesides being the first optimal algorithm t o solve any nontrivial class of DEC-MDPs, the new algorithm can help establish a baseline for evaluating fast approximate solutions.\nUsing the exact algorithm, other experiments have shown\nthat a simple hill-climbing algorithm with random restarts\nperforms quite well. In many cases, it finds the optimal joint\npolicy very quickly, which we would not have known without identifying the optimal solution using the coverage set\nalgorithm.\nThe new algorithm performed well on randomly generated problems within a simple experimental testbed. A\nmore comprehensive evaluation is the focus of future work.\nThis will include a formal analysis of the algorithm\xe2\x80\x99s running\ntime. and testing the algorithm with more complex problem\ninstances. This algorithm is also naturally an anytime algorithm, because the COVERAGESET function could terminate a t any time and return a subset of the optimal coverage\nset. We will explore this more in future work. We also plan\nto explore the range of problems that can be modeled within\nthis framework. For example, one problem that seems t o fit\nthe framework involves a pair of agents acting with some\nshared resources. If together they use more than the total available amount of one of t h e resources, they incur a\npenalty representing the cost of acquiring additional units\nof that resource.\nFinally, the optimal coverage set is an efficient representation of the changes in the environment that would cause\na n agent to adopt a different policy. This information could\nbe extremely useful in deciding when and what t o communicate or negotiate with the other agents. In future work,\nwe will explore ways to use this representation in order to\ndevelop communication protocols that are sensitive t o the\ncost of communication.\n\n6. ACKNOWLEDGMENTS\nWe thank Daniel Bernstein for useful feedback on earlier\nversions of this paper. This work was supported in part by\nNASA under grants NAG-2-1394 and NAG-2-1463 and by\nthe National Science Foundation under grant IIS-9907331.\nAny opinions, findings, and conclusions or recommendations\n\nexpressed in this material are those of the authors and do\nnot reflect the views of NASA or NSF.\n\n7. REFERENCES\n[I] D.S. Bernstein, R. Givan, N. Immerman, and S.\nZilberstein. The Complexity of Decentralized Control\nof Markov Decision Processes. To appear in\nMathematics of Operations Research.\n(21 C. Boutilier. Sequential optimality and coordination\nin multiagent systems. Proceedings of the Sizteenth\nInternational Joint Conference o n Artificial\nIntelligence, 478-485, Stockholm, Sweden, 1999.\n[3] M. Ghavamzadeh and S. Mahadevan. A multiagent\nreinforcement learning algorithm by dynamically\nmerging Markov decision processes. Proceedings of the\nFirst International Conference o n Autonomous Agents\nand Multiagent Systems, Bologna, Italy, 2002.\n[4] C. V. Goldman and S. Zilberstein. Optimizing\nInformation Exchange in Cooperative Multi-agent\nSystems. Submitted for publication, 2002.\n[5] K. Hsu and S.I. Marcus. Decentralized control of finite\nstate Markov processes. I E E E Transactions on\nAutomatic Control, 27(2):426431, 1982.\n11 M. Mundhenk, J . Goldsmith, C. Lusena, and E.\n6\nAllender. Complexity of finite-horizon Markov\ndecision process problems. Journal of the ACM,\n47(4):681-720, 2000.\n[7] J.M. Ooi and G.W. Wornell. Decentralized control of\na multiple access broadcast channel: performance\nbounds. Proceedings of the 35th Conference on\nDecision and Control, 293-298, 1996.\n[8] C.H. Papadimitriou and J. Tsitsiklis. On the\ncomplexity of designing distributed protocols.\nInformation and Control, 53:211-218, 1982.\n[9] C.H. Papadimitriou and J. Tsitsiklis. The complexity\nof Markov decision processes. Mathematics of\nOperatzons Research, 12(3):441-450. 1987.\n[lo] L. Peshkin, K.-E. Kim, N. Meuleau, and L.P.\nKaelbling. Learning to cooperate via policy search.\nSizteenth International Conference on Uncertaznty zn\nArtificial Intelligence, 484-496, 2000.\n(111 D. Pynadath and M. Tambe. The Communicative\nMultiagent Team Decision Problem: Analyzing\nTeamwork Theories and hlodels. Journal of Artzficial\nIntelligence Research, 389-423, 2002.\n[12] S. Sen, M. Sekaran, and J. Hale. Learning to\ncoordinate without sharing information. Twelfth\nNational Conference o n Artijicial Intellzgence,\n426-431, 1994.\n[13] R. Washington, K. Golden, J. Bresina, D.E. Smith, C.\nAnderson, and T. Smith. Autonomous rovers for Mars\nexploration. Proceedings of the I E E E Aerospace\nConference, 1999.\n(141 P. Xuan and V. Lesser. Multi-agent polices: From\ncentralized ones t o decentralized ones. Proceedzngs of\nthe First International Joint Conference o n\nAutonomous Agents and Multi-Agent Systems,\nBologna, Italy, 2002.\n[15] S. Zilberstein, R. Washington, D.S. Bernstein. and\nA.I. Mouaddib. Decision-theoretic control of planetary\nrovers. To appear in Springer Lecture Notes in AI.\nDecember 2002.\n\n'