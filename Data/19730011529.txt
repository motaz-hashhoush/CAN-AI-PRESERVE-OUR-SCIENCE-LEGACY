b'1\n\nELECTRKCAL\nI\nI\'\n\nIT\n\n-.--\n\n\xc2\xb7 --*.l;\n\n\xc2\xb7 \xc2\xb7 rr~-r~-\n\nc~ --- *rs\xc2\xb7-\n\nD-.r~\xc2\xb7\n\nB~:~..I\xc2\xb7\n\nr\n\nE\nN\nG\n\n.i\' : f_f\'\nI-IZ=\nH 3\n\n,[\n\nf\n\n113 0\nC\n\nu H (3\nD -\'\n4brP\nOH\nm\n\nd\n\nI\n\nO\'\n~u S\n\nLtn\nI\no\n\n}\n\nO I\n\nI\n\n\' -. \' ""\n\nO fo-,. I-\', L~\n\n0 t-o\n\nI.\n\nCdi --\n\n0\n\n0JO~\n\n\'I"\n\n;-PI _4\n\nI\n\n.\n\nN\n\nI n\n\ntf\n\n!\n\nr\n\nfq\n\nC:-.\n\nni~\n\nIJn t\nI~\n\'^ C <\nC\nF_\n_~i\n\n.~~~~~~~~~~~~~~~~.\n_WWW\'-\n\nWM\n\nI\n\nE\n\ni\n\nC)\n\nnD\n\nE\n\n.I\n\n_ _~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~1\n\nR\n\n0,\nC)\nt\no.\n\nc\n\nu-i C:f\n-,\n1W0 0\n\nz\n_f .\nw\n-I\n\n4\n\nsl)\nReproduced by\n\nun\n0w\n\nNATIONAL TECHNICAL\nINFORMATION\'SERVICE\nol Commerce\nUS Department\nSpringfield, VA. 22151\n\nENGONEERUNG EXPEROMENT STATDON\n\nAUBURN UNIVERWSTY\nAUBURN, ALABAMA\n\nfw\nU,\n\nN\nG\n\nAUBURN\n\nUNIVERSITY\n\nA U 8 U RN\n\nALABAMA\n\n36830\nSC HOOL\n\nOF\n\nENGINE\n\nER ING\n\nElectrical Engineering\n207 Dunstan Hall\n\nTelephone 826-4330\nArea Code 205\n\nMarch 20, 1973\n\nNational Aeronautics and Space Administration\nGeorge C. Marshall Space Flight Center\nHuntsville, Alabama 35812\nRE:\n\nContract NAS8-26580\nMonthly Progress Report\nFebruary 5, 1973 to\nMarch 5, 1973\n\nDear Sir:\nProgress in the assigned area of work is as follows:\nFeasibility Study of Reaction Control Systems\n(Huntsville Contact: C. Rupp)\nM. Polites\nTask 1.\nAdditional effort is being devoted to formalizing a method of\nestimating the error-time response for the class of MRAS systems\nstudied previously. In addition; a general set of conditions\ngoverning a class of coupled MRAS systems is being outlined.\nTask 2. (a). Using a torque axis transformation design procedure\na new 16 engine model has been developed and is presently under\nstudy.\n\n(b).\n\nStudy of the optimal CMG control law is continuing.\nSincerely yours,\n\nJoseph S. Boland, III\nProject Leader\nJSB/vht\n\n)\n\nA\n\nI\n\nL A N D - GRANT\n\nU N I V E R S I T Y\n\nFINAL TECHNICAL REPORT\nSURVEY OF DIGITAL FILTERING\n\nPREPARED BY\n\nDIGITAL SYSTEMS LABORATORY\nELECTRICAL ENGINEERING\nH. TROY NAGLE, JR\nPROJECT LEADER\nOCTOBER, 1972\n\nCONTRACT NAS8-20163\nGEORGE C. MARSHALL SPACE FLIGHT CENTER\nNATIONAL AERONAUTICS AND SPACE ADMINISTRATION\nHUNTSVILLE, ALABAMA\n\nSUBMITTED BY:\n\nAPPROVED:BY:\n\nH. Troy N\nsociate Professor and Head\nElectrical Engineering\n\nle, Jr.\n\nAssociate Professor\nElectrical Engineering\nI\n\nPRECEDfNG PAGE BLAN\n\nK\n\nNOT FYLU-\n\nFOREWORD\n\nThis is a technical summary reporting the progress of a study\nconducted by the Electrical Engineering Department of Auburn University\nduring the period 17 June 1965 through 15 October 1972.\n\nThis study\n\nreport completes Contract No. NAS8-20163, granted to Engineering\nExperiment Station, Auburn, Alabama, by George C. Marshall Space Flight\nCenter, National Aeronautics and Space Administration, Huntsville,\nAlabama.\n\nii\n\nACKNOWLEDGEMENT\n\nThe author wishes to recognize the following list of Faculty\nand Graduate Students who participated in this study during its\nvarious stages\n\nFaculty:\nChester C. Carroll, Project Leader, 1965-1970\nH. Troy Nagle, Jr., Project Leader, 1970-1972\n\nGraduate Students:\nJ. R. Heath\nRonald White\nJ. W. Jones\nW. L. Oliver\nG. E. Jordan\nH. H. Hull\nH. Troy Nagle, Jr.\n\nQuitman Liner\nRoger Cole\nJ. L. Raley\nJohn A. Childs\nDavid Kimsey\nLeRoy Bearnson\nR. H. Robison\n\niii\n\nSURVEY OF DIGITAL FILTERING\n\nH. Troy Nagle, Jr.\n\nABSTRACT\n\nA three part survey is made of the state-of-the-art in digital\nfiltering.\n\nPart one presents background material including sampled-\n\ndata transformations and the discrete Fourier transform.\n\nPart two,\n\ndigital filter theory, gives an in-depth coverage of filter categories,\ntransfer function synthesis, quantization and other non-linear errors,\nfilter structures and computer aided design.\nhardware mechanization techniques.\n\nPart three presents\n\nImplementation by general-purpose,\n\nmini-, and special-purpose computer are presented.\n\niv\n\nSURVEY OF\nDIGITAL FILTERING\n\nPart\n\nPage\n\n1.\n\nBackground\n\n1-i\n\n2.\n\nDigital Filter Theory\n\n2-i\n\n3.\n\nMechanization of Digital\nFilters\n\n3-i\n\nv\n\nPART ONE\n\nBACKGROUND\n\n1-i\n\nPART ONE:\n\nBACKGROUND\n\nTABLE OF CONTENTS\n\nI.\n\nIntroduction to Digital Filtering . . . . .\nA.\nB.\nC.\n\nII.\n\nImpulse Sampling. . . . . . .\nHold Devices. . . . . . . .\nDiscrete Transfer Functions .\nDifference Equations. . . .\nMapping Function . . . . . .\nFrequency Response. . . . .\n\nSampled Data Transformations.\nA.\n\nB.\n\nC.\n\nIV.\n\nThe Computer Model. . . . . . . . .\nThe z-Domain Model. . . . . . . . .\nScope of Digital Filtering. . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n1-1\n1-4\n1-4\n\n.\n\n.\n\n.\n\nStandard z-Transform. . . . . . .\nA.\nB.\nC.\nD.\nE.\nF.\n\nIII.\n\n1-1\n\n1-7\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n.e.e. .\n\n.\n.\n\n.\n.\n\n.\n.\n\xc2\xb7.\xc2\xb7.\xc2\xb7 .\n\n.\n\n.o\n\n.\n\n.e\n\n.\n\n.\n\no.\n\n.\xc2\xb7\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n\n. . . . . .\n\n1-ii\n\n.\n\n..\n\n1-7\n1-11\n. . 1-14\n. . 1-16\n1-19\n. . 1-22\n\n. . . . . . . . . . . 1-23\n\nApproximation Techniques. . . . .\n1. Backward Difference . . . . . .\n2. Forward Difference. . . . . .\n3. Rectangular Rule . . . . . . .\na. Left Side . . . . . . . . .\nb. Right Side .\n. . . . .\n4. Trapezoidal Rule . . . . .\n5. Simpson\'s Rule . . . . . . . .\n6. Impulse Invariance. . . . . . .\n7.\nImpulse Invariant Integrator.\nMapping Functions Summary . . . . .\n1. Standard z-Transform\n. . . . .\n2. Backward Difference . . . . . .\n3. Forward Difference. . . . . .\n4. Bilinear z-Transform. . . . .\n5. Matched z-Transform . . . . .\nOther Transforms. . . . . . . . .\n1. Simpson\'s Rule . . . . . . . .\n2.\n(w, v)-Transform. . . . . . .\n\nDiscrete State Variables.\n\n.\n\n. .\n....\n. .\n. .\n....\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n..\n\nee\xc2\xb7\xc2\xb7\n.\n\n.\n\n.\n\n..\n\n.\n\n..\n\n.\n\n..\n\n.\ne.ee.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\ne.ee.\n\n.\n\n.\n\nee\n\n.\n.\n.\n\n.\n\ne\n\n.\n\n.\n\neeee\n\n.\n\n.\n\n.eee\n\n.\n\n.\n\n.\n\n.\n\n. . ..\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n1-23\n1-23\n1-28\n1-30\n1-30\n1-32\n1-33\n1-34\n1-34\n1-36\n1-38\n1-38\n1-38\n1-40\n1-40\n1-45\n1-46\n1-46\n1-46\n\n. . . . . . . 1-48\n\nV.\n\nA.\nB.\nVI.\n\n1-50\n\nContinuous Linear Systems . . . . . . . . . . . . . . .\nDiscrete Linear Systems . . . . . . . . . . . . . . . .\n\n1-50\n1-52\n1-54\n\nDiscrete Fourier Transform ................\nA.\nB.\n\nC.\n\nVII.\n\n. . . . . . . . . . . . . . . . . . . . . . .\n\nConvolution.\n\n. .\nContinuous Fourier Transform\nDiscrete Fourier Transform . .\n1.\nSampling Process . . . . .\n2.\nDFT Derivation . . . . . . . .\n3.\nIDFT Derivation\n. . . . .\n4.\nDFT Pairs . . . . . . . .\nFast Fourier Transform . . . .\n1.\nCalculation Time . . . . .\n2.\nFFT Derivation . . . . . .\n3.\nIFFT Derivation. . . . .\n\n. . . . . . . . . . .\n\n1-54\n\n.. . . . . . . . . . 1-54\n. . . . . . . . . . 1-54\n. . . . . . . . . . . 1-58\n1-61\n......... ..\n\n.\n\n. ......\n.\n...1-64\n. ......\n..\n.\n\nRandom Processes ......................\n\n1-62\n\n..\n1-64\n1-67\n1-68\n1-73\n1\n\nA.\nB.\n\nContinuous Processes ..................\nDiscrete Processes ...................\n\nREFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n1-iii\n\n1-73\n1-73\n1-76\n\nI.\n\nINTRODUCTION TO DIGITAL FILTERING\n\nDigital Filtering may be described as the process by which input\ndiscrete-time sequences of numbers with discrete amplitudes are transformed into output discrete-time sequences of numbers with discrete\namplitudes.\n\nThe transformation process (the digital filter) may be\n\ndescribed as a set of difference equations which may be programmed on\na general-purpose computer, or realized with specially designed devices.\nThe Computer Model\nAn example digital filter is shown in Fig. 1.\n\nThe input signal\n\nei(t) is in analog form and is sampled every T seconds by an Analogto-Digital Converter (A/D).\n\nThe input samples ei(nT), n an integer, are\n\nin binary 2\'s complement form and are supplied to the computing device,\nwhich may be a general-purpose or special-purpose computer.\n\nThe com-\n\nputing device is programmed to calculate the filter output samples eo(nT)\nwhich are fed to an output hold register.\n\nThis register may actually be\n\nconsidered to be part of the computing device.\n\nThe output samples eo(nT)\n\nare held in the register until a new output sample is calculated and supplied to the output hold register.\n\nThe D/A converter produces an ana-\n\nlog output signal eo(t) whose characteristic form is shown in Fig. 2.\nThe digital filter of Fig. 1 operates as follows:\n\nA pulse from the\n\ndigital filter control unit at t--nT instructs the A/D to calculate\nei(nT).\n\nHowever this sampled value of ei(t) is not available to the\n\ncomputing device until time nT + Ta, where Ta is the total A/D\n1-1\n\n1-2\n\n0\n\n0\n\n~\n\nH\n\nP~\n\n3 n\n\n\xc2\xb0\xc2\xb0\n\nrT4\n\nz\n8 8\n\nE-(\n\n0.-\n\n4\n\n_)\n\nZZz\nH\n\n~E4\n\nz\nX\n\n-4\n\n1-3\n\ne (t)\n\ne o (nT+2T)\n\neo(nT+T)\neo (nT-T)\neo (nT-2T)\neo (nT)\nI\nI\nI\nI\nI\nI\n\nnT-2T\n\nFig. 2.\n\nI\nI\nI\nI\nI\nI\nI\n\n_1\nI\nI\nI\nI\nI\n\n-L\n\nnT-T\n\nnT\n\nI\nnT+T\n\nI\nI\nI\nI\nI\nI\nI\nI\nI\nI\nI\nI\n\n.N\n\nt\n\nnT+2T\n\nThe analog output eo(t).\n\nl\n\n1-4\n\nconversion time.\n\nOnce the input sample has arrived at the computing\n\ndevice, the output sample is calculated and sent to the output hold\ndevice at t=nT+Ta+Tc, where Tc is the computing time.\n\nThus the output\n\neo(nT) is actually eo(nT+Ta+Tc). With modern technology, Ta+Tc can be\ndesigned to take less than lps.\n\nHence for sampling rates of up to\n\n200KHz(T=5ps), T is much greater than Ta+Tc and hence,\neo(nT+Ta+Tc)\n\n- eo(nT).\nThe z-Domain Model\n\nThe digital filter of Fig. 1 has been examined and described from\na hardware or functional point of view.\n\nA mathematical model for this\n\nfilter is shown in Fig. 3 which employs the well known z-transform.\nFig. 3a demonstrates a discrete time model for the digital filter where\nthe switches labeled T represent impulse samplers and the block labeled\nGho(S) represents a "zero-order hold" device.\n\nFig. 3b illustrates the\n\ntransfer function representation of the computing device itself.\n\nFig.\n\n3 differs from Fig. 1 in that the computing device of Fig. 1 uses the\namplitude of the input (and output) samples to calculate new output\nsamples eo(nt); Fig. 3 uses impulse functions weighted by the amplitude\nof the input (and output) samples to calculate new output impulses\neo*(t).\n\nThe impulse samples, zero-hold, and z-transform will be dis-\n\ncussed in more detail later.\nScope of Digital Filtering\nThe digital filter models presented above were for conventional onedimensional processing of a single input variable.\n\nAlthough this con-\n\ncept of digital filtering is most widely accepted, many other researchers\n\n1-5\n\nA/D\n\nIi\n\nr\n\nI\nI.\nII\n\nI\nI\n\n-_\n\n_ Cmput.ig..Plyvic\n\n-\n\nI\n\nI\n\nI\n\n1! I\n\nI\n\nI-\n\nF\nI\n\nI!\nEi (s)\n\nI\nI\n\nI.\nI\nI\n\nI\n\n-\n\nT\n\nD/A\n---\n\nI\n\nI\nEo(s)\n\nI\n\nI\n\nI\n\n-I\nI\n\nI\n\nI\n\nII\nI\nIj\n(a) Model of entire filter\n\nG(z)\n\nEi(Z)\n\n(b) Model of computing device\n\nFig. 3.\n\nMathematical model of\nthe digital filter.\n\nEo(Z)\n\n1-6\n\nhave applied the label to more general schemes.\n\nDigital filtering in the\n\nlast few years has come to also mean optimal state estimation, discrete\nFourier transformation, high speed convolution, non-linear discrete\nfiltering, two-dimension image processing, random and multirate sampling,\nblock recursion, least-mean squares filtering, quantization optimization,\ncomputer programming, and hardware implementation. All of these topics,\nand others, will be introduced in what follows.\n\nEmphasis will be, how-\n\never, on the standard case of linear, one-dimensional digital filtering.\nA prerequisite to understanding the theory of digital filtering is a\nmathematical background in sampled-data transforms, Fourier transforms,\nconvolution, discrete state variables and stocastic processes.\nthese topics are now reviewed briefly.\n\nHence,\n\nII. STANDARD Z-TRANSFORM\nThe most common sampled-data transformation is called the standard\nz-transform.\n\nIt is used to describe both the sampling process for the\n\ndigital filter input signal and the discrete transfer function of the\nfilter itself.\nImpulse Sampling\nThe z-transform is used to represent mathematically a discretetime system.\n\nThe discrete-time intervals are produced by periodic\n\nimpulse samplers.\n\nConsider Fig. 4.\n\nThe Laplace transfer function G(s)\n\nis an analog filter; its input is a Dirac delta function.\n\nThe filter\n\noutput g(t) is periodically impulse sampled every T seconds.\n\nThe\n\nsampled output may be expressed as\n\ng*(t) = g(t)\n\nE\nk=O\n\n6(t-kT)\n\n(la)\n=\n\nco\nE\nk=O\n\ng(kT)\n\n6(t-kT)\n\n1-7\n\n1-8\n\nCONTINUOUS\nFILTER\nSAMPLER\n\nr-----i\n\nI\n\nI\n\nI\n\nI\n\nI\n\ng(t)\n\nG(s)\n\nI\n\nI\n\nT\n\nj\n\nSAMPLER\n\nI\nZ\nk=o\n\n1\n\n6(t-kT)\nI\n\nI\n\ng(t)\nI\n\nI\n\nI\n\nI\n\nI\n\ng*(t)\n\nI\n\nL\nFig. 4.\n\n- - -I\nA continuous filter\n\nand sampler.\n\n6(t)\n\n1-9\n\nThe Laplace transform of g*(t) is\n\n=O\n2\n\n-\n\ng(kT)e kTs\n\n[g*(t)] = G*(s) =\nk=O\n\nIf we define z m eTS, then\n\nG*(s)\n\n=E\ns -\n\n(lb)\n\ng(kT) z-k\n\nInz ko\n\nEquation (lb) is the standard z-transform of g(t), or\n\nG(z)\n\n=\n\nZ[gst) - G*(s)\n-\n\nz\n\ng(kT)z\n\n(2)\n\n,nz\n\n-T\n\nSuppose that the analog transfer function G(s) is of the form\nm\nII\ni=l (s+ai)\n\nG(s)=K\nn\nj=I\ni=1 (bj)\n(e--bj)\n\n(3a)\n\n1-10\nWhere\n-ai = complex zeroes of G(s)\n-b\n\n-= complex poles of G(s)\n\ns = a + jw -\n\nLaplace variable\n\nK - Constant\n\nn>m\n\nIf there are no repeated poles in C(B), then\n\nG(s) -\n\n-\n\nk-l\n\n(3b)\n\ns+bk\n\nWhere Rk is the residue at pole -bk.\n\nSince,\ns+u\n\n]\n\n=\n\nZ [aeut\n\nae-u\n\n1-e-UTz-1\n\nThe standard z-transform of (3b) results in\n\nn\n\nG(z) = Z\nk=l\n\nRk\n1-e-bkTz-l\n\nA third representation of interest is found by noting\nthat multiplication of g(t) and c\n\n6(t-kT) in the time domain corresponds to con-\n\nvolution of G(s) and\n\nT e- kTs =\n1\nk=o\ni\nconvolution the result is\nl-eeTs\nG*(s) = 1/2.g(O+) + 1/T\n\nZ\nk--co\n\nG(+k\n(s+js\n\nin the frequency domain.\n\nAfter\n\n(5\n\n(5)\n\n1-11\n\nwhere\n\nS\n\n=2rs/T.\n\nHence it is apparent that G*(s) is periodic in w, the\n\nsampling frequency.\n\nIt is required that G(jw) = 0 for I w I > ms/2 as\n\nshown in Fig. 5 so that the frequency content of G(jw) will be preserved\nin the primary strip of G*(jw).\n\nIf this relationship is preserved the\n\nenvelope of G(jw) can be recovered from G*(jw).\n\nThis phenomenon is\n\nknown as frequency aliasing.\nThe standard z-transform discussed above is best known of the\nsampled-data transforms.\n\nThe theorems and tables of z-transforms can be\n\nfound in any standard sampled-data text [1].\n\nHold Devices\n\nIn the previous section we have seen the sampling process used to\ndetermine values of an input signal at discrete time intervals.\n\nThe\n\ninverse process of data reconstruction from these sampled signals is\naccomplished by hold devices.\n\nConsider the problem of reconstructing the\n\nsignal g(t) given samples spaced T seconds apart.\n\nIf we expand g(t)\n\nin a Taylor\'s series\n\ng(t) = g(nT) + g\'(nT)(t-nT) +\n\nfor nT <\n\n"\n\n(t-nT)2 + . . .\n\n(6)\n\nt <t-nT ,,\n\nwhere g\'(t) = dg(t)\ndt\n\nThe derivatives may be approximated by\n\ng\'(nT) = y\n\n(g(nT) - g(nT-T))\n(7)\n\ng"(nT) = 1\nT\n\n(g\'(nT) - g\'(nT-T)) ,\n\netc.\n\n1-12\n\nI G(jw) I\n\n4\nA\nI\nI\nI\nI\nI\nI\nI\n\nI\n\nI\nI\nI\nI\n!\nI\nB\n\nWs\n2\n\n-Ws\n2\n\nI\n.I\n\nG*(jw)\n\nI\n\nI\n\nI\n\nI\n\nA/T I\n\nI\n\n.,\n\n(D\n\nI\n.I\n\nI!\n\nWs\n\n-Ws\n\nI\n\n--\n\n,\n2\n\nI\n\n2.\n\nPRIMARY\n\nSTRIP\n\nFig. 5.\n\nFrequency domain characteristics\n\nof G*(s).\n\n1-13\nIf equation (6) is truncated to just one term, this reconstruction is\ncalled a zero-order hold; if the first derivative is included, a firstorder\n\nhold; etc.\n\nZero-Order Hold [1]\nThe zero-order hold device in the mathematical model of Fig. 3\naccepts an impulse modulated input eo*(t) and produces an output eo(t)\nas shown in Fig. 2.\n\neo*(t) -\n\nThe input eo*(t) may be expressed as\n\nz eo(kT) 6(t-kT).\nk-o\n\nIts Laplace transform is\n\nE *(s)\n\nE 0 (z) -. =\n\n=\n\n0\n\n\'\n-\n\ne(kT)z\n0\nk=o\n\'\n\nk\n\nThe output (see Fig. 2) may be written\n\neo(t) =\n\n00\n\neo(kT)[u(t-kT)-u(t-kT-T).\n\nk=o\nIts Laplace transform is\nEo(s) =\n\n(kT)[e-kTs - e\'kTs_-Ts;\n\nZ\n\nk=o\n=\n\nE eo(kT)e kT s (l-e-Ts\ns\n\nk=o\n-\n\ns\n\ns\n-\n\nE (z) (l-e-T)\n6\n\n(8)\n(8)\n\n1-14\nThe transfer function of the zero-order hold device is\n(\n\nho\nGho (s)\n\n)\n\nDeo\n\n- 1-e\n\nTs\n\n.\n\n(9)\n\nE *(s)\n\nThe frequency domain characteristics of Gho(s) are shown in Fig. 6.\nSuppose that the sampling interval T is chosen very small.\n\nThen,\n\ne T= 1-Ts +O(T2\n\nl1-Ts\nThen,\nho (S)\n\n1\n\n-(1) =T.\n\n(10)\n\nThis result is verified by noting in Fig. 6 that, for w<<wi/2 (or\nT small), the magnitude function approaches T; also, the zero-order\nhold introduces phase lag which is linear with frequency into the\nsystem.\n\nDiscrete Transfer Functions [ 1 ]\n\nIn the mathematical model of Fig. 3, the transfer function of the\ncomputing device is shown as\n\nE *(s)\n\nEo(z)\n_-_-\n\nEi(Z)\n\nG(z),\n=\xc2\xb0\n\na\n\nEi*(s)\n\nFrom Fig. 3\n\nEo (s) - Ei*(s)G(s).\n\n1-15.\n\nI\n\nGho(ji) I\n\nsin\nT\n\n<*ygT\n|I\n\nAS\n\nH\n\n2\n\nws\n\nlO\n\n()w\n3\n\nws\n\n/ %o(jw)\n\n(\n\n0\n\n-Tr\n\nFig. 6.\n\nGain and phase characteristics\nof a zero-order hold.\n\n1-16\nHence,\n\neo*(t) =\n\n[ Z e i(kT)g(t-kT)]\n\nk=oc\n\n\xc2\xa3\n\n6(t-jT).\n\nj=o\n\nIn this expression g(t-kT) may be replaced with g(JT-kT) due to the\nDirac delta function. Next, let i-j-k and replace the summation\nindex j with i as follows:\n\neo*(t) Za--.\n\nE ei(kT) g(ZT).6(t-XT-kT).\n-o\n\nSince g(y) - 0 for y<O, the -k may be replaced by zero.\n\nThe Laplace\n\ntransform of eo*(t) is\n\nEo*(s) = [ Z ei(kT)e-kT[\nk=o\n10\n][=0\n\ng(T)e\n\n= Ei*(s) G*(s).\n\nHence, the descrete-time transfer function of the computing device in\nFig. 3 is indeed\n\nEO*(s)\n=\n\nG*(s) = G(z).\n\n..Ei* (s)\nDifference Equations [4]\nThe discrete transfer fincti.on of equation (11) is, in general,\nthe ratio of two polynomials in z--l\n\na0 +alz-l++.\n\nGW\n1\nl+blz\n\n.+an\n\n-\n\n+. +b\n\xc2\xb7\n\nnn\n-\n\nE(z)\n\n(12)\n\n-\n\n(12)\n\nE(\nEi(z)\n\nwhere the coefficients ai and bj are real numbers (can be zero). An\nequivalent expression for (12) is the equation\n\n1-17\n\nEo(Z) = aoEi(z) + a1 z-1Ei (z) +\n-blz- Eo (z)\n\n. . . + anZ-nEi (z)\n\n..-bnz-nEo(z).\n\nThe infinite series for the z-transforms Eo(z) and Ei(z) is now\nsubstituted into the above equation and the coefficients of like\npowers of z-\n\n1\n\nare equated, yielding\n\ne (kT) = a,e (kT)+a ei(kT-T)+...+a e (kT-nT)-bleo(kT-T)-...\n0\noi\n\nI\n\nn\n\nlo\n\n-bne (kT-nT).\n\nNote that z-\n\n1\n\n-\n\neT\n\ns\n\nwhich represent a time delay of T second.\n\n(13)\n\nEquation\n\n(13) may be programmed in the contfuting device of Fig. lIthe variable\nei(kT) is furnished by the A/D converter; delayed values of the input\nei(kT-nT) and delayed values of the output variable eo(kT-nT) are\nstored in the computing device.\nEquation (13) defines a programming scheme known as the direct\nform.\n\nA block diagram of this form appears in Fig. 7.\n\nAnother programming scheme known as the canonical form is determined\nbelow.\n\nThe transfer function is expressed as\n\nE (z)\nG(z)\n\n-\n\nM(z)\n\n0\n\nM(z)\n\nEi(z)\n\nr\xc2\xb7\n\n1-18\n\nei(k)I\n\nT\n\n\'\n\na0 /\n\n\'\n\n7\'\n\neo(k)\n\nFig.\n\n7.\n\nGeneralized block diagram of the direct\nprogramming form for a digital filter.\n\n1-19\n\nwhere\n\nE (z)\nM(z)\n\n= a\n\n+ a\n\nM(z) _\nEi(Z)\n\nz-l+...+anz\n1\n-\n\nl+blz-l+...+bnz\n\nn\n\n(14)\n\nThe time-domain equivalent expressions for (14) are\n\nm(kT)= e i (kT) -blm(kT-T)-...\n-bnm(kT-nT)\n\n(15)\n\neo (kT) = aom(kT)+alm(kT-T)+...+anm(kT-nT)\n\n(16)\n\nEquations (15) and (16) are the difference equations to be used in\nthe canonical programming form.\n\nA generalized block diagram of this\n\nform appears in Fig. 8.\n\nMapping Function\n\nThe standard z-transformation of an analog function in the s-plane\nmay be considered to be a mapping from the s-plane to the z-plane under\nthe rule\nz =eTs.\nSee Fig. 9.\n\n(17)\nThe mapping illustrates that the region of stability in the\n\ns-plane (the left half-plane) corresponds to the interior of the unit\ncircle in the z-plane.\nonto the unit circle.\n\nIn fact, the primary strip in the s-plane maps\nAll other strips also map to the unit circle\n\n1-20\n\neo (k)\n\nei (k)\n\nGeneralized block diagram\n8.\nof the canonical\nprogramming form for\na digital filter.\n\n1-21\n\nx/\n\n= \xc2\xb0\n\ne// ,)A\n\n(a\n\ns/2\n\n\\\n\na\nC\n\nI/z>-\'l/\n\n(\n\n/\n\nA\n\n/I z=l\n\nz-p=ej\n\n(b) z-plane\n\n(a) s-plane\n\nFig. 9.\n\ns-plane to z-plane mapping.\n\nRe(z)\n\n1-22\nfurther illustrating the frequency aliasing problem of Fig. 5.\n\nThus,\n\ntransfer functions which are stable in the s-plane will also be stable\nafter taking their standard z-transformation.\nFrequency Response\nThe frequence response in the s-plane is evaluated by\n\nIG(s) I s=j\n/G~~S_\n\ns=jw\n\n0\n\n<\n\nW < WB .\n\nThis corresponds to evaluating G(s) along the contour in the s-plane of\nFig. 9a.\n\nSome upper cutoff frequence WB is shown for illustration.\n\nIn\n\nthe z-domain the contour follows the unit circle so that\n\nI D(z) I z = ejwT\n\n/D(z)\n\nz= ej\nO\n\nT\n\n0\n\nB,\n\n< W <\n\nis used to calculate the frequency response of a discrete transfer function.\nFrom equation (5) we see that D(z) is periodic in ws so that in practice\nOB = ws/2 and the contour traverses the top half of the unit circle.\n\nHence\n\ndb = 20 log ID(ejwT)I\n\n(18)\n=/D (eiwT)\n\n0 <\n\nX\n\n<\n\nUs/2\n\nwill be used to calculate the frequency response of a digital filter.\n\nIII.\n\nSAMPLED DATA TRANSFORMATIONS\n\nSampled-data transformations are the techniques one uses t6 obtain\nnumerical solutions to integral and differential equations.\n\nAny linear\n\nsystem\'s transfer function may be written as\n\nG(s)\n\n-\n\nY(s)\n\nX(s)\n\nY(s) = Laplace transform of the output\n\nX(s) = Laplace transform of the input.\n\nAlternately the relationship between input and output may be described\nas a differential\n\nor integral equation.\n\nNumerical methods may be\n\nemployed to solve these equations; these methods approximate the integral\nand differential equations by difference equations.\n\nAs we have seen pre-\n\nviously the difference equations may be represented by a discrete transfer\nfunction.\n\nThe complete process is illustrated\n\nin Figure 10.\n\nNumerical Approximations\nSeveral numerical approximation techniques will now be presented,\nsome for differentiation\n\nand some for integration.\n\nBackward Difference\nThe backward difference is a simple technique which replaces the\nderivative of a function by\n1-23\n\n1-24\n\nSampled Data\n\nFigure 10.\n\nRelation between numerical approximations\nand sampled data transformations.\n\n1-25\n\nd\n\nY (t) - y(t - T)\n\ny(t)\n\ndt\n\nT\n\nSee Figure 11.\nIn the Laplace domain\n\nsY(s)\n\n- e-STy(s)\n\n-s)\n\nT\n1\n\n-sT\n\n-\n\nT\n. 1-\n\nz\nT\n\nHence,\n\nD(z) = G(s)\n\nS\n\n1 - z- 1\nT\n\nExample.\n\nG(s) =\n\nFind a discrete approximation for\n\ns\nS+\n\na\n\nY(s) = G(s) X(s)\n\nsY(s) + aY(s) =,sX(,s)\n\n(19)\n\n1-26\n\nx(t)\n\n1\n\nI\n\ny(nT)-y(nT-T)\nI\nI\nI I\nI\nI\nI\nI\nI\'\nI\nI\nI\nnT-T\n\n(a)\n\n-. t\nc\n\nnT\n\nBackward Difference\n\nx(t)\n\n\'\nI\n\nI\n\nI\nI\nI\n\n+ T) - y(nT)\n\nI\nI\nI\nI\nnT\n\n(b)\n\nFigure 11.\n\nI\nI\n\nnT+T\n\nForward Difference\n\nDifference Approximations.\n\n-o\n\nt\n\n1-27\nor\n\ndt y(t) + ay(t) =\nNow letdt\n\n--\n\nx(t).\n\nNow let\n\nd\nd\n\ny(t) =\n\nd\ndt x(t)=\n\ny(t) - y(t - T)\nT\n\nx(t) - x(t - T)\nT\n\nTherefore\n\ny(t) - Y(t\nT\n\n) + ay(t) = x(t) -\n\nx(t - T)\nT\n\nEvaluating at t = nT yields\n\ny(nT) =\n\n1\n1 + Ta\n\n(x(nT) - x(nT - T) + y(nT - T)) .\n\nEmploying equations (12) and (13),\n\nD(z) =\n\n1 - z\n\n1\n1 + Ta\n1 -\n\n1\n1 + Ta\n\n-1\n\n1-28\n\nAn alternate solution employs equation (19) as follows\n\nD(z) =\n\n1 -\n\nT\n-1\nz\n\n1 T\n\n1 - z\n\na+\n\n-1\n\nT\n1 -\n\nz\n\naT + 1 - z 1\n\nD(z) =\n\n1\n1+ aT\n\n11 -\n\n-1\n\nz\n\n1\n\nZ-1\n\n1 + aT\n\nForward Difference\nA similar numerical technique approximates\n\nd\n\n(t)\n\ndt\n\nSee Figure 11.\n\ny(t + T) - y(t)\nyT\n\n1-29\n\nThis represents the equivalent Laplace domain approximation\n\neSTY(s) - Y(s)\n.T\nT\n\nsY(s)=\n\nor\n\n* e sT - 1\ns =\nT\n\ns\n\n. z-\n\n1\n\n=\n\nT\n\nHence,\n\nD(z) = G(s)\n(20)\nz-1\nS\n\nT\n\nExample.\n\nFind a discrete version of G(s) using the forward difference.\n\nG(s) =\ns + a\n\nD(z) =\n\ns\ns+a\n\nz-1\n\nT\n\n1-30\nz. -1\n\nD(z) =--\n\nz-1\n\n+ a\n\nT\n\n1\n\nz\n\n-\n\n- 1\n\nD(z) =\n1 + (aT - 1) z\n\n1\n\nRectangular Rule\nSuppose now we try some numerical approximations to integrals and\ncompare results.\nLeft Side Rule.\n\ny(t) = it\n\nLet us determine the numerical approximation for\n\nx(t)dt\n\no\n\nAssume that the upper limit of the integral is t = nT.\n\ny(nT) = fnT\n\nx(t)dt .\n\nHence\n\n(21)\n\n0\n\nFigure 12a illustrates the rectangular rule using the left side of the\nrectangles.\n\nHence\n\nn-l\n\n\xc2\xa3\n\ny(nT) = T\n\nx(iT)\n\ni=o\n\ny(nT+T) = T\n\nn\nZ\ni=o\n\n= y(nT) + Tx(nT)\n\nn-i\nx(iT) = T Z\ni=o\n\nx(iT) + Tx(nT)\n\n1-31\n\nx(t)\n\nA\n.\n\n/\nT\n\nT\n\nLeft Side Rule\n\n(a)\n\nx t)\n\ni\n\n7\nX\ni\n\nI\n\n-\n\n.1\n\nT\n\n(b)\n\nFigure 12.\n\na-\n\n-- W\n\nT\n\nRight Side Rule\n\nThe rectangular rule.\n\nsaw\n\nt\n\n1-32\nTherefore using equations (12) and (13)\nTzD(z) =1-z-l\nT\nz-1\n\nHence we have approximated the integration transfer function\n\n1\n\nT\n\ns\n\nz-1\n\nwhich gives the same results as equation (20) for the forward difference.\n\nRight Side Rule.\n\nFigure 12b illustrates\n\nthe use of the right side\n\nof the rectangle in approximating equation(21).\n\nTherefore\n\nn\ny(nT) = T\n\nE\n\nx(iT)\n\ni=l\n\nn+l\ny(nT + T) --T\n\nx(iT) = T\ni=l\n\nn\n\nZ x(iT) + Tx(nT + T)\ni=l\n\n= y(nT) + T x(nT + T)\n\nLetting n = n - 1\n\ny(nT) = y(nT - T) + T x(nT)\n\nEmploying equations (12) and (13) one finds\nT\nD(z) =\n\n1 - -1\nz\n\n1-33\n\nHence, we have approximated the integrator\n\n1\ns\n\nT\nl-z-l\n\nA\n\nwhich yields the identical result of equation (19) for the backward\ndifference.\n\nTrapezoidal Rule.\nThe trapezoidal rule takes the average of the left and right side\nof the rectangles in Figure 12.\n\nT n-l\nT nZl\n\ny(nT)\n\n[x(iT) + x(iT + T)]\n\ni=o\n\n1\n2\n\n[T\n\nHence\n\nn-l\n\nZ\n\nx(iT) + T\n\ni=o\n\nn\n\nZ x(iT)\ni=l\n\ni\n\nUsing the results of the rectangular rule,\n1\nD(z) =\n\n-\n\n2\n\nT\n= 2\n\nTz-1\n\nl_-i-\n\nT\n\n+\n1-z-\n\n]\n\nl+z-1\n1-z 1\n\nThus we have approximated\n\n1\ns\n\n-\n\nT\n2\n\n1 + z 1\n1\' - z--\n\nThis approximation is the familiar bilinear z - transform.\n\n1-34\n\nSimpson\'s Rule\nSimpson\'s Rule evaluates equation (21) by the following formula\n\ny(nT)\n\n=\n\nT [x(O) + 4x(T) + 2x(2T) + ...+ 4x(nT - T)\n3\n\n+ x(nT)]\nBut\ny(nT + 2T) = y(nT) + T [x(nT) + 4x(nT + T) + x(nT + 2T)]\n\nLetting n -= n - 2 and following equations (12) and (13) yields\n-\n\nD(z) = T\n3\n\n-\n\n1 + 4z 1 + z\n1 - z- Z\n\n2\n\nHence, we have approximated\n\n1\ns\n\nT\n\'\n3\n\n1 + 4z- 1 + z- 2\n1 - z\n\nNote that this formulation is valid only at even iterations (n even).\n\nImpulse Invariance\nSuppose that we want to find a discrete equivalent filter for the\nLaplace transfer function G(s).\n\nFurther suppose that we desire the im-\n\npulse response of the discrete equivalent to match that of the analog\nfilter as shown in Figure 13.\n\ng(nT) = d(nT)\nThen\nco\n\nD(z) =\n\nE\n\ni=O\n\nd(nT)z n\n\n1-35\n\ng\n\nx(t) = 6(t)\nI\n\nt\n\nt\n\n(a)\n\nAnalog Filter\n\nd(t)\n\nx(t\n1\nt\n\nNo\n\n(b)\n\nFigure 13.\n\nDigital Filter\n\nImpulse Invariance\n\nA\n-\n\n*\' 1\n-"".\n\nT\n\n1-36\nco\n\nI g(nT)z - n\ni=O\n= G(z)\nwhich is the standard z-transform. Hence, for impulse invariance\nD(z) = Z[G(s)] = G(z)\n\nthe digital approximation is just the standard z-transform of G(s).\nImpulse Invariant Integrator\nLet us find the digital equivalent of an analog integrator using\nimpulse invariance and the models of Figure 14.\n\nG(z) =Z[1\n\nWe know that\n\n1\n\nand that\nGho(s) =\nfor small values of T.\n\n1\n\n- e -Ts\n\nT\n\nHence\n\nYd(z) =\nT\nX(z)\n1- z-\n\nand we have again approximated\n\n1=\ns\n\nT\n1-\n\nz\n\nTherefore, the backward difference, the right side rectangular rule, and\nthe impulse invariant integrator all indicate equation (19) as their\nequivalent sampled-data transformation.\n\n1-37\n\nY(s)\n\nX(s)\n\n(a)\n\nAnalog Integrator\n\nIntegrator)\n-/Digital\n\n.(s)(b)\n\xc2\xb7X\n\n(b)\n\nFigure 14.\n\nDigital Integrator\n\nImpulse invariant integrator.\n\nd(s)\n\n1-38\nMapping Functions Summary\nAs a result of our analysis of some elementary numerical approximation techniques we have identified several sampled data mapping functions.\n\nStandard z-Transform\nThe standard z-transform yields an impulse invariant filter the\nmapping function for this transformation is\n\ns = 1 In z\n\n.\n\n(22)\n\nT\nThis mapping has been previously defined in Figure 9.\n\nBackward Difference\nThe backward difference approximation for the solution of differential equations provides the following mapping\n-1\n\ns = 1 - z\nT\nSee Figure 15.\n\n(23)\n\nNote that the region of stability in the s-plane maps\n\ninto the right half plane z- ] > 1 of the z- 1 plane.\nof instability in the z\n\n1\n\nSince the region\n\nplane is the interior of the unit circle,\n\nstable analog filters will always result in stable digital equivalents.\nIn fact some unstable analog filters give stable digital ones.\n\nA major\n\ndisadvantage of this mapping is seen in the frequency response contour.\nThe jw axis in the s-plane does not map to the unit circle in the z\n\nplane (or the z-plane either).\n\n1\n\nHence, as we get farther from s = 0 or\n\nz = 1 the more degraded will be our desired frequency response.\n\nThus,\n\n1-39\n\n/,\nB\n\na\n\n(a)\n\ns-plane\n\nIm(\n\n- 1\n\n)\n\n4\nI I\n\nI\n\n/\n(b)\nFigure 15.\n\nz-\n\n1\n\nplane\n\nMapping s = 1 - z\nT\n\nRe(z-1)\n\n1-40\nwe must decrease T (increase fs) to improve this approximation.\n\nForward Difference\nThe forward difference approximation suggested the following\nmapping\ns = z - l\nT\n\n.\n\n(24)\n\nThis mapping function is shown in Figure 16.\n\nNote that the left-half\n\nplane in the s domain maps to the region to the left of z - 1 in the zplane.\n\nBut the interior of the unit circle represents the stability re-\n\ngion in the z-plane.\n\nConsequently, some stable analog filters will\n\ngive unstable digital ones.\n\nUnstable analog filters will also be un-\n\nstable digital ones under this mapping.\n\nYet a further disadvantage is\n\nthe same frequency contour encountered in Figure 15.\n\nHence, this is an\n\nundesirable mapping.\n\nBilinear z-Transformation\nThe trapezoidal integration approximation led to the sampled data\nmapping\n-1\ns = 2 1 - z\nT1+ l\nTz-\n\n.\n\nThis mapping is illustrated in Figure 17.\n\n(25)\n\nNotice here that the entire\n\nleft-half s-plane maps to the interior of the unit circle in the zplane.\nones.\n\nHence, all stable analog filters will result in stable digital\nAlso, the jw axis in the s-plane maps to the unit circle in the\n\nz-plane.\n\nHowever, the entire jw axis maps onto the unit circle which\n\ncauses a mismatching of frequencies.\n\nThis is a direct result of the\n\n1-41\n\na\n\n(a)\n\ns-plane\nIm(z)\n\nRe(z)\n\n(b)\nFigure 16.\n\nz-plane\nMapping s =\nT\n\n1-42\n\na\n\n= 0\n\n(a)\n\ns-plane\n\nz)\n\nO= 0\nz= 1\n\n(b)\nFigure 17.\n\nz-plane\nBilinear z-transform.\n\nRe(z)\n\n1-43\ncharacteristic that for a digital filter\n\n+ C = O\n\nZ =1\n\nZ = jl + C = Us/4\nz = -l1 +\n\n= Us/2\n\nas required by equation (18).\n\nFor the bilinear z-transform the frequen-\n\ncies in the z-plane (ID) are related to frequencies in the s-plane (wA)\nby\nej w D T - 1\njwA =jT\nejwDT +1\n\n2j sin wDT\n2\n2 cos wDT\n\n2\nor\niD\n\n=\n\n2\n\ntan\n\n\'A\n\n(26)\n\nT\nSee Figure 18.\n\nCorrection for this frequency scale warping may be accom-\n\nplished by redesigning (prewarping) the critical frequencies of the desired transfer function G(s) before applying the bilinear z-transform.\nThis transformation maps circles and straight lines in the s-plane\nto circles in the z-plane.\n\nIt works well for frequency characteristics\n\nwhich are piecewise linear.\n\nIt also insures that no frequency aliasing\n\ncan occur in the transfer function frequency characteristic because the\n+jw axis does map into the upper half of the unit circle.\nbilinear z-transform is quite popular.\n\nHence, the\n\n1-44\n\nWA\n\n1\n\n/Ideal\n\nI\n\nz\n\nI\n\n~WD\n\n2T\n\nFigure 18.\n\nChange in frequency scale for bilinear\nz-transform.\n\n1-45\nMatched z-Transforms [2]\nThe standard z-transform of equations (3b) and (4) required a\npartial fraction expansion of G(s) in order to complete the mapping\n\n1\n\n=\n\n1\n\ns +u\n\n- 1\n\n1 - e-UTz\n\nFor the purpose of simplifying the calculations, the matched z-transform\nmaps the poles and zeroes (-bj and -aj of equation (3a)) to the z-plane\nas follows:\ns + a +\n\nz\n\n-1\n\ne-aT\n\n(27)\n\nHence the matched z-transform of equation (3a) is\n\nG(z) = G(s)\ns + a i = 1 - z leaiT\nDs+\n\nm\n\nb\n\n= 1 - z le-bjT\n\n(\n1 le-ai T\n-(1 -z e\n\nKi= 1\nK\nn\n(-\n\n1\n\nz- e(28-a\n-\n\nT\n\n(28)\n\nj =1\nwhere K is adjusted to give the desired gain at d.c. (z = 1).\n\nThis\n\ntransform matches the poles and zeroes in the s and z planes.\n\nNote that\n\nthe poles of this transform are identical with those of the standard ztransform but that the zeroes are different.\n\nBecause of this difference,\n\nthe matched z-transform may be used on nonbandlimited inputs.\n\nIf G(s)\n\nhas no zeroes, it is sometimes necessary to multiply (1 + z-l)N, N an integer, times the expression (28).\n\n1-46\n\nOther Transforms\nIn general any transformation which maps the stable region of the\ns-plane into the stable region of the z-plane may be used.\n\nIt is help-\n\nful for the jw axis in the s-plane to map to the z-plane\'s unit circle.\nAnother important property is that rational functions G(s) should be\ntransformed into rational functions D(z) so that the proper difference\nequations may be determined for realization.\n\nSimpson\'s Rule\nThe Simpson\'s Rule approximation suggested that the mapping\n\ns =3\n\n1 - z\n\n(29)\n\nT\n-1\n-2\n1+ 4z\n+ z\nbe used as a transformation.\nan exercise for the reader.\n\nThe analysis of this mapping is left as\nPlease note that a second-order function\n\nG(s) will transform to a fourth-order D(z).\n\nThis is undesirable from a\n\ndigital hardware viewpoint.\n\n(w.v)-Transform [14]\nIn some applications, the system transfer function G(s,z,z") may be\na function of s, z = eT s , and za, where 0 < a < 1.\n\nIf all initial con-\n\nditions are zero and\n\nw = 2\nT\nv(a) =\n\n1 - z\nl+z - L\n-\n\n(1\n\nz\n1\n\n-+ a(a\n\n- 1) z-) 2 ,\n2(1\n\n-) 1\n\n2\n\n1-47\nthen for a system described by\n\nY(s) = G(s,z,za)\n\nX(s)\n\nits z-transform will be\n\nY(z) = G(w,z,v(a))\n\n[\n\nX(z)\n\n- x(O)\n\n]L\n\n1 + z1\n\nIf x(0) = 0, then\n\nD(z) = G(s,z,za)\ns =w\n\nza = v(a)\n\nThis completes the definition of the (w,v) transform.\n\nExample.\n\nScott [15] has shown that a desirable phase lock loop\n\nhas the transfer function\n\nG(s) =\n\n10\ns + lOz\n\nUsing the (w,v) transform to find a digital equivalent if x(0) = 0\n\nD(z)\n\n=\n\n10\ns + 10z - 0 .5\nw = 2\'1 -\n\nz-l\n\nT 1 + zz 0--= v(0.5)\nv(0.5) = 1 - 0.5(1 - z\n\n) + 0.5(-0.5) (1 - z-)\n2\n\n= 0.375 + .75z- 1 _ .125z- 2\n\nD(z)\n\n=\n\n+z 1 )\n5T(1\n(1\n(1 + 1.875T) -\n\n-\n\n5.625T)z\n\n+ (3 1\n\'\n\n25\n\nT)\n\nz\n\n- (625T)z-\n\nIV. DISCRETE STATE VARIABLES [5]\n\nAn nth order discrete-time system is generally described by difference equations.\n\nThe difference equation description of the system\n\ndynamics may be alternately presented in vector matrix (state variable)\nform by the following set of first-order difference equations.\n\nx(kT + T) = Ax(kT) + Bu(kT)\n(30)\n\ny(kT) = Cx(kT) + Du(kT),\n\nwhere x(kT), u(kT), and y(kT) are vectors of the discrete state variables,\ninput variables, and output variables respectively. The symbol T is the\nsampling period and k is any non-negative integer.\nFor the purpose of simplifying the notation, the sampling period\nT shall hereafter be omitted from the equation; thus, equation (30) becomes\nx(k + 1) = Ax(k) + Bu(k)\ny(k) = Cx(k) + Du(k)\n\n(31)\n\nThe solution of equation (30) can be found by rewriting the first\nof equations (31) in standard z-transform notation:\n\nz X(z) = AX(z) + BU(z) + zx(O),\n\nwhere x(O) is a vector of the initial condition of the state variables.\nSolving for X(z) produces\n\n(z) = (zI\n\n-\n\nA)-lzx(O) + (zI - A)\n\n1-48\n\nBU(z).\n\n(32)\n\n1-49\nThe inverse z-transform (Z-1) of (zI - A)-lz is\n\nZ-l[(zI - A)-lz] = 4(k) = Ak\n\nTherefore, the inverse z-transform of (32) yields\nk\n\nk-l\n\nx(k) = Akx(O) +\n\nAk-l-\n\n1n\n\nn()\n\n(33)\n\nn=O\nThis solution demonstrates that the present state of the system x(k) is\ndependent upon the initial state x(O) and the system inputs u(n) from\nthe initial time (t = 0) to the present time (t = kT).\n\nV.\n\nCONVOLUTION\n\nIn this section a review of continuous and discrete linear systems\nis presented.\n\nThe equations are discussed in rapid succession.\n\nContinuous Linear Systems\nFigure 19a illustrates the conventional continuous system under\nconsideration.\n\nAny linear system obeys superposition and is characterized\n\nby the impulse response g(t, i), the response at time t due to an impulse\nat i.\n\nHence\n\nco\n\ny(t)\n\nf\n\n=\n\nx(E)g(t, C)dE,\n\n(34)\n\nwhich is called the superposition integral.\n\nIf the linear system is\n\nshift invariant then\n\ng(t, i)\n\n=\n\ng(t - 6)\n\nand equation (34) becomes\nco\n\ny(t)\n\nf=x(C)g(t\n\n- E)dE,\n\n(35)\n\n. x(t)*g(t)\n\nwhich is the convolution integral.\nBut, by definition of the Laplace transform\noo\n\nY(s) = f y(t)e-stdt .\n0\n\n1-50\n\n1-51\n\nx(t)\n\nSystem\n\n(a)\n\nx(nT)\n\ny(t)\n\n-_-\n\ny(nT)\n\nContinuous System\n\nSystem\n\n(b)\n\ni.p\n\nDiscrete System\n\nFigure 19.\n\nConvolution.\n\n1-52\n\nTaking the Laplace transform of equation (35) produces\n\nY(s) = G(s)X(s),\n\nwhere\n\nf\n\nG(s) =\n\ng(t)e-stdt\n\n0\nis called the system transfer function;\nand G(jw), the frequence response of the system.\n\nDiscrete Linear Systems\nFigure 19b illustrates the conventional discrete-time system under\nconsideration.\n\nThe linear discrete system also obeys superposition and\n\nis also characterized by the impulse response d(nT, kT), the response\nat time nT due to and discrete impulse 6(kT), where\n\n6(kT) = 1\n\nif k = 0\n(36)\n\n=0\n\nif k\n\n0\n\nand\n\nx(nT) =\n\nX x(kT)6 (nT - kT)\nk=O\n\nBy superposition\nco\n\ny(nT) =\n\nE x(kT)d(nT, kT),\nk=O\n\nwhich is called the superposition sum.\nthen\n\n(37)\n\nIf the system is shift invariant,\n\n1-53\nd(nT, kT) = d(nT - kT)\n\nand equation (37) becomes\nco\n\ny(nT) =\n\nX x(kT)d(nT - kT),\nk=O\n\n(38)\n\nA x(nT)*d(nT)\n\nthe convolution sum.\n\nBut by definition of the standard z-transform\n\nco\n\nY(z) =\n\ny(kT)z\nk=O\nX\n\n-k\n\n.\n\nTaking the z-transform of equation (38) produces\n\nY(z) = D(z)X(z)\n\nwhich is the identical result in equation (11).\n\nHence, D(z) is called\n\nthe discrete system transfer function and D(eJWT) is called the frequency\nresponse (see equation (18)).\n\nVI.\n\nDISCRETE FOURIER TRANSOFRM\n\nThis section examines the properties of continuous Fourier transforms and derives the discrete approximation.\n\nContinuous Fourier Transform\nThe Fourier transformation may be defined by\n\nG(f) = f\n\ng(t)e-Ji 2 ftdt\n\n(39)\n\n_oo\n\nand the inverse Fourier transform as\n\ng(t) = f\n\nG(f)eJ2 rftdf .\n\n(40)\n\n_co\n\nThe similarity between equations (39) and (40) is illustrated by the\nsummary of Fourier transform pairs listed in Table 1.\nAnother useful property of Fourier transforms is shown below:\n\nf\n\ndt =\n\n1g(t)2\n\nf\n\nIG(f)1\n2\n\ndf.\n\n(41)\n\nThis is known as the Fourier Integral Energy Theorem.\n\nDiscrete Fourier Transform [6]\nSampling Process\nIn Figure 4, an impulse sampler was presented which sampled a\nsignal for t\n\n>\n\nO.\n\nHowever, if the signal is zero for negative t, the\n\nfollowing sampling function produces the same effect:\n\n1-54\n\n1-55\n\nTABLE 1.\n\nFourier Transform Pairs\n\nTime Function\ng(t)\n\nFourier Transform\nG(f)\n\n1\n\n6(t)\n1\n\nu(t + A) - u(t\n2\n\n6(t)\n\nAsinwAt\nrtAt\n6(t + A) - 6(t - A)\n\n2 cosrAt\ndg(t)\ndt\n-j2rtg(t)\n\nAsin~rAf\nrrAf\n\nA\n2\n\n- u(f -2\n\nu(f +\n\n2 cosrAf\n\n6(f + A) + 6(f\nj2TifG(f)\nd\ndfG(f)\n\nt\n\nf\n\nG(f)\nj2rf\n\ng(x)dx\n\n0\ng(At)\n\nAgl(t) + Bg2 (t)\ng(t + A)\n\n1 G(f)\nA\nA\nAGl(f) + BG 2 (f)\nej2\n\nGfAG(f)\n\ne2Atg(t)\n\nG(f + jA)\n\ng(t)*x(t)\n\nG(f)X(f)\n\ng(t)x(t)\n\nG(f)*X(f)\n\nO0\n\nI\nk=-I\n\n6(t - kT)\n\n1 ki (fk\n1 T\n6(f T k= -\'\n\nA\n2\n\n1-56\n\nA(t) =\n\nX\n\n6(t - kT).\n\n(42)\n\nk-co\n\nFrom Table 1 the Fourier transform of A(t) is\n\nF[A(t)] =1\n\n6(f\n\nk)\n\nTk=cO\n\nIn order to verify this result we may note that F[A(t)] is periodic and\nmay be expressed in a Fourier Series as\nF[A(t)]\n\n:\n\ncne\n\nwhere\n1\n2T\n\ncn=\n\nT\n\n1\n\nj2rnTf\nF[A(t)]eJ2n\nTfdf\n\n2T\n\n2T\nc =T\nn1\n\nf\n\ndf = 1\nT\n2T\n\nHence\n\nF[A(t)] =\n\n-j 27rnTf\n\ne\n\nas expected from equation (42).\nSuppose the sampling function in equation (42) is multiplied by\nan input signal g(t) to produce the sampled signal\n\n1-57\n\ng*(t)\n\n=\n\ng(t)A(t)\n\ng(kT)6(t - kT).\n-=\nk=-o\n\nAs illustrated in Figure 5, if B/2n, the highest frequency in the\nsampled signal is less than fs/2, then recovery of the original signal\nis possible with an ideal low-pass filter whose cutoff is fs.\n\nTo\n\nrecover the signal, gr(t) we multiply G*(f) by a square window function\nFep(f) .\n\n(43)\n\nGr(f) = G*(f)Ftp(f)\n\nSince multiplication of Fourier transforms represents convolution in the\ntime domain\n\ngr(t) = g*(t)* sin(rt/T)\n\n=rt/T\n\n*\n\ng(kT)6(t - kT)1\n\nco\n\nsinj(t-k\n\nsint/T/)\n\nT)\n\ng(kT)-\n\ngr(t) =\nk=-\n\nr t - kT)\n\nkT)\n\nIf fs > B/r, the low pass filter is ideal, and the samples g(kT) are\nexact, then\n\ngr(t) = g(t).\n\n1-58\n\nHowever, the samples are never exact, no signal is ever bandlimited, and\nno low-pass filter is ideal.\n\nTherefore, we can\'t exactly recover a\n\nsampled signal.\nDFT Derivation\nNow discrete versions for (39) and (40) will be determined.\n\nDefine\n\nthe following conditions for (39):\nfs\n\n=\n\nsampling frequency\n\nT = 1/fs\n\nsampling interval\n\n=\n\ng(t) = 0 outside the interval t = [0,NT]\n\nN = an integer, the number of sample points\nG(f) is bandlimited to + fs/2.\nPlease note that these conditions can never be completely satisfied.\nWith the time-limited function g(t), G(f) cannot be bandlimited.\npractice it can get quite small as jfI increases.\n\nIn\n\nHowever, since a\n\nfunction is never time and bandlimited, the time and frequency samples\nare corrupted by aliasing.\nUsing the above conditions in equation (39) one obtains\n\nG(f) =\n\nNT\nf g(t)e-J 2 ftdt\n0\n\nUsing the rectangular rule for numerical integration\nN-l\nG(f) = T\n\nI\n\nk=O\n\ng(kT) e j2ifkT .\n\n(45)\n\n1-59\n\nThe discrete version of equation (39) will compute samples of G(f)\nevery Af = fs/N = 1/NT Hertz.\n\nSubstituting f = nAf into equation (45)\n\nN-1\nj\nG(nAf) = T I\ng(kT)e i2\nk=O\n\nnA\n\nfkT\n\nor\nG(n/NT) = T\n\nN-1\n. g(kT)e- j(27/N)nk\nk=O\n\nIn another form\n\nG(n/NT) = T\n\nN-1\nI g(kT)Wnk\nk=O\n\n(46)\n\nwhere\n\nW = ej 2\n\n/ N\n\nSince g(t) = 0 outside the interval O<t<NT, one can construct\na periodic function h(t) from g(t), with period NT:\n\nh(t)\n\n=\n\nI\ng(t - mNT),\nm---\n\nwhich may be written\nt\nh(t) = f\n0\n\n6(t - mNT - T)dT\n\ng(T)\n\n= g(t)*\n\nI\nm=--\n\n6(t - mNT)\n\n(47)\n\n1-60\n\nwhere the * denotes convolution.\nso is H(f).\n\nSince G(f) is bandlimited to fs/2,\n\nTherefore,\n\nH(f) = G(f)[l/NT I\n\nm--\n\n6(f - m/NT)]\n\nm=-NT/NT\nNT\nNT\no\n\nand\nf\nD\n\n.H(f)\n\n=\n\nI\n\nH\'(m/NT)6(f - m/NT) ,\n\n(48)\n\nm=--00\n\nwhere H(f) is the continuous Fourier transform of h(t).\n\nThe weighting\n\nfunction H\'(m/NT) in equation (48) is defined below:\n\nH\'(m/NT) = (1/NT)G(m/NT) .\n\n(49)\n\nEquation (46) may be inserted\'in (49),\n\nH\'(m/NT) = (1/N)\n\nN-1\nI g(kT) W -mk\nk=0\n\n(50)\n\nFrom (47),\n\ng(kT) = h(kT):\n\nk=0, N-1.\n\nTherefore (50) becomes\nN-1\nH\'(m/NT) = (1/N)\n\nI\n\nk=0\n\nh(kT) W - k\n\n(51)\n\n1-61\n\nIDFT Derivation\nThe inverse discrete Fourier transform is found by considering\nequation (40)\n\nh(t) = f\n\n2\nH(f)eJ ftdf.\n\nwO\n\nUnder the conditions of the previous section\n1/2T\nh(t) = f H(f) eJ 2 ftdf\n\n(52)\n\n-1/2T\nsince H(f) is bandlimited to fs/2 = 1/2T.\nSubstituting equation (48) into (52) and evaluating at t = kT,\n\nh(kT) =\n\n1/2T\n2\nf\ni\nH\'(m/NT)eji\nfk\n-1/2T m=->\n\nT6(f - m/NT)df\n\nSince the integrand is periodic (1/T) in f,\n\nh(kT)=\nh(kT)\n\nI\nf\n0\n\n(m/NT)e (2 /N)mk\nH\'(m/NT)e j\n6(f\n\n-\n\nm/NT)df\n\nm=-o\n\nThe limits of integration truncate the sum to\nN-1\n\nh(kT)=\n\nH(m/NT)Wk\n\n(53)\n\nm=O\nThe prime is dropped in equations (51) and (53) for convenience, and the\nresulting relations are\n\n1-62\nDFT\nN-1\nH(m/NT) = (1/N)\n\nE\n\nh(kT)W mk\n\'\n\n(54)\n\nk=O\n\nIDFT\nh(kT) =\n\nN-1\nI\nm=O\n\nH(m/NT)Wmk\n\n(55)\n\nThese equations define the discrete Fourier transform pair.\n\nThis\n\ntransform may be thought of as a mapping of N points in the time domain\nto N points in the frequency domain.\nDFT Pairs\nAlthough equations (54) and (55) are discrete approximations of\n(39) and (40), we can show that they form exact transform pairs.\nFrom equation (54),\n\n1\n\nH = N [W] h\nj\n\n(56)\n\nwhere H and h are vectors constructed of the N samples in the frequency\nand time domains, and\n\n1\n\n1\n\n1\n\nW- 1\n\n.\n\n.\n\n.\n\n1\n\n. . w (N-l)\n-2\n\n[W] =\n\n(57)\n1\n\nW- (N - l)\n\n.\n\n..\n\n2\nW-(N-1)\n\nFrom equation (55)\n\nh = [W*] H\n\n(58)\n\n1-63\n\nwhere [W*] is the complex conjugate of [W].\n\nIn order to prove that the\n\ntransform pairs are exact one must show that\n\n[W\n\n1\n\n=\n\n1 [W*]\nN\n\n(59)\n\nThis proof follows:\nLet\n\n[P] = [W] [W*]\n\nthen a general element of [P] is\n\nP.m\n\n=\n\n[1 W- (-1)... . W-\n\n(N -\n\n1\n1)\n\n(\n\n- 1 )\n\n]\n\nw(m-l)\n\n1I\n\nw(N-l) (m-l)\n\n= 1 + W - (-m) + ...\n\n+ W- ( N\n\n-\n\nl)\n\nThen all diagonal elements of [P]\n\nPtQ = 1 + 1 + ...+ 1 = N\n\nand the off diagonal elements\n\nPem\n\n=\n\n1\n\nm\n\n= .\n\nHence\n\n[P] = N[I]\n\nand the exact relationship is proved.\n\n(l-m)\n\nI\n\n1-64\n\nA summary of DFT pairs is listed in Table 2.\n\nSeveral other\n\ninteresting relations are displayed below:\n\nh(0)\n\n=\n\nN-1\nZ H(m/NT)\nm=0\n\n(60)\n\nN-1\nH(O) = (1/N)\n\nX\n\nh(kT)\n\n(61)\n\nk=0\n\nand\nN-l\n\nN-l\n2\n\nIh(kT) 1 =\n\nI\nk=0\n\nIH(m/NT) 2 .\n\n(62)\n\nm=0\n\nThis last relation is known as Parseval\'s Theorem.\n\nFast Fourier Transform\nCalculation Time\nThe fast Fourier transform (FFT) is a high speed technique for\ncalculating the DFT.\n\nN = rlr2...rn\n\nIf the number of samples N may be written\n\nri an\n\ninteger\n\nthen\n\n[W] = [W 1] [ W 2 ]\'\'\'\n\n[ W n\n]\n\n(63)\n\nwhere [Wi] is an N x N matrix with only riN non-zero elements.\ncalculation of\n\nH\n\n=\n\nH\n\n1\nN [W] h\n\nThe\n\n1-65\n\nTABLE 2.\n\nDFT Pairs\n\nh(kT)\n\nH(m/NT)\n\nAhl (kT) + Bh 2 (kT)\n\nAH(m/NT) + BH(m/NT)\n\nh(kT - nT)\n\nW-mnH (m/NT)\n\nh l (kT)h 2 (kT)\n\nHl(m/NT) *H2 (m/NT)\n\nh*(kT)\n\nH* (-m/NT)\n\nh(-kT)\n\nH(-m/NT)\n\n6(kT)\n6(kT - nT)\nN-1\n1 E hl(t + k)h 2 (t)\nN t=o\n\n1/N\n(1/N)W-mn\n\nH 1 (m/NT)H2 (-m/NT)\n\n1-66\n\nrequires N2 operations of complex multiplication, whereas\n\nH\n\n1 [hW1][W2]...[W\n\nI\n\n(64)\n\nrnN operations\n\nrequires (rl + r2 + \'-.\n\n+ rn) N operations.\n\nFor the special case ri = 2,\n\nN = 2n, the total number of operations is\n\n#oper = (2 + 2 +\n\n\'-- + 2)N\n\n= 2nN\n= 2Nlog2 N\nExample.\n\n.\n\n(65)\n\nCompare the time to calculate the DFT and FFT of a sequence\n\nof 1024 samples of a time function given that a typical computer calculates a complex multiplication in about 40ps\n\nDFT:\nCalc. Time \' N2(40ps)\n= 1.053 x 106 x 40 x 10-6\n= 42.1 seconds\n\nFFT:\nCalc. Time\n\n2Nlog2N(40ps)\n= (2048)(10)(40 x 10-6)\n= .82 seconds\n\n1-67\n\nFFT Derivation\nSince the DFT is a linear operation and N = 2 , we may break equation\n(54) into two functions, the even samples and the odd ones:\n\nH(m/NT) =\n\n(N/2)-m\n2\n[h(2kT)W 2mk + h(2kT +T)W- mW\n1\nN\nk=O\n\n(N/2)-l\n=\nk=O\nN\n\n-2mk\n\nm\n\n]\n\n-m (N/2)-1\nh(2kT+ T)W- mk\n)W\n2\nN\nk=O\n\nor\n\nH(m/NT) = DFT[h(2kT)] + W-mDFT[h(2kT + T)]\n\nfor m\n\n- 1.\n\n=\n\nw\n\n-.\nm\n\nN\n\nBut\n\nN\n\n-\'W\n\n2 =W-mW\n\nW2(m +\n\n(66)\n\n=\n\n=WW\n-)\n\n-M\n\nW-2m\n\nTherefore, the remaining samples may be determined by\n\nH(m/NT + 1/2T)\n\n=\n\n(N/2)-1\n2mk\nh(2kT)W 2\nk=O\n\n1\nN\n\n-m (N/2)-1\n\nW\nw\nN\n\n-2mk\n\n) h(2kT +T)W\nk=O\n\nor\n\nH(m/NT + 1/2T) = DFT[h(2kT)] - W-mDFT[h(2kT + T)]\n\n(67)\n\n1-68\nN\nfor m = 0, 2 - 1.\n\nThe above equations may be successively applied in\n\norder to achieve the maximum reduction in computation time indicated\nby equation (65).\n\nThe technique of dividing the time samples into\n\neven and odd parts is sometimes called "decimation in time."\n\nThe FFT\n\nof eight-points is illustrated in Figure 20.\nNote that the time samples are entered in "bit reverse" order:\n\n0\n1\n2\n3\n4\n5\n6\n7\n\nbit\nreversal\n000\n100\n010\n110\n001\n101\n011\n111\n\nbinary\n000\n001\n010\n011\n100\n101\n110\n111\n\n0\n\n4\n2\n6\n1\n5\n3\n7\n\nIFFT Derivation\nSince the IDFT is a linear operation and N =\n\nn, we may separate\n\n2\n\nequation (55) into two functions, the even samples and the odd ones:\n\n(N/2)-1\nh(kT) =\n\nI\n\n2mk\n2mk k\n[H(2m/NT)W\n+ H(2m + 1/NT)W\nWk]\n\nm=O\n(N/2)-1\n(N/2)-1 .\nI\nH(2 m/NT)W2mk + Wk\nI\nH(2 m + 1/NT)W2 mk\nm=0\nm=0\n\nor\n\nh(kT) = IDFT[H(2m/NT)] + WkIDFT[H(2m + 1/NT)],\n\nfor k = 0, N2\n\n1.\n\nBut\n\n(68)\n\n1-69\n\nEs\no\n\nEs4\n00)\nO\n\nZ\n\nZ\n\nZ\n\n00\n\nZ\n\nZ\n\nIruC\n\nZ\n\nEXo\n\nZ\n\nvE\no\n\nc0\n-\n\n.,\n4\n\nZ\n\n4q\nEco\n\nco\n\nQ)\n\nII\n\nz\n\nm\na)\n3\n\n.v\n\nI\n\na)\n\n-,\nP14\n\nII\nJZ\n\n,c\n\no:\n\n1-\n\n%-\n\n.Z\n\nc,\nE.\n\n-,~\nsr\n\nOD\n_S\n\na\n\n_\n\n%0\nE:\n\nC4\n\nI\'D\n\\_\n\n%-\n\n%-\n\np\n_\n\nUV\n\nv\nC\n\ni-v\n,_\n\n1-70\n\nWk +\n\nN\n\nN\n\nkw = _Wk\n\nW2(k + N) = W2 kWN = W2k\n\nTherefore the remaining samples may be calculated by\n\nNT\n(N/2)-l\nH(2m/NT)W\nh(kT + -) =\nm=0\n\n(N/2)-l\n\n2mk\n\nH(2m + 1/NT)W\n-\n\nm=0\n\nor\n\n)\n\nh(kT +\n\n=\n\nfor k = 0, 2- 1.\nfor the IFFT.\n\nIDFT[H(2m/NT)] - WkIDFT[H(2m + 1/NT)]\n\n(69)\n\nThe repetition of this process yields the algorithm\n\nThe above derivation is sometimes called "decimation in\n\nfrequency."\nEquations (66) through (69) suggest an algorithm for calculating\nthe IFFT as shown in Figure 21.\n\nIn this figure the equations (68)\n\nand (69) are employed at each stage of the transformation of eight\nfrequency samples into eight time samples.\n\nNote that the frequency\n\nsamples are again inserted in "bit reverals" order.\nThe reader will please note the similarity of the Figures 20 and\n21.\n\nThe basic element is sometimes called a "butterfly" as shown in\n\nFigure 22.\n\nThe gains on a few multipliers are different.\n\nsuggests the mechanication of FFT hardware.\n\nThis structure\n\n1-71\n\nO\no\n\nvE _\n\n\'~s\n\nv\n_\n\nv\n\n=\n\ns\n\n=~~~~~~b\n\nC\xc2\xb7\n1C\n\no\n\nt io4N =H\n\nX SZ~~~~\nX- vj\nm\n\nX\n:C\n\nw\n\n\'\n\nCO\nC\'\n\n1-72\n\na-\n\nb\n\nb=a+c\nd= a- c\n\nc\n\nX\n\nd\n-1\n\nFigure 22.\n\nButterfly structure.\n\nVII.\n\nRANDOM PROCESSES\n\nIn the analysis and synthesis of digital filters one frequently\nencounters signals which are random in nature that must be examined\nwith special techniques.\n\nContinuous Processes [6]\nIf G(f) is the Fourier transform of a continuous signal g(t), the\npower density function or power spectrum may be defined as\n\ngg(f) = lim\ngg\nA-~\n\nIG(f)\n\n{ A\n\n}\n\nThe auto correlation function\n\nA\nlgg(T) = lim\nA-\n\nA\n\nf g(t)g(t + T)dt.\n0\n\nThese two functions form a Fourier transform pair\nO0\n\nTgg(f) =\n\nf\n\nPgg(T)\n\nej2 fTdT\n\nBoth the power spectrum and the auto correlation function are real and\nsymmetric.\n\nDiscrete Processes [16]\nFor the discrete case, the cross-correlation function is first\ndefined\n1-73\n\n1-74\nN-1\n\nPy(kT) =im\n\nN\n\ny(nT)x(nT + kT)\n\nE\n\nN-+~\n\nn-0\n\nWhen both functions are the same (x\n\n=\n\ny), the cross-correlation becomes\n\nthe auto-correlation\n\n*,x(kT) = lim\nN+x\n\nN-1\nI\nn=O\n\nN\n\nx(nT)x(nT + kT).\n\nThe auto-correlation function evaluated at k = 0 yields\nN-1\nxx(O) = lim\nN-w\n\nN\n\nI\n\nx (nT)\n\nn=O\n\n= x2 (kT)\n\nthe mean squared value of the signal x(kT).\nSince the power spectrum is the Fourier transform of the autocorrelationswe see from Table 2 that\n\nxx(m/NT) = X(m/NT)X(-m/NT)\n\nif the signal x(kT) is time limited in the interval [0, NT].\n\n\'xx(m/NT)\n\nTherefore\n\n2\n\n= IX(m/NT) 1 .\n\nThe Fourier transform of the discrete cross-correlation function is\n\nxy (m/NT) = X(-m/NT)Y(m/NT)\n\nand is sometimes called cross power spectrum or cross-periodogram.\n\n1-75\n\nLastly, consider the discrete system of Figure 13b with a random\ninput whose power spectrum is known.\n\nWe may find the mean squared\n\nvalue of the filter\'s output by the following:\n\ny2 nT)\n(\n\n2\n\nTrJ\n\nr\n\nx(Z)D(z)D(z)D(l/z)dz/z\nx\n\nwhere\n\nr = the unit circle\n\nxx(Z)=\nk=O\n\nxx(kT)z k\n-\n\n= power spectrum\n\n1-76\nREFERENCES\n[1]\n\nB. C. Kuo, Analysis and Synthesis of Sampled-Data Control Systems,\nEnglewood Cliffs, New 3ersey, Prentice Hall, Inc. 1963.\n\n[2]\n\nR. M. Golden, "Course Notes - Designing Digital Filters z-Transforms and Fourier Analysis," National Electronics Conference,\nSt. Charles, Ill., June, 1969.\n\n131\n\nA. P. Sage and S. L. Smith, "Real-Time Digital Simulation for\nSystems Control," Proc. of IEEE, Vol. 54, No. 12, Deo,,\'1966,\npp. 1802-1812.\n\n[4]\n\nH. T. Nagle, Jr., "The Organization of a Special-Purpose Computer\nto Implement a Generalized Digital Filter for Sampled-Data Control Systems," Doctoral Dissertation, Auburn University, Auburn,\nAla., June 3, 1968.\n\n[5]\n\nP. M. DeRusso, R. J. Roy and C. M. Close, State Variables for\nEngineers, New York, N. Y., John Wiley and Sons, Inc., 1965.\n\n[6]\n\nG. K. McAuliffe, "Course Notes - The Fast Fourier Transform and\nSome of Its Applications," National Electronics Conference, St.\nCharles, Ill., June, 1969.\n\n[7]\n\nA. R. M. Noton, Introduction to Variational Methods in Control\nEngineering, New York, Pergamon Press, 1965.\n\n(8]\n\nC. Williamson, "Optimal Controllers for Homing Missiles," Report\n#RE-TR-68-15, U.S. -rmy Missile Command, Redstone Arsenal, Alabama, Sept., 1968.\n\n(9]\n\nR. E. Kalman and R. S. Bucy, "New Results in Linear Filtering and\nPrediction Theory," J. Basic En&., March, 1961, pp. 95-108.\n\n[10]\n\nT. C. Hsia, "On Synthesis of Optimal Digital Filters," Proc. First\nAsilomar Conference on Circuits & Systems, Nov., 1967.\n\n[11]\n\nJ. B. Slaughter, "Quantization Errors in Digital Control Systems,"\nIEEE Transactions on Automatic Control, Vol. AC-9, January, 1964,\npp. 70-44.\n\n[12]\n\nB. T. Nagle, Jr., "Comments on \'A Least Upper Bound in Quantization Error,"\' IEEETAC, Vol. AC-14, No. 4, Aug., 1969.\n\n[13]\n\nB. Widrow, "Statistical Analysis of Amplitude Quantized SampledData Systems," AIEE Transactions on Applications and Industry,\nNo. 52, January, 1961.\n\n1-77\n\n[14]\n\nC. A. Halijak, "The (w,v)-Transform," Proceedings of the IEEE\n1972 Region 3 Convention, Knoxville, Tennessee, April 10-12,\n1972, pp. C4.1-C4.3.\n\n[15]\n\nR. E. Scott, "An Improved Phase Lock Loop Derived from Ideal\nSingle Sideband Modulation," Ph.D. Dissertation, Univ. of Denver,\nDenver, Colorado, June, 1966, pp. 20-27.\n\n[16]\n\nA. J. Monroe, Digital Processes For Sampled Data Systems, John\nWiley and Sons, Inc., New York, 1962.\n\nPART TWO\n\nDIGITAL FILTER THEORY\n\n2-i\n\nPART TWO:\n\nDIGITAL FILTER THEORY\n\nTABLE OF CONTENTS\n\nI.\n\nDigital Filter Categories. .\nA.\n\nB.\n\nC.\n\nII.\n\n2-1\n\n. . . . . . . . .\n\nNonrecursive Filters .....................\n2-1\n1. General . . . . . . . . . . . . . . . ....\n. . . . 2-1\n2. Finite Impulse Response Filters . . . . . . . . . . .\n2-2\n3.\nFast Convolution .\n..................\n2-5\n..\n2-7\n4. Linear Phase Filters ................\n5. Frequency Sampling Filters . . . .......\n.\n2-10\n6. Windowing Filters ......\n..\n..\n.....\n2-13\n7. Moving Average Filter . . .\n.........\n2-14\n8. Least Mean-Square Digital Filters . . . . . . . . . . 2-17\n9. Least Squares Polynomial Moving Arc Filter . . . . . . 2-19\n10. Digital Inverse Filtering . . . . . . . . . . . . . . 2-20\nRecursive Digital Filters. . . . . .\n1. General. . . . . . . . . . . . .\n2. Block Recursion. . . . . . . . . .\n3. Flat Group Delay Digital Filters.\n\n2-23\n.........\n2-23\n... . 2-23\n..\n2-27\n\nAdvanced Topics .\n. .. . . . . . . . . . . . . . . . . .\n1.\nComplex Digital Filters . . . . . . . . . . . . . . .\n2. Randomly Sampled Filters . . . . . . . . . . . . . . .\n3. Multirate Digital Filtering. . . . . . . . . . . . .\n4. Two Dimensional Digital Filters. . . . . . . . . . .\n5. Adaptive Digital Filters . . . ....\n....\n6. Floating Point Digital Filters ..\n........\n7. Optimal Digital Filters . . . . . . . . . . . . . . .\n8. Nonlinear Filtering . . . . . . . . . . . . . . . . .\n9. Range Adaptive Digital Filtering . . . . . . . . . . .\n10. Random Sample Skipping\n........ .......\n.\n11. Block-Floating-Point Filters ..\n. . . . . . . . .\n12. Sample-Rate Reduction Digital Filters.\n.. . . . . .\n\n2-30\n2-30\n2-32\n2-36\n2-39\n2-45\n2-45\n2-47\n2-57\n2-57\n2-57\n2-59\n2-60\n\nTransfer Function Synthesis............\n\n2-62\n\nA.\n\n2-62\n2-62\n2-64\n\nNonrecursive Filters ...................\n.\n1. Specification of Frequency Domain Zeroes .......\n2. Frequency Sampling ..................\n.\n2-ii\n\n.\n........\n.......\n........\n\n3.\n4.\n\nWindowing . . . . . . . .\nEquiripple Filters . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-64\n2-65\n\n.\n\n.\n\nB.\n\nC.\n\nIII.\n\nRecursive Filters . . . . . .\n............. .2-66\n1. Direct Synthesis in the Frequency Domain . . . . . . .\n2-66\n2. Sampled Data Transformation . . . . . . . . . . . . .\nIs\n2-68\n3. Digital Compensators . . .\n2-90\n4. Frequency Sampling . . . .\n.. . . . . . . . . . . .\n2-93\n5. Nonlinear Programming. . .\n. . . . . .... .....\n.\n.\n..\n2-94\n6. Optimal Digital Equivalent . . . . . . . . . . . . . .\n2-95\nSample Designs . . . . . . . .\n2-101\n1. Bandstop Filter. . . . .\n2-101\n2. Digital Resonators . . . . . . . . . . . . . . . . . . 2-101\n3. Digital Differentiators.. * . .............\n.\n.\n.\n.2-102\n4. Low-Pass Filters . . . . . \xc2\xb7. . . ..\n.. ..\n. . . . ...\n2-103\n\n2-104\n\nA.\n\nGeneral.\n\n2-104\n\nB.\n\nInstability Thresholds . . . . . . . . . .\n\n2-104\n\nC.\n\nIV.\n\nCoefficient Quantization . . . . . . . . . . .\n\nReduced Coefficient Wordlengths.\n\n2-105\n\n. . . . . . . . . . . . . . . .\n\nNonlinearities in Fixed Point Arithmetic . . .\nA.\n\nQuantization Errors. . . . . . .\n1. Quantizer Types. . . . . . .\n2. Steady-State Analysis . . . .\n3. Statistical Analysis . . . . .\n4. Quantization Error Bounds.. .\n5. Open-Loop vs. Closed-Loop. . .\n\nB.\n\nLimit Cycles and Deadband Effects.\n\nC.\n\nSaturation and Overflow.\n\nD.\n\nV.\n\n. . . .\n\nDynamic Range.\n\n2-108\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . .\n\n.\n\n.\n\n.\n\n.\n\n..\n\n.oooooooo.\n\n.\n\n.\n\n.\n\n.\n\n.o.o\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-108\n2-108\n2-111\n2-116\n2-118\n2-122\n\n. . . . . . . . . . .\n\n2-123\n\n. . . . . . . . . . . . . . . .\n\n2-124\n\n. . . . . . . . . . . . . . . . . . . . .\n\n2-125\n\nNonlinearities in Floating Point Arithmetic.\n\n. . . . . . . .\n\n2-126\n\nA.\n\nNotation .........................\n\n2-126\n\nB.\n\nError Sources.\n\n2-127\n\n. . . . . . . . . . . . . . . . . . . . .\n\n2-iii\n\nC.\n\n2-129\n\nD.\n\nVI.\n\n\xc2\xb7\nCoefficient Quantization\nOutput Error . . . . . . .\n\n2-131\n\nProgramming Forms. . . . . . . . .\n. . . . . . . .\n\nA.\n\nDirect Form.\n\nB.\n\nModified Direct .\n\nC.\n\nStandard Form .\n\nD.\n\n.\n\n\xc2\xb7\n\n2-137\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-138\n\n.\n\n2-140\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-143\n\nModified Standard Form . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-145\n\nE.\n\nCanonical Form . . . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-147\n\nF.\n\nModified Canonical Form.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-149\n\nG.\n\nParallel Form.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-152\n\nH.\n\nCascade Form . . . . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-154\n\nI.\n\nModified Cascade .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-157\n\nJ.\n\nX1 Structure .\n\n.e\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.e\n\n2-160\n\nK.\n\nX2 Structure . . . . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-162\n\nL.\n\nVII.\n\n. . . . . .\n\nSummary of Programming Forms -\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-164\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-168\n\n. . . . . . .\n\n. . .\n\n. . . . . . .\n\n.\n\nComputer Aided Design . . . .\nA.\n\n\xc2\xb7\nTransfer Function Synthesis.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-168\n\nB.\n\nCoefficient Quantization \xc2\xb7 . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-169\n\nC.\n\nProgramming Form Selection \xc2\xb7\n1. General... . . . . . . .\n2. Flow Charts . . . . . . .\n3. Source Listing . . . . . .\n4. Summary. . . . . . . . .\n5. Stored Program Mode- .\node.\nSpecial-Purpose Computer Mc\n6.\n\xc2\xb7\n7. Closed Loop Comparison \xc2\xb7\n8. Conclusion . . . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2-iv\n\n.:\n.\n\n*\n. . ..\n.e\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n\xc2\xb7.\xc2\xb7\n\n. . . . ..\n\n2-169\n2-169\n2-172\n2-174\n2-174\n2-176\n2-178\n2-181\n2-185\n\nVIII.\n\nApplications of Digital Filtering. . . . . . . . . . ....\n\nREFERENCES. . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n2-v\n\n....\n\n2-186\n\n2-187\n\nI.\n\nDIGITAL FILTER CATEGORIES\n\nThe generalized transfer function of a digital filter has been\nshown to be the ratio of two polynomials in z.\n\nGenerally the coefficients\n\nof the polynominals are real numbers which must be determined in some\nmanner to force the digital filter transfer characteristics to meet\nsome criteria.\n\nThe manner in which the coefficients are found as well\n\nas other considerations (for example, implemention details) allow us\nto categorize digital filters into several classifications.\n\nIn this\n\nchapter we examine non-recursive, recursive, and several other major\ncategories for digital filters.\n\nNon-Recursive Filters\nGeneral\nNon-recursive digital filters are those whose transfer function\ncan be written as\n\nm\n\nD(z) =\n\naiz\n\n(1-1)\n\n- i\n\ni=O\n\nNon-recursive filters have no feedback terms, and hence they have a\nfinite impulse response.\n\nThey are sometimes called transversal filters,\n\na name used for delay line filters in radar moving-target-indicator\napplications.\n\n2-1\n\n2-2\n\nMuch emphasis has been placed on these filters in the literature\nand several design techniques will now be illustrated.\n\nFinite Impulse Response Fllters [1]\nFinite impulse response (FIR) filters satisfy equation (1-1).\nConsider the first order filter\n\nH(z) =\n\n1a\n1 - az-l\n-\n\n= 1 + az\n\n2\n+ (az-1)\n\n+\n\n(az l=0\n)\n\n(\n\n=\n\n1\n\n< 1\n\nSuppose we truncate the series to M terms to produce the FIR below\n\nM-1\nHM(z) =\n\nX\n\nZ=o\n\n(1-2)\n\n(az)\n\nAlso,\n\nHM(Z)\n1 - (azHM(Z)=\n=\n1- az 1\n\nM\n\n- zM\n\nM\n(1-3)\n\nzM-l (z - a)\n\nThis is another way of expressing the FIR as filter with feedback.\nFig. 1 illustrates the z-plane pole-zero locations for both H(z)\nand HM(z) for M = 8.\nFig. 2b, (1-3).\n\nFig. 2a shows the implementation of (1-2);\n\n2-3\n\nV\nI\n\n1\n\n.1\nI,\n\nFig. 1.\n\nPole-zero Plots in the\nz-plane.\n\n2-4\n\nx(kT)\n\ny(kT)\n\n1\n(a)\n\nx(kT)\n\na\n\n(b)\nFig. 2.\n\nBlock Diagrams for (1-2) and (1-3)\n\n2-5\n\nFast Convolution [2]\nFast convolution is a technique which employs the FFT and IFFT\nto determine the filter output response (see Fig. 3a).\n\nDirect convolu-\n\ntion is expressed by\n\nN-1\ny(kT) =\n\nI\n\nh(ZT)x(kT - tT).\n\n(1-4)\n\nt=0\n\nTo calculate N output points, this requires N 2 real multiplications.\nFor fast convolution\n\ny(kT) = IFFT{H(-- \'FFT(x(kT))} .\n)\nNT\n\n(1-5)\n\nHere the FFT and IFFT require 2Nlog2 N operations each, while the multiplication requires N operations.\n\nThis totals\n\n# operations = N(4 log2 N + 1).\n\nIf each operation (a complex multiplication) is assumed to take approximately 4 real multiplications, the result is\n\n# multiplications\n\n=\n\n16 N log 2 N.\n\nSuppose N = 1024, then N2 Z 106 and 16 N log2 N - 1.6 x 105.\n\n(1-6)\n\nHence,\n\nfor large numbers of output points, the fast convolution technique\nis faster than direct convolution.\n\n2-6\n\n---\n\nx(ks--W\n\ny(kT)\n\nNT\n(precalculated\nand stored)\n\n(a)\n\nh(t)\n\nNT\nx(t)\nMT\ny(t)\n\n(M+N)T\n(b)\n\nConvolution of h(t)*x(t)\n\nh(kT)\n\nH\n\nx(kT)\n\nh\n\nla\n\ny(kT)\n\nI\n\nI\n\nI\n\n(c)\n\nFig. 3.\n\nFast Convolution\n\nLC\n\n2-7\nThe reader must be careful in using the fast convolution technique\nbecause the results can be misleading.\nthe analog signals in Fig. 3b.\n\nConsider the convolution of\n\nIf we sample these signals and use\n\nthe FFT and IFFT, we are convolving the periodic functions shown in\nFig. 3c.\n\nHence the output y(kT) can differ greatly from the desired\n\nsequence.\nIn order to improve the results one may add zeroes into the input\nand transfer function sample sequences as shown in Fig. 4.\nimproved output response.\n\nNote the\n\nHowever, in adding zeroes we have increased\n\nthe calculation time unless we modify the FFT algorithm.\n\nLinear Phase Filters [3]\nA linear phase filter is an FIR filter with exact linear phase.\nThey may be used to approximate an arbitrary magnitude frequency\nresponse without causing phase errors.\n\nThe linear phase filter is\n\ngood for standard lowpass, bandpass, and highpass filters.\nIf the number of sample points in a FIR filter is\n\nN = 2T + 1,\n\nthen linear phase with delay\n\nh(kT) = h(NT - T - kT)\n\nT\n\nis realized if and only if\n\n(1-7)\n\n2-8\n\nh(kT)\n\nt\nNT\n\nLT\n\nx(kT)\nMT\n\nt\n\ny(kT)\n\nFig. 4.\n\nAdd a Zero Sequence\n\n2-9\n\nHence, for N even\n1.\n\nThere is no unique peak in h(kT)\n\n2.\n\nh(kT) = h(NT - T - kT) k\n\n3.\n\nThe center of symmetry is between (N) and (?N) - 1.\n\n4.\n\nThe delay is\nT\n\n=\n\nO,\n\n(N) - 1\n\nN-l=N\n2\n\nthe center of symmetry.\nFor N odd\n1.\n\nThere is a unique peak in h(kT) at (N - 1)/2.\n\n2.\n\nh(kT) = h(NT - T - kT) k = 0, (N - 1)/2\n\n3.\n\nThe center of symmetry is at (N - 1)/2\n\n4.\n\nThe delay is\nN-\n\n1\n\nthe center of symmetry.\nIf the above conditions are met, the frequency samples H(\n\n) will be\n\ngiven by\n\nH(<)\n-=\n\nH()\n\nI\n\nejOm\n\nwhere, for N even\n\nem =\n\n8m\n\n-\n\nL .mT\nNm\n\n2\n\nN\n\n(N -m\n\nm = O,\n\n(2) - 1\n\n(1-8)\n)\n\nm = (N),\n2\n\nN\n\n2-10\n\nand for N odd\n\no\n\nm\n\n= _ 2 mT\nN\n\nem = 2\n\nN\n\n(N - m) T\n\nm = 0, (N - 1)/2\n(1-9)\nm = (2) + 1, N .\n2\n\nand\n\n1\nf\nl\nH(- ) = H(-) = 0 .\n2T\n2\n\n(1-10)\n\nThis concludes our brief description of linear phase filters.\n\nFrequency Sampling Filters [3]\nThe term frequency sampling filters refers to a class of digital\nfilters specified by sample points in the frequency domain and implemented in the manner of Fig. 5.\n\nMany techniques have been suggested\n\nfor choosing the sample points\n\nH( m) =\n\nIHmI ejOm\n\nm= 0, N - 1\n\n(1-11)\n\nincluding optimization techniques which adjust the points in the\ntransition region to give a good ripple between sample points.\nreal impulse response filters\n\nFor\n\n2-11\nTransition Region\n\n.. O\n00000\n\nPass\nBand\n\nI b\nI\nI\nP\n\nI\n\nI\'\nI\nI\nI\n\nb\nI\n\nIp\nIw\n\nStop\nBand\n\nm\n\n4Ihv *\n\n(a)\n0\n\n.. *.\nN-1\nNT\n\ny(kT)\n\n(b)\n\nFig. 5.\n\nFrequency Sampling Filter\n\n2-12\n\nIHmlI\n\n=\n\nIHNm\n(1-12)\n\nm\n\nN-m\n\nBy the IDFT\n\nN-1\nh(kT) =\n\nH(--m) ei(2r /N\nNT\n\nI\n\nm=O\n\n)m\nk\n\nk = 0, N - 1\nand\n\nN-1\nH(z) =\n\nI\n\nh(kT)z- k\n\nk=O\n\nwhere\n\n= H(-)\nNT\n\nH(z)\n\nz = ej 2\n\nm / N\n\nThen\n\nH(z) =\n\nN-1\ni\nk=0\n\nrN-1\nX\nm=O\n\nz\n\nH NT )\n\n-k\n\nor\n\nN-1\nH(z) = (1 - z-N)\n\nH- z)\n(1-13)\n\nm=O\n\n1 -\n\n-1 m\n\n2-13\n\nwhere\n\nW = e j 2r/N\n\nEquation (1-13) is the motivation behind the frequency sampling\nimplementation illustrated in Fig. 5b.\n\nWindowing Filters [4]\nIn equation (1-2) a truncation was performed (a fairly drastic\nmeasure) to produce a FIR filter from an infinite impulse response\nfunction.\n\nWindowing is the process of orderly termination of an\n\ninfinite series by truncating the series and adjusting the remaining\nterms to mask the truncation effects.\n\nThe transfer function for the\n\nFIR is given in equation (1-1); its output response is\n\nm\ny(kT) =\n\ni\n\naix(kT - iT).\n\ni=O\n\nBriefly stated, the problem is to find the coefficients ai of the\nFIR filter H(z) such that\n\nj\nH(e\n\nT\n\n)\n\njwT\n\nwhere F(e\n\nz F(ej W\n\nT )\n\n) is some specified desired frequency response.\n\ndesign procedure is outlined below:\n\nThe\n\n2-14\n\n1.\n\nFrom F(eJiT), use the IFFT algorithm to find f(kT).\n\n2.\n\nMultiply f(kT) by a window function w(kT), or\n\nh(kT) = f(kT)w(kT)\n\nThe process is outlined in Fig. 6.\n\n(1-15)\n\nMultiplication in the time domain\n\nis convolution in the frequency domain, and hence\n\nH(f) = F(f)*W(f) .\n\nThe window function shown is a rectangular one which duplicates the\ntruncation process.\n\nNotice the ringing effect in Fig. 6c.\n\nlobes for this window are about 20%.\nwindows.\n\nThe side-\n\nFig. 7 illustrates two other\n\nThe triangular one reduced the sidelobes to about 4%.\n\nraised cosine window is the best one shown.\n\nIts function is\n\nIrt\nw(t) = a + (1 - a) cos --\n\nIf a = 0.50 it is called a Hamming window.\nabout 0.54.\n\nThe\n\n(1-16)\n\nThe optimal value of\n\nThis value yields the Hanning window and reduces the\n\nsidelobes to about 1%.\n\nMoving Average Filter [5]\nA moving average filter is a FIR filter which calculates the\naverage of the N most recent observations of the input:\n\na is\n\n2-15\n\nF(eJ wT\n\nf\n\n-\n\n(a)\n\nI\n\n)\n\n/1\n\nDesired Frequency Response\n\nW(w)\n\n2A1\nA\n\n(b)\n\nWindow Function\n\nH(ej\n\nT\n\n)\n\nA\n\n(c)\n\nFig. 6.\n\nWindowing Filter\n\nWindowing Filter Construction\n\n2-16\n\nw(t)\n\nI1\n\nI\n\n1\nA\n\n:-A\n\nI\n\nI\n\nI\n\nI\n\nI\nI(a)\n\nI\n\nRectangular Window\n\nI\nI\n\nI\n1\n\nlI\n\n1\n\nI\n\nI\nI\n\n1\n\nAI\nAI\n\n-A\n(b)\n\n-\n\nTriangular Window\n\n:I\nI1I\n\n/w(t)\n\nI\n. t\n-A\n(c)\n\nA\nRaised Cosine Window\n\nFig. 7.\n\nWindow Functions\n\n2-17\n\nN-1\n\nx(kT- T)\n\nI\n\ny(kT)\n\n\xc2\xa3=0\n\nand\n\nN-1\n\nH(z) =\n\nz\n\n1\n\n\xc2\xa3=0\n\nIn another form\n\n-N\n\n11z) Z\n-N\nH(z) = N\n1- z\n\n(1-17)\n\nLeast Mean-Square Digital Filters [6,7]\nAssume that the filter input is x(nT), a random signal whose\nautocorrelation Rxx(t) is known, and that the crosscorrelation\nRdx(t) of this input and a desired output d(nT) is also specified.\nLet the impulse response of the filter be g(kT), and its output,\nz(kT).\n\nAllowing a shifted time scale,\n\nN\nz(nT)\n\n=\n\nI\nx(nT - KT)g(kT).\nk=-M\n\nDefine the signal D to be expected value of the difference in the\nactual and desired filter outputs squared:\n\nD = E[d(nT) - z(nT)]2\n\n.\n\n2-18\n\nBy definition\n\nR\n\nxx\n\n(t) = E {x(t + T)X(T)}\n\nRdx(t) = E {d(t +\n\n(1-18)\n\nT)X(T)}\n\nRdd(t) = E {d(t + T)d(T)}\n\nSubstitution of these relations into D yields\n\nD\n\nN\nRdx(nT)y(nT)\n- 2 X\nRdd(O)\nn=-M\n\n(1-19)\nN\n\nN\nRxx(kT - nT)g(kT)g(nT).\n\n+\nk=-M\n\nn=-M\n\nThe purpose of the least-mean squares filter is to minimize D by\nHence, if one takes the partial derivative of D with\n\nchoosing g(nT).\n\nrespect to g(nT) and sets the result to zero, the following solution\nis generated\n\nN\nRdx(nT) =\n\nI\n\nRxx(kT - nT)g(kT) .\n\n-M < n < N\n\n(1-20)\n\nk=-M\n\nIn equation (1-20), all quantities are known except g(kT).\nthe filter weights may be calculated from (1-20).\n\nHence,\n\nLeast mean-squares\n\nfilters are sometimes called digital Wiener filters.\n\n2-19\n\nLeast Squares Polynomial Moving Arc Filter [5]\nThe problem here is to solve for the coefficients ai of a polynomial\nto best fit the input data y(ti) in a least squares sense.\n\nEach input\n\npoint is approximated by\n\na0 + alti + a2 ti +\n\nY(ti)\n\n=\n\n+ adtdi\n\nd\nX ak tk i\nk=O\n\nIf the input samples are evenly spaced, ti =\n\niT and\n\nd\ny(iT) =\n\nak(iT)k\n\nE\n\nk=O\n\nFor n input samples define\n\nrd\n\nk\n\nX ak(iT)k\n\nS=\n\n-\n\nY(iT)\n\nk=O\n\ni=0\n\nIn order to minimize S by choosing ak, one may take the partial\nderative\n\nas =\naa ers\nk\n\n2\nr\n\nak(iT)\nc\n\n-\n\ny(iT)]\n\n(T)\n\n=o\n\n=\n\nThis expression reduces to\n\nI\'\n\n2-20\nd\n\nd\n\ni\n\nI\n\nak(iT) (tT)Z\n\nd\nI\n\nk\n=\n\ny(iT) (tT)\n\nR=O\n\nk=0 t=0\n\nWritten in matrix form\n\nCA = B\n\nor\n\n(1-21)\n\nA = C-l1 B.\n\nIn (1-21) the matrix C- 1 represents the filter itself (whose coefficients\nare precomputed) and B represents the system input.\n\nThe output is A\n\nwhich represents the polynomial coefficients ak.\nAnother form of polynomial filtering termed exponential filtering\nallows the polynomial to grow by one term as each new input occurs.\nSuch schemes are called "growing memory" filters.\n\nDigital Inverse Filtering [8,9]\nDigital inverse filtering is a special case of least mean-square\nfiltering as described in equation (1-20).\n\nSuppose that the desired\n\nfilter output is\n\nd(kT) = 1, 0, 0, 0,\n\n\'\n\nthe discrete impulse function.\n\nHence the crosscorrelation\n\n2-21\n\nRdx(nT) = x(O), O, 0, 0,\n\n-\n\nwhich can be scaled to unity (x(O) = 1).\n\nEquation (1-20) with M = 0\n\nthen becomes\n\nro\n\nr1\n\nr2\n\nrN\n\nrl\n\nr0\n\nr1\n\nrN-1l\n\nO\n(1-22)\n\nrN\n\nrN-1\n\nLo\n\nN-2\n\nwhere\n\nri = r i = Rxx(iT).\n\nIf the filter coefficients gi are used in an FIR filter\n\nN\nH(z) =\n\nI\n\ngi\n\nz - i\n\ni=O\n\nand the random signal x(nT) is applied to the input, the output will\nbe a digital impulse function.\ninverse digital filter.\n\nTherefore, H(z) is said to be an\n\nThe calculations involved are shown in Fig. 8.\n\n2-22\n\nRxx(iT)\n\nd(kT)\n(a)\n\nFilter Design\n\nN\n\nx(kT)\n\n3\n\niE\n\ni=O\n\n(b)\n\nFig. 8.\n\n-i\n\ngi\n\nZ\n\n-\n\nFilter Application\n\nDigital Inverse Filtering\n\n- z(kT) : d(kT)\n\n2-23\n\nRecursive Digital Filters\n\nGeneral\nA recursive digital filter is a filter with feedback which, in\ngeneral, has an infinite impulse response.\n\nIts transfer function is\n\nn\naizi\nH(z)\n\n=\n\n(1-23)\n\ni=O\nn\n1 +\n\nbiz\n\ni\n\ni=l1\n\nwhere at least one ai and bi is not zero.\nRecursive filters generally require fewer terms (lower order)\nthan a non-recursive filter with similar characteristics. Higher\norder recursive filters are usually factored into second order stages\nwhich are either cascaded or paralleled.\n\nBlock Recursion [101\nOne technique for implementing a desired recursive digital filter\nof the form\n\nHp(Z) = D(z)\n\nis called block recursion and is shown in Fig. 9.\nin Fig. 9b is\n\n(1-24)\n\nThe implementation\n\n2-24\n\nX(z)\n\n[Hp(z)3\n\n(a)\n\nY(z)\n\nThe Desired Filter\n\nHB(Z)\n\nl\n\nI\n\nX(z~)\n\nHz)B\n\nY(Z)\n\nFIR\n\nG(z)\n\n-M\n\nFIR\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n- (b)\n\n-\n\n-\n\n-\n\nBlock\nDelay\n- - - -\n\n-\n\n-\n\nThe Implemented Filter\n\nFig. 9.\n\nBlock Recursion\n\n-\n\nI\n\n2-25\n\nH (z)\nH B(Z)\n1-\n\n(1-25)\n\nHM\nz-MG(z)\n\nBut the desired filter is\n\n1\n\nm\n\nm-1\nrn (- . ,-1\n\ni=l\n\na\n1-\n\nziz\n\ni=1\nwhere z i are the poles of the function Hp(z).\nThe finite impulse response filter HM(z) is found by truncating each\ncomponent of Hp(z) to M terms, or\n\nai[ l - (Ziz-1)M]\n\nm\ni=l\n\n1 - ziz-1\n\n= Hp(Z)\n\n1-z-\n\n- z-M\n\n) Mm\n\ni=l 1 - Zi\n\nQ(z)\n\nD(z)\n\nwhere\n\nm\n\nQ(z)\n\nM\naizi\n\na.zMD(z)\nii\n3\ni=l 1 - ziz\n\nThus,\n\nHM(z)D(z) = 1 - z-MQ(z)\n\n-1\n\n(1-26)\n\n2-26\nand\n\nQ(z) = zM(1 - HM(z)D(z))\n\n(1-27)\n\nwhere Q(z) is a polynomial of order M-1.\n\nFrom the above relations it\n\nis clear that if G(z) in HB(z) is chosen as Q(z), then\n\n= Hp(z)\n\nHB(Z)\n\n(1-28)\n\nG(z) = Q(z)\n\nand the block recursive implementation exactly produces H (z), the\ndesired filter.\n\nThus, we have shown that a recursive filter can be\n\nimplemented using one FIR HM(z) in the feed forward path and one FIR\nG(z) in the feedback path, where\n\nHM(Z) = truncated version of Hp(z)\n(1-29)\nG(z) = zM (1 - HM(z)D(z)).\n\nSome researchers have used the FFT to implement the two FIR filters\n[11-13].\nExample.\n\nConsider the filter\n\n1\nHp(z) =Dz\n1 + az 1 + bz 2\n\n=\n\n1\nD(z)\n\n2-27\n\nand let M = 3\n\n1 + az-\n\n1\n\na 1 - az\n+ bz-2/ 1\n\nb)z-2\n\n+ (a2\n\n1\n\n1 + az\'l + bz 2\n- az 1 -_ bz 2\n_ az\'l _ a2z-2\n-2\n\n(a2-b)z\n\n_\n\nabz-3\n-\n\n+ abz\n\n3\n\nHence\n\n-\n\nH3 (z) = 1 - az\n\n1\n\n-\n\n+ (a2 -b)z\n\n2\n\nG(z) = z3(1 - H3 (z)D(z))\n\n= (2ab - a3 ) + (b2 _ a2 b)z-1 .\n\nOne can check the impulse response of HB(z) by dividing the\ndenominator into the numerator and comparing it with Hp(z).\n\nFlat Group Delay Digital Filters [14]\nIn order to achieve a linear-phase digital filter one must choose\na non-recursive structure.\n\nHowever, when the order of the non-recursive\n\nfilter is unacceptably larger, one is led to approximate the linear\nphase using a recursive filter design whose error norm is the maximally\nflat criteria.\nConsider the recursive filter\n\n2-28\n\nn\nX\n\n1+\n\nH(z)\n\nai\n\n(1-30)\n\n=\n\n1 +\n\naiz-i\ni=l\n\nwhose d.c. gain is unity.\n\nX\n\ntan1\n\nThe phase response (T = 1) is given by\n\naisin i w\n\n= ~( )\n\nain\n\n(1-31)\n\nX aicos i w\ni=O\n\nThe ideal phase\n\n(-UT),\n\nwhere\n\nT\n\nis the desired delay is approximated\n\nby minimizing\n\n6(m) = -WT -\n\nO(W)\n\nor\n\nc(w)\n\n=\n\ntan(6(w)) = - tan WT - tan (O(w))\n\nThe procedure is to make s(w) vanish at d.c., together with its\nderivatives up to some order depending on n.\nThe solution yields the filter\n\n(1-32)\n\n2-29\n\nH(z)\n\n2n!\nn!\n\n1\n2n\nTn\n\n(2T + i)\n\ni=n+l\n(1-33)\n\nn\nn\n2k +i\nI (-l)k () 1n 2T+ k + i\nk=O\ni=O\n\n-k\n\nwhich is stable for all finite positive values of\n\nT.\n\n2-30\n\nAdvanced Topics\n\nIn addition to the simple division of digital filters into recursive or non-recursive categories, there are many other ways of identifying\ntheir characteristics.\n\nComplex Digital Filters [15]\nA complex digital filter has a complex input x(nT), a complex\noutput y(nT), and a complex transfer function H(z).\nshown in Fig. 10.\n\nAn example is\n\nA lowpass envelope is centered at fc by replacing\n\nz by e-jwcT z = yz in H(z).\nn\n\nZ\n\nH(z) =\n\nn\n\n1 +\n\nX bz\n\n-\n\nl=1\n\nHence\n\n(1-34)\n\nHS(z) =\n\nyttz-\n\n1+\nl=1\n\nComplex digital filters have application in communication and\ninformation theory, signal detection, randomly time-invariant channels,\netc.\n\nIn one application they are used to generate the Hilbert transform\n\nof a real signal x(nT).\n\n2-31\n\nH(f)\n\nj^~~~~~~~~~~~~~~~~~~~~~\n\nHS (f)\n\nf\n\nFig. 10.\n\nComplex Bandpass Filter\n\n2-32\n\nRandomly Sampled Filters [16]\nA randomly sampled digital filter H(z) takes input samples of the\nanalog input signal x(t) at some random time\n\nnT < tn < nT + T\n\nand stores them in an input buffer.\n\nThe numbers x(tn) are then fed\n\nto the filter hardware as evenly spaced samples x(nT).\n\nHence, the\n\ndirect convolution of x(nT) and h(nT) produces the output y(nT)\nwhich is interpreted as y(tn).\n\nThe question arises what errors are\n\ngenerated by the random sampling?\nDefine\n\ntn = (n + Zn)T\n\n-A<Z <\n\nn\n- -\n\nA\n\n(1-35)\n\n= cT\n\nO < a < 1\n\nwhere Zn is a random variable.\n\nAlso define\n\nx(tn ) = xn = x(n + Zn)\n(1-36)\nx(nT) = xn\n\n2-33\nIf we expand xn in a Taylor series about Zn\n\nn =xn +\n\nnZn + 1/2xnZ\n\nd\nwhere xn = d- xn\n\nin =-Xn\n\n2\n\n+\n\nand define an input error En\n\n\'\n\nnZn + 1/2n\n\nXn\n\n+...\n\nThe output error due to random sampling is defined as\n\nn\n\nYnY\n\nen= Yn\n\nYn\n\nin\n=\n\nhn-iSi\nn\n\nfor a non-recursive filter.\n\nOther pertinent relations are\n\nv 2 = E(Z2)\n\nn4 = E(Z 4 )\nn\nE(C n\n\n= 1/2\'xv 2 + ...\n\n)\n\n(1-37)\nE(2) = %2\n\'\n\n+ (1/4x\n\n2\n\n+ l/3~x~x)n4 + ..\n\nv\n= Exe) hn(1/4)\nn\n\nE(en) I\nhn iE(i)\nS\n--- - i=O---\n\n2\nE(e2) = E2 (en) + n hn-ivar(i)\n\xc2\xb7\ni=O\n\nc\n\n2-34\n\nThe frequency response error is determined for sinusoid inputs\n\nx(t) = cos Wt\n\nO <w < Nf\n\nwhere Nf is the Nyquist frequency n/T.\n\nThe expected values of the\n\noutput steady state errors are\n\nE(en)ss = (-1/2w2 v2)H(ejW) cos nw\n2\nE(en )ss\n\n+\n\n2\n=\n\nE (en)ss +\n\n-\n\nO\ni=O\n\n2\n\n)\n\nI ij2) ( 2 2\n-ie\'OW\nh\n\n2\n_4v4 04V4\nV\n_2 w4 4 _- 4v48\n(i\n\n24\n\n8\n\n)\n\ns\ncos2nw\n\nThe physical interpretation of the results is shown in Fig. 11, where\n\nE(Hm(eJw)) = (1 - 1/2w2 v2 )H(ejw)\n\n(1-38)\n\nIn random sampling only the expected amplitude response is distorted\nwhile the expected phase response is unchanged.\n\nThe noise to signal\n\nratio for noise generated by random sampling is approximated by\n\nNSR = 10 log 10\n\n(1-39)\n\n2-35\n\ncos\n\nat\n\n(a)\n(a)\n\nH(z)\nExact\n\nYn\n\nExact Model\n\nCos wt\nt\n\nYn\n\nT\n\n(b)\n\nApproximate Model\n\nFig. 11. Randomly Sampled Filter\n\n2-36\n\nRandom sampling finds wide application in time-sharing filters,\nradar filters, and faulty samplers - all samples are faulty to some\nextent.\n\nExample.\n\n[9]\n\nH(z) =\n\n0.1\n1 -\n\n.9z\n\n1\n\nand Zn has a rectangular distribution with a = 0.1, or 10% jitter\nin the input sampler.\n\nThe curves of Fig. 12 illustrate that as\n\nfrequency increases, the noise component increases making the\nfilter unusable above w/Nf = 0.3\n\nMultirate Digital Filtering [17]\nA multirate digital filter is one in which the samplers for the\ninput and output are operating at different rates, one usually being\nan integral multiple of the other.\n\nMuch analysis of multirate sampled\n\ndata control systems has been treated in the open literature.\n\nHere\n\nwe examine three configurations of multirate filters demonstrated\nin Fig. 13.\n\nW\n\nSolutions for the sampled output frequency responses are\n\n(jo) =\n\n1\nKT\n\nG(jw + j K-T) R*(jw + j 27T)\n\nX\n\nn\n\n=-\n\n-\n\n2-37\n\n0\nIH(eiJ) 12\n-10\n\n-20\n\n-30\nNSR\n-40\n\n-50\n\n-60\nwf\n\n0.5\n\n0.1\n\nFig. 12.\n\n0.2\n\n0.3\n\n0.5\n\nRandom Sampling Example\n\n1.0\n\n2-38\n\nR~s\nR(s)\n\n-\n\n/\n\nR*(s)\n\nR*(s)\n\n,\n\nW*(s) /\n\nW**(s)\n\nKT\nslow\n\nTypical Multirate Filter\n\ns\nG(s)\n\nH(s)(s)\nR~s)\n\n(b)\n\nX**(S)\n\n(s)\n\nKT\nslow\n\nKT\n\nT\nfast\n\nR(s)\n\nW(s) /\nT\n\n(a)\n\nR(s)\n\nGs)\nG(s)\n\nT\nfast\n\nDigital Prefilter\n\nY**(s)\n\n(s)\n\nF(s)\nKT\n\nKT\nslow\n\n(c)\n\nFig. 13.\n\nAnalog Prefilter\n\nMultirate Digital Filters\n\n2-39\n\nFJLwn\n\n**(J1)\n\nj2Trn\n(1-40)\n\nY**(jw) =\n\n1\n**(w)F(jw + j2Rn )\nKT\'\nKT n=-OD\n\nR(jw + j\nKT\n\nThese expressions simplify greatly if the filter function G(s) is\nband limited\n\nIG(jw) I = 0\n\nIw\n\n>\n\n(1-41)\n\n2KT\n\nKT\n\nThe functions H(s) and G(s) represent prefilters used to band limit\nthe input signal r(t) to prevent frequency aliasing.\n\nTwo Dimensional Digital Filters [18,19]\nTwo dimensional digital filters are used in digital image\nprocessing.\n\nThey are used to transform characteristics in photographs\n\nor CRT images.\n\nThe transformation is described by\n\namnzz2\nH(Zlz 2 ) =\n\nm=O n=O\n\nA(z1\n\n\'\n\nz2 )\n\n1\nB(z ,\'Z2 )\n\nm=O n=O\n\nm=0 n=0\n\nand\n\n(1-42)\n\n2-40\n\n- eSlA\n\nz2\n\ne-s2B\n\nwhere sl and s2 are Laplace variables; A and B are the sampling intervals\nin the x and y planar coordinates of the image being processed.\n\nThe\n\ntwo dimensional filter may also be expressed as\n\nH(zlZ2 ) =\nm=0 n-O\n\nhmnzlz\n2\n\n(1-43)\n\nwhere hmn is the impulse response.\nLet us consider the stability of a two dimensional digital filter.\nFor a stable filter\n\nIhmnl\n\nX\n\nm=0 n=0\n\n<\n\no\n\n(1-44)\n\nA two dimensional digital filter is stable if and only if no value of\nZ1 and z2 exist such that\n\nB(zl,Z2 ) = 0, and\n\n1Z 1 < 1, and\n1\n\nz2 1\n\n<\n\n1.\n\n2-41\n\nEquivalent conditions are listed below:\n\nH(zl,z2 ) is stable if and\n\nonly if\n1)\n\nThe map B(zl,z2 )\n\n=\n\n0 of the unit circle Izlj = 0 to the z2\n\nplane is outside the unit circle 1z1 = 1.\n2\n2)\n\nNo point in Izll < 1 maps into z2 = 0; or z2\noutside the unit circle in the z1 plane.\n\nExample.\n\nGiven the two-dimensional filter\n\n1\n\nH(Z,1Z2 ) =\n\n1 + az1 + bz2\n\nwe may set the denominator to zero.\n\nB(zl,z2 ) = 1 + azl + bz 2 = 0\n\nto determine the following map\n\n=-l-a\n1\n\nz2\n\n.\n\nCondition 1 is shown in Fig. 14.\n\nr =jal\nl+ r < -1\n\n-\n\nb\n\n0=\nmaps\n\n2-42\n\nLIi\n1\n\n1\n\n1\n\nb\n\nFig. 14.\n\nStability in Two-Dimensional\nDigital Filters.\n\n2-43\n\nor\n\nlal + IbI < 1\n\nfor stability.\nCondition 2 checks the point z2 = 0 in the z1 plane:\n\n1\no = - Z\n\n-\n\na\n-Zi\n1\n\nb-\n\nZi\n\na\n\nbut ZI must lie outside the unit circle, so\n\n1- \'1 > 1\n\nlal < 1\n\nwhich is included in condition 1.\n\nTherefore, the example filter\n\nis stable if the sum of the magnitudes of the coefficients is\nless than one.\nNon-recursive two-dimensional digital filters may also be designed\nusing windows, just as their one dimensional brothers.\ngood one-dimensional window, then\n\nIf wl(x) is a\n\n2-44\n\n(1-45)\n\nw 2 (x,y) = wl( /x2 + y2 )\n\nwill be a good two-dimensional window function.\nExample.\n\nConsider the one-dimensional window\n\nw(x) = 1 - Ix\n\nxlI <1\n\n=0\n=\n\nIxl > 1\n\nThen\n\nW(w) = sin2 (w/2)/(w /4)\n2\n\nwhich has sidelobes of about 4%.\nThe two-dimensional counterpart is\n\nw 2 (x,y) = 1 -\n\nXx2 + y2\n\n= 0\n\n1\nx\n\n2\n\n+ y21\n\nIX2 + y21 > 1\n\nThen, in the frequency domain\n\np\n)\nW2(wlI\'2\n\n= 2n[p-3\n0\n\nP =\n\n12 +\nh\n\n\' 1\n\nJo(t)dt-p 2 Jo(P) ]\n\non\n\nwhich has sidelobes of only 2%.\n\n2-45\n\nThe reader is referred to the open literature where much twodimensional digital filtering theory is reported.\n\nAdaptive Digital Filters [20]\nA major advantage which digital filters hold over analog ones is\nthe ease in which a digital filter\'s coefficients may be changed while\nthe filter is processing data.\n\nAdaptive digital filters change their\n\ncoefficients to minimize some specified criteria.\nadaptive digital filter is depicted in Fig. 15a.\n\nK\nd(iT) =\n\nX\n\nAn example non-recursive\nThe filter output is\n\nC(i)\ngk\n\nx(iT - kT)\n\n(1-46)\n\nk=0\n\n(i)\n\nwhere gk\n\nare time varying coefficients calculated as shown in Fig. 15b.\n\n(i+l)\ng(i+l)\n\nThe term\n\n\xc2\xa3\n\n=\n\n(i) +\ngk\n\nCi)\nC(i) x(iT - kT).\n\n(1-47)\n\n(i) is found by subtracting the filter response from an ideal\n\nresponse d(iT).\n\nThe factor A is a variable step length which is\n\nadjusted to improve the filter response in driving c(i) toward zero.\n\nFloating Point Digital Filters\nA floating point digital filter is one which is implemented by\na computing device which executes floating point arithmetic in calculating the filter\'s difference equations.\n\nBoth the filter\'s coefficients\n\nand the signal variables are represented in the following format\n\n2-46\n\nx(iT-T)\n\nhe~i)\n\nx(iT-KT)\n\nE(i) _\n\nd(ir)\n\nIdeal\nResponse\nGenerator\n\nff+d(iT)\n\n(a) Filter Block Diagram\n\n~A(i)\n\n|.\n\n-\n\nx(iT-kT)\n\ng(i)\nk\n(b) Time Varing Gain Generation\nFig. 15.\n\nAn Adaptive Digital Filter.\n\n2-47\n\nF x RE\n\n(1-48)\n\nwhere F is a fraction expressed in radix R and E is the exponent value.\nR is usually 2 in 16-bit minicomputers but is 16 in IBM 360/370 machines.\nDigital filters are not usually implemented in floating point for several\nreasons.\n\nFloating point hardware is slower than fixed point and is\n\nmore costly.\n\nPerhaps a more important reason is that floating point\n\nquantization errors in signal variables can cause system instability\nwhereas with fixed point arithemtic is guaranteed to be stable if the\nfilter coefficients yield stable poles in the z-plane.\n\nOptimal Digital Filtering\nOptimal digital filters are filters used to minimize some performance.evaluation criterion set for the discrete filter.\nsection, three topics will be presented:\n\nIn this\n\n1) the concept of optimi-\n\nzation, 2) the optimal control law, and 3) state estimation.\nConcept of Optimization [21].\n\nA system may be described by n\n\nfirst order linear or non-linear differential equations in the\nindependent variables xl, x 2,\n\n\'xn.\n\nAny system can be so described\n\nby the introduction of the appropriate number of variables, henceforth\nreferred to as the state variables.\n\nThe n differential equations are\n\nx = f(x,u,t)\n\n(1-49)\n\nSuppose that a function\n\n-.\n\nAt~~~\n\n2-48\nmT\nV(u) =\n\n/\n\n(1-50)\n\nL(x,u,t) dt\n\no\n\nis to be minimized by choosing the forcing functions u(t) or some\nother system parameters.\n\nL represents the performance criterion\n\ntogether with any terms which penalize or restrict the use of\nforcing signals.\n\nThe minimum value of V(u) is termed the cost.\n\nA linear optimal system has the following characteristics:\n(a)\n\nlinear differential equations\n\n(b)\n\nthe performance criterion has a quadratic form in the\nstate variables and forcing functions\n\n(c) unrestricted forcing functions and state variables.\nAny system which does not possess all three characteristics is nonlinear.\nConsider the linear system described by the following set of\nfirst order differential equations:\n\ni\n\n= Fx + Gu\n\n(1-51)\ny = Hx\n\n.\n\nNow it is desired to calculate u(t) (given the initial values\nx(O)) such that the cost function V(u) is minimized.\nIt is proposed to approximate the system by a discrete time version.\nThe time interval is divided into m equal sub-intervals T and the forcing\nfunction u is to be held constant during each subinterval.\n\nThe system\n\n2-49\n\nis considered to be described by a sequence of transitions from the\n(k-l)th to the (k)th state.\nSolving the set of first order differential equations, we find\nthe following transition equation:\n\nx(kT + T) = 4(kT + T, kT)x(kT) + r(kT + T, kT)u(kT)\n\n(1-52)\n\nwhere\n\nr(kT + T, kT) A\n\nkT+T\n+(kT + T, T)G(T) dT.\nkT\n\nand 0(t,T) is found as follows:\n1.\n\nWhen F is time varying, D is computed from\n\nd[_(t,T)] = F(t) q(t,T)\ndt\n\n2.\n\nWhen F is constant $(t,T) = 0(t-T) is computed by\n\n0(t-T)\n\n=\n\neF(t-T) =\n\no [F(t-to)]\nk=o\nk!\n\nThe relationship between continuous and discrete systems is shown in\nFig. 16.\nIn addition, the integral to be minimized is replaced by the\nsummation\n\n2-50\n\nX:\n_\n\nV\nAS\n\n~~~~~~~~~~~~U)\n\n" I\n\n0)\n\nr.,\naJ\na,\nJJ\n\n-I4\n\nUEd\nU\n\nxl\n4.\n\nv\n\nU)1\n\n3\nU)\nu\n\nt^\nX\n\n0\n\nci\n\n+\n\n-\n\n<2\n\nh\n\na,\n\nI\n\nI\n\n\xc2\xb7~~~~~~~~~~~~~~~~~~~~~C\n\n0)\n0\n\nv\n\ncn~~\n\n+\n\nci\n\na,\n\n3\n\nv\nI\n\n3~~U\n\n0\n\nu~~~~~~~~~~~~~~~~U\n)\n0~~~~\n\nlu\n\nUI\nU)\n\xc2\xb7re\n\nvl\n~\n\nU\n~~~~~~~~\nora~~~~H\n)\n-I\n\n\'-\n\n+~~\n\nG~~~~~~~~~~~\n1.\n\n\'.0o\n-4\n\n-\n\nC.\n\nD0\n\nM~c;\nt3\n\n~\n\n~\n\n~\n\n~\n\n~\n\n~\n\n41\n\n4.1~~~~~~~~~~~~~~~~~~~~~~~~r\n\'-S~~~~\n\n\'1\n\n:1\n\n\xc2\xb7~~~~~~-\n\n2-51\nm-1\nV(mT) = T\n\nX\n\nL[u(kT),\n\n(1-53)\n\nx(kT), kT]\n\nk=o\n\nThe minimization of V(mT) for discrete systems will be considered\nfor two cases:\n\na) the optimal control law, and b) state estimation.\nConsider the continuous system equations\n\nOptimal Control Law [22].\nto be of the form,\n\nx(t) = F(t) x(t) + G(t)u(t)\n\n(1-54)\n\ny(t) = H(t)x(t)\nmT\nV(mT) = xT(mT)Ax(mT) +\n\nf\n\nxT(t)B x(t) dt\n\nmT\n+\n\nf\n\nuT(t) C(t)u(t) dt\n\no\n\nwhere A = terminal state weighting matrix\nB(t) = state weighting matrix\nC(t) = control cost matrix.\nThe optimal controller is obtained by solving the nonhomogenous matrix\nRiccati equation\n\ndS =\ndt\n\nSF - FTS + SGC-1(O)CTS - B(O).\n\n(1-55)\n\nIf F and G are constant,\n\nS(mT) = [02 1 (mT) + 02 2 (mT)A][Oll(mT) + 01 2 (mT)A]-\n\n1\n\n(1-56)\n\n2-52\n\nWhere\n\ne\n\nTA\n\n[A\n%1\n021\n\n121\n22\n\nand\n\n-F\n\nGC-1GT\n\nB (0)\n\nFT\n\nOnce S(mT) is known, the optimal control vector can be obtained from\n\n(1-57)\n\nUopt(t) = D(mT - t)x(t)\n\nwhere\n\nD(mT - t) = C-l(t)GTS(mT - t)\n\nIn block diagram form, the optimum controller can be depicted as in\nFig. 17.\n\nNotice that to find uopt the state vector x(t) is necessary\n\nfor calculation of the optimal input.\n\nIn most systems x(t) is not\n\navailable; y(t) is available instead.\n\nHence, we "estimate" x(t) using\n\ny(t) as shown in Fig. 17.\n\n2-53\n\nG4\n\nI\n0,\n\nJI\n\nco\n\nl\nI\n\nV\nI\n\nI\n0\n-4J\n>\'\n\nol\n\n-4\n\n0\n\nCI\n\nO\n\n+\n\n:3\n\n0\n\n\'\n\nC-\n\nI\nI\n\nI~~~~~~\nL\n\n-\n\nr.l\n4\n.\nr\n4i\n\n0\n\n4\n\n-\'-\n\nrZ4\n\nII\n\n2-54\n\nState Estimation [23].\n\nIt is desired to find an optimal estimate\n\nx for the state variables x for a system defined in Fig. 17.\n\nThe\n\nsystem output y is measured every T seconds; call the measurement\n\nz(nT) = y(nT) + v(nT)\n\n= H(nT) x(nT) + v(nT).\n\nsimplifying the notation\n\n(1-58)\n\n-n = Hrr-n +-n\n\nwhere vn is measurement noise and\n\nE[\n\nT] = R\n\n-n-m\n\nRn\n\n6\n\nm\n(1-59)\n\nE[v\n-n\n\ni = 0.\n\nThe estimation scheme is to predict the present value of the state\nvector by using the last predicted value and updating it with the\npresent measurement.\n\nx (+) =\n\n() + Kn\n\n[\n\nn\n\nH\n\nn\n\nx\n\n-\n\nn( )]\n\nwhere xn(+) and xn( - ) are estimates of the state vector xn after\n\n(1-60)\n\n2-55\n\nand before the measurement\nweighting matrix.\n\n,n\'\n\nat time nT.\n\nThe Kn is the optimum\n\nLet the error in the estimate be\n\nxn (+) = _Xn(+) - Xn\n\n(1-61)\nxn(-) = xn()\n\n- x-n\n\nSubstituting (1-58) and (1-61) into (1-60)\n\n(1-62)\n\nxn(+) = (I - KnHn)Xn(-) + KnVn\n\nDefine\n\nPn(+)\n\nE[xn(+) xn(+)]\n\n(1-63)\n\nHowever,\n\nE[Xn(-)vT] = E[v x(-)]\n-n\n-n\n-n-n\n\n= 0\n\nbecause of uncorrelated measurement errors.\n\nPn(+)\n\n=\n\nThus, (1-63) becomes\n\n(I - KnHn)Pn(-)(I - KnHn ) T + KnRKT\nK~H~\nI1K\n\nThe cost function to be minimized in state estimation is the sum of\nthe diagonal elements of the error covariance matrix Pn(+):\n\n(1-64)\n\n2-56\nm-1\nV(mT) =-\n\nE [Xn(+) Xn (+)I\n\n}\n\nn-o\n\nThe V(mT) is minimized by Kn\n\nK\n\n= P (H\nn\n\nT =\n\nP(-)H\n\ninLTIHnn\nn\n\nSubstituting K\nnI\n\nn\n\n.\n\n+T\n+P\n\nThe solution is\n\n-1-\n\npn(+)HTRn\n(+)HTRl\n\nn\n\n(1-65)\n\ninto Pn(+) results in\n\n1\nPn(-)Hn[HnPn(-)Hn + Rn] -\n\nPn(+)\n\nHnPn(-).\n\n(1-66)\n\nThe equation set for the state transitions of the discrete system\n\nXn+\n\n=\n\nn-xn\n\n(1-67)\n\n+ ~n\'\n\nAgain, using (1-67) and (1-61) in (1-63)\n\nPn+l(-) =\n\nnPq+)\n\n+ Qn\'\n\nwhere\n\nw\n\n-n\n\n= r[kT + T, kT]u(kT)\n\nE [wwT] = Qn6\n!n!m\nn mn\n\n(1-68)\n\n2-57\n\nE[n] = O\n\nThe results given above are now summarized in Fig. 18.\n\nNonlinear Filtering [24]\nReference [24] presents a class of nonlinear systems which obey a\nprinciple of superposition.\n\nIn particular, the synthesis of nonlinear\n\nfilters for signals which can be expressed as a product or convolution\nof components is examined.\n\nPractical applications in speech and image\n\nprocessing are illustrated.\n\nRange Adaptive Digital Filtering [25]\nIn many applications the digital filter\'s input signal tends to\ndwell near zero with occasional perturbations away from null.\n\nRange\n\nadaptive digital filtering has automatic scaling of its input, internal,\nand output signals to prevent arithmetic overflow.\n\nThis is a hardware\n\nconcept and will be further examined in PART 3, Mechanization of Digital\nFilters.\n\nRandom Sample Skipping [26]\nIn certain time-shared applications of digital filter hardware\nseveral input/output sequences, say n, of numbers can be handled by\na single special-purpose computer which looks like n digital filters.\nIf the sampling rates for each filter is different, then inevitably\nconflicts for the arithmetic unit will take place and certain input\nsamples will essentially be lost.\n\nThis process can be described as\n\n2-58\nIC\n\n1\nI-\n\nI-\n\n--I\n\nI\n\nI\nI\n\nII\n\nO0\n\n3\n\nI\n\nI\n\xc2\xb7\n\n"1\n0\n\nr.\n\n0\nCt\n\n+\n\nim\n\nI\n\no\n\nI\n\nB\nI\n\n0\n0)\no:\nct\n\nI\n\n__\n\n0\n\n\'r _\n\xc2\xb7\n\nr-\n\n,.\nr\'\n\ne,\n\nI\n\nI\'\n\nI\nI\n\n0\n\nI\n\n-h\nvl\n\nI\n\nI.\n\nI\n\nI-.\n\nI\n\n+\n\nI\n\nh~~ 1\nvlvlv\n"\'\n\n~~~~el\'dr~~~~~~~~~~~\n\nI\nI\n\nbj6~~~~~~~~~~~~~~~ ?~ I\n~~~\\\ni h\nI \'d~~~~~~~~~~\nv~~~~~~l\n17~~~~~~~~~\nI\n_\n\n__\n\n_\n\n0\n\nI\nCa\n00\nlo\n\nI\n\n_ _\nOA\n1_04\n\nI\n\nct\nFo\n\n0\n\nIcF~~\':\nrf;~~\n\nFt\nF,\n0\n\n:3\n\nI\nI\n\nI\n\nI\n\nI\n\nI\n\nI\n\nI\nI\n\n_g;~~~~~~~~~~~~~~\n\nI\n\nI~~~~~~~~~~~~~~~~~~~\n~~~~~~~~~~~~~~~I\nI~~~~~~~~~~~~~~~\n\np\n\nIN\n\nI\n\n0\n\nIA\nI0\nI\n\nFne\nS\n\nI\nI\nI1\n0 IN >\n\nI\nI.\n\nII\n\nv\n\nD\n-\n\nrftrt\n\nct\n\n,[\n6~~~~~~~~~~~~~~~~~~~\n\nt\n\nI\n\n2-59\n\nnonlinear random sample omission.\n\nReference [26] shows that in some\n\ncases random sample omissions in a closed-loop system\n\nwith random\n\ninputs can be beneficial in reducing the mean-square value of a nulling\nerror signal.\n\nBlock-Floating-Point Filters [27]\nBlock-floating-point is a compromise between fixed-point and floatingpoint arithmetic.\n\nIn fixed point arithmetic no scaling is used for\n\naddition or multiplication of numbers.\n\nIn floating-point, automatic\n\nscaling is performed for each product or sum calculated.\n\nIn block-\n\nfloating point arithmetic, numbers are expressed as a fraction and\nexponent (as in floating point); however, scaling is performed once\nfor an entire expression instead of for each operation.\nFor example,\n\nYn = xn + alYn-1 +\n\n+ aNYn-N\n\n(1-69)\n\nwould be calculated as\n\n^\n\nYn =_1\nn\nY\nA\nn\n\nn\n\nYn\n\n=\n\nAnxn + alAnwln +\n\nXn\n\n= An-lxn\n\n\' + aNAnWNn\n(1-70)\n\nWin = An_lYni\n\n2-60\nThe scaling factors An and An are powers of 2 and are determined as\nfollows\n\nAn =\n\nCn\n2\n\nAn = AnAn-l\n\nwhere Cn is the maximum characteristic of the variables Xn, Wln,"\',\nwNn.\n\nIn (1-70), the calculations for Yn, xn, and win involve only\n\nscaling (shifting).\n\nOnce scaling is performed in Yn\' then all the\n\narithmetic calculations are performed in fixed-point.\n\nThe block-floating-\n\npoint realization is summarized in Fig. 19.\n\nSample-Rate Reduction Digital Filters [28]\nThere exists a direct relation between input sampling frequency\nand the computational rate of the digital filter hardware implementation.\nIn order to prevent input frequency aliasing, two common practices\nare to sample at a high rate or to use an analog low-pass filter before\nthe A/D converter.\n\nReference [28] suggests sampling at a high rate\n\nand using a digital low-pass filter whose output can be sampled at\na much lower rate to furnish the input signal for some digital signal\nprocessing system.\n\nAdvantages include the elimination of phase\n\ndistortions which are inevitable in analog aliasing filters.\n\n2-61\n\nxn =-\n\nnX~~~~~~~~~~~~An\nYn\n\nna-1\n\nAn\n\naN\'\n\nj\n\na2\n\nAT\n\nWNn\n\nFig. 19.\n\nBlock-Floating-Point Filter\n\nII.\n\nTRANSFER FUNCTION SYNTHESIS\n\nThe synthesis of transfer functions for digital filters is\nsurveyed in this section.\n\nThe survey is subdivided into nonrecursive\n\nfilters, recursive filters, and sample designs.\n\nNonrecursive Filters\nThe synthesis of nonrecursive digital filters consists of determining\nthe coefficients hi of the expression\n\nH(z) =\n\nM-1\n-i\nI hiz\ni=O\n\n(2-1)\n\nIn the z-plane this amounts to placing zeroes anywhere in the plane\nwith all poles falling at the origin.\n\nSpecification of Frequency-Domain Zeroes [29]\nThe design of nonrecursive digital filters in the frequency domain\nconsists of specifying a finite trigonometric polynomial which satisfies\nsome criteria.\n\nHere the polynomial is defined by placing its zeroes.\n\nThe frequency charactrristic of (2-1) is defined as\n\nM-1\nH(f) =\n\nn=0\n\nhn e-j2fn\n\n2-62\n\n2-63\n\nReplacing f by the complex variable ~ = f + ja\n\nM-1\n[1 - e-j2 (-\n\nH\n\nH(O) = K1\n\nn\n\n(2-2)\n\n)]\n\nn=l\n\nwhere\n\nI{n {fn\n=\n\n}\n\nIfnI\n\njan}\n\n+\n\n< 1/2\n\nThe {(n} are the zeroes of H(O) in the central period.\n\nHence, the\n\nscale factor K1 and the M-1 central zeroes completely specify H(f).\nThe function is factored into stopband and passband zeroes\n\nH(f) = HS(f)Hp(f)\n\n(2-3)\n\nwhere\n\nHs(f) = K\n\nNS\nn)]\nII [1 - e-j2(f-0\nn=l\n(2-4)\nNp\n\nHp(f) = Kp\n\nn\n\nn=l\n\n[1 - e-j27(f-\n\n]\n\nn)\n\n2-64\nOnce the zeroes have been apportioned between the stopband and passband,\nthe passband and stopband zeroes are positioned to give a "good" shape\nfor H(f).\n\nThe procedure is demonstrated in [29].\n\nFrequency Sampling [30,31,32]\nThe technique of frequency sampling may be used to synthesize\nnonrecursive filters as follows:\n1.\n\nChoose a set of frequencies at which the sampled frequency\nresponse is specified.\n\n2.\n\nObtain the values of the continuous frequency response of the\nresulting filter as a function of the filter parameters\n(defined below) using the sampling theorem.\n\n3.\n\nCompare the interpolated frequency response with the desired\nfilter and search for a minimum of some filter characteristic.\n\n4.\n\nWhen the minimum is found, the parameters are used to realize\nthe nonrecursive filter.\n\nThe frequency samples in step 1 are specified in the passband\nand stopband; however, in the transition region several samples are\nleft adjustable and these are the parameters used in steps 2 and 3.\nThe references [30,31] describe computer aided design programs which\nessentially automate the optimizing process.\n\nWindowing [19,31,33]\nWindowing filters, discussed in Chapter 1, are nonrecursive\nfilters whose finite impulse response is found by terminating an\n\n2-65\n\ninfinite impulse response by means of a window function.\n\nThe details\n\nof the procedure have been demonstrated earlier.\n\nEquiripple Filters [34,35]\nEquiripple nonrecursive digital filters may be designed by\nminimizing the maximum error between some desired complex frequency\nresponse F(f) and the FIR response as shown below\n\nP/2\nEk =\n\n/ hte-2fk\nt=-(/2\n\n- F(2Wfk)\n\nwhere\n\nj = /hl = filter coefficients\nP = even integer\n\nfk = normalized sampled frequency\n\nIfkl\n\n<\n\n1/2\n\n2W = sampling rate.\n\nEquation (2-5) may be minimized using the simplex method of linear\nprogramming.\n\nDigital filters designed in this manner are sometimes\n\nsaid to have minimax responses.\n\n(2-5)\n\n2-66\n\nRecursive Filters\nThe recursive digital filter has the form\n-i\n\nn aiz\n\nH(z) = i=O\n\n(2-6)\nE bi z - i\ni=l\n\n1 +\n\nThe synthesis of recursive filters is the task of choosing the\ncoefficients ai and bi in order to force the filter to behave in some\nspecified manner.\n\nDirect Synthesis in the Frequency Domain [36]\nThe frequency response for (2-6) is found by substituting z = ej2 IfT\nn\nH(f) =\n\na\n\n-je fiT\n2\n\ni=0\n\n(2-7)\n\nn\n1 + ~ bie-j2rfiT\ni=l\n\nIf N(f) is the numerator of (2-7), then\n\nIN(f)1 2 = (\n\n=H\n\nwhere\n\no\n\nE\n\naie- j 2 fiT)(\nX\n\naiej 2 nfiT\n\nn\n+ 2 k\nHk cos(2rkTf)\nk=l\n\n(2-8)\n\n2-67\nn\n\nH\n\no\n\n=\n\nk\nk=O\n\n2\n\nHk\n\napaq\n(p-q)=k\n\nEquation (2-8) may be further reduced to\n\nn\n\nIN(f)1 2\n\n2k\n\nak cos\n\n=\n\n(2-9)\n\n(nTf),\n\nk=0\n\nHence equation (2-7) may be written as a\n\nwhere ak are constants.\n\nrational function in cos (rTf) [or sin(RTf)].\n\nAny such rational\n\nfunction may be specified by the roots of the two polynomials.\nConsider the Butterworth lowpass filter in the analog domain\n\n{Hl(f)1 2 =\n1 + (-)2p\n\nfc\n\nSince the term sin(nfT) corresponds to f in the discrete case\n\n{H2 (f)1\n\n1\n1 +\n\n[sin (rfT )\n[sin(rfcT)\n\n(2-10)\n2p\n\nrepresents a lowpass digital filter.\n\nTo find the filter coefficients\n\nsolve for the roots of the polynomial.\n\nAn example design in the\n\ncontinuous case is presented later in this chapter.\n\n2-68\n\nOther filter types (bandpass, highpass, band stop, etc) may be\ndesigned using this technique.\n\nSampled Data Transformations [37]\nThis section describes a mapping technique for designing recursive\ndigital filters.\n\nFirst, a suitable continuous filter G(s) is found,\n\nand then a mapping function from the s-plane to z-plane is employed to\nfind the digital equivalent filter D(z).\n\nHence, first we review\n\ncontinuous filter design and then employ the sampled-data transformations.\nContinuous Filter Design.\n\nThe design of continuous filters can\n\nbe accomplished by first designing several low pass filter transfer\nfunctions G(s), called prototype or normalized designs; the prototypes\nhave a critical or break frequency of one radian/sec.\n\nThe prototype\n\nis used to realize a filter for a given specification by using the\nfrequency transformations listed below:\n\nLow Pass:\n\ns + s/Wu\n\nBand Pass:\n\ns +\n\ns 2 + Wuul\ns (Wu-w)\n\ns(Wu-W)\nBand Stop:\n\ns +\ns2 + wuO\n\nHigh Pass:\n\nwhere\n\ns + Wu/s\n\n(2-11)\n\n2-69\n\nw\n\n= upper cutoff\n\n\'~\n\n= low cutoff\n\nu\n\nFive prototype filters will be discussed in this section:\nButterworth, Bessel, Transitional, Chebyshev, and Elliptic designs.\n\nButterworth:\n\nThe Butterworth approximation to the ideal low pass\n\nfilter is defined by the squared frequency magnitude function\n\nIG(W)12 = 1/[l\n\n(2-12)\n\n+ (u2)n]\n\nwhere n is the order of the filter.\n\nThe Laplace transfer function is\n\ngiven by\n\nG(s) G(-s) = 1/[1 + (-l)ns2n]\n\nor\n\nG(s) =\n\n1\nn\nn\nj=l (s + bj)\n\nwhere\n\nbj = - _ei7[(l/2) + (2j-1)/2n]\nbj\n=\n\ni\n\n= --\n\n2-70\n\nThe Bessel filter approximation for the linear delay function\n\nBessel:\ne-\n\nTs\n\nmay be written\n\n(2-13)\n\nG(s) =\nBn(s)\n\nwhere Ko is a constant term and Bn(s) are Bessel polynomials.\n\nB0\n\n=\n\n1\n\nB1\n\n=\n\nS + 1\n\nBn = (2n-l) Bnl + s2Bn-2\n\n1/n\nThe roots of Bn(s) are normalized using the factor (Ko\n\nTransitional:\n\n)\n\nThe transitional filter combines roots of the nth order\n\nButterworth and normalized Bessel filters according to a transitional\nfactor TF.\n\nLet\n\nrj = magnitude of jth transitional pole\n\nrlj = magnitude of jth Bessel pole\n\nj = angle of jth transitional pole\n\n8\n\n1j = angle of jth Bessel pole\n\n02j = angle of jth Butterworth pole.\n\n2-71\n\nthe poles of the transitional filter are then described by\n\nrj = rl j TF\n\nej\n\n=\n\nChebyshev:\n\n2j + TF(e1 j -\n\ne2 j\n\n(2-14)\n\n)\n\nChebyshev filters exhibit better cutoff characteristics\n\nfor lower order filters than do the above designs.\n\nChebyshev type I\n\nand type II filters are defined by\n\nG(W)1 2 =\n\nI\n\n1\n\n(2-15)\n\n1 + s2T2(m)\nn\n\nand\n\n1 ()\nG2\n\n1\n\n=\n1 + e2\n\n(2-16)\n2\n\nT .Tn(r)\nn\nLT (r/)\n\nJ\n\nwhere\n\n=\n-\n\nTo = 1\n\nT1=\n\ncos(n cos-lw)\ncosh (n cosh-lw)\n\nO <0 < 1\nX > 1\n\n2-72\n\nT 2 = 2w2 - 1\n\nT3 (W) = 4-3 - 3w\n\nThe order of the filter n is determined by specifying inband ripple E\nand the lowest frequency at which a loss of a db is achieved.\n\nHence,\n\ne = (1 0 E/1 0 - 1)1/2\n\n(2-17)\nA\n\n2\n\n=\n\n1 0\n\na/10\na/\n\nand\n\ncosh-1/A2 - 1/\nn=\ncosh- l(r)\n\nIn equation (2-17), the variables E, a, or wr must be adjusted so the\nn will be an integer.\n\nThe type I filter differs from the type II in\n\nthat the type I exhibits equiripple in the pass band while type II\nhas equiripple in the stop band.\n\nElliptic:\nstop bands.\n\nThe Elliptic filter has equiripple in both the pass and\nHence, this type design usually achieves the desired\n\nfrequency response with a lower order n than any of the above types.\nThe elliptic filter is determined by\n\n2-73\n\nIG(w)12 =\n\n1\n\n(2-18)\n\n1 + E2+2(w)\nwhere\n\nI\n\n)\n\nn\n\nK(k\n\nn odd\n\nK(k)K\n\n= n,\n\nsn[K(kl)+ K(k\n\n)\n\nK(k)\n\nsn-l(;k);k\n\nn even\n\nwith\n\ndw\n\nx= J\n\n=\n\no\n\n[(1-W2)(1-k2W2)]1\n\n/ 2\n\nElliptic integral of the\nfirst kind\n\nsn[x;k] = w = Jacobian Elliptic function\n\nK(k) = complete Elliptic integral of the first kind\n\n=\nk =\n\nt/2\n\nd(1\n(1 - k2sin2 )1 / 2\n\no\n\n/wr\n\nkl = e(A2\n\n- 1)-1/2\n\n1\nc = (1 0 E/0\n\nA2\n\n-\n\n1 0\n\na/\n\n- 1)1/2\n\n1 0\n\nwhere e, a, wr were defined for the Chebyshev filter, the order n\nis found by\n\n2-74\nK(kl)K(k)\nK(kl)K(k\')\n\nwith\n\nk\' = (1 - k2)1 / 2\n\nk;\n\n=\n\n(1- k21)/2\n\nThe result of any of the five design methods results in a Laplace\ntransfer function G(s) for the desired frequency response.\n\nSampled-Data Transformations.\n\nOnce the continuous transfer function\n\nG(s) has been determined, the transformation to the discrete or z-plane\nis made.\n\nThree methods of transformation will be presented:\n\nthe\n\nstandard z-transform, the bilinear z-transform, and the matched\nz-transform.\n\nStandard z-transform:\n\nThe problem of converting a continuous filter\n\nto a discrete one was presented earlier.\n\nEo*(s)\n= G(z) = Z[G(s)]\nEi*(S)\n\nand that\n\nIt was shown that\n\n2-75\n\nEo(s)\nGho(s) Z[G(s)]\nEi*(s)\nBut for small T\n\nGho (S) Z T\n\nHence,\n\nEo(s)\n0\n\n=\n\nTZ[G(s)]\n\nEi*(s)\n\nDefine the digital filter D(z) equivalent to G(s) to be\n\nD(z) = TZ [G(s)],\n\n(2-19)\n\nwhere Z[G(s)] is the standard z-transform of G(s).\n\nn\n\nD(z) = T\n\nHence,\n\nRk\n\nk=l 1 - e-Tbk -1\n\nNote that the standard z-transform can be used only on bandlimited\nsignals (f < fs/2).\n\n2-76\n\nBilinear z-transform:\n\nThe bilinear z-transform may be used to obtain\n\na discrete equivalent of G(s) as follows:\n\nD(z) = G\'(s)\n\n1\ns = (2/T)(1 - z- 1 )(1 + z-l)\n-\n\n(2-20)\n\nWhere G (s) is a continuous filter whose critical frequencies differ\nfrom G(s) by\n\n(2-21)\n\nf\' = 1/7T tan (7fcT).\n\nRelation (2-21) is used before the continuous filter G(s) is designed.\nThe new filter G (s) is designed instead and then transformed to the\nz-plane by (2-20).\n\nThe bilinear z-transform is a bandlimiting trans-\n\nformation with relatively flat magnitude characteristics in the pass\nand stop bands.\n\nHowever, the time response will be considerably\n\ndifferent.\nMatched z-transform:\n\nThe matched z-transform matches the poles and\n\nzeroes of the discrete function to those of the continuous one.\n\nThe\n\ndigital equivalent of the G(s) function is calculated as follows:\n\nD(z) = G(s)\ns + ai = 1 - z-le-aiT\ns + bj = 1 - z-lebj\nT\n\n(2-22)\n\n2-77\n\nIf G(s) has no zeroes, it is sometimes necessary to multiply (2-22) by\nN\n(1 + z-1) , N is an integer.\n\nSummary:\n\nThe standard z-transform is suitable for only bandlimited\n\nfunctions, while the bilinear and matched z-transforms are suitable\nfor all filter types.\nfactored form;\n\nThe matched z-transform requires G(s) in\n\nstandard, in partial fraction form; and bilinear,\n\nin prewarped frequency form.\n\nThe standard z-transform preserves the\n\nshape of the impulse-time response; the matched, the shape of the\nfrequency response; and bilinear, the flat magnitude gain-frequency\nresponse characteristics.\n\nAn example filter is designed and discretized\n\nin the following example.\nDesign Example.\n\nIn this section a digital filter will be designed\n\nusing the techniques summarized above.\nSuppose it is desired to design a bandstop filter Gl(s) with\n\nwu = 200 = 2X(31.831)\n\n-t\n\n=\n\n170 = 2=(27.056).\n\nMultiplied times this filter will be a low pass filter G2 (s) with\nmn = 600 = 2r(95.493), with a d. c. gain of 1.356.\n\nThe band stop filter\n\nwill be designed from Butterworth, Bessel, and Chebyshev I prototypes\nwith n = 2.\n\nThe low pass filter will be designed with n = 1.\n\nprototype of G2 (s) =\n\n1\ns+l\n\nThe\n\n2-78\n\nThe prototype filters for Gl(s) are found below.\nButterworth: The Butterworth filter is defined by\n\nil<(w,)12 =\n\n1\n1 +\n\nGl(s)G (-s) =\n1\n\n1\n\n1 +\n\n4\n\nG1 (s) =\n4\nC1 )\n(\n(s - ei3wt/ )(s - ei5W / 4)\n\nGl(s) =\n\n1\ns + /2 s + 1\n\nBessel:\n\nThe Bessel prototype is defined by\n\nG(s) =\n\nK0\n\n3\n\nB2 (s)\n\nChebyshev I:\n\ns2 + 3s + 3\n\nThe Chebyshev I filter is defined by\n\nIG,(w)12 =\n\n1\n\n1 + 6 2 T2 (W)\n2\n\nT2 (w) = 2X2\n\n-\n\n1\n\nc = (1 0 E / 1 0\n\n-\n\n1)1/2\n\n2-79\n\nA\n\n2\n\na/10\n\n. 10\n\ncosh1 - (/A\n)\nn =A\n-1/c\ncosh\n\n=\n\n2\n\n(wr)\n\nr\n\nLet E = 1.33 db, then\n\nE = (10 133\n\n- 1)1/2\n\n.5\n\nLet the filter gain be down 6 db at w\n\na= 6\n\nA\n\n2\n\n=\n\nn =\n\n10.\n\n6\n\n= 4.\n\n1/ )\ncosh-l(A2\nA\nh- l/E\ncosh\n\n=2\n\n(w )\nr\n\ncosh-l ( r) = 1 cosh-1(i ) = .44\nr\n\nw\n\nr\n\n= 1.098\n\nr\n\n2-80\n\nHence,\n\n1\n\nIG (W) 12\n1\n\n4\n(\n\n-w\n\n2\n\n+ 1.25\n\n1\nG1 (s) =\n\n(s + 1.057 /31.750)(s + 1.057 /-31.75\xc2\xb0\n\n1\n\nG1 (s) =\n\n2\n\ni\ns\n\n+ 1.308s + 1.118\n\nThe analog filters are designed from the prototypes by setting\n\nG(s) = G1 (S)\n\nX\n\nG 2 (s)\n\ns (u - "w)\nS = S/W ,\n\ns\n\n2\n\nn\n\n+ww\n\nand adjusting the d.c. gain to be 1.356.\nare given by\n\nThe resulting filter equations\n\n2-81\n\nButterworth:\n\n9\ns4 +68000s2 +1.156X10\n7\n(s4 +1272.8s3 +68900s2 +4.3275XlO s+l.156X109 )(s+600)\n\nBessel:\n\ns4 +68000s2 +1.156X10\n9\nG(s) = 2440.8\n(3s4 +27000s3 +2.049X105s2+9.18X107s+3.465X109)(s+600)\n\nChebyshev I:\n\ns4 +68000s +1.56X109\n2\nG(s) = 909.6\n3\n9\n(1.118s4+1177.2s +76924s2 +4.0025X10 s+1.2924X10 )(s+600)\n7\n\nThe filter equations above were plotted for db and phase, % , versus\nfrequency as shown in graphs 1, 2, and 3.\n\nSince the plots are nearly\n\nidentical, the Butterworth G(s) was chosen to be discretized by the\nstandard, bilinear, and matched z-transforms, with T = .001.\nThe Butterworth design for G(s) may be written in partial fraction\nexpantion\n\nG(s) =\n\n40.567\ns + 27.314\n\n+ 2.5502X10 - 4\n\n+\n\n2.4402X10 4 + j7.5033X10 5\n-\n\nx + .35375 + j185.39\n\n- j7.5033X10-5\n\nx + .35375 - j185.39\n\n2-82\n\nI\n\n9ItI-\n\n.l:Ui\n2 ilu\n\n!\nI\n\nN\n\n1Dj\n,f\njl7\n\n4\n\nMlM\n4Wi\nTF\n\n:1 t-:tfi\n\nR\n\n\xc2\xb0 I $# e\n\nFtl I\n\n4i\n\nWS\'\n\niI\nX \'\n\nIe\n\n-L\n\nI\n\n1T\nI i.\n\nPl- u1\n\n-1\n[Tht\n\nI\',,\n\nWE! t\n\n7\n-\n\ntE!:\nI\n\nIR\n\nI i FT\nTi.,4\nmi -F , --\n\n\' I.-\n\n-rtL\n\nIi I\n.\n\n9\n\n7\n\nI .\n\na-\n\n--6s\n4-\n\nV\n3-\n\n:l\n\nm.\n\n2_\n\na4\n\nI.\n0:\n\nLzo\n\no\n11\n\nP~i\na\'\n\n2\n0\n\n9-\n\n4\n\n7-\n\nUr\n\nW\n\n5-\n\nL\n\n3-\n\nA-L\n\ncL\n\nII\nIlll l l lll ll\nHHHHHHHHHHHHll I I l I Il! HllWH ii\n0\n\n\xc2\xb7. ,o\nto~I\n-\n\nA\n\no\n\nHi\n\nll\n\ni\n0\n\nI\n\nll\nlll ill ll 1 1!11!11111 1!\n\no\n\n4\n\nill 1111 lll mlll\n\nk\n\n2-83\n\nI\n\ni\n\n8_\n\nI.L\n.\n\n;\n\n\'\n:\n\'\nI 3I\nl131T\n\n,\n\n-j-\n\n:\n-4_~X\n\nI.\n\n;i--, It\nmrl\niIT1\n1\n\nLI\n\n2\n\n. I\n\n~\ntIM-i 1-4 I. `i-iTiBFEi\n-1X\n_L57..:If ga\n-I\nII~\nI\n11\n\n,i\n.f\nTR\n-\n\n111\nTITitoH\n\n1\n\niS\n\nI\n\n_s\n\n-1-1t::t-[\' I ir-1F\' :1\n\n-.\n\n1L\n\n\'-t l t i i1\n_I\n\n_~\'-\n\n:i\n\nT\nteli,l9\n\n:t\n\nX\n\nI\n\n+ i4\njL-Lr\'\n\n-U\n*I\n\n_"\n\n*cM- :\' .~t \'4-[\n\nj\n\n-t .1.tiRU -l\n\n, .if;,\n-\n\n-1I l\n-t\nj\'R 14t1\n\n~L\n4i. TI.\n\nI:\n\n.I\nI-Ti-c\xc2\xa2% :F L: i\n\nF1\n\nBaeFHM\n\nF\n\nv-\n\nRFjrIT.l\n\nI\n\nI\n\n, : , .I;\n\nH\n\xc2\xb7r m\n\n..-\n\n:441 44~~1:4rl:P~r~~14~rrl\n\n1-.: P1:1:1\nI\nl\ni :: \'l1l -Vr4 -il, -\n\n4_i\n:\n\n-ZH .s.\n\n1\n\n2BBE:,IWg. _,\n17TIN\n~\n\n:\n.!\n\n-4\n\nII\n0k\n\nOm\n\ne~~4S~3\n\nI\nO-\n\nI\n.%\n\'4\n\nI ..i\'} 4\n, ,:.\n41-1 I\'\'V ,4\n\nTilHa -\n\nLL\n\nIII\n11\n\ni,-\n\nLI\n\nI+\n\n-11\ni1,\n\nm\n\n:\nEl\n\nI V)\n\nIz\n6_\n\n\xc2\xb1LIimLi t\n\n5-\n\np~\n\nttit\n\nt:Hi\'\n4"4\n\nI\' -\'?~\n\'tJHi tTiqF~\n\'\n\nimt\n\'In.\n0,\n\nr\n\n1\n\nlitl[\n\n55\nto.\nIDW\nc\nz\n\nil:\n\nzo\niur\n- .-\n\now\n\n- I- F;irmt\nIs -.-t\n\n8mei\n\n-u\n.\n\nigI-\n\n_U\n\nW\nW\n0I\n\ni\')\n\n0\n\nC)\n\ng\n\na\n\no<o\n\nCO\nI\n\nI\n\nI\n\nI\n\nI\n\n2-84\n\nI\n\nI\n\n_r,\n\n_"1.t\n\nf~.~:1\n\no\n\n8\n\nI\n\nI\ni\n\n,\n\n-I\n\nH\n\nI I It,\n\ng.\nT\n\nREEZE (71..\'\n.t\n1\n1\n.\n\nfa\n\no\n\n-,\n\nlm\nl\n\nR\n\n\xc2\xb0,o\n\ntitHt1\n\ni-\n\nIIII\n\n,4-\n\nIKc\n\nfrP\n\nHIOF\n\ni iF\n\xc2\xb7-i i ! ItT7l44:--, - t\nI{\n\nT-1- iiF\nrITi!T\n\n~~~~~~Ilititi\nMUtTIt\nI\nH I::.\np\nM=\nL;i I\'-I;l :t-1,;r -t: tIfH i-r+iH-i 1hhH!7J 1FT\n>\n-i\nI .Lrr ..f f tliII .H I 1ftlA I T4TTWIh lFf +-i F i LA U1A, -If-\n\nTh\n\nri4\n\n.i\'r\n\nii\n\nmCt-fif+t#fhU.\n\ni i\n\nI,;\n:il- . I "1t1:l,4tt I I-I l\n\n-. -\n\nI--\'i-\n\nAi\n\nIFl\n\niI:r\n\nI~\n~\n\niIT\n\n~\n\n\'\n\n-H\ni\n\n.1\n\n1\niI ZrI tI1\n!tl I IDIlilil ll Ii\nF1111 II\nI I1\n\niil\n\nM11 I\n\nli iL I I\n\nil1\ntII\nTll\n\nIII\n\nI0\n\n-it\'-\n\n6_\n\n-4,\n\xc2\xb7t\n_L7\'\n\nFr T\n\n: 7\n\n1\nI\n\n-1\nL-4.41 i\n~ i\n,\n\nq-ti-l3\' 1\n\nII EP1\nlli\nitI \'-J-__H-9ti\nIli-r,-tf\nP-41-41-TI-flT2-4iff-MMEMEJ+I-h \'I\nj\n-t-t- +f- ii ,H-h\nt+IHIII-i-tH-HH IiH-ft H il- ttl\nt-i It-ti - ii-I\n.1\n\n,.T\n\n_\n\ni <1\n4-f#LEU1\n\nLiJ\n-HIHerll\nH-f\xc2\xb1i7L1\xc2\xb1LHf\xc2\xb1.i r-14 J i ,\n\nTl\nI\'I~ ~~TT\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nii:~\n\n:!-i-F\n\n-I\n\nI\n\n.xF[-\n\n14:\n4, 4f\n\n:i~~~~~~~~~~~~~\n\n2i\n\n-I\n\n\'l4:t[\'\n\n!iL 1i141 -FtTTIiT\ni17iH77ll !-L~tBi\n-:T\ni1\ngid-l\'\n\nI I!1\niH!I\n\n$\n\nlt4 ;-1\nir rKi\nI\nI\ni!ri\'1 i -!\n\ntr\n\n\xc2\xb7\n\ntI\n\nm\nE\n\nI" ,-+rHl\nle\n--37\n\n5-\n\nII I\nqt\n\n5\n\nI\n\n-i-iTt- -i\n\nt-t-H\n\n43-\n\n( ii\n\n41ididM\n\nWDt\n\nW,1\n\nAIt\n\nIi\',\n\n:-Bt4 -tH\n\n0\n\nzo\n\noCU\n\nI[ I,1111111111\nIII\n\nI0\n\n,\n\n~ ii\n\nI\n\nD1n-,,\n::\n\ntI\n\nI\n" if\n\n-i\n\niI-SH.SlXlWt~\n\ni\n\n\'0~\n\n-U\n\nI u\nWU\n\n7-\n\nwU\nLM\nla\n4;\n\nrli\n5-\n\n4-\n\n:1\n\n3-\n\n(V)\n\n11121.\nIr Ii111\n\nI[[\nIII\n1IIIII.....Trr\n\nIi\n\nII\n\nIIIII1\n\nII\n\nIII\nII\n\nII I I IIII\nII\nIII\n\nrmrmm\nIII III II II\nI\n\nIII\n\n-\n\n08"\n\nIII\n\n11111\n...\n\n.....\n\n11111\n0\n\nI\n\n2-85\n1642.1\n\n-869.03\n\nS + 1244.8\n\n8 + 600\n\nThe standard z-transform is taken\n\na\n+\naT\n\'\ns + u\n1 - e-Ulz\n\nand\n\na + ib\n\n+\n\ns + u + iv\n\na - ib\n\n+\n\ns + u - iv\n\nT [2a] + [2e-T(bsinvT - acosvT)]z-1\n-\n\n1 + [-2e-uTcosvT]z1 + [e-2 uT]z-2\n\nHence,\n\nD(z) =4.0567X10 2\nD(z)\n1 - .97306z\n\n1.6421\n1 - .28800z - 1\n\n4.8803X10 7 - 4.420xX10-7z1\n\n+\n\n1 - 1.9654z\n\n1\n\n+ .99929z\n\n2\n\n-.86903\n\n+\n\n1 - .54881z-\n\n1\n\nThe frequency response of this function is found by letting z = ej)T,\nThe plot is shown in graph 4.\ninadequate.\n\nNote that this response is entirely\n\nThe standard z-transform is accurate only when G(s)\n\nis limited to frequencies less than 1/2T, or in this case, 500 Hz.\nThis condition is violated as is seen in the plot of the continuous\nButterworth design G(s).\n\nI\nI\nI\n7\n\n2-86\n\no\xc2\xb0a\n\n::\ni\n\n-t\n\nl\'-p\n\nO\n\no a\n\xc2\xb0 o ?\n\n2\n\nO\n\n:\n\niii\n\n4JM\ni\n\nI"\':t\n\n6-\n\n1\n\n4_-*****-\n\nMt\n14\n\nP\n\n-14~~~~F\n\nI HI I + I\n\n+~~H~~\n\ni\'" ,:\'-\n\nt:mrl\n\nI If\n\nVlt\'H\'.\n\nfm i\n\nS\n\nr\n\ntt1LIM\n12, t\n\n1\n\n:\nItIw-t~\n\n\'+H+1~~~~~~~~~.4K\n\'\nLU VtHHtt:\n\nLt~~Hjfifi~ltd bulfl4\n\n\',Htr! I\n\nl-H\n\n. -1144-I-\n\nI11\nil 1III IIII 1\nIII\nI11\n1 Il ll11111\n1\n-tt111111111ff\nlIllffH\' +H-14H-PI---11t-IH+I\n1111\n\nI\n\nTll\n\nit\n\nf\n\nlllli11l\n\nfi\n7\n\n5-\n\n-,\n\nItii\n44i 4i\n\n3-\n\n11 1\nA\nI\n::\n:1:4 flIII III -Ii It-I\xc2\xb1i1 ttt IM]T I 1 1\n:2\n1-LLLL-LH\nIIflmlII I i\nH\nH: : : :f1-1 441 Hl!i-LI ft -i-14 IIIII III I IL IH-1+1\xc2\xb1 IIIII IIII ilTIr F.]\n-I\n_T\n\nai\nzo\nu\n\n.H_\n\n__ __\n\nffith f\xc2\xb6lfttflfLL\n\nI11\n\n.\n\nIII\nL\n\nWI\n\nHHl\n\nIII\nIII\n\nIIIIIIIIII\n\nmill\n\nIrIlII1\n\nIt\n\n41T\n\nI\n0LO\n41\n\'4\n\nil\n\n1it\n\nrllll II\n\n,I!i E l1. ...\nI,_t\n\nI .IIQ\n\n1t-I1\'\n\n8_\n-\'o\n\n7-\n\nHt\n\nN)\n\n4_\n\n3-\n\n;4\n11111 11 II mI II\nmIIIIIIIIIIIIIIII\n\nIIIIIIIIIIIIII II\n\nII\n\nTII11111\n\nMI II\nIIIIIII III II II\n\nII\n\nIII\nIII\n\n1111111\n\nI\n\nII\n\n.. . ........\nI ....\nII II\n\nIII\n\nII II\n\nIII\n\nIII III II 1111\nII\nIIII IIII II\n.. . ..\n.\n..\n\nI\n\n.\n.\n0~~~~~~~~~~~~~~~~~~\n\n\\t\n\n2-87\n\nThe bilinear z-transform requires a prewarped frequency scale for\nthe Butterworth G(s) design, so\n\nUs = T tan ( 2z T).\n2\n\nunwarped\n\nwarped\n\n200\n\n170.41\n\n600\n\nOn\n\n200.67\n\n170\n\nWu\n\n618.67\n\nThe Butterworth design to be used in this case is\n\nG(s) =\n\nX\n\n1\ns2 +\n\n/i\n\ns + 1\n\nS=\n\ns(200.67 - 170.41)\n\n+\n\n2006717041\n\n1\ns + 1\n\ns = s/618.67\n\ns2 + (200.67)(170.41)\n\nG(s) = 838.92\n\n2\ns4 +68392s +1.1694X10\n9\n\ns4 +1294.8s3+6930\n\nG(s) = 838.92\n\n8s2 +4.4279X107s+1.1694X10 9 )(s+618.67)\n\n2\n(s2+3.4199X104)\n\n(s+26.987)((s2+.70708s+34199)(s+1267.1)(s+618.67)\n\n2-88\n\nThe bilinear z-transform is found by letting\n\nD(z) = G((s)\n\nT\n\n-1\n1 + z\n\n(1-1.9661z-l+z 2 )2(l+z-l)\n1\n(1-.97337z )(1-k9654z-+.99930z2 )(1-.22433z- 1 ) (1-.52749z- )\n\nD(z) = .19509\n\n- 1\n\nThe frequency response for this function is found with z\nplotted in graph 5.\n\n=\n\nejwT and is\n\nNote that this plot closely matches graph 1.\n\nThe Butterworth G(s) may be factored as follows:\n\nG(s) = 813.6\n\n(s+j84.39)\n(s-j184.39)(s+jl84.39)(s-j84.39)\n(s+27.314)(s+.35375+jl84.39)(s+.35375-j184.39)(s+1244.8)(s+600).\n\nThe matched z-transform is given by\n\nD(z) = G(s)\n\ns+a1l-e\n\n-aT -1\nz\n\n(s + u + iv)(s + u = iv) = 1 - 2e-uTcosvTz 1 + e\n\n2\n\nuTz -2\n\nI\n\nJL\n\n9-\n\n8\n\n1L1 Li\n!:, iI,j , r\n;i\n\nI\n\nI\n\nI;\n\n8\n\n%\n\noI\n\nI\n\nI\n\n\xc2\xb111+E\n\nFh - Mht-\n\nt Iil\n\nift\n1i-ilitiiLftHii\n\nIti.\n\nffl\n\n7l\n!,\n\n111\n\n1-t-, I:\nEg 41:l11t jli:tl:jiS\ng :-t--ttt \xc2\xa2tt44W4tt\n\nI\'IT\n.1..I\n\nI\n\nIIi\nI\nIf\nI\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nmufRTo.oQ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n7 44,PIIt\noH\n\nI\n._.! IifIt,\'\n\no0Q$\n\n2-89\n\nLF-\n\n1 i\n\n1\n\n5\n\nI\nI\n\nI1t\n\n. __g\n\nEl-,l-?l:\n\n\'-:tt\n\nltfii-i \'i.\n\n|\n\nw\n\n_t -W\n\n0\n\nt\n\n__ _ _ .i __|\n\ng\n\nAT:\n\n7-i - X\nt-\n\nj1.\n\n1:-mttt -l t1\ni!r\n\n-!I\n!\n\nIl It\n\n7i:\'~-tI\n\n-IT,\nl t! i1-\n\n1\'4t4: Lw\n\n1\n\n-0\n\ng4lr04\n\n11\n\n151\n\nL. t1\'\n!_164\n\n-1M-\n\n2_\n\n3\'ll\n\nw\n-tffffffl"f\n\n-I\n-i1\n\nW14fXi-,-\n\ng\nXX -0--W1\nX\n\nt\n\n.,tv\n+t- \'\n\n\'i\n\nI\n\n!i\n\n4 III-I\n\n,f;\n\nL\n\nfW~~~~~\n\n_\ngf W _. __ ._g\n-t _\n\nE ii ._ ._\ng ; W\n\nS\nX W X X : w ililililS40-lil!III\n111\n,\n\nl1\n\n\'111\n\n.IF ,I..\nIL:\nt\n\nII !\n\nR_\n\nI\n\n11\nn\n\n1\nn,\n\n8\n%J\n\n1111\n\nI\n\no\nTi\n,_\n\n_\n6_ . ..\n\nS_ H\n4_-\n\nI11\n\nEN\n\nm.\nII!\n\nz\nIi.\n\n.\na\n\n0\no\n\n9-o\narl\n\n7-\n\n1\nI ii\nxe\nLIM\n\n0\n4-\n\nII11\n-F1-,\n\nbL\n\nA-\n\nOIC\no\n\n-1\n\no\n\nX\nI\n\n0\n\nI\n\nI\n\na\nI\n\n2-90\n\nand D(1) is set equal to 1.356, the d.c. gain.\n\nHence,\n\n2\n(1-1.9661z-l+z-2)\nD(z) = .34607 (1-.97036z-1)(1-1.9654z-1 +.99929z-2)(1-.28800z-)(l-.5488lz 1 )\n1\n\nThe frequency response of this function is plotted in graph 6.\n\nNote\n\nthat the matched z-transform (like the bilinear) gives a good approximation to the response of graph 1.\n\nDigital Compensators [38]\nDigital filters are often employed as compensators for discrete\ncontrol systems.\n\nTwo common techniques for designing these compensators\n\nare root locus and Bode plots.\nRoot locus.\n\nA typical discrete-time closed-loop control system\n\nis demonstrated in Fig. 20a.\n\nLet\n\nn\naiz-i\n(2-23)\n\nD(z) = K i=0\nX biz- i\n\ni=0\n\nwhere K is a variable constant and ao = bo = 1.\ntechnique is outlined below\n1.\n\nFind the characteristic equation\n\n1 + D(z)Z[Gho(s)G(s)H(s)]\n\nThe root locus\n\nI\n\n2-91\no\no\n\nI\n\nW~~\n~\n\nQ\n\no000o\no\n\no\n\n~~~~\n\n6\'\n\nI\n\n9\n\n,>\n\ni\n\ng\n\nV ,2\n\n\xc2\xb0\n\n$\xc2\xb0\n\nQ\n\nIJf ,\n:_\n\n_\n\n,\n\ni.\n\nt\n\n\'. i\nzi\n\nlliP.!.i-il\n\n:-IL\n\n......\n\n-T\n\n4\n\n4~~~~~~~~~~~~~~~~~~~~1\n\nT\ng i@ mlIX1 gS .F t\nI41\ne~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nH\n;mttg\n\nI9\n\ni\n\n\'\n\n.\n\nX m? F1X X H1F~ff 1- <\n\nshX\n\n4-r -T\n\ntr\n\n~-\n\n~ :T\'\n\n,-iL-L l..\ntF,2F\ni::i\n\nI\nI\n\n~~~~~~~I41\n5-4 -\n\n+_\n\n.:+. .\n\n|i\n\nt\n\nX\n\nE ff g -i+\n\n....\n\nu\n\n... 2t.J.2\n\n. i(l th\n\n1\n\n3 ili0| 0Xee\n\ng\n\nE\n\nS\n\nm\n\n3t X11\n\nt~~~~~~~~~~~~~~\nLiJ\n\n:,t-d\n\nri\n:\n\nQC\n\nIn).\n\n0.\n\n4-- ~~\n~\n~\n\n;(i4a\nI-\n\nu\n\n11-I\n\n~ ~ ~ ~ ~ ~ ~~\n\ntt00\nX~~~~~~~r0Sle\n\n1-X\nt\n\n--\n\n---- w w\n\nSm!Xg Ei o~~-J\n\nT\n\n3~~~~~~~~~~~~~~~~~~~~~I\n\n- u\n\nMM\n0\n\n111141WfI111 411-Wh\n+-\n\n7-\n\nI\n\ng\n\ng\n\nW\n\n2--~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\njlli\n\n~llll!lmlmllllllllllLLLL\n\n!~~~~~~~~~~~~~1\n\n_.\n\n4\no\n\ni\n\nT 1-11-111111\n-, 4\'~~~~~~~~~~\n\n1I\n\n;t\n\ntn\n\ni\nI\n\n0\n\n1~~~~~~~\n\n2-92\n\nR(s)\n\nD()\n\n_\n\nC(s)\n\nH(s)\n\n(a)\n\nClosed-loop Control System\n\nEL\nincreasing K\nd,----\n\n1\n\n(b)\n\nTypical Root Locus\n\nFig. 20.\n\nRoot Locus\n\n2-93\n\n2.\n\nPlace the poles and zeroes of D(z) inside the unit circle\nin order to make the roots-of the characteristic equation\nstable for some range of K.\n\n3.\n\nVary K from 0 to - and solve for the closed-loop roots of\nthe characteristic equation.\n\n4.\n\nChoose an appropriate value for K.\n\nIn practice steps 2 and 3 are repeated on a trial and error basis.\n\nOnce\n\nthe procedure is complete, D(z) in (2-23) is completely specified.\nBode plots.\n\nBode plots are amplitude and phase plots for a\n\ntransfer function constructed using the asymptotic behavior of simple\nfirst and second order factors in the numerator and denominator of\nthe function D(s).\n\nThe plots are\n\ndb = 20 logID(j2rrf)I\n\n% = /D(j2nf)\n\nOnce the proper frequency response has been found, D(s) may be mapped\nto the z-domain using the bilinear z-transform.\n\nFrequency Sampling [3]\nEarlier in this report the technique of implementing a finite\nduration impulse response filter in a recursive manner was presented.\nThe-coefficients must be integer powers of the first one for this\ntechnique to be applicable.\n\n2-94\n\nNonlinear Programming [34]\nNonlinear programming can be used to design both recursive and\nnonrecursive digital filters.\n\n1 + ai z\n\ns\nH(z)\n\n=\n\n1\n\nThe filter is written as\n\n+ biz\n\n2\n\n(2-24)\n\ng I 1\niz\n+\ni= 1 + ciz-1 + diz-2\n\nor\nS\n\nH(z) = g +\n\n-\n\nai + biz 1\n\nE\n\ni=l\n\n(2-25)\n\n1 + ciz-1 + di\n\nz - 2\n\nAn error function is formed\n\nEk = IH(ej2 nfk)\n\n2\n\n-\n\nIF(2Wfk)I 2\n\nk = 1,N\n\n(2-26)\n\nwhere fk are the discrete frequencies, 2W is the sampling rate, and F\nis the desired continuous frequency response.\n\nNote that Ek is every\n\nwhere a differentiable function of ai , bi, ci , di, and g.\n\nThe errors\n\nEk must satisfy\n\n-Lk < Ek < Uk\n\nFrom (2-27) we may define\n\nk = 1,N.\n\n(2-27)\n\n2-95\n\n=k QUk\n\n- Ek\n(2-28)\n\nHk = QLk + Ek\n\nwhere Q\n\nk = 1,N\n\n> 0.\n\nA penalty function such as\n\nN\nQ +\n\nN\n\nr +_\nk=l Gk\nk=l\n\nis formed.\n\nr\n\n(2-29)\n\nGk\n\nA suitable computer program (such as the Fletcher-Powell\n\nalgorithm [39]) is used to minimize the penalty function with respect\nto Q,\n\ng, ai, bi, ci, and di.\n\nThen the factor r is divided by a factor\n\nand the process is repeated until Q becomes nearly constant.\nless\n\nIf Q is\n\nthan unity the procedure stops; otherwise, increase the number\n\nof stages s of the filter and repeat the above procedure until a Q\nis found less than unity.\n\nOptimal Digital Equivalent [40]\nIn this section the problem of determining an optimal digital\nequivalent D(z) for a continuous filter G(s) is considered (see Fig. 21).\nThe coefficients of D(z) are determined by fitting the input and outputs\nof the two filters.\n\ne d(J) = eo(j),\n\nLet\n\n(2-30)\n\n2-96\n\n31\n\nEi (s)\n\nG(s)\n\n1\n\nEi(s)\n\nEo (s)\n\nE\nT\n\n(s)t\n\nFig. 21.\n\nD(z)\n\n-T\n\n*\n\neo(t)\n\nT\n\nI.\n\nThe Equivalent Filters\n\neod (t)\n\n2-97\nand\n\nE (z)\nD(z) = od\nEi(z)\n\n=\n\n-l\n-n+l\na + alz-1 +,"+a\nzn\no\n1\nn1 + blZ- 1 +\'. +bnz-n\n\nThe problem then becomes one of choosing at and bl\n(2-30) is satisfied.\n\nd\n\n(J)\n\nI\n\nZ=o\n\nin D(z) such that\n\nA difference equation for (2-31) is\n\nn-1\neo\n\n(2-31)\n\nn\natei (jet) l\nle=l\n\nbeeod(J-,)\n\xc2\xb7\n\n(2-32)\n\nSubstituting (2-30) into (2-32)\n\nn-l\n\nn\n\nI\n\napel (j-t) - [eo(J) +\n\nZ=o\n\ne(j) =\n\n1\n\nC\n\nbte o (j-Z)]\n\n(2-33)\n\nwhere e(j) is driven to zero by minimizing e2 (j), the mean squared\nerror.\nIn vector form (2-33) becomes e(j) = .T(j)c - eo(j) where,\n\n0\ncT = [ao ,.,an l,-bl,"\'\',-bn]\n\n(2-34)\nqT(j) = [e\n\nl\n,(j),ej(j-n+\n),eo(j-1),\n(j\n)... ,e(j-n)].\n\nThe mean square error is\n\n_\nN\ne 2 = lim 1/(2N+l) I\ne2 (j)\nN-)j=-N\n\n(2-35)\n\n2-98\n\nEquation (2-35) is minimized by\n\n= 2 lim\nN+\nNac\n-a\n\n1/(2N+1)\n\nN\nI\n\nS(j)e(j) = 0\n\nJ=-N\n\nor\n\nN\n\nN\n(lim 1/2N+1\nN-x\n\nq(j)q\'(j))c = lim\n\nN-o\n\nj=-N\n\nI\n\nI\nR\n\n1/2N+1\n\nE.\n\nq(j)eo()\n\nj=-N\n\nI\n\nI\nr\n\nand\n\nc = Rir2\n\n(2-36)\n\n2-99\n\ncT)\n1I\n\nI\n\noo\nI\n\n\xc2\xb7r,~~~~~~~~~~~~~~\n\n-4\n_\n\n0\n-4\nU\n\n0\n0\xc2\xb7\xc2\xb7\n\n-\n\n(U\n\n\xc2\xb7\no\n0.\n\n=\n\no\n\n0 o-\n\nc\n\n:\n\nI\n\n.rn\no\n\nIm\n\n*\n_\n\no\n\n.;,-,,.\n\n0\n0J\n\no\n\n.\n\n_\n\n.r"\n\n0\n\no\n\n0\no\nal\n\n.:\nO\n\nO\n\n.\n\n,-4\nC.\n\nI\n0\n\nx\n\n,-4\n\xc2\xb7,\nI\n\n+p,oI\n\n*,)\n\n\xc2\xb7\n\ncr\n\n-4\nI\n\n0\n\n"-4\nr.\n\nv4 r< r\nI)\n\n-%0\nao\n\n-%o\n\nO\n\n*,,\n0\n\no\n\n0\n\n41\n\n0\n\n,-4\\\nI\n\n\xc2\xb7H\n\n0\nU,\n4.1\no-\n\n0\n\n.l\n\nI\n\n0\no\n\n+\n\n0\n0\n\n.v\'\n\nI \xc2\xb7r.\n"\n+\n\n-4 ,\n-I\n\n_4\nI\xc2\xb7,\n\n,\n\n.\n\n"-_\n\n.\n\nI\n\nca)\n\n-4\nI0\n\n,-4\n\n1,\n\n,\n0\n\n*\n0\n\nI\n\nO)\n\n.4\n\n"-4\n*\n\no\n\narl\n\no\n\n_\n\n-4\n\n<~~~~4\n\nOri\n\n*\n\n*1-\n\n_"\n\n0\n-4\n\nO\n\nI"\n\n0r\n.r,\n_\n\n-4\n\n,,\n_\na)\n\na) o\n\n*n,\n_\na)~~~\'o\n-\n\nz\n\no,\n\n,-\n\n0\nl-1\n\n-4\n\n+\n\nCe\n\nr=l\n\n--i\n\nr.\nII\n:1\n\n\'-I\n\n1I\n\nll\n\n2-100\n\nEquation (2-37) and (2-38) may be written\n\nAB\nR = [\n\nrT = [E Fl].\n\n]\n\n(2-39)\n\nCD\n\nThe elements of (2-39) are of the form\n\nA:\nB:\nC:\nD:\nE:\nF:\n\n4eiei(kT)\n$eieo(kT)\n$eoei(kT)\n$e e o (kT)\no\nOeie o (kT)\nOeo e o (kT)\n\n(2-40)\n\nwhere\n\nN\n\nxy(kT) = lim 1/(2N+l)\nN+>\n\nE\n\nx(j)y(j-k);\n\nJ=-N\n\nthe Oxy is the correlation function for discrete sequences.\n\nSince the\n\ninput signal power spectrum $eiei(s)and the analog filter G(s) are\nknown, (2-40) is determined by\n\n1\n\n4eiei(kT)\n=\n\nOeie o (kT) =\n\nand\n\nOeiei(T)IT = kT\n\n-l [eiei(s)G(s)]\n\n- [$eiei(s)]\n\n=\n\nkT\n\nT = kT\n\nkT = Deoei(kT),\n\nIT = kT\n\n(2-41)\n\n2-101\n\neoeo(kT) = -\n\n[4eiei(s)G(s)G(-s)]\n\n=\n\n|T =\n\nkT\nkT.\n\nThe digital filter determined above should have higher order than\nits analog counterpart so that the mean squared error will be small.\nSample Designs\nIn this section some example digital filters are listed.\nBandstop Filter\nA digital bandstop filter was designed earlier in this chapter:\n\n.34607(1-1.9661z-l+z\n\n2\n\n)2\n\nD(z) =\n\n(1-.97036z-1)(1-1.9654z +.99929z-2)(1-.2880z )(l-.54881z 1 )\n(2-42)\n\nThe frequency response for T = 0.001 is shown in graph 6.\n\nDigital Resonators\nA digital oscillator is formed by placing complex poles on the\nunit circle:\n\nD(z)\n\n(2-43)\n1 - 2 cos(2 fT)z- 1 + z-2\n\nwhere T is the sampling period and f is the frequency of oscillation.\nExperimental results are available in [41].\n\n2-102\n\nDigital Differentiators [42]\nThe differentiator is a necessary part of many practical systems.\nThe digital differentiator may take many forms; perhaps\n\nthe best is\n\na forth order recursive design shown below:\n\n-\n\n-\n\nD(z)\n\n=\n\n-\n\n1\n1\nA (1 - az 1 )(l - bz- )(l - cz )(l - dz 1 )\n(1 - ez 1 )(l - fz-1 )(l - gz-l)(l - hz- 1 )\n\n(2-44)\n\nwhere\n\nA\na\nb\nc\nd\n\n=\n=\n=\n=\n=\n\n0.36804011\n0.99999949\n-0.86810806\n0.32672838\n-.44183252\n\ne = -.10779165\nf = -.87602073\ng = 0.33494085\nh = 0.51312758\n\nThis differentiator was designed using nonlinear programming.\nA nonrecursive wideband differentiator can be constructed for N\nsamples by the relation\n\nGk = k/(N/2)\n\nk = 0, N/2\n(2-45)\n\n= (N-k)/(N/2)\n\nk = N/2 + 1, N - 1.\n\nIf the center samples are adjusted to optimally minimize the magnitude\nerror for N = 16, then\n\n2-103\n\nGN/ 2 = 0.92890015\n\nG (NI2)-i = 0.86994255\nG(N/2)_\n\n2\n\n(2-46)\n\n= 0.75000000\n\nyields a peak error magnitude of 7 x 10- 5 for an 80% bandwidth.\n\nLow-Pass Filters [43-45]\nReference [43] presents some 9 example nonrecursive low-pass filters\nof order 11.\n\nThe designs are found using prolate spheroidal functions,\n\nleast mean-square error, Fourier coefficients, windowing, binary\nweighting, and minimax techniques.\n\nThe reader is referred to Table 1\n\nof [43] for the appropriate coefficients.\n\nIII.\n\nCOEFFICIENT QUANTIZATION\n\nGeneral [46]\nOne effect of finite wordlengths in digital computers is that the\nfilter\'s parameters, or coefficients, must be chosen from a finite set\nof allowable values.\n\nClassical design procedures yield filter transfer\n\nfunctions with coefficients of arbitrary precision which must be altered\nfor implementation using digital computing devices.\n\nOne approach to\n\nthis problem is to select a filter structure (programming form for\nthe difference.equations) which is not sensitive to coefficient\ninaccuracies.\n\nFor example, realizing a filter directly allows a\n\ngreater chance for instability than cascading or paralleling second\norder modules because it is well known that the roots of polynomials\nbecome more sensitive to parameter changes as the order of the polynomial\nincreases.\nAny programming form, or structure, produces a grid of allowable\npole/zero locations in the z-plane.\n\nThe proper structure to choose is\n\none for which the grid is most dense in the areas at which the poles/\nzeroes must be placed for a particular design.\n\nIt is obvious that\n\narbitarily rounding or truncating denominator coefficients could cause\npoles to migrate outside the unit circle causing filter instability.\n\nInstability Thresholds 147]\nFor low-pass filters, a measure of the number mb of bits required\nto represent the coefficients of a stable filter may be expressed as\n2-104\n\n2-105\n\nmb = i - N log2 (2ir\n\nBT)\n\n(3-1)\n\nwhere\n\nB = minimum attainable bandwidth\n\n2i-1 <\n\nN/2]\n\n< 2\n\nfor the direct programming form.\n\nB2\n\nFor the cascade form\n\n2 - (mb - 2)/2\n. 2\n\n(3-2)\n\nThese stability thresholds are valid for filters designed using direct\nsynthesis in the frequency domain for sine and tangent Butterworth lowpass filters.\n\nThe results may be extended to other filter types.\n\nReduced Coefficient Wordlengths [48]\nThe cost of implementing a digital filter via a special-purpose\ncomputer is directly related to the wordlength of its coefficients.\nHowever, a short wordlength can cause large deviations in pole/zero\nplacement.\n\nHence a compromise must be found.\n\nThe following procedure\n\nrepresents one solution to the problem.\nLet the transfer function of the digital filter be\n\nH(z)\n\nm\nX bi -i\niO=0\nm\nm\n-i\ni ciz\ni=O\n\n(3-3)\n\n2-106\nwhere c\n\n= 1.\n\nIf we examine the desired frequency response H\n\naround\n\nthe unit circle\n\nIH (eWT)I = 1\nj\n\nin passband\n\n= 0\n\nin stopband\n\n(3-4)\n\nand is unspecified in the transition regions.\n\nIf the maximum passband\n\nand stopband deviations are defined as 6p and 6\n\nIH,(eJT)\n)I\n\nj\nH(eWT)I < 6(e\n\nT )\n\nor\n\nE(e j\n\nT\n\n)\n\n=\n\n1jJWT\n1 - IH (ej\n1\n\nE\n\nI in PB\n\njwT\n\n6s IIHn(e\n\nwhere\n\n)I\n\n(3-5)\n\n)I\n\nin SB\n\nis the normalized error function and H (z) = K H(z), a normalized\n\ntransfer function.\nThe design of H(z) minimizes max\n\nE\n\nsuch that\n\nmax E < 1\nusing standard minimax procedures.\n\n(3-6)\nIf (3-6) holds for a set of parameters\n\na, then justification for searching for a second set a\' of reduced wordlength which also satisfies (3-6), where\n\n2-107\nT\na = [bo,\nlb\n\n, bm\' co,\n\nm]\n\nThe coefficients are usually found for the cascade or parallel form.\nThe search for a new set a\' follows a modified univariate procedure\nwhich is described below:\n1.\n\nSeveral sets of parameters, say 10, are stored in order of\nminimum max E.\n\n2.\n\nPerform a univariate search on the best set a1 .\ncan be found, try a2.\n\n3.\n\nStop the procedure when no better improvement is found for\nany stored coefficient ai.\n\nIf no improvement\n\nGenerally, rounding of the coefficients is first performed.\n\nA\n\nunivariate search reduces max E by 25 to 50% over rounding, while a\nmodified univariate search produces the best results reducing max E\nby 25 to 50% over the univariate search.\nIn general, the development of synthesis procedures for quantized\ndigital filter coefficients remains an active area of research.\n\nIV.\n\nNONLINEARITIES IN FIXED POINT ARITHMETIC\n\nIn digital computer implementations for digital filters, the\nrestriction of finite wordlength produces several nonlinear phenomenon.\nQuantization occurs at the input sampler and in the internal arithmetic.\nSaturation and overflow also manifest themselves.\n\nInaccuracies in\n\ncoefficient representation has been discussed previously.\n\nOther noteable\n\neffects which must be examined are limit cycles and deadbands.\nQuantization Errors\nA digital filter specified by equation (3-3) is implemented by programming constant coefficient linear difference equations.\n\nThe program\n\nfor the difference equations will consist of the arithmetic operations,\nmultiplication and addition (subtraction), and data transfer operations.\nThe arithmetic unit of the computing device must be furnished binary\nnumbers for the coefficients and variables of the difference equations.\nSince each coefficient and variable is represented by a finite number of\nbinary digits, the binary numbers supplied to the arithmetic unit are\nquantized versions of the real numbers expected in the difference equation.\nHence the digital filter introduces quantization errors into the system\nof which it is a part.\nQuantizer Types [49]\nSignal amplitude quantization results from A/D conversion of the\ndigital filter input signal, and from arithmetic operations with in the\ncomputing device itself.\n\nThree common types of arithmetic quantizers\n\nare shown in Fig. 22; the step-length of each quantizer is h.\n\nFig 22a\n\nillustrates the quantizing characteristic for a roundoff quantizer.\n\nThe\n\nroundoff quantizer approximates the input signal ei by the closest quantized value eiq as follows:\n2-108\n\n2-109\n\neiq\n\neiq\n\nh\n\nh\n\nh\n\nei\n\nh\n\n(A) ROUNDOFF\n\nei\n\n(B) TRUNCATION\neiq\n3h\n\n2h\n\nh\n\nh\n\n2h\n\nFig. 22.\n\n(C) LSB-1\n\n3h\n\nei\n\nThree Common Arithmetic\nQuantizers.\n\n2-110\n\n_ h < ei 2\n\niqe < 2\n2\n\nfor e ii > 0\n(4-1)\n\nh < ei\n\neiq\n\n\'2\n\nfor ei < 0.\n\nTherefore, the maximum error magnitude is h .\ntruncation quantizer is shown in Fig. 22b.\n\nThe properties of the\n\nThis quantizer is less diffi-\n\ncult to implement than the roundoff type; however, the approximation eiq\nis less accurate:\n\n0 < ei - eiq < h\n\nfor ei > 0\n(4-2)\n\n-h < ei\n\n-\n\neiq <\n\nfor e i\n\n< .\n\nHere the maximum error magnitude is h.\nThe third arithmetic quantizer presented in Fig. 22c is labeled LSB-1.\nIn LSB-1 the least significant bit of quantized binary words is always\nset to "one."\n\nFor this quantizer eiq is never equal zero.\n\n-h < ei - eiq < h\n\nfor ei > 0\n(4-3)\n\n-h < ei - eiq \'<h\n\nfor ei < 0.\n\nAgain the maximum error magnitude is h.\nSignal amplitude quantization at the A/D converter usually takes two\nforms.\n\nIf the A/D converts the input signal magnitude to binary form,\n\nthen the quantizer characteristic of Fig. 22b for truncation adquately\n\n2-111\ndescribes the effect of the A/D.\n\nHowever, if a bipolar A/D is used, the\n\nbipolar property is usually obtained by an offset bias voltage which\ncauses the bipolar A/D quantization characteristic shown in Fig. 23.\n\nFor\n\nthis quantizer\n\nO < ei - eiq < h\n\n(4-4)\n\nand the maximum error is h.\nIn summary, the maximum error magnitude introduced by a quantizer\nat a sampling instant is\n\nRoundoff:\n\nhi = h/2\n\nOthers:\n\nh!\n\n(4-5)\n=\n\nh\n\nThe quantizers of Figs. 22 and 23 may be represented in a system as\nan additional input error signal; this process is shown in Fig. 24.\n\nUsing\n\nthis model for the quantizers, their effect on system response will now\nbe considered.\n\nMathematical analysis of quantizing errors may generally\n\nbe described as steady-state analysis, statistical analysis, and error\nbound analysis.\n\nEach of these analysis techniques will now be presented.\n\nSteady-State Analysis [50]\nThe steady-state analysis may be divided into three steps.\n\nFirst,\n\nfind the z-plane transfer functions Tj(z) from the jth quantizing error\n\n2-112\n\n(2\n\nn-1\n-1)h\n\n\xc2\xb7\n\nLq\n\nSATURATION\n-\n\n-\n\nI-\n\nt\n\nh\n\n2n-1h\nSATURATION\n\nN = WORDLENGTH OF A/D\n\nFig. 23.\n\nBipolar A/D.\n\n2-113\n\nei(kT)\n\neiq(kT)\n\n(A) THE J\n\nth\n\nj\n\nr I\nI\n\n--\n\nQUANTIZER\n\nQUANTIZER\n\n-\n\nI\nnj(kT)\n\nI\n\nI\n\nMATHe\n\nei(kT)\n\nl\n\n(kT)\n\nI\n\n_ _ J\n(B) MATHEMATICAL REPRESENTATION\n\nFig. 24.\n\nMathematical Model for a Quantizer.\n\n2-114\nsource Nj to the system output, eo .\nthe system is s.\n\nThe total number of quantizers in\n\nHence,\n\nS\n\nEon(z)=\n\nI\nj=l\n\n(4-6)\n\nTj(z)Nj(z),\n\nwhere Eon (z) represents the output due to quantization errors.\nSecond, assume each error source is a step input of the maximum\nerror amplitude h\' for the type quantizer being analyzed.\n\nTherefore,\n\nh\'\n3\n1 - z-\n\nNj(z) =\n\n(4-7)\n1\n\nSubstituting (4-7) into (4-6)\n\ns\nEon(z) =\n\nTj(z) h\'\n\nI\n\nj=l\n\n(4-8)\n1 - Z\n\n1\n\n1\nLastly, apply the z-transform final value theorem [y(-) = lim(l - z- )\nz+l\nY(z)] to (4-8); thus,\n\nS\n\neon(O) = lim I Tj(z)h\'\nz-l j=l\n\nl=im Tj(z)\n\nhji.\n\nIf one defines\n\nKssj\n\nlim T\n\nsj\nz+l\n\n(z)9)\n\n2-115\nthen\n\neon (co)\n\ns\n\nK5\nss\n\nI\n\n=\n\nh\'\n\n(4-10)\n\nj=l\n\nEquation (4-10) may be used to evaluate the effect of each quantizer on the\nsystem output under steady-state conditions.\nAnother technique for finding the Kssj weighting constants for (4-10)\nis derived as follows.\n\nThe standard z-transform for tj(t) is\n\nco\n-\n\nTj(z)\n\nt (kT)z\n\n=\n\nk\n\n(4-11)\n\nk=0\n\nwhere kT represents a sample instant.\n\nKss j\n\n=\n\nI\n\nk=O\n\nHence (4-9) becomes\n\ntj(kT)\n\nIf tj(kT) tends to zero as kT gets large, say NT,\n\nN\nKssj -\n\ntj(kT)\n\n(4-12)\n\nk=0\n\nmay be used in (4-10) to calculate the steady-state error.\n\nThe terms\n\ntj(kT) in (4-12) may be obtained from a simulation of the system by applying Njdz)\n\nw\n\n1, a discrete impulse function.\n\nNote that the weighting con-\n\nstants are functions of the system characteristics and not of the quantizers.\n\n2-116\n\nStatistical Analysis [51]\nIf the input signal to a roundoff quantizer Qj has a dynamic range\nof more than three step intervals hi, the effect of the quantizer may be\ndetermined by replacing it with a unity gain and an additive white noise\nnj(kT) (see Fig. 24) with a rectangular amplitude distribution density\nfunction p(nj) of bounds +hj/2 and height 1/hj.\n\nThe LSB-1 quantizer can\n\nalso be replaced in this manner with p(nj) bounded by +hj and 1/2hj.\n\nThe\n\ntruncation quantizer cannot be represented exactly in this manner, but\nthis technique does give a good approximation with p(nj) bounded by\n+hj and 1/2hj.\n\nLet us continue by analyzing the roundoff case which\n\ncan be easily extended to the others.\nThe variance anj2\n\nan;\n\n=\n\n|\n\nof this rectangular distribution is\n\nnjp(ni)dn. =\n\n-00\n\n~\n\n~\n\n-\n\n(4-13)\n\n<\n\n12\n\nWhen the dynamic range of the input signal is greater than three\nquantization levels, the noise input of the quantizer is essentially uncorrelated between successive sampling intervals, and the autocorrelation\nof the quantization noise becomes\n\nco\n\nnjnj() =\n\n(T - ITI)/T\n\nOn\n\nn-= O\n\nIT\n\n<\n\nT\n\nJ\n\n(4-14)\nITI >\n\nT\n\n2-117\nThe sampled power density spectrum is defined by\n\nn n (nT)z-n\nn=-nj\n\n=\nI\n\nnjn (z)\n\n=\n\n2\n\nan\n\n(4-15)\n(4-15)\n\n12\n\nThe mean-squared error output due to one quantizer error is\n\neon2 (kT)\n\n=\n\n1/2ri f\n\nn n (z\n\nTjzT(z)T(\n\n)dz/z\n\n(4-16)\n\nr\nwhere T(z)\n\n=\n\nEon(z)/N(z), r is the unit circle, and i =\n\nI4.\n\nSubsti-\n\ntuting (4-15) into (4-16) and assuming that the total rms output error is\nbounded by the sum of the s rms errors due to the quantizer inputs yields\ns\n\n[eon]rms <\n\n(4-17)\n\n1 Kstj hi\nj=l\n\nwhere\n\nKstj\n\n=\n\n24i J\n\nTj(z)Tj(1/z)dz/z]\n\n(4-18)\n\nr\nThe integral in (4-18) may be evaluated by calculating the residues of the\nintegrand.\nAnother technique for calculating the mean-square output error is by\nusing the following identity:\n\n12i\n\n2lf i f\'\n\nF(z)F(l/z)dz/z =\n\nHence (4-18) becomes\n\nKstj\n\n=\n\nk\n\nk=0\n\ntj(kT)2\n\nI\n\nk=O\n\nf(kT)2\n\n2-118\nwhere tj(kT) is found from the impulse response in a simulation of the\nIf tj(kT) converges to zero for k large, say N, then\n\nsystem.\n\nKt\n\nKstj\n\nN\n\n[12 kO\n\nj\n\nk2]\n\nij i\n\n1/2\n\n(4-19)\n\nThis relation may be used instead of (4-18) for many applications.\nEquations (4-18) and (4-19) are for roundoff only.\n\nThey should be\n\naltered by substituting hj = 2h\' into (4-17) for the general case.\n\nQuantization Error Bounds [521\nConsider an nth order system described by\n\nx(k + 1) = Ax(k) + Dr(k)\n(4-20)\neo (k)\n\n=\n\ncTx(k) + dTr(k)\n\nwhere r(k) is a vector of the system inputs and eo(k) is the output.\nThe sampling interval T has been eliminated for convenience.\n\nThe\n\nintroduction of quantizers into the system results in\n\nxq(k + 1) = Axq(k) + Dr(k) - BI(k)\n\n(4-21)\ne\n\n(k )\n\n= cTxq (\n\nk\n\n) + dTr(k) - fT (k)\n\nwhere q(k) represents a vector of the s quantizer error inputs nj(k).\nstate variable representation for the quantization error\n\nA\n\n2-119\n\nv(k) = x(k) - x2q(k)\n(4-22)\neon(k)\n\neo (k)\n-\n\neo(k)\n\nresults from subtracting (4-21) from (4-20)\n\nv(k + 1) = Av(k) + Bq(k)\n(4-23)\neon(k) = cTv(k) + fTq(k).\n\nThe general solution for (4-23) is.\nN-l\nv(N) = ANv(O) +\n\nI A9B&(N - 1 JZ=o\n\n)\n\n(4-24)\n\nN-1\neon(N) = cTAN_(O) +\n\nc\n\nSTA B (N - 1 - Q) + fTq(N).\n\nQ=O\n\nFor N large,\n\ns\nv(N) =\n\nN-1\n\nI\nJ=1\n\nA\nQ=o/\nI\n\n\\\n\njb)qj(N - 1 - Q)\n(4-25)\n\nj=\n\n=N-1o\nT\n\n-1- Q) + f, (N).\n\nwhere qj(N) is the jth quantizer and\nmatrix B.\n\nj\n\nis the jth column of the nxs\n\nSince, if a - bc + de, laL<lblxcl+ldlxlIel\n\n2-120\nN-1\n\ns\n\nl5(N)I - J-,\n-o\n1\n\nIA\n\nm\n\nb\n\nj I)qj(N - -1\n)\n\nand\n\ns\n\nN-1\n\nIv(N)Imax <- j1(o\n\nwhere hi =\n]qj(n - 1 -\n\nA\n\nI) h\n\n(4-26)\n\n)Imax is given in (4-5).\nS\n\ns( N-\n\nleon\n\nSimilarly,\n\nmax _- j l(\n\nIc..A\n\nJ=l\nmax 9.=0\n\n)hjI\nh +\n\n(4-27)\n\nIfjlh.\n\n(4-27)\n\nj=l\n\nIn another form,\n\ns\n\nlv(N) Imax - j\n\nml\nh\n\nj=1\n(4-28)\ns\n\nleon(N)lImax <_ I\nJ=l\n\nbjhj\n\nwhere\n\nN-1\nmj\n\n2\n\nIAN\nl\n\nI, j\n\n,\n\nKbj =11 (cTAj )+ IAjl\n,\n\n(4-29)\n\nNote that (4-29) gives weighting vectors mj and weighting constants Kub\nj\nwhich are functions of the system and not of the quantizers.\n\nHence, (4-28)\n\n2-121\nand (4-29) are useful in helping to choose quantization error schemes for\nsystems with digital filters.\nA second method for bounding the output error due to quantizer Q\nis from the transfer function\n\nTj(z)\n\nE on(z)\n\non\n\n=\n\nNj(z)\n\nThe impulse response is found with Nj(z) = hj. Therefore,\n\nT (z)h; -\n\nEon(z)\n\n112\n\nt(k)z ]h\n\n0,\xc2\xb7\n\ni lj-\n\nOr\xc2\xb7I,"J\n\n(4-30)\n\nTo calculate the worst case output error eon due to quantizer Qj\n\nIt (k)I] h\n\nIeon(N)Imax <\n\n(4-31)\n\nSimilar to the argument employed for equations (4-12) and (4-19)\n\ns\n\nIeon(N)Imax ! i\n\nKubjhj\n\nwhere\n\nN\nKubj :O\n\nItj(k) , j = 1, s.\n\n(4-32)\n\nEquation (4-32) may be used to calculate the weighting constants Kubj\ninstead of (4-29).\n\n2-122\n\nA summary of the results of the quantization analysis presented in\nthis section is displayed in Table 1.\n\nTABLE 1:\n\nQuantization Analysis\n\nFigure of Merit\n\n=\n\nE\n\nConstantj h\'\n\nAnalysis\nMethod\n\nFigure of\nMerit\n\nConstantj\n\nSteady\nState\n\nSteady\nState Error\n\nKss\n\nlimi\nTj(z)\nz + 1\nN\n\nKss j\n\nStatistical\n\nRoot mean\n\n:\n\nKstj =l\n\nKsbtj\n\nMaximum\n\nBound\n\nerror\n\nf\n\nTj(z)Tj(l/z)\n\ni\n\nsquare error\n\nError\n\ntj(kT)\n\nk=O\n\nz\n\nN\nI\ntj(kT)2\n[3 k=O0]\nN-1\n\nKubj = Z\n\nI ICTA\nfiI\n\nN=\n\nKub\n\nj\n\nN\n=Z k=-O\n[ltj(kT)[\n\nOpen-Loop vs. Closed-Loop [53]\nThe quantization analysis procedures above are equally applicable\nto open-loop or closed-loop systems.\n\nHowever, open-loop analysis of the\n\ndigital filter itself is perhaps the easier approach.\n\nIt has been\n\nshown in [49,53] that open-loop analysis can give satisfactory results\neven if the filter is to reside in a closed-loop system.\n\n2-123\nLimit Cycles and Deadband Effects [46,54,55]\nConsider the digital filter\n\nYn\n\n=\n\n(4-33)\n\n=\'0.5\n\nxn + B Yn-1\n\nimplemented in fixed-point arithmetic with roundoff quantization.\n\nIf\n\nthe input xn is a impulse function of value 7/8\n\nYo = 7/8\nYl = 1/2\n\n(4-34)\n\nY2 = 1/4\nY3 = 1/8\n\nn> 4\n\nYn = 1/8\n\nis the resulting output sequence.\nzero.\n\nIdeally the output should go to\n\nThis type error is called a limit cycle, and the amplitude\n\nintervals within limit cycles are called deadbands.\n\nThe deadband for\n\n(4-33) is\n\ntYn-ll\n\n-\n\nlYn-ll\n\n1\n\n<\n\n( 2 )\n\n2-b\n\nwhere b is the number of magnitude bits.\n\n1)2[Yn-l I <\n\n( -1 ) 2-\n\ni- 161\n\nHence\n\nb\n\n(4-35)\n\n2-124\nFor the second-order filter\n\nn =\n\nn -\n\nlYn--\n\nYn2\n2\n\n(4-36)\n\nthe deadband is\n\nIYn.2\n\n<\n\n-b-1\n2\n\n(4-37)\n\n1 - 1821\nThe deadband for higher order filters is directly dependent upon\nthe programming form.\n\nIn general, the parallel form yields better\n\nresults because one need not be concerned with the ordering of cascaded\nsections [54].\n\nSaturation and Overflow [56]\nWhen a filter is implemented in one\'s or two\'s complement arithmetic\nand signal values exceed the finite register length upper limit, a\noverflow condition occurs and the results usually changes sign.\n\nThis\n\ncondition can cause large limit cycles, called overflow oscillations,\nto be excited.\n\nThese oscillations may be avoided by using saturation\n\narithmetic as designed in [57,79].\n\nOne must be wary of this solution\n\nfor in many closed-loop control systems, saturating the signals causes\nsystem instability.\n\nSaturation changes the filter output which\n\neffectively alters, temporarily, the transfer function.\n\n2-125\n\nDynamic Range [46]\nThe dynamic range of a binary signal\n\n0 < Ixn\n\n< 2b\n\nxn of b + 1 bits is\n\n(4-38)\n\n1.\n\nIncreasing the number of bits by one doubles the dynamic range.\n\nAs\n\nseen in the last section, it is important that the dynamic range of\na digital filter in many applications never be exceeded.\n\nHence, several\n\ntechniques may to employed to find b.\nOne technique finds the least upper bound on the signal xn and\nuses (4-38) to specify b and hence this limit can never be exceeded.\nMore practical solutions use simulation of the filter with typical\ninputs to define the dynamic range of the internal variables.\n\nSome-\n\ntimes statistical methods are used for non-deterministic input signals.\n\nV.\n\nNONLINEARITIES IN FLOATING POINT ARITHMETIC\n\nIn the past there has been little emphasis placed on research and\nanalysis of quantization errors at the output of a floating point filter,\nthe reason for this being that most filter implementations use fixed\npoint arithmetic.\n\nSandberg [58] was the first to study quantization\n\nerror analysis for floating point filters with [59-62] being more recent.\nAs in the case of fixed point filters, quantization error for floating\npoint filters has three sources due to finite word length.\n1)\n\nThey are\n\nthe quantization of the input signal xn into a set of discrete\nlevels;\n\n2)\n\nthe representation of the coefficients of the filter, ak and\nbk, by a finite number of bits;\n\n3)\n\nthe accumulation of roundoff errors caused by arithmetic\noperations.\n\nNotation\nIf we assume the ideal output of the filter is wn and the actual\noutput Yn, the error at the output of the nth sample en may be defined\nas\n\nen = Yn -\'wn\n\n(5-la)\n\nwhere\n\n2-126\n\n2-127\nM\nWn =\n\nN\nakxnk\n\nk=O\n\nbkwn-k\n\n-\n\n(5-lb)\n\nk=l\n\nBefore the effects of the above error sources are discussed, the representation of floating point numbers with a fixed number of bits should\nbe considered.\nA floating point number is written in the form (sgn)2b\'a, where b\nis a binary integer called the exponent and a is a fraction between\n1/2 and 1 called the mantissa,\n\nAs expected, the range of numbers that\n\ncan be represented is determined by the number of bits of the exponent.\nIn order to represent a number v in floating point form with a t-bit\nmantissa, the smallest integer exceeding log2 v is first determined.\nThis number is denoted by [log2 vl.\n\nThe binary expansion of the fraction\n\nv/[log2 vl is then rounded to t bits.\n\nIf (v)t denotes the t-bit mantissa\n\nfloating point approximation, it is seen that\n\n(V)t = v(l + E)\n\n(5-2)\n\nwhere the error is bounded by -2- t\n\n< E <\n\n2\n\n,\n\nor [-2,2).\n\nError Sources\nBoth addition and multiplication in floating point arithmetic\nintroduce roundoff error.\n\nLet (vl\'v2 )t and (v1 + v2 ) t denote,\n\nrespectively, the actual computed product and sum of two numbers v1\nand v 2 ; then\n\n2-128\n\n(vl\'v 2 )t = (V1\'V )(1 + 6)\n2\n\n(5-3)\n\n(vl + v2)t = (v1 + V2)(1 + C)\n\n(5-4)\n\nwhere the errors 6 and c are bounded by [-2 -\n\nt\n\n, 2-t).\n\nThe above errors will be regarded as random quantities and they\nwill be uniformly distributed in their range [-2 - t,\n\n2\n\n-t).\n\nMaking these\n\nand the above assumptions, a statistical approach will be discussed\nwhich predicts floating point quantization errors.\nFirst, consider the effect of input quantization.\n\nSupposing\n\nthe quantizer has equal step size h, the input to the filter is\nn\n\nQ\n+ en where each en is bounded by -(h/2) < eA < (h/2).\n\nSince the\n\nfilter is linear, the output is the sum of the two components, xn\nand e .\nQ\n\nIn determining the effect of input quantization, eS is\n\nconsidered as white noise with a zero mean and variance h 2 /12.\n\nThe\n\nsteady-state output component due to eQ is a zero-mean wide-sensestationary (w.s.s.) sequence with power spectral density\n\n2\nH(z)H(l/z)(h /12)\n\n(5-5)\n\nwhere H(z) is the transfer function of the filter as repeated below\n\nM\nH(z) = (\n\nE\n\nk=O\n\nN\nbkz-k)\n\nakz-k)/(l +\nk=l\n\n(5-6)\n\n2-129\n\nThe effect of coefficient inaccuracy on roundoff accumulation has been\nignored.\nAn expression for the mean-squared value of the error at the\nfilter\'s output due to input quantization is obtained by integrating\nthe power spectral density (Equation (5-5)).\n\nIt is equal to\n\n1/2rj fHR(z)H(l/z)(h2/12)]/zdz\n\n(5-7)\n\nCoefficient Quantization\nConsidering the effect of coefficient quantization, it is seen\nthat each coefficient is replaced by its t-bit representation\naccording to (5-2).\n\nThis means the coefficient ak is replaced by\n\n(ak)t, which equals ak(l + ak), with ak bounded in absolute value\nby\n\n2 -t .\n\nLikewise, each bk is replaced by (bk)t which is bk( 1 + Sk).\n\nBecause of this, it is abvious that the filter characteristics will\nchange.\n\nThe problem can be approached in several ways.\n\nThe first,\n\nand the simpliest, is to compute the frequency response of the actual\nfilter with t-bit rounded coefficients by using the actual transfer\nfunction\n\nM\n[H(z)]t = (\n\nI\n\nk=O\n\nN\n(ak)t z-k)/(l +\n\nI\n\n(bk)tz-k\n\n(5-8)\n\nk=l\n\nand then comparing the result with the ideal response of the original\ndesign.\n\n2-130\n\nCoefficient rounding can cause movements of the poles and zeroes\nWhen this happens, network sensitivity\n\nof the transfer function.\n\ntheory can be applied to study the changes of the filter response.\nIf the poles of H(z) are at zi , i = 1, N, and the poles of [H(z)]t\nare at zi + Azi, it can be shown that\n\nN\n\nN\nk=l\n\n(z\n\n)/\n\nzi/n))]/Aak\n\nm=l\n\n(5-9)\n\nmai\nwhere Aak is the change in the coefficient ak.\nbe obtained for the movement of the zeroes.\n\nLikewise, results can\n\nThe change in the filter\n\nresponse can be studied from these movements.\nInstability of a filter may occur, due to coefficient error, when\na filter has poles that are close to the unit circle in the z-plane.\nThe problem can be serious when the sampling rate of the filter is\nrelatively high, even for low order filters in the direct form.\nKaiser 162] has demonstrated that for an Nth-order low-pass filter\noperating at a sampling rate of 1/T with distinct poles at e-PkT,\nstability is guaranteed if the number of bits used m b satisfies the\ninequality\n\nN\nmb > [-log2 [15f\n\n/2N + 2) (\n\nn PkT)]]\nk=l\n\n(5-10)\n\nwhere the bracket denotes the samllest integer exceeding the quantity\ninside.\n\nIt is also possible to extend the result to include multiple\n\n*\n\nX~C~\n\n2-131\n\npoles and to derive similar results for filters of other than low-pass\ntype.\nThe effect of coefficient inaccuracy is more pronounced for a\nhigh-order filter when it is realized in the direct form than when\nit is realized in the parallel or cascade form, which suggests the\nparallel or cascade form should be used for high-order filters when\nFurther details on coefficient quantization are given in\n\npossible.\nChapter 3.\n\nOutput Error\nRoundoff accumulation error for floating point filters 159-61]\nis quite different from that of fixed point filters and consequently\nwill be treated with more depth than that of fixed point.\n\nThe errors\n\nintroduced are relative to Equations (5-2), (5-3) and (5-4).\n\nThe\n\ncalculation of the statistical mean-squared error at the output will\nbe discussed for the direct programming form with the understanding\nthat extension to other forms is easily accomplished [61].\nIt has been shown\n\nthat for floating point arithmetic\n\nfilter coefficients are ak(l + ck) and bk(l + Sk\n)\nbounded in absolute value by\n\n2-\n\nt.\n\nthe actual\n\nwhere ck and 8k are\n\nThe actual computed output yn is\n\ngiven by\n\nM\nYn\n\n=\n\nftJ\n\nN\n\nI ak(l + ak)Xn-k I bk(l + ak)Yn-k]\nk=O\nk=l\n\n(5-11)\n\n2-132\n\nwhere fe[\n\n] denotes the actual computed result by floating point\n\narithmetic of the quantity inside the brackets.\n\nIt is assumed that\n\nthe computation of (5-11) is carried out in the following order:\n\nthe\n\nproducts ak(l + ak)xnk and bk(l + Bk)Ynk are first formed; the two\nsums are then calculated; and finally the difference is taken to give\nYn.\n\nEach of these arithmetic operations introduces a round-off error\n\nwhich is characterized by (5-3) and (5-4).\n\nA flowgraph of this operation\n\nmay be drawn, as is shown in Fig. 25, which includes all the roundoff\nerror introduced in the calculation of Yn.\n\nFrom Fig. 25, it is seen\n\nthat 6n,k is introduced when the product of ak(l + ak)xnk is formed,\nand\n\n6\n\nn,1 is introduced when the computed products of ao(l + ao)xn and\n\nal (l + al)Xn_l are added.\n\nThe actual output Yn is then\n\nM\nYn =\n\nN\n\nk=O\n\nak(l + ak)en,kxn-k -\n\nX\n\nbk(l + Bk)\n\nk=l\n\n4\n\nn,kyn-k\n\n(5-12)\n\nwhere\n\nM\n0\n\nn,o = (1 +\n\nn)(\n\n+\n\n6\n\nn,o\n\n)\n\ni\n\n(1 +\n\nn,l)\n\n(1 +\n\nn,i )\n\ni=l\nM\nOn, k = (1 +\n\nn)(l + 6n,k) i\ni=k\n\nk = 1, 2,",\n\nM\n\nN\n%n,l = (1 + Cn)(1 + En,1 )\n\n(1 + nn,i)\ni=2\nN\n\nn,k\n\n=\n\n(1 + Cn)(1 + Enk)\n\nH (1 + nn,i ) ,\ni=k\n\nk = 2, 3,---,L\n\n(5-13)\n\n2-133\n\na (l+a )xn\n0\no\n\nbl(l+01)Yn_\n\nn\n\n1\n\nOb 2 (1+B2 ) n-2\nL+ n,1\n\na2 (l+c\n\n2\n\n-ob3 (1+s 3 )Yn_ 3\n\n2( 2)\n)x 2\nn\n\n14ln,3\n\nI\nI\nI\nI\nI\n\nI\n1+6\n\nI\nI\n\nI\n\nl+e n\n)bN-(1+N)Yn-N\n\naM (l+aM)\nn-M\n-(1+n N)\n-(1+l n,N)\n\nYn\n\nFig. 25.\n\nFlowgraph of Equation (2-81).\n\n2-134\n\nThe quantities\n\n6\n\nn,k; Cn,k; nn,k; en,k; and En are the errors introduced\n\nat each arithmetic step and they are independent random variables uniformly distributed in [-2- t,\n\n2\n\n-t).\n\nFrom Equations (5-1) and (5-12) it can be shown that the error\nen satisfies the following equation:\n\nN\n\nY bkek\n\n= U\' + u"\n\n(5-14)\n\nk=O\n\nwhere b o = 1 and\n\nM\n"n\n\n=Z\n\nk=O\n\nN\nakakXn-k\n\n-\n\nk=l\n\nbk kWn-k\n\nM\n\n"=f\n\nk=O\n\nN\n\nak(-n,k\n\n)xn-k -\n\nk= 1\n\nbk( n,k\n\n)Wnk\n\n(5-15)\n\nIn the above equations un is due to coefficient rounding; un is due\nto roundoff accumulations and the input x n is zero mean and w.s.s.\nBoth components u\' and u" have zero mean and are w.s.s., and they are\nuncorrelated, this being because On,k and On,k have a mean equal to 1\nand are independent of x n and wn.\nFrom Equation (5-14), the error sequence en is zero mean and w.s.s.\nwith a power spectral density related to those of u\' and u" by\nn\nn\n\nbee(Z) = [l/(D(z)D(l/Z))][Putu,(z) + Ou,,u,,(z)].\n\n(5-16)\n\n2-135\n\n%uu,(Z) is calculated from Equation (5-15) and is given by\n\n, \'(z)\n\n=\n\nIB(z) - H(z)A(z)]\' B(l/z) - H(l/z)A(l/z)]xx\n(Z)\n\n(5-17)\n\nwhere H(z) (Equation (5-6)) is as previously defined and\n\nN\nA(z) =\n\nkk\n\nbkkz-k\nk=l\n\nM\nB(z) = C akckz\nk=O\n\nk\n\n(5-18)\n\nConcluding from Equations (5-13), and (5-15), u" is white noise with\nn\npower spectral density as follows;\n\n(u ,u,(Z) = q2 /2rj f(F(z) + G(z)H(z)H(1/z)\n-N(l/z)[D(z) - 1]H(z)\n\n-N(z)ID(l/z) - 1]H(l/z))xx(z)/Z dz\n\nM\nwhere N(z)=\nakz\nk=O\nEquation (5-6) and\n\nM\n\nF(z) =\n\nis the numerator of the transfer function in\n\nM\nI akaFk,iZ\nk=O i=O\n\n(5-19)\n\n2-136\nN\n\nG(z) =\n\nN\n\nI\nk=l i=l\n\nbkaiGi,i\n-\n\ni\n\nM + 2 - max(k,i),\n\nFk,i\n\nk\'i\n\n=\n\n-M + 3 - k,\n\nk 0 i or k = i = 0\n\nk= i\n\n0\n\nN + 2 - max(k,i),\nGk,i\n\n=\n\nk # i or k = i = 1\n\nN + 3 - k,\n\nk= i\n\nl\n1\n\n(5-20)\n\nThe mean squared value of the error en can now be calculated from\nbee(Z) by using\n\nEe\n\n2\n\nI = 1/2nj f\n\nee(Z)/z dz.\n\n(5-21)\n\nVI.\n\nPROGRAMMING FORMS\n\nThe structure of a digital filter is described by a unique set\nof constant coefficient linear difference equations.\n\nThese difference\n\nequations constitute the digital filter\'s programming form.\n\nAs a general\n\nrule, for any programming form the lower the order n of the filter\ntransfer function the smaller the error introduced into the system by\ncoefficient and signal amplitude quantization.\n\nConsequently, a nth\n\norder filter is usually factored into second-order modules which are\nparalleled or cascaded to realize the higher orders.\n\nThe second-order\n\nis chosen so that complex poles and zeroes are realizable.\nThe z-transfer function for any second-order module may be expressed\n\na o + alz-1 + a2 z-2\nD(z) =\n\n(6-1)\n1\n\n+ blz\n\n+ b 2 z-2\n\nThe eleven programming forms presented here will be for the second-order\nmodule of equation (6-1).\nprocedure applies:\n\nFor a higher-order digital filter, the following\n\n1) Section D(z) into second-order modules, 2)analyze\n\neach module using the computer-aided design procedure to be developed\nlater, and 3) cascade (or parallel) the resulting designs to realize the\noriginal D(z).\nThis section will summarize, for equation (6-1), eleven different programming forms and the attributes of each needed for quantization analysis\n\n2-137\n\n2-138\n\nby steady-state, statistical, and upper-bound techniques.\n\nIn particular,\n\nthe transfer function Tj(z) from the jth quantizer to the filter output\nfor equation (4-9) and (4-18), and the discrete-time difference equations\nnecessary for the impulse response from the jth quantizer to the filter\noutput for equations (4-11), (4-19), and (4-32), will be listed for each\nprogramming form.\n\nThe tabulation of the eleven programming forms is\n\na result of [38, 63-66].\n\nMany others are possible as seen in [67-70,76].\n\nDirect Form\nThe direct programming form for equation (6-1) is shown in Fig. 26.\nThis form has an A/D or input quantizer, Q 1 , digital-to-analog (D/A) or\noutput quantizer Q2 and one internal feedback quantizer Q 3 .\n\nThe transfer\n\nfunctions from each quantizer to the output are\n\nT l (z) = D(z)\n\nT2 (z) = 1\n\n(6-2)\nblZ-l + b2z-2\n\nT3 (z)\n\n-\n\n1\nz\n1 + blz-l + b2-\n\n2\n\nThe integrands for equation (4-18) are thus\n\nT (z) T (l/z)\nZ\n\nT 2 (Z) T2 (l/z)\nz\nz\n\n(a0z2 + alz\n\n)(a2)/b + a2\n\nz(z2 + blZ + b 2 )(z2\n\n2\n\n+ blz/b2 + 1/b 2 )\n\n(6-3)\nz(6-3)\nz\n\n2-139\n\nei(k\n\neo (k)q2\n\nFig 26.\n\nThe Direct Form.\n\n2-140\n(blz + b 2 )(bl\nZ\n\nT3 (z) T3 (1/z)\nz\n\nz(z2 + bl\n\nz\n\n+ b 2 )(z2\n\n+ b 2 z )/b 2\n+ blz/b 2 + 1/b 2 )\n\nFor programming and impulse testing the difference equations for the\ndirect form are\n\nei(k)q\n\nei(k) + nl(k)\n\neo(k) = a0 ei(k)q+ alei(k - l)ql+ a 2 ei(k- 2)ql\n\n(6-4)\n\n-bl eo(k - l)q 3 - b 2 eo(k - 1)q3\n\neo(k)q2 = eo(k) + n 2 (k)\n\neo(k)q3 = eo(k) + n 3 (k).\n\n.\nThe filter output variable is eo(k)q 2\n\nThis completes the description of\n\nthe direct programming form.\nFor all eleven programming forms the standard notation of Q1 for\nthe filter input quantizer and Q2 for the filter output quantizer has been\nadopted for convenience. The transfer functions Tl(z) and T 2 (z) are then\nalways to be for the input and output quantizers respectively.\n\nThese\n\ntransfer functions will be identical for all the programming forms as\ngiven in equation (6-2).\n\nModified Direct Form\nThe modified direct programming form for equation (6-1) is shown in\nFig. 27. This form differs from the direct form only in the feedback\n\n2-141\n\nei(k)\n\neo(k)q2\n\nFig. 27.\n\nThe Modified Direct Form.\n\n2-142\n\nloop.\n\nThis form has two internal quantizers; Q3 is identical to the\n\ndirect form hence T3 (z) is given by equation (6-2); Q4 has been added and\nits transfer function to the filter output is displayed below:\n\nz-1\n)\n\nT4(z\n\n=\n\n1 + blz-1\n\n(6-5)\n\n+ b 2 z-1\n\nThe integrand for equation (4-18) for Q4 becomes\n\nz2/b2\n\nT4 (z) T4 (1/z)\n\n2\nblz/b2 + l/b2 )\n= 2 + b1 z + b2)(Z z + 2\nz(z\n\n(6-6)\n\nFor programming and impulse testing the difference equations for\nthe modified direct programming form are\n\nei(k)q = ei(k) + n1 (k)\n\neo(k)\n\naei(k\n\n+ alek\n\n-\n\n)q + a2ei(k - 2)q\n\n+ m(k - l)q\n\neo(k)q2 = eo(k) + n2 (k)\neo(k)q3 = eo(k) + n 3 (k)\n\nm(k) = -bleo(k)q3 - b2eo(k - 1)q3\n\nm(k)q = m(k) + n 4 (k).\n\n(6-7)\n\n2-143\nStandard Form\nThe standard programming form for equation (6-1) is presented in Fig.\n28,\n\nThis form has two internal quantizers, Q3 and Q4 .\n\nTheir transfer\n\nfunctions to the filter output are\n\nT3 (z) =\n\n1\nz+\n\nbz + b2\n\n(6-8)\nT4 (z) =\n2\n\nz + b\n+ blZ + b 2\n\nThe integrands for equation (4-18) are\n\nz2/b2\n\nT3 (z) T3 (1/z)\n\nz(z 2 + b1 z + b2 )(z2 + blz/b 2 + l/b2 )\n\n(6-9)\n2\n(z + bl)(Z + blZ )/b\n2\n\nT4 (z) T4 (1/z)\nz\nz(\n\n2\n\n+ b1 z + b2\n\n2\n\n+ blZ/b 2 + 1/b 2\n\n)\n\nThe difference equations for this form are\n\nei(k)q = ei(k) + nl(k)\n\neo(k) = aoei(k)q + m 2 (k - l)q\n\neo(k)q = eo(k) + n2 (k)\n\nm l (k) = a2ei(k)q - blml(k - l)q - b2m2(k - 1)q\n\n(6-10)\n\n2-144\n\nFig. 28.\n\nThe Standard Form.\n\nD\n\n2-145\n\nm l (k)q = m\n\nk) + n3 (k)\n\n(\nl\n\nm 2 (k) = alei(k)q + m l ( k - l)q\n\nm 2 (k)q = m 2 (k) + n 4 (k)\n\nwhere\n\na1 = al - a0 b\n\na2 = a2 - a0 b 2 - blal.\n\nModified Standard Form\nAgain the modified standard form is for D(z) as expressed in equation\n(6-7) and is demonstrated in Fig. 29.\n\nThis programming form differs from\n\nthe standard form in its feedback loops.\n\nThe same internal quantizers\n\nare present as before with a fifth quantizer added.\n\nThe transfer func-\n\ntions for the three quantizers are\n\nT 3 (z) = T 3 (z) in (6-2)\n\nT4 (z) = T 3 (z) in (6-8)\n\nT5 (z) = T4 (z) in (6-5)\n\nHence, the integrands for equation (4-18) have been previously shown.\n\n2-146\n\neo(k)q2\n\nFig. 29.\n\nThe Modified Standard Form.\n\n2-147\n\nFor programming, etc., the difference equations for the modified\nstandard form are\n\nei(k)q = ei(k) + nl(k)\n\neo(k) = aoei(k)q + m 2 (k - l)q\n\neo (k)q2\n\n=\n\neo(k) + n 2 (k)\n\neo(k)q3 = eo(k) + n3 (k)\n(6-11)\nm l (k) = a2 ei(k)q - b 2 eo(k)q\nml(k)q = ml(k) + n4 (k)\n\nm 2 (k) = alei(k)q + ml(k - l)q - bleo(k)q\n\nm 2 (k)q = m2 (k) + n5 (k).\n\nCanonical Form\nThe block diagram for the canonical programming form limited to\nthe second-order module of equation (6-1) is shown in Fig. 30.\n\nThis\n\nform has only one quantizer Q3 whose transfer function to the filter\noutput is given by\n\nT3 (z) = D(z).\n\n2-148\n\n.eo(k)q\n\ne (k)q\n\nei(k)\n\nFig. 30.\n\nThe Canonical Form.\n\n2-149\n\nTherefore, Q3 has the same effect as the input quantizer on the filter\nThe difference equations including quantization are shown\n\noutput.\nbelow:\n\nei(k)q = ei(k) + nl(k)\n\nm(k)\n\n=\n\nei(k)q - blm(k - 1)q - b2m(k\n\n2)q\n\nm(k)q = m(k) + n3 (k)\n(6-12)\neo(k) = aom(k)q + alm(k - l)q + a2 m(k - 2)q\n\neo(k)q = eo(k) + n 2 (k).\n\nModified Canonical Form\nThe modified canonical programming form for the second-order D(z)\nof equation (6-1) is depicted in the block diagram of Fig. 31.\n\nThis pro-\n\ngramming form differs from the canonical form by its forward transfer\nBy moving the multiplier coefficient from m(k)q to ei(k)q\n\npaths.\n\nthe\n\ntransfer function for Q3 is changed:\n\nT3\nT3(z)\n\n=\n\nalz + a2\n2\nz + blz + b2\n\nwhere\n\n1\n\n=\n\nal - a b\n\na 2 = a2 - a0 b 2 \'\n\n(6-13)\n\n2-150\n\ne o (k)q\n\nei(k)\n\nFig. 31.\n\nThe Modified Canonical Form.\n\n2-151\n\nThe integrand for equation (4-18) for this transfer function is\n\n(31 z + a2 )(alz + a2 z\n\nT3 (z) T3 (1 /z)\n\n~z\n\n=\nz(z\n\n)\n\n2\n\n2\n+ blz + b 2\n\n) (2\n\n(6-14)\n+ blz/b 2 + 1/b2 )\n\nThe difference equations for the modified canonical programming form are\nshown below:\n\nei (k)q = ei (k) + n l (k)\n\neo(k) = aoei(k)q + alm(k - l)q + a 2 m(k - 2)q\n\neo(k)q\n\n(6-15)\n\neo(k) + n2 (k)\n\nm(k) = ei(k)q - blm(k - l)q - b 2 m(k -\n\n2\n\n)q\n\nm(k)q = m(k) + n3 (k).\n\nThe six programming forms discussed to this point have all required\nthe programming coefficients ai and b i of equation (6-1), or were easily\ncalculated from them.\n\nThe last five forms which are to be presented now\n\nwill require more effort to find the correct form for D(z) and the proper\nprogramming parameters for the difference equations.\n\n2-152\nParallel Form\nThe general block diagram for the parallel programming form for\na second-order D(z) is shown in Fig. 32.\n\nThe form may be used if\n\nand only if the second-order module has real poles P1 and P2 .\n\nHence,\n\nD(z) must have the form\n\nD(z) = ao + R1\n+ R2\nz-pl\nz-p 2\n\n(6-16)\n\nThe constants R1 and R2 are real numbers representing the residues of\npoles P1 and p2 , and P1 should be different from P 2.\nThe coefficients gi shown in Fig. 32 must satisfy the following\nrelationships:\ngl g2 =\n\n(6-17)\n\ng 3g 4 = R 2\nIn order to minimize the magnitude of the parameters gi the following\nchoices were arbitrarily made:\n=\n\ngl=\n\nr\n\n1\ng2 = R /gl\n\n(6-18)\ng3 =\n\nvR\n\ng4 = R2 /g3\nThe transfer functions from the internal quantizers to the filter\noutput were obtained:\n\nT3(z)\n3\n\ng2\n9-1\nz-p\n\n(6-19)\n\n2-153\n\neo (k)q\n\nFig. 32.\n\nThe Parallel Form.\n\n2-154\n\nT (z) =\n\ng4\nZ-P2\n\nThe integrands for equation (4-18) corresponding to (6-19) are\nT 3 (z)T3 (1/z) = -g2 z/P1\nz\n\nz(z-p1 ) (z-1/p1 )\n(6-20)\n\nT4 (z)T4 (1/z) = -g4 Z/P2\nz\n\nz(z-p 2 ) (z-1/P 2 )\n\nThe difference equations for the parallel form are\nei(k)q = ei(k) + nl(k)\n\neo(k) - aoei(k)q + g2ml(k-l)q + g4m2 (k-l)q\neo(k)q = eo(k) + n2 (k)\n\n(6-21)\nml(k) = glei(k)q + Plml(k-l)q\n\nml(k)q = ml(k) + n3 (k)\n\nm 2 (k) = g3 ei(k)q + P2m2(k-1)q\n\nm2 (k)q = m2 (k) + n4 (k)\n\n.\n\nPlease note that the parallel form can realize only real poles, but\nit is capable of realizing either real or complex zeroes.\n\nCascade Form\nThe cascade programming form for a second-order digital filter\nmodule essentially factors the module into first order stages and\nrealizes each stage individually.\n\nIf each first order stage is\n\nimplemented in the manner of Fig. 30; the resulting cascade form is\n\n2-155\nshown in Fig. 33.\n\nA requirement for this form is that\n\nD(z) = ao (z-ql) (z-q2 )\n(z-P 1 )\n\n(6-22)\n\n(z-P 2 )\n\nwhere qi and Pi are real zeroes and poles.\n\nAlso, the following\n\nrelationships must be satisfied:\n\na o = glg2 g 4\ng3\n\n-gl 2\n1g\n\ng5\n\n9gg 4\n2\n\n(6-23)\n\nThe cascade form has two internal quantizers which are described\nby the transfer functions\nT3(z) = D(z)/g1\n(6-24)\nT4 (z) = g4 z-q2\nz-P2\nThe integrands for equation (4-18) are\nT3 (z) T 3 (1/z) = 1\n\nD(z) D(l/z)\n\ng\n\nz\n\nz\n(6-25)\nT4 (z) T4 (1/z) = -g4(z-q2)(2-q2z)(p2\nz\n\nz(z-P2 )(z-l/P\n\n2\n\n)\n\nThe difference equations for this cascade form are displayed below:\nei(k)\n\n= ei(k) + nl(k)\n(6-26)\n\nm l (k) = glei(k)q q + Plml(k-l)q\n1i\n+ pim1 ( lq\nml(k)q = m (k) + n 3 (k)\n\n2-156\n\nei (k)q\n\nei (k)\n\neo (k)q\n\nFig. 33.\n\nThe Cascade Form.\n\n2-157\nm 2 (k) = g2 ml(k)q + g3 ml(k-l)q + P2m2 (k-1)q\n\nm 2 (k)q = m2 (k) + n 4 (k)\n\neo(k) = g4 m 2 (k)q + g5 m2 k-1)q\neo(k)q = eo(k) + n 2 (k)\n\nThe parameters gi, i=l, 5 in equation (6-26) must be found using (6-23).\nSince there are three equations with five unknowns, an arbitrary choice\nfor gl and g2 is made as follows:\n\ngl = 1.0\n(6-27)\n\ng2 =\n\nVTao-\n\nIf ao is zero, this form cannot be realized.\nThis completes the cascade form.\n\nIn summary, this programming form\n\nis applicable to a second-order digital filter module when it is\npossible to cascade first-order stages programmed in the canonical form.\nModified Cascade Form\nOf the many possible ways of implementing first-order stages,\none other technique was selected which employs the modified canonical\nform for each first-order section (see Fig. 34).\n\nThis programming\n\nform is labeled modified cascade; it requires D(z) to be factorable\ninto real poles and zeroes as in equation (6-22).\nThe transfer functions from the three internal quantizers to the\nfilter output are given below:\nT 3 (z) = g 3 g4 (z-q2 )\n(z-p 1 ) (z-P 2 )\n\n(6-28)\n\n2-158\n\nei (kq\n\nei(k)\nm2 (k)q\n\neo(k) q\n\nFig. 34.\n\nThe Modified Cascade Form.\n\nK\n\n2-159\nT4 (z) = g4\n\nZ -q2\nz -P2\n\nT5 (Z) = g6\nz-P2\nwhere the parameters gi are restricted by\ng1 g4 = ao\ng2 g3\n\n=\n\n(P1 -ql)g1\n\ng5 g6 =\n\n(2-q2)g4\n\n(6-29)\n\nSince there are three equations and six unknowns, arbitrary choices\nare again made for gl, g2 , and g4 as follows:\n\ngl\n\n=\n\n1ao.\n\ng2= (P1 +l)/2\ng3\n\n=\n\n(P1 -ql)gl/g2\n\n(6-30)\n\ng4 =a/gl\n\ng5 = (P2+ 1 ) / 2\ng6 = (P2 -q2 )g4 /g5\nUsing these parameters, the following difference equations may be used\nto implement this programming form:\n\nei (k)q = ei (k) + n l ( k)\nm 2 (k) = glei(k)q + g 3 ml(k-l)q\nm2 (k)q = m 2 (k) + n4 (k)\neo (k) = g4 m2 (k) + g6 m 3 (k-l)q\ne,(k)q = eo(k) + n 2 (k)\nm l (k) = g2 ei(k)q + Plml(k-l)q\nml(k)q = ml(k) + n3 (k)\n\n(6-31)\n\n2-160\nm 3 (k) = g5m\n\n2\n\n(k)q + p2m3(k-1)\n\nm 3 (k)q = m 3 (k) + n 5 (k) .\n\nX1\n\nStructure\n\nThe last two programming forms to be presented are designed\nfor a second-order D(z) with complex poles.\n\nThe appropriate\n\nexpression for the transfer function is\nD(z) = a\n\n+ A\n\n+ A*\n\n6-32)\n\nz+p,\n\nz+p\n\nwhere a o has been previously defined, p and p\n\nare complex conjugate\n\npoles, and A and A* are complex conjugate residues.\nThe first implementation of (6-32) is depicted in the block dialgram\nof Fig. 35.\n\nThe parameters indicated in the figure are defined beloiW:\n\ngl = -Re (p)\ng2 = Im (p)\n(6-33)\ng3\n\n=\n\n2 Im (A)\n\ng4 = 2 Re (A)\n\nThe two internal quantizers, Q3 and Q4, are described by the\ntransfer functions\n\nT3(z) =\n\ng2\nzz+blz+b\n2\n\nT4 (z) =\n\nz-g1\nz2+blz+b\n2\n\n(6-34)\n\nThe corresponding integrands for equation (4-18) are\n\nT3 (z) T3 (1/z) = g2 z2/b2\nz\n\nz(z2 +blz+b2 ) (z2 +blz/b2 +l/b2 )\n\n(6-35)\n\n2-161\n\nei (k)q\n\neo(k)q\n\nei (k)\n\nFig. 35.\n\nThe X1 Structure.\n\n2-162\nT.(z) T 4 (1/z) = (z-g) (-glz2 )/b2\nz(z+blz+b ) (Z\'+blz/b +l/b2 )\n2\n2\n\nz\n\nThe difference equations for the XI structure are enumerated below:\nei(k)q = ei(k) + nl(k)\neo(k) = aoei(k)q + m 2 (k-l)q\n\n(6-36)\n\neo(k)q = eo(k) + n (k)\n2\n\nm l (k) = g3 ei(k)q + glml(k-l)q - g2 m 2 (k-l)q\nml(k)q = ml(k) + n 3 (k)\n\nm 2 (k) = g4 ei(k)q + glm2 (k-l)q + g2 ml(k-1)q\n\xc2\xb7\nm2(k)q = m 2 (k) + n4 (k)\nX2\n\nStructure\n\nThe last programming form presented in this paper is the X2\nstructure of Fig. 36.\n\nThe transfer function D(z) must be expressed\n\nin the format of equation (6-32) in order to use this form.\nThis programming form has two internal quantizers whose transfer\nfunctions to the filter output are\n\n)\n\nT3(z) = g3 z + (g2 g4 - glg3\nz 2 + blZ + b 2\n\n(6-37)\n\nT4 (z) = g4 z - (g2g3 + glg4)\nz2 + blZ + b 2\nwhere\ngl =-Re (p)\ng2\n\nI\n=\n\nm\n\n(p)\n\ng3 =-Im (A)\ng4\n\n=\n\nRe (A)\n\n(6-38)\n\n2-163\n\nei (k)q\n\nei(k)\n\nFig. 36.\n\nThe X2\n\nStructure\n\n2-164\n\nThe integrands for equation (4-18) are\n\nT 3 (z)T3 (1/z) = (g3 z+6 1 ) (g3 z+6 1 z2 )/b 2\nz\n\nz(z 2 +blz+b 2 ) (z2 +blz/b2 + 1/b 2 )\n\n(6-39)\n\nT4 (z)T4 (1/z) = (g4 z+62 ) (g4 z+6 2 z2 )/b2\nz\n\nz (z2 +blz+b2 ) (z2 +blz/b2 +l/b 2 )\n\nwhere\n61= g2 g4 - glg3\n62 =-g2 g 3 - g1 g4\nThe difference equations for the X2\n\nstructure are listed below:\n\nei (k)q = ei (k) + nl(k)\neo(k) = aoei(k)q + g3 ml(k-l)q + g4 m2(k-1)q\n\n(6-40)\n\neo(k)q = eo(k) + n2 (k)\nml(k) = glml(k-l)q - g2 m 2 (k-l)q\nml(k)q = ml(k) + n3 (k)\nm2 (k) = 2 ei(k)q + glm2 (k-l)q + g2 ml(k-l)q\nm2 (k)q = m 2 (k) + n4 (k) .\nThis completes the X2\n\nstructure.\n\nSummary of Programming Forms\nThis section has summarized the essential characteristics of\neleven programming forms for a second-order digital filter module.\nAll of the equations necessary to perform steady-state, statistical,\nand error bound analyses have been determined.\n\nA pattern may be\n\nobserved in the formats of the relations for equation (4-18), the\n\n2-165\nresidue evaluation equation for statistical analysis.\n\nAll of the\n\nintegrands fall into the following formats:\n\nF1(Z) =\n\nY3(YoZ2+Y1z+Y2)\n\n(Yo+YlZ+Y2z 2\n\n)\n\nz(z 2 +b Z+b 2 ) (z2 +blz/b2 +l/b2 )\n(6-41)\nor\nF2 (z) = Y2 (z-y 1 ) (1-Ylz)\n\nz(z-YO)\n\n(z-l/yo)\n\nTable 2 displays the respective equations for each programming form\nusing equation (6-41).\nMany other characteristics of each programming form should also be\ninvestigated; for example, the coefficient sensitivity and the deadband\neffects are also important for good digital filter operation.\n\n2-166\n\nTable 2.\n\nIntegrands for (4-18).\n\nY\n\nParameters\nY2\n\nF1\n\nao\n\nal\n\na2\n\n1/b2\n\nQ3\n\nF1\n\n0\n\nbl\n\nb2\n\n1/b 2\n\nQ3\n\nF1\n\n0\n\nb\n\nb2\n\n1/b 2\n\nQ4\n\nF1\n\n0\n\n0\n\n1\n\n1/b 2\n\nQ3\n\nF1\n\n0\n\n0\n\n1\n\n1/b2\n\nQ4\n\nFF\n\n0\n\n1\n\nb1\n\n1/b 2\n\nQ3\n\nF1\n\n0\n\nb1\n\nb2\n\n1/b2\n\nQ4\n\nF1\n\n0\n\n0\n\n1\n\n1/b2\n\nQ5\n\nF1\n\n0\n\n0\n\n1\n\n1/b2\n\nCanonical\n\nQ3\n\nF1\n\nao\n\nal\n\na2\n\n1/b 2\n\nModified\nCanonical\n\nQ3\n\nF1\n\n0\n\nal\n\na2\n\n1/b 2\n\nParallel\n\nQ3\n\nF2\n\nP1\n\n0\n\nF2\nF2\n\nP2\n\nF1\n\nao\n\nF2\n\nP2\n\nProgramming\nForm\nA11\n\nQuantizer\n\nQ1\n\nFormat\n\nY3\n3\n\nQ2\n\nDirect\n\nodified\nirect\n\nStandard\n\nodified\nStandard\n\nQ\nQ4\n\n3\n\nCascade\n4\n\n2\ng2/P\n\n\xc2\xb0\n0\n\n2\n2/P,\n-g4/PF\n\na1\n\n0\n\na2\n\nI\n\n2\n\n1/g2b\n42F\n\n2 2\nModified\n\nQ3\nCascade\nQ\n4\nQ5\n\nF\n\n0\n\nQ4\nF2\nF2\nF2\n\n2\n2\nP2\nP2\n\n1\n\n2\n-g4/P2\n42\n-2p\n6g6/P2\n\n3\n\ng/pp2\n\n2-167\n\nTable 2.\n\nIntegrands for (4-18).\n\n(Cont\'d)\n\nVII.\n\nCOMPUTER AIDED DESIGN\n\nIn the design of complex system, the digital computer serves as\nan essential tool in synthesis and design verification.\n\nComputer\n\naided design (CAD) programs are effectively employed in the synthesis\nof digital filters in three ways:\n\ntransfer function synthesis,\n\ncoefficient quantization, and programming form selection.\n\nTransfer Function Synthesis\nThe digital computer has been used extensively in the design of\ndigital filter transfer functions [30,71-74].\n\nNonrecursive designs\n\nusing linear programming has been implemented by Rabiner [73] while\nParks and McClellan [72] using polynomial interpolation techniques.\nRabiner et al [30] also used a steepest descent technique to obtain\nFIR filters with minimax error in selected bands.\nRecursive digital filters have been synthesized using sampled\ndata transformations by Golden [71].\n\nRobinson and Robinson [74]\n\nhave demonstrated a CAD program for taking z-transforms.\n\nSteiglitz\n\n[75] has used nonlinear optimization techniques to obtain recursive\ndigital filter approximations to arbitrary frequency responses.\n\n2-168\n\n2-169\n\nCoefficient Quantization\nAvenhaus [48] has investigated the effects of coefficient optimization for reducing the coefficient wordlength.\ndesigned and its coefficients are founded.\n\nA given filter is\n\nThen an optimizing search is\n\nundertaken to find other sets of coefficients which meet the design\ncriteria with a shorter wordlength.\nMuch work is left to be done in the proper quantization of digital\nfilter coefficients and CAD will surely play a major role in future\ndevelopments in this area.\n\nProgramming Form Selection\nA CAD program, listed in [49], has been developed which analyzes\nthe signal amplitude quantization errors in the eleven programming\nforms presented in Chapter 6.\n\nThe program, written in FORTRAN IV,\n\nis an aid to implementing digital filters for any application, the only\nrestriction is that the filters be expressable as second order stages\nas shown in equation (6-1).\n\nGeneral\nThe filter implementation program actually consists of eleven\nparts, one for each programming form discussed in the previous section.\nEach programming form is analyzed using steady-state, statistical,\n\n2-170\n\nand upper bound techniques.\n\nThe system weighting constants Kssj, Kstj,\n\nand Kubj are calculated using the equations of Table 1.\n\nKssj and Kstj\n\nwere computed by both equations for debugging purposes; Kub j was\ndetermined using the second equation.\n\nA weighted average of these\n\nconstants was also used:\n\nKwaj = XlKss j + x2 Kstj + A3Kubj ,\n\n(7-1)\n\nwhere\n\nx\n1\n\n+\n\n2\n\n= 13\n1.\n\n+\n\nTherefore, a weighted average error can be calculated by\n\nS\n[eO]wa\n\nI\nKwaj j\n\nj=l\n\nThe weighting constants \\i may be adjusted by the designer to emphasize\nthe steady state, statistical, or upper bound errors.\nThe filter implementation program may be used in two modes, one\nfor stored-program computers and one for special-purpose hardware; the\ntwo modes are distinguished by the manner in which the quantizer step\nlengths are chosen.\n\nIn both the assumption is made that truncation\n\n2-171\n(or LSB-1) quantizers are used in the system.\n\nAll errors must be\n\nhalved by the user if roundoff quantizers are present.\nIn the stored program mode the maximum error hj of the jth\nquantizer is fixed by first simulating the ideal digital filter\nresponse to a "worst case" step input, which is an A/D input word\nof all "ones."\n\nDuring the transient response to this step, the\n\nmaximum value of the filter output and internal variables is recorded.\nAfter the simulation has run a sufficient number of iterations for convergence, sav 100, the maximum values are rounded up to the nearest\npower of two.\n\nSince the computer wordlength is a fixed number Lr,\n\nthe quantizer intervals are found by\n\nhj\n\nVarmax\n\nrounded-up\n\n(73)\n\n2Lr\nh1 is always assumed equal one.\nIn the special-purpose computer mode the register lengths are\nnot fixed; therefore, a different method is used to find hj.\n\nThe\n\nphilosophy of this mode is to balance the effect of each quantizer\nin the system so that they all have relatively equal error contributions.\n\nThis balancing is done by dividing equation (7-2) by Kwal\n\n(with hi - 1):\nEeo]wa\n=-l+ r\nJ=2\n\nKwaj\n\nhj\n\nEach term in the summation is forced to be less than or equal one\n(to insure that the A/D will introduce an error as large or larger\n\n(.77-4)\n\n2-172\nthan the other quantizers) by choosing\n\nhj\n\n< Kwal\n\n(7-5)\n\n*\n\nKwaj\nA further restriction is that hj be a power of two; hence the ratio\nKwal/Kwaj is rounded down to the nearest power of two to find the\nactual hj to be used:\n\nhj = [Kwal/Kwa ] rounded downj\n\n(7-6)\n\nFlow Charts\nFig. 37 demonstrates the flow of information in the main section\nof the filter implementation program.\n\nThe input data to be given to\n\nthe program is summarized below:\n1)\n\nTransfer function coefficients in (6-1)\nao, al, az, bl, b 2\n\n2)\n\nRegister lengths\nA/D, D/A, and wordlength of the stored-program computer\nor coefficient wordlength for the special-purpose computer.\n\n3)\n\nWeighting coefficients in (7-1)\nX1 , X2, X3\n\nThis is all the information needed to completely analyze the quantization errors for all the programming forms.\nThe first major task in the program is to find the poles and\nzeroes of the transfer function D(z) and to set three flags which omit\nthose programming forms which are unrealizable.\n\nThe main program\n\nthen calls a subprogram for each realizable programming form.\n\nEach\n\ncalled subprogram completely analyzes the quantization errors\ncharacteristic to that particular form and prints their detailed\ndescription.\n\nAt the end of the program, final summaries of each\n\nprogram mode are listed for easy cross-reference.\n\np\n\n2-173\n\nTt\n\nSET FLAGS TO SKIP\nUNREALIZEABLE FORMS\n\nCALCULATE XI AND\nXII STRUCTURES\n\nI o\nYE5\n\nFig. 37.\n\nFlow Chart of Main Program.\n\n2-174\nA general flow chart describing a subroutine for any given\nprogramming form is shown in Fig. 38.\n\nThe first task is to calculate\n\nall of the parameters needed for the difference equations of the\nspecified programming form; next, these parameters are quantized.\nThe simulation difference equations are then calculated once for\nthe step response and once for each quantizer in the system.\n\nDuring\n\nthese simulations the system constants Kssj, Kstj, and Kubj are\ncalculated. Finally the steady-state, statistical, and upper bound\nerrors are calculated, as well as the weighted average error of\nequation (7-2), for both modes of program operation.\n\nSource Listing\nThe filter implementation program consists of approximately 1800\nsource statements and is available in [49].\n\nAlso, a limited\n\nnumber of printed listings are available from Auburn University.\n\nSummary\nThe filter implementation program has been developed using an\nIBM 360/50 using FORTRAN IV and OS360.\n\nIn its final form the program\n\ntakes approximately 3.5 minutes to compile and 25 seconds to load and\nexecute.\n\nThe execution time may be trimmed by limiting the simulation\n\niterations to a smaller number, say 10 to 20.\nNow the CAD program will be used to analyze two digital filters,\none for each program operating mode.\n\n2-175\nENTER\n\nCALCULATE AND QUANTIZE\nDIFFERENCE EQUATION PARAMETERS\n\nINITIALIZE VARIABLES, SET STEP\nINPUT, SET IJK=O\n\n\'\nCALCULATE SIMULATION EQUATIONS\nFOR 100 ITERATIONS TO OBTAIN\nMAXIMUM VALUES AND SUMS OF\neo ,\n\nCALCULATE hj FOR\n\nI\n\nleol, and eo2\n\nYES\n\nSTORED PROGRAM MODE\nNO~~N\n140~~~\nNO\n\nSET QUANTIZER(IJK)\n\nIJK=4\n\nVES\n\nPRINT Kss, Kst, Kub\nFOR IMPULSE TEST\n\nf\'\n\nPRINT ERRORS FOR STORE\nPROGRAM MODE\n\nCALCULATE hj AND PRINT ERRORS\nFOR SPECIAL-PURPOSE\n\nFig. 38.\n\nCOMPUTER MODE\n\nFlow Chart for Programming Form.\n\n2-176\n\nStored Program Mode\nConsider the second order digital filter\n\nD(z) =\n\nz2 + .75z + 0.125\n2\n\nz\n\n(7-7)\n\n+ .50z + .0525\n\nSuppose that this filter is to be realized using a 16-bit minicomputer\nusing a 11-bit A/D and 13-bit D/A as input-output equipment.\n\nThe com-\n\nputer-aided design (CAD) program may be used in the stored-program mode\nof operation to aid the designer in programming the minicomputer. Table\n3 is the final summary of quantization errors attributed to the filter\nabove for its realizable programming forms.\n\nThe D(z) in (7-7) has real\n\npoles and zeroes; therefore, the X1 and X2 structures may not be used.\nUsing the weighted average errors in Table 3, the CAD program recommends that the filter in (7-7) be programmed by first the modified canonical form; and second, the parallel form.\n\nNote that all the programming\n\nforms give relatively good results; this is due to the fact that the\ninternal quantizers and output quantizers contribute only a minor part\nof the total quantizing error.\n\nThe A/D and D/A wordlengths chosen in\n\nthis example are responsible for these results.\n\n2-177\n\nV4 oo\n\nN\n\nL-n\n\n\'4\n\n,U\n\nN\n\ne<4\n\nes\nco\n\n00\n\n\'\n\n1-4\n\nr\n\nI\n\nO\n\nC\n\n,I\n\nI\nV]\n\nO\n\nO\n\n<\n\nN\n\n0o\n\nN\n\nc\n\n-,0\n\n00\n\no\n\n0.\n\nH\n\nOD\n\no\n0G\n\n0.\n\n0\n\n00\nOA\n\nN\n\nO\n\nO\n\nU . r.\n\n0\n\n01\n\nco\n0%\n\no\n\n0% . 0\n\nax\n\no\n\n00\n\n0%\n\ns\n\noo\no\nQ4\n\nc%\n\nU0\ni-4\n\nC, o\n\nI0\n\nzk0 XW\nW\n\n>)\n0a\n0V)\n\nr-.\n\n0 u10\nw\n\n.\n\n0\n\n\'\n\n00\nU\n\n-.\n\n0\ng\n\n,-.\n$\n\nH\n\nco\nr00\n\n-\n\nw-.\n\nr~.\n\n.\n\ni%\n\n-0\n,-4\n\nI\n\n.0\n\nco\n\n,\n\n.-\n\nN\n\nr-.\n\nU\nU\n0\n\nCd~i\n\n.\n\n.\n\nU\n\n..\n\n-4\n\n0I\n\nC_\n\nHa\n0\n\n4\n\nU\n\n2-178\n\nSpecial-Purpose Computer Mode\nSuppose that a special-purpose computer is to be constructed to implement the following z-douain transfer function:\n\n2\n\nD(z)\n\n(7-8)\n\n1.862z + .895\nz2\n\n_ .2500\n\nAgain, if an 11-bit A/D is to be used, the CAD program gives the results\nshown in Table 4.\n\nFrom the table, the weighted average error suggests\n\nthat the direct form is best; the modified canonical form, second.\n\nDirect form.\n\nThe program prints out an analysis of each programming\n\nform which may be used for (7-8).\n\nSee Table 5.\n\nThe system error weight-\n\ning constants (KBS, Kst, and Kub) are summarized as well as the X\'s of\n(7-1), the maximum quantizing error h\' of each quantizer, and the form\nfactor.\n\nThe form factor is interpreted as follows\n\nFORM = I,J,\n\nwhere\n\nI = total number of bits for the register\n\nJ = number of bits to the right of the binary point.\n\nA negative J indicates the least significant bit has a value greater\nthan one.\n\nFrom Table 5, h2\n\n=\n\n2hI and h; - 4hi .\n\nThe CAD program always\n\n2-179\n\no\n\nc\n\nw\n\n\'\n\nO\n0\nCn\n\n0\n\n\\O\n?\'\n\n4.\n\nw.\n\ntn\n\n~~\n\nO\n0\n\n,0\n\n~\n\nsr\n\n0\n\nCn\n\n0%\n\n~\n\n0\n\nr\n\nrI\n\n(4\n\no4\n\nG\n-\n\n0\n\n,-\n\n0%\n\n0\n\nO\n\n@n\n\nc\n0T\n\nO\n\nH\n\n-I\n\no\n\nu\no\n.\n\nI\n\no\'4\nz\n\nr\n\no\n\nrq\nt-\n\nco\n\nCDVt\n\n0\n\n-eU\n9.\n0\n`..\n\nI\n\n*n\n\n.\n\nU)\n\n51\n\n,\n\nW\n61\n\nE,\n\n,.4\n\n,0\no\n\nt\n\nG\n\nu\'\n\n0\n\no\n\no\n\n0\n\n-H\n\n1\n--\n\n0\n\ne4 i\n\nU\n\nco\n\nS\n\n4\n\n0\n\no9\n4\n\n0\nSt\n\n0\n\nW o\n\nC\nX\na\n\nO\n\n0\n\nA l\n\nU\n0\n\nH\n\nCO\n\na\n\nX\n\ncn\n\nE\n\n2-180\n\nTABLE 5:\n\nThe Direct Printout\n\nSTEADY-STATE ANALYSIS\nKSS(1) = 0.044\nKSS(2) - 1.000\nKSS(3) - 0.333\n\nSTATISTICAL ANALYSIS\nKST(1) = 1.426\nKST(2) = 0.577\nKST(3) = 0.149\nERROR BOUND\nKUB(1)\nKUB(2)\nKUB(3)\n\nANALYSIS\n= 5.009\n= 1.000\n= 0.333\n\nSPECIAL-PURPOSE COMPUTER MODE\nLAMBDA(1) = 0.333\nLAMBDA(2) = 0.333\nLAMBDA(3) = 0.333\n\nH(1) - 1.0\nH(2) = 2.0\nH(3) - 4.0\n\nSTEADY-STATE ERROR = 3.377\nPERCENT Q1 = 1.3\nPERCENT Q2 = 59.2\nPERCENT Q3 = 39.5\nRMS ERROR = 3.177\n\nPERCENT Q1 = 44.9\nPERCENT Q2 = 36.3\nPERCENT Q3 - 18.8\nMAXIMUM ERROR BOUND - 8.343\nPERCENT Q1 = 60.0\n\nPERCENT Q2 = 24.0\nPERCENT Q3\n\n=\n\n16.0\n\nWEIGHTED AVERAGE ERROR = 4.966\nPERCENT Q1 = 43.5\nPERCENT Q2 = 34.6\nPERCENT Q3 = 21.9\n\nFORM - 11,0\nFORM = 10,-1\nFORM = 9,-2\n\n2-181\nassumes h\' - 1.\n\nAlso, the form factor of Q 2, the output quantizer, sug-\n\ngests that a 10-bit D/A may be used.\nModified canonical form.\n\nThe CAD program output for the modified\n\ncanonical form is shown in Table 6.\n\nNote that h\n\n-\n\n2hi and h3 = hi for\n\nthis programming form.\n\nClosed-Loop Comparison\nThe second-order digital filter in (7-8) has been analyzed in [53]\nfor a closed-loop sampled-data control system.\nthe control loop is shown in Fig. 39.\n\nThe block diagram for\n\nStatistical and upper bound tech-\n\nniques were employed to design the compensator of the control loop for\nboth the direct and modified canonical forms; system simulations were\nemployed to verify the results.\n\nTable 7 presents a comparison of the\n\nopen-loop results of this paper and the closed-loop results of [531].\nNote that they agree very closely.\nOne observation should be made at this point.\n\nThe register lengths\n\ndetermined by the open-loop design procedures of this paper are in general larger than those required in closed-loop applications. Stable\nfeedback systems generally tend to reduce the maximum values of the digital filter variables and thus the number of bits needed to represent\nthese variables in the special-purpose computer.\n\n2-182\n\nTABLE 6:\n\nThe Modified Canonical Printout\n\nSTEADY-STATE ANALYSIS\nKSS(1) = 0.044\nKSS(2) - 1.000\nKSS(3) = -0.956\n\nSTATISTICAL ANALYSIS\nKST(1) = 1.426\nKST(2) = 0.577\nKST(3) - 1.303\nERROR BOUND\nKUB (1)\nKUB(2)\nKUB(3)\n\nANALYSIS\n- 5.009\n= 1.000\n= 4.009\n\nSPECIAL-PURPOSE COMPUTER MODE\nLAMBDA(1) = 0.333\nH(1) - 1.0\nLAMBDA(2) = 0.333\nH(2) - 2.0\nLAMBDA(3) = 0.333\nH(3) - 1.0\nSTEADY-STATE\nPERCENT\nPERCENT\nPERCENT\n\nERROR - 3.000\nQ1 = 1.4\nQ2 = 66.7\nQ3 = 31.9\n\nRMS ERROR = 3.884\n\nPERCENT Q1 = 36.7\nPERCENT Q2 = 29.7\nPERCENT Q3 = 33.6\nMAXIMUM ERROR BOUND = 11.019\nPERCENT Q1 = 45.5\nPERCENT Q2 = 18.1\nPERCENT Q3 = 36.4\n\nWEIGHTED AVERAGE ERROR - 5.968\nPERCENT Q1 = 36.1\nPERCENT Q2 = 28.8\nPERCENT Q3 = 35.1\n\nFORM - 11,0\nFORM - 10,-1\nFORM - 12,0\n\n2-183\nN\n\n+\narC\n\nI\nCE4\n\nC\'C\nN\n\nN\nII\nN\n1-\n\n\xc2\xa2\n\nC\n\nr-\n\n-4\n\n0\n0\n2\n\nI\n\n0\n-4\n\n0\n\nh+N\n\no0\n\n+\nN\n\n+\n\nCo\n\n04\nN\nI\nN\n\nN\n\na,\nI\nC\n\nr-\n\nN\n\n1.\n\nII\n0.\n\nN\n\n2-184\n\nTABLE 7:\n\nOpen-Loop Versus Closed-Loop\n\nProgramnning\nForm\n\nOpen-Loop\nResults\n\nDirect\n\nh i - 2h;\n\nhi\n\nh=- 4h;\n\nh3 = 4h\n\nh; \' 2h\'\n\nh\' = h;\n\nModified\n\nCanonical\n\nh2\n\nh\n\nh 3 = hi\n\n2\n\nClosed-Loop\nResults [17]\n- hi\n\n=h\n\nh3 = .5h1\n\n2-185\n\nConclusion\n\nThis section has presented a computer-aided design technique useful\nin implementing digital filters expressed as z-domain transfer functions.\nTwo examples have been given to illustrate the stored-program and specialpurpose modes of operation of the CAD program.\n\nAlso, the program,.which\n\nanalyzes the filter\'s "open-loop" quantization errors, gives results closely\nmatching a "closed-loop" design.\n\nThis CAD program should be used as a\n\ntool for obtaining a "first guess" at the best way to program a digital\nfilter.\n\nIf a closed-loop simulation is available for the system in which\n\nthe digital filter will be used, then the CAD program design may be adjusted to give better loop performance.\nAlthough the program as presented has been designed for second-order\nmodules, it can be used as a subroutine in larger programs to match polezero pairs for higher order realizations, or to indicate the proper\ncascade ordering of second-order modules.\n\nThe CAD program may be a power-\n\nful tool to the digital filter (or controller) designer if its results\nare properly interpreted.\n\nVIII.\n\nAPPLICATIONS OF DIGITAL FILTERING\n\nDigital Filtering has found many diverse applications in recent\nyears.\n\nThis section lists several of them and points the interested\n\nreader to the open literature for detailed descriptions.\nThe following list presents typical applications for digital filters:\n1.\n\nSampled-Data Control Systems\na. General [38, 77]\nb. Pendulous Integrating Gyroscopic Accelerometer [78, 79].\nc. Saturn V Thrust Vector Control [80, 81]\n\n2.\n\nSpeech Processing\na.\nGeneral [82]\nb. Vocoder [83]\nc.\nEqualizers [84]\n\n3.\n\nRadar and Sonar Signal Processing\na. General [85, 86]\nb. MTI Filters [87, 88]\nc. Tracking Filters [89, 90]\n\n4.\n\nSpectral Analysis and Synthesis\na. Narrow Band Filters\nb. FFT [91]\nc.\nFrequency Synthesis [92]\n\n5.\n\nVibrations and Acoustic Testing [93]\n\n6.\n\nImage Processing\na. General [24, 94, 95]\nb.\nImage Enhancement [94, 95, 96]\nc. Pattern Recognition [97]\n\n7.\n\nSeismic Processing [7, 9]\n\n8.\n\nBiomedical Processing [94, 95, 97, 98]\n\n9.\n\nSynthesis of Speech and Music [99]\n\nMany other applications of digital filtering are also important\nwith the number of new ones ever increasing.\n2-186\n\n2-187\n\nREFERENCES\n\n[1]\n\nR. R. Read and C. S. Burrus, "Use of the Geometry of Partial Sums\nin Digital Filter Analysis," IEEETAU, Vol. AU-20, Aug. 72,\npp. 213-218.\n\n[2]\n\nT. G. Stockham, Jr., Chapter 7, Digital Processing of Signals\n(B. Gold and C. M. Rader), New York: McGraw-Hill, 1969.\n\n[3]\n\nL. R. Rabiner and R. W. Schafer, "Recursive and Non-Recursive\nRealization of Digital Filter Designed by Frequency Sampling\nTechniques," IEEETAU, Vol. AU-19, September 71, pp. 200-207.\n\n[4]\n\nT. S. Huang, "Digital Signal Processing - Applications to Speech\nand Image Processing," Course Notes, UCLA, July, 1972.\n\n[5]\n\n"General Principles of Digital Filtering and a Survey of Filters\nin Current Range Use," IRIG Document 122-71, Data Reduction and\nComputing Group, Range Commanders Council, U.S. Air Force, December,\n1971.\n\n[6]\n\nW. C. Kellogg, "Time Domain Design of Nonrecursive Least MeanSquare Digital Filters," IEEETAU, Vol. AU-20, June, 1972,\npp. 155-158.\n\n[7]\n\nE. A. Robinson and S. Treitel, "Principles of Digital Wiener\nFiltering," Geophysical Prospecting, Vol. 15, September, 1967,\npp. 311-333.\n\n[8]\n\nJ. D. Markel, "Digital Inverse Filtering - A New Tool for Formant\nTrajectory Estimation," IEEETAU, Vol. AU-20, June, 1972,\npp. 129-137.\n\n[9]\n\nK. L. Peacock and S. Treitel, "Predictive Deconvolution: Theory\nand Practice," Geophysics, Vol. 34, April, 1969, pp. 155-169.\n\n[10]\n\nH. B. Voelcker and E. E. Hartquist, "Digital Filtering via Block\nRecursion," IEEETAU, Vol. AU-18, June, 1970, pp. 169-176.\n\n[11]\n\nH. 0. Helms, "Fast Fourier Transform Method of Computing Difference\nEquations and Simulating Filters," IEEETAU, Vol. AU-15, June,\n1967, pp. 85-90.\n\n2-188\n\n[12]\n\nD. Chanoux, "Synthesis of Recursive Digital Filters Using the FFT,"\nIEEETAU, Vol. AU-18, June, 1970, pp. 211-212.\n\n[13]\n\nR. Reed and J. Meek, "Digital Filters with Poles Via the FFT,"\nIEEETAU, Vol. AU-19, December, 1971, pp. 322-323.\n\n[14]\n\nJ. P. Thiran, "Recursive Digital Filters with Maximally Flat\nGroup Delay," IEEE Trans. on Circ. Theory, Vol. CT-18, November,\n1971, pp. 659-664.\n\n[15]\n\nT. H. Crystal and L. Ehrman, "The Design and Application of Digital\nFilters with Complex Coefficients," IEEETAU, Vol. AU-16,\nSeptember, 1968, pp. 315-320.\n\n[16]\n\nE. P. F. Kan and J. K. Aggarwal, "Randomly Sampled Digital Filters,"\nIEEETAU, Vol. AU-20, March, 1972, pp. 52-57.\n\n[17]\n\nE. P. F. Kan and J. K. Aggarwal, "Multirate Digital Filtering,"\nIEEETAU, Vol. AU-20, August, 1972, pp. 223-224 (Correspondence).\n\n[18]\n\nT. S. Huang, "Stability of Two-Dimensaional Recursive Filters,"\nIEEETAU, Vol. AU-20, June, 1972, pp. 158-163.\n\n[19]\n\nT. S. Huang, "Two-Dimensional Windows," IEEETAU, AU-20, March,\n1972, pp. 88-89.\n\n[20]\n\nJ. G. Proakis, "Adaptive Digital Filters for Equalization of\nTelephone Channels," IEEETAU, Vol. AU-18, June, 1970, pp. 195200.\n\n[21]\n\nA. R. M. Noton, Introduction to Variational Methods in Control\nEngineering, New York, Pergamon Press, 1965.\n\n[22]\n\nG. Williamson, "Optimal Controllers for Homing Missiles," RE-TR68-15, U. S. Army Missile Command, Redstone Arsenal, AL,\nSeptember, 1968.\n\n[23]\n\nR. E. Kalman and R. S. Bucy, "New Results in Linear Filtering and\nPrediction Theory," J. Basic Eng., March, 1961, pp. 95-108.\n\n[24]\n\nA. B. Oppenheim, R. W. Schafer, and T. G. Stockham, Jr., "Nonlinear\nFiltering of Multiplied and Convolved Signals," IEEETAU, Vol. AU-16,\nSeptember, 1968, pp. 437-566.\n\n[25]\n\nJ. R. Heath and C. C. Carroll, "Special-Purpose Computer Organization\nfor Double-Precision Realization of Digital Filters," IEEETC,\nVol. C-19, December, 1970, pp. 1146-1152.\n\n2-189\n\n[26]\n\nC. C. Carroll and J. W. Jones, "A Special-Purpose Computer\nRealization of a Time-Shared Digital Filter," Technical\nReport Number 10, NAS8-20163, Engineering Experiment Station,\nAuburn, Alabama, August, 1968.\n\n[27]\n\nA. V. Oppenheim, "Realization of Digital Filters Using BlockFloating-Point Arithmetic," IEEETAU, Vol. AU-18, June, 1970,\npp. 130-136.\n\n[28]\n\nA. W. Crooke and J. W. Craig, "Digital Filters for Sample-Rate\nReduction," IEEETAU, Vol. AU-20, October, 1972, pp. 308-315.\n\n[29]\n\nA. A. G. Requicha and H. B. Voelcker, "Design of Nonrecursive\nFilters by Specification of Frequency-Domain Zeroes," IEEETAU,\nVol. AU-18, December, 1970, pp. 464-470.\n\n[30]\n\nL. R. Rabiner, B. Gold, and C. A. McGonegal, "An Approach to the\nApproximation Problem for Nonrecursive Digital Filters," IEEETAU,\nVol. AU-18, June, 1970, pp. 83-106.\n\n[31]\n\nB. Gold and K. L. Jordan, Jr., "A Direct Search Procedure for\nDesigning Finite Duration Impulse Response Filters," IEEETAU,\nVol. AU-17, March, 1969, pp. 33-36.\n\n[32]\n\nT. J. McCreary, "On Frequency Sampling Filters," IEEETAU,\nVol. AU-20, August, 1972, pp. 222-223.\n\n[33]\n\nH. D. Helms, "Nonrecursive Digital Filters: Design Methods for\nAchieving Specifications on Frequency Response," IEEETAU, Vol.\nAU-16, September, 1968, pp. 336-342.\n\n[34]\n\nH. D. Helms, "Digital Filters with Equiripple or Minimax Response,"\nIEEETAU, Vol. AU-19, March, 1971, pp. 87-93.\n\n[35]\n\nR. W. Hankins, "Design Procedure for Equiripple Nonrecursive Digital\nFilters," Technical Report 485, MIT Research Laboratory of Electronics,\nCambridge, MA, May 12, 1972.\n\n[36]\n\nR. K. Ontes, "An Elementary Design Procedure for ;Digital Filters,"\nIEEETAU, Vol. AU-16, September, 1968, pp. 330-335.\n\n[37]\n\nR. M. Golden, "Digital Filter Synthesis by Sampled-Data Transformation," IEEETAU, Vol. AU-16, September, 1968, pp. 321-329.\n\n[38]\n\nB. C. Kuo, Analysis and Synthesis of Sampled-Data Control Systems.\nEnglewood Cliffs, NJ, Prentice-Hall, Inc., 1963.\n\n2-190\n\nNew York:\n\nAcademic Press, 1969.\n\n[39]\n\nR. Fletcher, Optimization.\n\n[40]\n\nT. C. Hsia, "On Synthesis of Optimal Digital Filters," Proc.\nFirst Asilomar Conference on Circuits and Systems, November,\n1967.\n\n[41]\n\nD. B. Kimsey and H. T. Nagle, "Digital Filter Implementation\nby Minicomputer," Proc. IEEE Region 3 Convention, April 1012, 1972, pp. C3-1, C3-4.\n\n[42]\n\nL. R. Rabiner and K. Steiglitz, "The Design of Wide-band Recursive\nand Non-Recursive Digital Differentiators," IEEETAU, Vol. AU-18,\nJune, 1970, pp. 204-209.\n\n[43]\n\nD. W. Tufts and J. T. Francis, "Designing Digital Low-Pass\nFilters - Comparison of Some Methods and Criteria," IEEETAU,\nVol. AU-18, December, 1970, pp. 487-494.\n\n[44]\n\nS. C. D. Roy, "On Maximally Flat Sharp Cutoff Low-Pass Filters,"\nIEEETAU, Vol. AU-19, March, 1971, pp. 58-63.\n\n[45]\n\nM. C. Agarwal and A. S. Sedra, "On Designing Sharp Cutoff LowPass Filters," IEEETAU, Vol. AU-20, June, 1972, pp. 138-141.\n\n[46]\n\nA. V. Oppenheim, "Effects of Finite Register Length in Digital\nFiltering and the Fast Fourier Transform," Proceedings of the\nIEEE, August, 1972, pp. 957-976.\n\n[47]\n\nR. K. Ontes and L. P. McNamee, "Instability Thresholds in Digital\nFilters Due to Coefficient Rounding," IEEETAU, Vol. AU-18,\nDecember, 1970, pp. 456-463.\n\n[48]\n\nE. Avenhaus, "On the Design of Digital Filters with Coefficients\nof Limited Wordlength," IEEETAU, August, 1972, pp. 206-212.\n\n[49]\n\nH. T. Nagle, Jr., and M. M. Edgeworth, "Computer Aided Design of\nDigital Filters," TR#14, NAS8-20163, George C. Marshall Space\nFlight Center, Huntsville, AL, September, 1971.\n\n[50]\n\nJ. B. Slaughter, "Quantization Errors in Digital Control Systems,"\nIEEETAC, Vol. AC-19, January, 1964, pp. 70-74.\n\n[51]\n\nB. Widrow, "Statistical Analysis of Amplitude Quantized SampledData Systems,\' AIEE Trans. on Appl. and Ind., No. 52, January,\n1961.\n\n[52]\n\nH. T. Nagle, Jr., "Comments on \'A Least Upper Bound on Quantization\nError\'," IEEETAC, Vol. AC-14, August, 1969, pp. 433-434.\n\n2-191\n\n[53]\n\nH. T. Nagle, Jr., and C. C. Carroll, "Memory Sizing for Digital\nFilters," Proc. of the IFIP Congress \'71, Ljubljana, Yugoslavia,\nAugust 23-25, 1971, pp. TA-4-129, 133.\n\n[54]\n\nE. P. F. Kan and J. K. Aggarwal, "Minimum - Deadband Design of\nDigital Filters," IEEETAU, Vol. AU-19, December, 1971, pp. 292296.\n\n[55]\n\nS. R. Parker and S. F. Hess, "Limit-Cycle Oscillations in Digital\nFilters," IEEETCT, Vol. CT-18, November, 1971, pp. 687-697.\n\n[56]\n\nP. M. Ebert, J. E. Mazo, and M. C. Taylor, "Overflow Oscillations\nin Digital Filters," BSTJ, Vol. 48, 1969, pp. 2999-3020.\n\n[57]\n\nH. T. Nagle, Jr., and C. C. Carroll, "Organizing a Special-Purpose\nComputer to Realize Digital Filters for Sampled-Data Systems,"\nIEEETAU, Vol. AU-16, September, 1968, pp. 398-412.\n\n[58]\n\nI. W. Sandberg, "Floating-point Roundoff Accumulation in Digital\nFilter Realization," BSTJ, Vol. 46, October, 1967, pp. 1774-1791\n\n[59]\n\nB. Liu and T. Kaneko, "Error Analysis of Digital Filters Realized\nwith Floating-Point Arithmetic," Proc. IEEE, Vol. 57, October,\n1969, pp. 1735-1747.\n\n[60]\n\nB. Liu, "Effect of Finite Wordlength on the Accuracy of Digital\nFilters - A Review," IEEETCT, Vol. CT-18, November, 1971,\npp. 670-677.\n\n[61]\n\nE. P. F. Kan and J. K. Aggarwal, "Error Analysis of Digital Filter\nEmploying Floating-Point Arithmetic," IEEETCT, Vol. CT-18,\nNovember, 1971, pp. 678-686.\n\n[62]\n\nJ. F. Kaizer, "Some Practical Considerations in the Realization\nof Linear Digital Filters," Proc. 3 rd Annual Allerton Conf. on\nCircuit and System Theory, 1965, pp. 621-633.\n\n[63]\n\nJ. W. Henderson, Jr., "A Comparison of Different Realizations of\na Second Order Digital Filter with Regard to Quantization Errors,"\nMasters Thesis, Auburn University, Auburn, AL, June 9, 1970.\n\n[64]\n\nR. E. Crochiere, "Digital Ladder Structures and Coefficient Sensitivity," IEEETAU, Vol. AU-20, October, 1972, pp. 240-246.\n\n2-192\n\n[65]\n\nA. E. Vereshkin, et.al., "Two New Structures for the Implementation\nof a Discrete Transfer Function with Complex Poles," Automation and\nRemote Control, September, 1968, pp. 1416-22.\n\n[66]\n\nP. M. DeRusso, R. J. Roy and C. M. Close, State Variables for\nEngineers, New York, N. Y., John Wiley and Sons, Inc., 1965.\n\n[67]\n\nS. K. Mitra and R. J. Sherwood, "Canonic Realizations of Digital\nFilters Using the Continued Fraction Expansion," IEEETAU, Vol.\nAU-20, August, 1972, pp. 185-194.\n\n[68]\n\nA. Antonion, "Realization of Digital Filters," IEEETAU, Vol. AU-20,\nMarch, 1972, pp. 95-97.\n\n[69]\n\nC. S. Burrus, "Block Realization of Digital Filters," IEEETAU,\nVol. AU-20, October, 1972, pp. 230-235.\n\n[70]\n\nL. B. Jackson, "Roundoff Noise Analysis for Fixed-Point Digital\nFilters Realized in Cascade or Parallel Form," IEEETAU, Vol. AU-18,\nJune, 1970, pp. 107-122.\n\n[71]\n\nR. M. Golden, "A Computer Program for the Design of Continuous and\nDigital Transfer Functions," Autonetics Publication TM68-572-21-8,\nAugust, 1968.\n\n[72]\n\nT. W. Parks and J. H. McClellan, "A Program for the Design of\nLinear Phase Finite Impulse Response Digital Filters," IEEETAU,\nVol. AU-20, August, 1972, pp. 195-199.\n\n[73]\n\nL. R. Rabiner, "Linear Program Design of Finite Impulse Response\n(FIR) Digital Filters," IEEETAU, Vol. AU-20, October, 1972,\npp. 280-288.\n\n[74]\n\nP. N. Robinson and G. S. Robinson, "A Computer Method for Obtaining\nz-Transforms," IEEETAU, Vol. AU-20, March, 1972, pp. 98-99.\n\n[75]\n\nK. Steiglitz, "Computer-Aided Design of Recursive Digital Filters,"\nIEEETAU, Vol. AU-18, June, 1970, pp. 123-129.\n\n[76]\n\nA. Fellweis, "Some Principles of Designing Digital Filters\nImitating Classical Filter Structures," IEEETCT, Vol. CT-18,\nMarch, 1971, pp. 314-316.\n\n[77]\n\nJ. A. Cadyow and H. R. Martens, Discrete-Time and Computer Control\nSystems. Englewood Cliffs, NJ: Prentice-Hall, Inc., 1970.\n\n2-193\n\n[78]\n\nC. C. Carroll, et.al., "The Hybrid Realization of a 1)igital\nController\nfor the PICA Control Loop," TR#6, NAS8-20163, George C. Marshall\nSpace Flight Center, Huntsville, AL, September, 1967.\n\n[79]\n\nR. White, H. T. Nagle, and C. C. Carroll, "Organization of a\nHigh-Speed Stored-Program Special-Purpose Computer for the\nRealization of Digital Filters," TR#13, NAS8-20163, George C.\nMarshall Space Flight Center, NASA, Huntsville, AL, May, 1971\n\n[80]\n\nC. L. Phillips, et.al., "Digital Compensation of the Thrust Vector\nControl System," TR#8, NAS8-11274, George C. Marshall Space Flight\nCenter, NASA, Huntsville, AL, May, 1967.\n\n[81]\n\nH. T. Nagle, Jr., and C. C. Carroll, "A Special-Purpose Realization of a Third-Order Digital Filter for the PIGA Control Loop,"\nTR#9, NAS8-20163, George C. Marshall Space Flight Center, NASA,\nHuntsville, AL, May, 1968.\n\n[82]\n\nR. W. Schafer, "A Survey of Digital Speech Processing Techniques,"\nIEEETAU, Vol. AU-20, March, 1972, pp. 28-35.\n\n[83]\n\nL. K. Schweizer, "Problems in Realizing a Digital Vocoder and\nNovel Solutions," IEEETAU, Vol. AU-19, March, 1971, pp. 94-96.\n\n[84]\n\nF. Eggimann, "Computer Simulation of an Automatic Adaptive\nEqualizer for Real Telephone Channels and Free Data Format,"\nIEEETAU, Vol. AU-18, December, 1970, pp. 434-438.\n\n[85]\n\nK. V. Schlachta, "Digital Radar Recording and Analysis," IEEETAU,\nVol. AU-18, December, 1970, pp. 399-403.\n\n[86]\n\nJ. D. Echard and R. R. Boorstyn, "Digital Filtering for Radar\nSignal Processing Applications," IEEETAU, Vol. AU-20, March, 1972,\npp. 42-52.\n\n[87]\n\nA. I. Zverev, "Digital MIT Radar Filters," IEEETAU, Vol. AU-16,\nSeptember, 1968, pp. 422-432.\n\n[88]\n\nR. Roecker, "The Application of Digital Filters for Moving Target\nIndication," IEEETAU, Vol. AU-19, March, 1971, pp. 72-77.\n\n[89]\n\nA. J. Monroe, Digital Processes for Sampled-Data Systems.\nYork: John Wiley and wons, Inc., 1962\n\n[90]\n\nN. Morrison, Introduction to Sequential Smoothing and Prediction.\nNew York: McGraw-Hill, 1969.\n\n\'1\n\nNew\n\n2-194\n\n[91]\n\nS. Bertram, "Frequency Analysis Using the Discrete Fourier\nTransform," IEEETAU, Vol. AU-18, December, 1970, pp. 495-500.\n\n[92]\n\nJ. Tierney, C. M. Rader, and B. Gold, "A Digital Frequency\nSynthesizer," IEEETAU, Vol. AU-19, March, 1971, pp. 48-57.\n\n[93]\n\nA. G. Ratz, "Statistical Effects in Automatic Random Equalizer,"\nIEEETIM, Vol. IM-16, December, 1967.\n\n[94]\n\nSpecial Issue on Digital Picture Processing, Proceedings of the\nIEEE, Vol. 60, No. 7, July, 1972.\n\n[95]\n\nSpecial Issue on Two-Dimensional Digital Signal Processing,\nIEEETC, Vol. C-21, Number 7, July, 1972.\n\n[96]\n\nH. C. Andrews, A. G. Tescher, and R. P. Kreiger, "Image Processing\nby Digital Computer," IEEE Spectrum, Vol. 9, July, 1972, pp. 2032.\n\n[97]\n\nSpecial Issue on Digital Pattern Recognition, Proceedings of the\nIEEE, Vol. 60, No. 10, October, 1972.\n\n[98]\n\nG. Dumermuth, et.al., "Numerical Analysis of Electroencephalographic\nData," IEEETAU, Vol. AU-18, December, 1970, pp. 404-411.\n\n[99]\n\nL. L. Beranek, "Digital Synthesis of Speech and Music," IEEETAU,\nVol. AU-18, December, 1970, pp. 426-433.\n\nPART THREE\n\nMECHANIZATION\nOF DIGITAL\nFILTERS\n\n3-i\n\nPART THREE:\n\nMECHANIZATION OF DIGITAL FILTERS\n\nTABLE OF CONTENTS\n\nI.\n\n... . . . . . . . . . . .\n\nIntroduction . . . . . . . . ..\n\n..\n\n3-1\n\n3-4\n\nSimulation . . . . . . . . . . . . . . . . . . . . . . . .\n\n3-4\n\nB.\n\nIII.\n\n. .\n\nA.\n\nII.\n\nReal Time Programming. .\n\n. . . . . . . . . . .\n\n3-9\n\n. . . . .\n\nGeneral Purpose Computer Implementations .\n\n. . . ..\n\nMinicomputer Implementations\n\n.\n\n.\n\n. . . . . . . . . . . . 3-11\n\n.\n\n3-11\n\n. .... . . . . . . . .\n\nA.\nB.\n\nOperating System . . . .\n\n.\n.\n\n..\n\n..\n\n.\n\n. . . . . . . . . . . . 3-16\n\nC.\n\nAssembly Programs. . . .\n\n.\n.\n\n..\n\n.\n.\n\n.\n\n. . . . . . . . . . . . 3-19\n\nD.\n\nIV.\n\nHardware Requirements.\n\nExperimental Results\n\nSpecial Purpose Computers.\nA.\n\n. . . . . . . . . . . . 3-22\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.. . .\n\n. 3-25\n\nImplementation by Sample and Hold Devices with\nAnalog Networks . . . .\n\n. . .. .\n\n3-25\n\nB.\n\nHybrid Implementation. -\n\n.. .\n\n.\n\n3-30\n\nC.\n\nDigital Implementation . . . . . . . . . . . . .\n1. Input/Output Components . . . . . . . . . .\n2. Arithmetic Unit . . . . . . . . . . . . . .\n3. Memory Design . . . . .\n4. Controller Design . . .\n\n..\n..\n..\n..\n..\n\nD.\n\nImplementation by Microprogrammable SP Computer\xc2\xb7\n1. Input/Output Unit . . . . . . . . . . . . .\n2. Arithmetic Unit . . . . . . . . . . . . . .\n3. Memory ..................\n4. Control Unit ................\n\n. .. . .\n. . . . .\n. . . . .\n,. . ...\n..\n,.\n\n.\n\n\xc2\xb7.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n..\n\n3-ii\n\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n\n.\n.\n.\n.\n\n3-41\n3-45\n3-49\n3-63\n. 3-67\n3-70\n3-76\n3-77\n3-82\n3-93\n\n.\n\nE.\nF.\n\nRange Switching Digital Filter Implementation. .....\n\n3-105\n\nG.\n\nLSI Digital Filter Implementation. . . . . . . . . ...\n\n3-112\n\nH.\n\nV.\n\nTime-Sharing of a Digital Filter Implementation ...\n\nCommercial Digital Filters ..\n\n3-113\n\n.\n\n.\n\n. . . . . . . . ...\n\n. . . . . . . .....\n\nFFT Hardware ...A.\n\nCommercial Equipment .........-\n\nB.\n\nMIT Fast Digital Processor .* . . . . . . . . . . . . ..\n\nREFERENCES ........\n\n3-iii\n\n3-117\n3-117\n\n-.......\n\n.-.-.-.-.-......\n\n3-98\n\n3-118\n\n.\n\n3-125\n\nI.\n\nINTRODUCTION\n\nIn recent years a trend has been developing to replace analog systems with digital systems.\n\nThis rate of replacement has been directly\n\nrelated to the technological advances in the manufacturing of digital\nlogic building blocks.\n\nWith the advent of large-scale-integration, a\n\nparticular class of digital networks, called digital filters, has become economically practical in such areas as stabilization of control\nsystems, spectrum analysis, voice and speech analysis, radar, medical\nelectronics and virtually any other analog filter function 11,2].\nDigital filtering has been defined in PARTS ONE and TWO as a\ncomputational process consisting of digital multiplications, additions\nand delays whereby one sequence of numbers is transformed into another\nsequence.\n\nThis transformation may be specified by a transfer function\n\nin the z-domain, D(z), or by a set of linear difference equations\nwith constant coefficients.\n\nAssuming knowledge of these coefficients,\n\ndigital filter realization procedures [1,3,4,5] consist of the design\nof a digital system to solve these difference equations.\n\nThe difference\n\nequations may be solved with a software program and a general purpose\ncomputer or with the use of a special-purpose (SP) computer [6,7,8,9,\n10,11,12], a technique which has become increasingly popular.\n\n3-1\n\n3-2\n\nIn the SP computer realizations, a particular digital filter programming form is selected and the computer is designed accordingly\n113,14,15,16,17]. Particular attention must be given to assure that\nthe hardware organization meets the system specifications for coefficient\nquantization, signal amplitude quantization, and quantization noise\nlevels introduced into the system by the digital filter implementation.\nAt the present time no systematic design procedure has been developed\nto accomplish these goals.\n\nTypically one designer specifies the digital\n\nfilter coefficients and another specifies a hardware implementation.\nThe state-of-the-art in digital filter implementation is represented\nin 11,3,7,8,9,10,12,18,19,20,21]. Perhaps the most interesting are the\nIC model in [1] and the programmable design of 112].\n\nThe IC model is\n\navailable from Autonetics Division of North American Rockwell.\n\nA\n\ndigital filter implemented with this technology is small and can\nrealize third-order filters at sampling rates of up to 5kHz.\n\nHowever,\n\npoles must be real and the parallel programming form is the only one\navailable.\n\nThe commercial units also have restricted programming\n\nforms, or implementation is done by frequency transformations which\nlimit their use to applications in which minimum time delay and high\nspeed sampling are not specified.\n\nIt has been shown in [14] that some\n\nof the programming forms have different characteristics, and it is\ndesirable in many cases to be able to select the programming form.\nThe need for a selectable programming form along with the desirable\nfeatures of LSI implementation offer a challenge to the system designer.\n\n3-3\n\nAdd the necessity for real-time fault diagnosis and standardized CAD\nprocedures to the list and the system design goals are complete.\nNow that the theory of digital filtering has been presented in\nPART TWO, we will examine four mechanization techniques for digital\nfilters.\n\nThe four techniques are 1) general-purpose computers,\n\n2) mini-computers, 3) special-purpose computers, and 4) FFT hardware.\nA discussion of all techniuqes will be presented starting with\nmechanization (implementation) by general-purpose (GP) computers.\n\nII.\n\nGENERAL PURPOSE COMPUTER IMPLEMENTATIONS\n\nOf the four implementation techniques, the GP computer is the\nleast\n\nattractive, with the main reason being that most GP computers\n\npossess excessive computing capabilities to be used only for difference equation calculations.\n\nIf this were done, there would be large\n\nportions of the computer hardware that would never be used thereby\nmaking this type implementation overly expensive.\nGP computers do have a useful application in digital filter\nimplementation in that they may be used to simulate other implementation\ndesigns (an example being by special-purpose computers) or for realtime programming of a GP computer to implement a digital filter as well\nas other computational chores.\n\nLet us now look at these two aspects\n\nof using a GP computer in the design of a digital filter system.\n\nSimulation\nThe most common implementation of a digital filter is by specialpurpose computer.\n\nWhen designing a special-purpose computer for the\n\nimplementation of the filter in a particular programming form, one of\n\n3-4\n\n3-5\n\nthe first steps that must be done is deciding on word length requirements for the input word, output word and internal variable (m(kT - T),\nm(kT - 2T), etc. of the difference eqs.) wordlengths and possibly\narithmetic schemes.\n\nThis can be accomplished by techniques such as\n\nthe CAD program presented earlier.\n\nOnce a design is recommended, it\n\nis good engineering practice to simulate the system on a GP computer\nto verify all the design parameters.\n\nWith most higher level languages,\n\nlogical programming may be done such that every aspect of the design\nmay be simulated.\n\nIf this approach is taken the system designer may\n\n"change something" and observe its effects; this technique may be\nused to "optimize" the final system design.\nAs an example of a digital filter implementation simulation, the\nprogram below was written in FORTRAN and run on an IBM 360 digital\ncomputer to simulate the "range-switching" filter described in [8]\nemployed in a nulling type control loop.\n\nThe program was written so\n\nthat the "range-switching" effects on the output of the loop could be\nobserved and the effect the wordlengths had on the output response for\na particular input, which in this case is a sine wave of specified\namplitude and frequency.\n\n3-6\n\nFORTRAN SOURCE PROGRAM FOR SIMULATION OF PIGA LOOP\nWITH A NOISE INPUT\n\nSOURCE DECK\nTIME DOMAIN SIMULATION OF THE COMPENSATED SYSTEM\nC\nD(2)\nDIMENSION X1(2),X2(2),X3(2),\nITER,TES\nCOMMON/COM2/RM(1001),\nCOMMON/COM1/XP(3,2),BQ(1),C(2)\n1 FORMAT(1H ,13,2X,1P9E13.4)\nCALL INPUT\n2 FORMAT(1H1)\nN=1\nH=1.o 0E+05\nWN=184.0\nT=0. 001\nGP=56. 2\nGT=321000.0\nGDA IS THE TOTAL LOOP GAIN\nC\nGDA=0.083\nW1=SIN(WN*T)/WN\nW2=COS(WN*T)\nW3=(1.0-W2)/WN**2\nW4=(T-W1)/H\nW5= (1.0-W2)/H\nW6=WN*SIN(WN*T)\nW7=W6/H\n*H\nGDIG=GAD/GT/GP/(T-W1)\nWRITE(6,10) GDIG\n10 FORMAT(1H0,7HKDIG = ,1PE11.4)\nTEST=+2000.0*980.0/4.0\nDO 669 NAGL=1,2\nXP(NAGL,1)=O.O\n669\n8 BDC=O.O\nBQ(1)=0.0\nx1(1)=o.o\nX2(1)=0.O\nX3(1)=O.O\nD(1)=0.0\nC(1)=0.0\n0\nCOFS=O\nDO 5 1=1,ITER\nR=TES*980.*RM(I)\nWRITE(6,1) I,BDC,BQ(N) ,I (N),XP(1,1),D(N),R,COFS,C(1)\n4\nBEGIN ANALOG PORTION SIMULATION\nC\n(N)+W4*(R-GR*D(N))\nX1 (N+1)=X1(N)+W1*X2(N)+W3*X3\n*\nX2(N+1)=W2*X2(N)+W1*X39N)+W5 (R-GR*D(N))\nX3 (N+1)=W2*X3(N)+W6*X2(N)+W7*(R-ST*D(N))\nBDC=GP*X1(N+1)\n\n3-7\nC\nC\n\nC\n\nZ\n\n2\n3\n\n4\n5\n\n7\n\n6\n\nEND\nANALOG PORTION SIMULATION\nBEGIN DIGITAL UNIT SIMULATION\nUI=BDC\nCALL DIGCOM (UI,YP)\nD(N+1)=GDIG*YP\nEND\nDIGITAL UNIT SIMULATION\nCOFS=GT*D(N+1)\nXl(N)=Xl(N+1)\nX2 (N)=X2 (N+1)\nX3(N)=X3(N+1)\n5 D(N)=D(N+1)\nSTOP\nEND\nSUBROUTINE DIGCOM(U1,YP)\nCOMMON/COM1/SP(3,2),BQ(1)\n,C(2)\nAO=1.0\nAl=-119./64.\nA2=57./64.\nB1=0.0\nB2=0.0\nFX=256.\nUI=UI/3.0*FX\nEX=1.0\nCALL ROUND (UI,EX,FX)\nUP-UI\nBQ(1)=UP*3.0/FX\nUI-UI*3.0/FX\nIF(ABS(UP)-16.0) 2,3,3\nC(2)=0.0\nGO TO 4\nC(2)=1.0\nUP=UP/16.\nIUP=UP\nUP=IUP\nFX=16.\nIF(C(2)-C(1)) 4,6,7\nXP(1,1)=16.*XP(1,1)\nXP(2,1)=16.*XP(2,1)\nAX=4.0\nBX=63.75\nCALL ROUND(XP(1,1),AX,BX)\nCALL ROUND(XP(2,1),AX,BX)\nGO TO 6\nXP(1,1)=XP(1,1)/16.0\nXP(2,1)=SP(2,1)/16.\nAX=4.0\nBX=63.75\nCALL ROUND (XP(1,1),AX,BX)\nCALL ROUND (XP(2,1),AX,BX)\nSP(1,2)=-Bl*XP(1,1)-B2*XP(2,1)+UP\nXP(2,2)=SP(1,1)\n\n3-8\nAX=4.0\nBX=63.75\nCALL ROUND(XP(1,2),AX,BX)\nYP=(A1-AO*B1)*XP(1,1)+(A2-A*B2)*XP(2,1)\n+AO*UP\n1\nCX=64.\nDX=255./64.\nCALL ROUND(YP,CX,DX)\nDO 1 I=1,2\n1 XP(I,1)=XP(I,2)\nC(1)=C(2)\nYP=YP/FX*3.0\nRETURN\nEND\nSUBROUTINE INPUT\nC\nC\n\nRANDOM INPUT\nCOMMON/COM2/RM(1001),ITER,TES\nTES=200.\nITER=301\nNRANB=6\nCALL RANBIT(NRANB)\nCALL RCON1(35187269)\nRMAX= (2.**NRANB-1.)/2.\nD019 I=1,ITER\nRM(K)=IRAN(5)\n19 RM(I) = (RM(I)-RMAX)/RMAX\nRETURN\nEND\nSUBROUTINE ROUND (A,AN,BN)\nX=ABS (A)\n\nS=A/X\nIX=X*AN\'\n\nXQ=IX\n\n1\n2\n\nXQ=XA/AN\nIF(XQ-BN) 1,2,2\nA=S*XQ\nRETURN\nA=S*BN\nRETURN\nEND\n\n3-9\n\nReal Time Programming\nA digital filter implemented on a general-purpose computer,\nwhether large or small, is said to be realized by real-time programming.\n\nThe machine language version (translated from some higher\n\nlevel language) of the difference equations must execute quickly\nenough to meet the sampling rates imposed by the system specifications.\nIn some applications the general-purpose computer will handle other\ncalculations as well and will be "time-shared" to perform both duties.\nOther times a small process control computer can be dedicated solely\nto the digital filter calculations.\n\nAn example system is shown in\n\nFig. 2.1.\nGenerally speaking, future trends will be to design specialpurpose computers to shoulder the digital signal processing tasks, and\nrelieve the general-purpose computer for more complicated tasks which\nexploit its entire computational power as embodied by its versatile\ninstruction set.\n\nD(z)\n\nFig. 2.1.\n\nA GP computer being used as a digital\nfilter in a discrete control loop.\n\nMINICOMPUTER IMPLEMENTATIONS\n\nIII.\n\nA minicomputer Implementation of a digital filter as described\nin [22] will be discussed.\n\nOnly one reference is used as a background\n\nsince it is the only one that has been seen in the literature of digital\nfiltering.\n\nIt will be sufficient since any other minicomputer imple-\n\nmentation would follow the guidelines presented.\nHardware Requirements.\nThe hardware used for the minicomputer implementation is shown in\nFig. 3.1.\n\nIt consists of a Honeywell H316 minicomputer with two 4096-\n\nword memory modules, a 10-bit analog-to-digital (A/D) converter, a 12-bit\ndigital-to-analog (D/A) converter, a crystal-controlled real-time clock\nand the ASR-33 teletypewriter.\nH316 minicomputer.\nwordlength.\n\nThe H316 is a GP minicomputer with a 16-bit\n\nArithmetic is performed in two registers, A and B, and\n\nit is a one-address machine with the A register serving as the accumulator\nwhich will be described in detail later.\n\nThe memory is divided into\n\nsectors or pages of 512 words each, with the computer having the\ncapability to reference any of the 512 words within a certain sector.\nSingle-level indexing and/or multiple-level indirect addressing can be\nused to address words outside the current sector or the base sector.\nWith respect to the arithmetic instructions of the computers\ninstruction set, there are two modes of operation:\nand double precision.\n\nsingle precision\n\nEach mode of operation may be entered by the use\n\n3-11\n\n3-12\n\nH316\nMinicomputer\n\nFig. 3.1.\n\nHardware used in minicomputer implementation.\n\n3-13\n\nof one instruction, "SGL" for single-precision arithmetic operations\nand "DBL" for double precision arithmetic operations.\n\nWhen operating\n\nin the single-precision mode, the A register is used solely as the\naccumulator.\n\nIt is 16-bits long with the left-most bit being the\n\nsign bit and the 15-bits to the right being the most-significant\nthrough the least significant of the magnitude bits which are in a\ntwo\'s complement code.\n\nWhen operating in the double precision mode,\n\nthe A and B registers are used as the accumulator with the sign bit\nbeing in the left-most bit position of the A register.\n\nThe rest of\n\nthe A register contains the 15 most significant bits of the double\nprecision word with the 15 least significant bits being contained in\nthe 15 right most bit positions of the 16 bit B register.\n\nThe left\n\nmost bit position of the B register does not take part in arithemtic\noperations.\nWhen performing the "add" instruction which will have to be done\nmany times in difference equations calculations, the contents of the\naddressed memory word are added to the contents of A leaving the sum\nin A for single-precision addition.\n\nIf done in double-precision the\n\ncontents of the addressed memory word (two memory locations for double\nprecision) are added to the contents of the A and B registers and the\nsum left in them.\nThe same procedure occurs for multiply for the single or double\nprecision mode.\n\nThe addressed word in memory is multiplied by the word\n\nstored in the A or A and B registers and the product left in the A or\nA and B registers.\n\n3-14\n\nFixed point arithemtic is used for all difference equation calculations.\n\nSince an imaginary binary point is assumed, after a multipli-\n\ncation instruction is executed, the computer shifts the product as\nrequired to align the binary point.\nInput to the minicomputer is accomplished through 16 input bus\nlines into the A register.\n\nSeveral peripheral devices may be connected\n\nto this bus as inputs to the computer.\n\nIn the case of the.minicomputer\n\nimplementation of a digital filter this bus inputs information from the\nA/D, ASR-33, and the real-time clock.\nOutput is accomplished through 16 output bus lines which are tied\ndirectly to the A register and always reflect its contents.\n\nFor the\n\ndigital filter implementation, the output device is the D/A or the ASR-33.\nThe different input devices are checked by the computer by placing\na code unique to each device on the address bus.\nMost peripheral devices are slower than the computer, thereby\nmaking the computer spend much of its time waiting for a peripheral\ndevice to perform its function.\n\nIt is for this reason that it is\n\npractical to let the computer process other information while a particular peripheral is performing its I/O function.\n\nThen when the peripheral\n\nis finished, it can inform the computer and the computer can give it\nanother command.\nThe method of informing the computer of the completion of a task\nis called an interrupt.\n\nWhen a peripheral interrupts the H316, it\n\nA:\n\n3-15\n\nfinishes executing the instruction presently being performed and then\nperforms a subroutine jump indirectly through a dedicated memory\nlocation.\n\nIn short, the dedicated memory location contains the address\n\nof a subroutine to which the computer jumps when an interrupt occurs.\nWithin this subroutine the computer may poll the peripherals to find\nout which one interrupted.\nAn A/D converter is used as the input interface element to the\ncomputer.\n\nThe A/D which was interfaced to the H316 is a bipolar con-\n\nverter having a range of -10v to +10v.\n\nIt has a 10-bit plus sign-bit\n\noutput which is input into the most significant 10-bits of the A\nregister.\nThe D/A converter accepts and transforms the binary output of the\ncomputer into an analog voltage.\n\nA hold register is employed so that\n\nthe output voltage will remain constant until the next output occurs.\nThe D/A used in the minicomputer implementation was built from Honeywell\nP-Pac DTL logic.\n\nThe converter is built from three cascaded Honeywell\n\nCE-071 four-bit converters which consist of a resistive ladder plus\nswitching network.\nTo provide for a sampling rate other than that determined by the\ncomputers execution time, a real-time clock was employed.\n\nIt initiates\n\neach cycle consisting of input, calculation, and output and is built\nas a peripheral which furnishes periodic (sample rate) interrupts to\nthe computer.\ncycle\n\nWhen an interrupt occurs, the computer goes through one\n\nand then waits for the next interrupt before it goes through the\n\n3-16\n\ncycle again.\n\nThis allows the operator of the minicomputer to obtain\n\nany desired sample rate.\nOperating System\nIt is the purpose of the minicomputer implementation to be able\nto realize in real-time one of eleven different digital filter programming\nforms.\n\nIt is the function of the operating system to set up, control,\n\nand possibly run diagnostic tests if something goes wrong, on the\nminicomputer and its peripherals.\nA functional block diagram of the operating system (OS) is shown\nin Fig. 3.2.\n\nSolid arrows indicate a passing of control from one\n\nroutine to another, while dotted arrows indicate a passing of parameters.\nOnly one filter form is shown, but it should be remembered that eleven\nsuch forms are present with similar links to the operating system\nBriefly, to realize a digital filter, a particular form is picked\nand the parameters which determine the transfer function are input.\nThe OS will then type back these parameters if desired.\n\nOnce the filter\n\nform is set up, the OS is instructed to begin execution of that form.\nLet us now discuss the different parts of the OS.\nExecutive.\n\nThe executive routine (EXEC) initially types a question\n\nmark on the teletype.\n\nWhenever the question mark appears the operator\n\ntypes in one of four commands:\n\nMODIFY, LIST, RUN, or TEST.\n\nThe first\n\nthree refer to a particular programming form and are followed by a\nnumber between one and eleven.\n\nThe TEST command refers to one of seven\n\ndiagnositc routines and should be followed by a number from one to seven.\n\n3-17\n\n0\nn\n>,\n0\n.,,\n\n\'4\n\n0\n\nbO\nco\n0\n,CS\n\n00\n\nFX4\n\n3-18\n\nAfter one of the four commands is typed in EXEC turns control over to\none of the four routines having the same name.\n\nLet us briefly discuss\n\nthese routines.\n1.\n\nModify.\n\nThe modify routine inputs the coefficients, quantization\n\nformats, and sample rate for a particular filter form.\n\nEXEC determines\n\nwhich of the eleven forms has been typed in following the command MODIFY,\nthen transfers control to the modify routine, passing the filter form\nnumber as a parameter.\n2.\n\nList.\n\nThe list routine types out the coefficients of a pro-\n\ngramming form followed by the quantization formats and finally the\nsample period.\n3.\n\nRun.\n\nTo begin the filter processing the operator would type\n\nin RUN followed by the number of the form he desires to use.\nRUN has a list of all entry points of the filter forms.\n\nWhen\n\nthe RUN routine is entered it immediately obtains the address of the\nnormal entry point and passes it to the interrupt processer which will\nneed it at a later time.\n\nThen RUN selects the sample period which the\n\nuser has specified for that filter form and outputs it to the RTC.\nNext, RUN sets the mask of the real-time clock (RTC) and teletype,\nstarts the RTC, types out a question mark, and transfers control to\nthe initialization entry of the filter form specified.\n\nThe filter form\n\nmakes its first pass and hangs up in the idle routines at the end.\nWhile in the routine the RTC should interrupt.\n\n3-19\n\nInterrupt processor.\n\nWhen the interrupt occurs, control is passed\n\nto the interrupt processor.\n\nThis routine must identify what caused\n\nthe interrupt and act accordingly.\nUser\'s interface.\n\nThe user interface consists of the teletype\n\nroutines plus the data conversion routines.\n\nThe teletype routines\n\nare relied upon by all the other routines which have to communicate\nwith the user.\n\nThe teletype routines handling mumerical data rely on\n\nthe conversion routines to convert from decimal to binary and binary\nto decimal.\nDiagnostics.\n\nSeven diagnostic routines are implemented in the\n\nOS to test the hardware and the software structure.\n\nOne of these routines\n\nmay be executed by typing in the request TEST followed by a number from\none to seven.\ncategories:\n\nThe errors that are checked for are divided into three\n\nhardware errors, errors in the programming of the OS, and\n\nlast, user errors.\nThis completes the discussion of the OS.\n\nWe will now look at the\n\nassembly programs.\nAssembly Programs.\nEach of the eleven filter programming forms is realized by a\nseparate subroutine which has the following format:\nENTRY\nINPUT\nCALCULATION\nOUTPUT\nTIME DELAY\nPRECALCUATION\nIDLE\nEXIT\n\n3-20\n\nThere are two entry points to each program:\n\none being an initializa-\n\ntion entry point which zero\'s the internal variables the first time\nthrough the program and the second a regular entry point that is\nentered everytime except the first.\n\nAfter entering the normal entry\n\npoint a "start A/D" command is given and, while waiting on the input\nto become available, a partial sum is formed.\n\nAs soon as the input\n\narrives it is shifted to a correct format, multiplied by A0 , and the\nsum is then completed.\n\nThe sum is then quantized for output, presented\n\nto the D/A, then quantized in a different format for storage and feedback.\n\nIf overflow is detected during quantization, the word is saturated,\n\ni.e. filled with the largest possible number.\nAfter the output is complete, the internal variable must be shifted\nto perform time delay.\n\nThen the partial sum for the next pass is begun.\n\nJust enough of the formation of the partial sum is left for the next\npass to occupy the arithemtic unit while waiting on the A/D.\n\nDuring\n\nthe "idle" period, the RTC interrupts and the interrupt subroutine\ndirects control back to the normal entry point.\nThe coefficients as well as the three shift instructions used in\nthe quantizing routines are declared as external names so that they\nmay be altered by the OS.\nAs an example of one of the eleven assembly language programs the\nassembly language program for a second order D(z) in modified canonical\nprogramming form is shown below.\n\n3-21\n\nSUBR MCAN1,ENT1\nSUBR MCAN2,ENT2\nENT\nSHFT61,S1\nENT\nSHFT62,S2\nENT\nSHFT63,S3\nENT\nCOEF6,AO\nBEL\n* INITIALIZE INTERNAL VARIABLES\nENT1 CRA\nSTA\nXM1\nSTA\nXM2\n*CALCULATE OUTPUT DIF]FERENCE EQ.\nENT2 OCP\n\'41\nLDA\nXM1\nMPY\nALl\nDBL\nDST\nTEMP\nLDA\nXM2\nMPY\nAL2\nDAD\nTEMP\nDST\nTEMP\nINA\n\'1041\nJMP\n*-1\nS1\nLRS\n4\nSTA\nEI\nMPY\nAO\nDAD\nTEMP\nSGL\nSTA\nSGN\nS2\nLLS\n9\nSSC\nJMP\nOK1\nLDA\nSGN\nCSA\nLSA\n=\'77777\nSRC\nTCA\nOK1 OTA\n\'40\nJMP\n*-1\n*CALCULATE FEEDBACK DIIFFERENCE EQ.\nLDA\nEI\nMPY\nONE\nDBL\nDST\nTEMP\nLDA\nXM1\nTCA\nMPY\nB1\n\nSTART A/D\n\nINPUT FROM A/D\nWAIT FOR INPUT\n\n3-22\n\nDAD\nTEMP\nDST\nTEMP\nXM2\nLDA\nTCA\nB2\nMPY\nDAD\nTEMP\nSGL\nSTA\nSGN\n3\nS3\nLLS\nSSC\nOK2\nJMP\nSGN\nLDA\nCSA\nLDA\n=\'77777\nSRC\nTCA\n* PERFORM TIME DELAY\nOK2 STA\nXM\nXM1\nLDA\nSTA\nSM2\nLDA\nXM\nSTA\nXM1\nENB\nNOP\n*-1\nJMP\nXM1 DBP\n0\nEO\nXM\nEI\nXM2\nTEMP\nSGN\nAO\nALl\nAL2\nB1\nB2\nONE\n\nBSS\nBSS\nBSS\nBSS\nBSS\nBSS\nOCT\nOCT\nOCT\nOCT\nOCT\nOCT\nEND\n\n1\n1\n2\n2\n2\n1\n10000\n-22753\n7357\n3146\n231\n10000\n\nExperimental Results\nExperimental results were obtained of the minicomputer implementation\npreviously described.\n\n3-23\n\nFirst it realized the transfer function of a Euler integrator\n\nD(z) =\n\n1\n\n(II-1)\n\n1 -z\n\n-1\n\nin the direct form at its maximum sampling rate (5.5 KHz).\n\nThe response\n\nwas obtained for an input square wave and as wished, the output was a\ntriangular waveform with a fine-grained stair-stepped appearance.\nSecondly it realized the transfer function of a digital differentiator\n-\n\nD(z) = 1 - z\n\n(II-2)\n\n1\n\nin the direct programming form.\n\nIt\'s response to a triangular wave-\n\nform, a square wave, was as expected.\nLastly, the transfer function of a digital oscillator was realized\n\nD(z) =\n-1\n\n1 - 2cos(2nfT)z\n\n(II-3)\n\n-\n\n+ z\n\n2\n\nwhere T is the reciprocal of the sample rate (5.5 KHz) and f is the\nfrequency of oscillation.\n\nThe equation was programmed with b1 =-1.75\n\nwhich resulted in an output frequency of 450 Hz as predicted by\nEq. (II-3).\nExperimentation demonstrated that the direct and canonical forms\nhad the highest maximum sampling rate.\n\nThe direct, canonical and\n\nmodified canonical forms have minimum sample intervals of less than 200\np-secs.\n\nThe modified direct form has a minimum interval of about 225 p-secs.\n\n3-24\n\nThe parallel and cascade forms have a minimum interval of about 275 P-secs,\nwhile the remainder of the forms have intervals of approximately\n300 U-secs.\nThe number of instructions required to implement the filter forms\n(including coefficient storage), ranges from 98 for the canonical to\n109 for the modified cascade.\n\nThe entire operating system occupies\n\napproximately 5000 memory locations including indirect links in the\nbase sector.\nThis concludes the discussion on minicomputer implementations of\ndigital filters.\n\nFrom this discussion it was seen that a minicomputer\n\ncan be adapted well for a real-time digital filter implementation; in\nfact, much better than the larger SP computers because of the smaller\nsize.\n\nWe will now look at even a smaller digital computer implementation,\n\nthat of implementation by special-purpose computers.\n\nIV.\n\nSPECIAL-PURPOSE COMPUTERS\n\nIt is obvious to one that for most realizations of a digital filter,\nthe general purpose computer and the minicomputer approach have several\ndisadvantages.\n\nThe most easily seen disadvantage, as previously mentioned,\n\nis the wasted hardware incurred because of the relative simplicity of\nthe difference equations that must be calculated for a realization.\n\nIt\n\nis for this reason that the special-purpose computer approach to realization is taken for a majority of the applications of digital filtering.\nIt will be shown in the following discussion of special-purpose (SP)\ncomputer realizations that they are the most economical (hardware wise)\nand demonstrate a great amount of versatility.\nThe realization techniques by SP computers will begin with the very\nearliest method, which was sample and hold devices with analog networks\nand conclude with present day commercial models that are available on\nthe market.\n\nImplementation by Sample and Hold Devices with Analog Networks.\nThe first SP computer realizations of a digital filter were by sample\nand hold devices with analog networks I23].\n\nThere are two main ways of\n\nrealizing a digital filter in this manner with them being:\n\n1) series\n\ndiscrete data networks, and 2) feedback discrete data networks.\n\nThere\n\nis a third way of realization which is a combination of the above two\n\n3-25\n\n3-26\n\nthat will not be discussed since it is felt it is not essential to\nillustrate the realization technique.\nThe series discrete data network which is used for the realization\nof the discrete transfer function of a digital filter is shown in\nFigure 4.1.\n\nThe transfer functions of the two systems in Fig. 4.1\n\nare related by\n\nD(z) = GhoGc(Z)\n\n(4-1)\n\nSince the transfer function of the zero-order hold is (1-e\n\n)/s,.it\n\ncan be obtained from Eq. (4-1) that\n\nZ[\n\nG (s)\nc\n] =\n\n(4-2)\n\nD(z)\n1\n\n-z\n\nFrom this it is seen that given a specific D(z), the transfer function\nG (s) of the discrete-data network can be determined from Eq. (III-2)\nC\n\nby taking the inverse z-transform.\nIf Gc(s) is to be an RC realizable transfer function, all the poles\nof GC(S) must be simple and lie on the negative real axis of the s-plane\nwith the exception of the origin and infinity.\nbe located anywhere in the s-plane.\n\nThe zeroes of G (s) may\n\nTherefore, Gc(s)/s can be expanded\n\ninto the following form by partial fraction expansion:\n\nG (s)\n\nA\ns\n\nm\n\nAk\n\n++\n\n-c\ns\n\nk=l s + sk\n\n(4-3)\n\n3-27\n\nel (t)\nT\nZero-order\nhold\n\nD (z)\n(a)\n\n(t *e2(t\neI\nel(t)\n\n;G\nG\n1\xc2\xb7 I I\n\nDigital filter.\n\nonG\nho~\n\nI\n\nG\n\n1\n\nZero-order\nhold\n\nF~\n(b)\n\n-D(z)\n\nr\n\nr\nI\n\nts)\niqpj~ho (s):I\n\' ---- i~~~~~~~~~~~~~~~~~~\n\nZero-order\nhold\n\nI1\n\nEquivalent series discrete-data network.\n\nFig. 4.1\n\n3-28\n\nwhere Ao and Ak are constants and sk = [k = 1, 2, 3,"\', ml are simple\nnegative real poles.\n\nG (s)\ns\n\nThe z-transform of Eq. (rIs-3) is\n\nA\n\nm\n\nC1\n\nz\n\n- 1\n\n+\n\nk~\n\nAk\n\nkcl\n\nT(4-4)\n\n=1-SkT -1\n1 - e\n\nZ\n\nwhich has simple positive real poles inside the unit circle\nwith only one pole at\n\nK\n\n- 1.\n\nIzI = 1,\n\nComparing Eq. (4-4) with Eq. (4-2),\n\nit is seen that in order for G (s) to represent an RC network, the\nc\ndiscrete transfer function D(z) must have the following properties:\n(1)\n\nThe number of poles of D(z) must be equal to or greater than\n\nthe number of zeroes of D(z).\n(2)\n\nThe zeroes of D(z) are arbitrary in location.\n\n(3)\n\nThe poles of D(z) must be simple, real and positive, and\n\nlie inside the unit circle\n\nIzl = 1 in the z-plane.\n\nIt can be shown that for a feedback discrete-data network, the\nfeedback structure with a zero-order hold and the RC network shown in\nFig. 4.26 is equivalent to the digital filter of Fig. 4.2a [23].\nThe transfer function of the two systems are related by\n\nD(z) - 1 + 1\nGhoH(z)\n\nor\n\ns\nZ[ MU I]\n\n1\n1\n\n1\n\n1\n- 1\n\nz\n\nD(z)\nI__\n\nD(z)\n\nI_\n\n.\n\n(4-5)\n\n3-29\n\n(t)\nT\n\nT\n{-"\n\nD(z)\n\n[-\'1\n(a)\n\n-\n\nZero-order\nhold\n\nDigital filter.\n\nel(t)\n\ne2 (t)\n\n%.\n\nZero-order\n\nhold\n\n-\n\nH(s)\n\nRC Network\n\n(b)\n\n\'\n\nEquivalent feedback discrete-data network.\n\nFig. 4.2\n\n3-30\n\nIn order to realize H(s) by an RC network, the discrete transfer\nfunction D(z) must have the following properties:\n(1)\n\nD(z) must have the same number of poles and zeroes.\n\n(2)\n\nThe poles of D(z) are arbitrary.\n\n(3)\n\nThe zeroes of D(z) must be simple, real, positive, and lie\n\ninside the unit circle of the z-plane.\nThe reasons behind these restrictions are discussed in detail in\nI23] and therefore are not repeated here.\nIn summary it may be said that the listed restrictions and limited\nflexibility of the above implementation techniques make their use and\npracticality almost negligible.\n\nHybrid Implementation.\nAfter observing the difference equations of several of the programming forms which may be used for the realization of a digital\nfilter, such as the direct and canonical forms, one can see that these\nequations might be computed by a device which performs the arithmetic\nfunctions of addition, subtraction, multiplication and time delay.\nThis suggests the use of summing amplifiers, constant multipliers and\ndelay elements for the implementation.\n\nAlso, knowing that digital\n\nfilters may be implemented by SP and GP computers suggests the realization\nof a digital filter by hybrid techniques, with hybrid meaning that analog\nand digital methods are used for the implementation 110].\n\nFor most hybrid\n\nrealizations digital techniques are used for analog-to-digital (A/D)\n\n0/\n\n3-31\n\nconversion, digital-to-analog (D/A) conversion and time delay with analog\ntechniques used for the arithmetic functions of addition, subtraction,\nand multiplication. This type realization exploits the best and most\nnatural functions of both analog and digital elements to eliminate a\nmajority of the restrictions placed on the realization of the previous\nsection of the literature which used sample and hold devices with analog\nnetworks.\n\nAnother advantage of the hybrid realization which will be\n\nillustrated shortly is that once a D(z) is obtained, the filter can be\nrealized directly from it.\n\nThis will eliminate the need for additional\n\nmathematical manipulation required for the derivation of an s-plane\ntransfer function from the D(z) as was required by the previous implementation technique.\nThe hybrid implementation of a digital filter fits itself to a\nmajority of the programming forms of a given D(z).\n\nFor simplicity sake,\n\nwe will look at the hybrid realization of a D(z) in two programming\nforms; the direct form and the canonical form.\n\nThese two forms usually\n\nhave the simpliest difference equations which must be implemented for\ntheir realization.\nFor most hybrid designs an A/D converter functions as the sampling\ndevice as well as an interfacing element for the input to the filter.\nIntegrated circuit buffer registers are usually used to store previous\nvalues (in digital form) of an intermediate variable that is internal to\nthe controller. D/A converters provide the interface for the output of\n\n3-32\n\nVariable resistors at the input of a summing amplifier are\n\nthe filter.\n\nusually used to adjust the coefficients of the compensation function\ncontinuously over a wide range of values.\n\nThe equivalent of a zero-order\n\nhold device is realized at the output of the filter as a result of the\ndigital data-storage elements within the hybrid unit.\nThe hybrid filter is designed, in this case, to realize almost\nany compensation function up to three zero\'s over three poles in the\nz domain with any sampling frequency up to several thousand hertz.\n\nIf\n\na filter of order higher than three is required, several second or third\norder filters are cascaded until the desired order is obtained.\n\nThis\n\nis usually done over constructing a higher order filter because of the\ngreater coefficient sensitivity for a higher order filter.\nthe\n\nIt is seen that\n\nhybrid controller is extremely versatile and can be used to realize\n\na wide range of sampled-data compensation functions; thus, as previously\nmentioned, it is not necessary to redesign the unit to change the\ncompensation function.\nTo obtain an insight into the design of a hybrid digital filter\nfor a given D(z), a hybrid implementation will be derived for the direct\nand canonical programming forms.\nLet us look at the direct form realization first.\n\nA second order\n\ntransfer function for a realizable digital filter can be written as\n+ a-2\n+ alz\na2\n= o\n-1\n-2\n+ b2z\n1+bz\na\nD(z)\n\nE (z)\nE. (z)\n\n\'\n\n(4-6)\n\n3-33\n\nsT\nwhere the coefficients ai and bi are real numbers and z = e\nEquation (4-6) can be rewritten as\n\nEo() = aEi(z) + alz-Ei(z) + a2-2Ei(Z) - blz- Eo(\n\n)\n\n-\n\nb2 z 2E (z)\n\n(4-7)\n\nor in the time domain as the difference equation\n\neo(kT) = aoei(kT) + alei(kT - T) + a2 ei(kT - 2T) - bleo(kT - T) (4-8)\n\nb2 e0 (kT - 2T)\n\nwhere f\n\n= 1/T is the sampling frequency.\n\nFig. 4.3 is a block diagram\n\nof the hybrid realization of the second order direct programming form\nwith double lines denoting digital information and the single lines\nanalog information.\n\nA detailed explanation of the components used for\n\nthe realization will be given after the block diagram of the canonical\nform is given.\nIn order to realize the canonical form, we have previously seen\nthat for a second order D(z), the following difference equations must\nbe realized.\n\nm(kT) = ei(kT) - blm(kT - T) - b2 m(kT - 2T)\n\n(4-9)\n\neo(kT) = aom(kT) + alm(kT - T) + a2 m(kT - 2T)\n\n(4-10)\n\nPRECEDING PAGE BLANK NOT FUMM\n3-35\n\nThe block diagram that results for a canonical hybrid realization is\nshown in Fig. 4.4.\n\nIt is easily seen how this form results after\n\ncareful consideration of the form of the two difference equations that\nmust be implemented.\nThe actual construction of the canonical implementation will now\nbe discussed in more detail.\n\nThis form was chosen to be discussed\n\nbecause of several advantages it has over the direct progranmming form.\nOne of the primary reasons, which is obvious from Figs. 4.3 and 4.4,\nis that the canonical form is much more economical.\n\nFor example, the\n\ndirect form requires two A/D converters whereas the canonical requires\none.\n\nAlso, if n is the order of the numerator of the D(z) being realized\n\nand m the denominator, the direct form requires n + m delays whereas the\ncanonical requires the greater of n and m.\nD/A converters.\n\nThis is also the case for\n\nIt can be said that in general anytime a D(z) is to\n\nbe realized by a hybrid realization, choose the programming form which\nrequires the least hardware.\nLet us now discuss the analog and digital components of a hybrid\nimplementation.\n\nObserving the canonical realization of Fig. 4.4, the\n\nfirst step in a hybrid realization is theconversion of analog information into digital information by the use of an A/D.\n\nThere are two types\n\nof A/D\'s which might be used, the first being the successive approximation\ntype and the second the count-up-to type.\nFig. 4.5a is a block diagram of a successive approximation type\nA/D converter.\n\nThe comparator compares the A/D input signal m\n\nPreceding page blank\n\n(the\n\n3-36\n\neo (kT)\n\nFig. 4.4.\n\nBlock diagram of a hybrid realization of\na digital controller in canonical programming\nform.\n\n3-37\n\nLSB\nMSB\n\n(a)\n\n-\n\nLeast significant bit\nMost significant bit\n\nSuccessive approximation type\nA/D converter.\n\nLSB\nmq(kT)\n.SB\n\n(b)\n\nCounter type A/D converter.\n\nFig. 4.5.\n\nA/D converters.\n\n3-38\n\nsignal mo is the output of the feedback summing amplifier as shown in\nFig. 4.4) to the quantized output moq of a D/A converter.\n\nThe signal\n\nmoq is determined wholly by the digital word m(kT) which is in sign\nmagnitude code.\n\nThe logic circuitry is programmed to "search for" or\n\n"home-in on" the analog signal mo . Successive approximation type\nconverters are faster (commercial models are available that will convert\nan analog signal to an 8 bit sign magnitude code approximation in\n200 nsec) than the counter types which will be described shortly,\ntherefore they are used in a majority of applications.\nFig. 4.5b is a block diagram of the counter type A/D converter\nIn general it is slower than the successive approximationtype converter\nand is therefore used when the input analog signal m o is of lower\nfrequencies. This type of converter operates on the principle of\nletting a binary counter count until its output decoded through a D/A\nconverter is equal to the input signal.\n\nWhen this occurs the counter\n\nceases operation and its output bit sequence (m(kT)) at this time is\na sign magnitude code approximation of the analog input mo.\n\nBefore the\n\nconverter can be ready for the next conversion the binary counter will\nhave to be set such that all its bits are logic "0", with this being\ncompleted before each conversion.\nOf the two types of converters discussed above, it is recommended\nthat the successive approximation type be used for digital filter\nimplementations because of its ability to handle high frequency input\nsignals.\n\n3-39\n\nThe time delay indicated by Eqs. (4-9) and (4-10) can be\nprovided by clocked flip-flops.\n\nThis means the information at the\n\ninputs of the flip-flop is held or stored until the clock terminal\nis pulsed, at which time the information that is on the input is\ntransferred to the output terminals.\nThe D/A converters shown in Fig. 4.4 are the ladder type networks\ncommonly used in constructing D/A\'s.\n\nFrom the Figure it is seen\n\nthat the contents of the time delays constitute previous values of\nm(kT).\n\nThese digital words are decoded by the D/A converters into\n\nanalog signals and are then available for further analog processing;\nthat is, multiplication and summation.\nFrom Eqs. (4-9) and (4-10) it is seen that the ai and b i are\nreal numbers and may be positive or negative.\n\nThis suggests the use of\n\noperational amplifier circuits to perform the arithmetic operations of\nmultiplication and summation.\n\nFrom Fig. 4.6, which illustrates in\n\ndetail the summation and multiplication techniques for a second order\nimplementation in canonical programming form, it is seen that the\nalgebraic sign of the coefficients is obtained by inverting the output\nmiq of the ith D/A (multiplying by -1) so that miq and -miq are available.\nFrom Fig. III-6 it is also seen that the magnitude of the coefficients\nis obtained by variable input resistors to an operation amplifier.\nAny hybrid digital filter implementation should generally be\nimplemented in the same procedure as the above.\n\nIf there are variations,\n\nthey are usually small, and are left up to the individual designer.\n\n3-40\n1.\n\n1\n\nx\n\ny\n\n+\n\n+\n1\n\nA/D control\n\nTiming and\nControl\n\ny\nShift pulse\n\nFig. 4.6.\n\nSign of oef.\nSign of coef.\n\nHybrid implementation of a secondorder digital compensator.\n\n3-41\n\nDigital Implementation.\nThe adaptation of a small SP computer for the implementation of\ndigital filters only seems natural after a careful consideration of the\nrequirements that must be met for the realization of the difference\noperations of a particular programming form.\n\nLet us consider the\n\nrequirements of the difference equations of an arbitrary programming\nform since the requirements would hold for all forms.\n\nLet our choice\n\nbe the difference equations required for the realization of a D(z)\nin the modified canonical programming form.\n\nThese equations for a\n\nsecond order D(z) are shown below:\n\neo(kT) = a0 ei(kT) + alm(kT - T) + a2 m(kT - 2T)\n\n(4-11)\n\nm(kT) = ei(kT) - blm(kT - T) - b 2 m(kT - 2T)\n\n(4-12)\n\nObserving the above equations we see that for them to be physically\nrealized a device must be used which can add, subtract, multiply,\nperform time delay, truncate and provide data storage.\ncan do all of this is a small SP computer.\n\nA device which\n\nAll of the components of a\n\ndigital computer can be organized into four main functioned units as\nshown in\n\nFig. 4.7.\n\nConsidering the functional requirements of a digital filter, we see\nthat the Arithmetic Unit of the computer can perform the addition,\nsubtraction, multiplication, and the truncation required for the\nrealization of the digital filter.\n\nThe Memory can be used to perform\n\n3-42\n\nFig. 4.7.\n\nFour functional units of a digital computer.\n\n3-43\nthe time delay, coefficient storage, internal variable storage, and\ninput/output storage required for a filter implementation. The Input/\nOutput functional block of the digital computer will accomodate the\nA/D and D/A converters if required for interface for a digital filter\nrealization.\n\nThe Control Unit functional block of the digital computer\n\nwill accomodate the controller for the digital filter and the data\ntransfer logic which insures correct routing of data for the realization of the required difference equations.\n\nAt this point we see\n\nthat all of the arithmetic, storage, input/output, and control requirements necessary for the implementation of a digital filter have all\nbeen incorporated into the four functional blocks of a digital computer;\ni.e., a digital filter may be realized by a small SP computer and its\nfunctional diagram is shown in Fig. 4.8 [7].\n\nThe computer is said\n\nto be small because it will be designed to only calculate the difference\nequations for a particular programming form.\n\nAnother reason the resulting\n\nSP computer is small is because of reduced word lengths that are required\nthereby requiring less hardware.\nNow that it has been shown that a small SP digital computer can\nbe used for the realization of a digital filter, it is now appropriate\nto discuss how one would go about designing a SP computer realization\nof a digital filter and the considerations that must be made while doing\nthis.\nThe design consists of three parts:\n\nfirst, the determination of\n\nquantization levels in the computer (input quantizing, round-off errors,\n\n3-44\n\nTruncator\nInternal\nVariable\nStorage\n\nCoefficient\nStorage\n\nI\n\n,lI\nAdder\n\nli D-\n\nSubtracto\nK MultiplierI Arithmetic\nUnit\n\nOutput Storage\n\nAccumulator\n\nInput Storage\n\nInput\n\nI\n\nA\n\nI\nI\n\n_\n\n___\np\n\nI\n\nInput-Output Equipment\n\nFig. 4.8.\n\nFunctional diagram of a digital filter.\n\n3-45\n\nand filter coefficient quantizing); second, the logical design of the\ncomputers components; and third, the interconnection of the computers\ncomponents to implement the above mentioned quantization levels.\nThe first step in the design procedure, the determination of\nquantization levels, will not be covered here since it would be a\nreiteration of earlier sections of this work.\n\nAs a reminder\n\nthough, one could use several techniques to do this, among them being\nthe CAD program and different quantization error analysis techniques.\nNext consider the second step, which has not been presented\npreviously and will be discussed in depth.\nThe second step in the design of any SP computer implementation of\na filter, as previously mentioned, consists of the logical design of\nthe computer components.\n\nAll of the components will be grouped into\n\nfour main component groups with these being, 1) Input/Output components,\n2) Arithmetic components, 3) Memory components and 4) Controller\ncomponents.\n\nThe discussion will begin with the design of the input/\n\noutput components.\n1.\n\nInput/Output Components\nWhen designing the input/output equipment for a digital filter the\n\nfirst decision to be made is that of what type information will be the\ninput and output of the filter, i.e. is the input/output of the filter\ngoing to be in analog or digital form.\nIf the input and output of the filter is in digital form the design\nproblem will usually be minimal since the filter normally operates on\n\n3-46\n\ndigital inputs and outputs in digital form.\n\nThe only problem that might\n\nbe encountered if the application of the filter is such that it will\nhave\n\ndigital\n\ninput/output is that of synchronization between the device\n\nthat is supplying the filter its input and the filter.\n\nProvisions must\n\nbe made when designing the filter such that it will accept a digital\ninput word from a device which is supplying it.\nFor many applications of digital filtering, the\nfilter will be operating in an environment composed mainly of analog\nsignals which will necessitate input/output interface elements for the\nfilter.\n\nThese interface elements will be A/D converters for the input\n\nto the filter and D/A converters for the output of the filter as shown\nin Fig. 4.9.\nFirst let us discuss the selection of an A/D converter.\n\nThe first\n\ndecision to be made is how fast should its conversion speed be.\n\nIn\n\ngeneral this is dictated by the frequency of the input signal or the maximum sample rate of the filter.\n\nIf the input analog signal is of high\n\nfrequencies and the filter is to sample at a high rate the successive\napproximation type converter previously discussed usually would be\nbetter since it is faster than the counter type.\n\nNext, a\n\nconsideration of the number of bits required for the digital approximation of the analog signal would be required.\n\nThis can be determined\n\nby knowing the maximum analog input voltage (Vmax)\n\nto the filter and\n\nthe maximum allowable quantization step length h.\n\nIf these two parameters\n\nare known the number of quantization steps required can be determined by\n\n3-47\n\n(SB)\n\n,\n\n(SB)\n\n_W\n4\n\nMemory\nD _Controller\n\nV\n\nD\nA\n\nAnalog signal\ne i (kTJ\n\nFig. 4.9.\n\n\\\n\nD g ita l\ni\n\nS ig n a l \'eO T\ns\n~ n\n\nsignal\nl(k\n\nA digital filter with its interface elements.\n\n3-48\n\nV\nNumber of Quantization Steps =\n\n= QS\n\n(4-13)\n\nh\nKnowing QS, the number of magnitude bits (n) required for the A/D\nconverter can be chosen if n is the smallest integer such that\n\n(2n - 1) > QS\n\n(4-14)\n\nNow that the bit length and speed of the A/D converter is known,\nthe next step is the construction or selection of a commercial A/D.\nIn selecting a commercial A/D, there are many distributors available.\nAll one has to do to find them is to thumb through an electronics\nor computer oriented magazine.\n\nA few pointers to remember when\n\nselecting commercial A/D converters are that the faster their conversion\nspeed, the more they cost, and if internal reference voltages are supplied\nthey are also more costly than when the user supplies the references.\nIf a user were to construct his own A/D converter, its cost would\nalso be determined by its bit length, speed, and the construction of\ncircuits to supply reference voltages if they are not already available.\nThere is abundant material available on how to construct successive\napproximation and counter type converters.\n\nChosing the output interface element, the D/A converter, is generally\none of the easier design tasks of designing a digital filter.\n\nIf a D/A\n\nconverter is required, to construct an analog approximation\nof the digital output of a filter, there are two basic types from\n\n3-49\n\nwhich to choose.\n\nOne type has a current output proportional to the magni-\n\ntude of the digital word it is converting and the other type has a voltage\noutput.\n\nWhich type is chosen depends on the application of the digital\n\nfilter being designed.\n\nMost D/A converters are constructed from resistive\n\nlatter type networks.\nThere are many commercial types of D/A\'s on the market today and\ntheir manufacturers are easily found by simply, as for A/D\'s, thumbing\nthrough computer oriented magazines.\nThere are several characteristics of D/A\'s that must be considered\nwhen buying one.\nor bipolar.\n\nOne consideration is if the D/A will have to be unipolar\n\nSome D/A\'s will only operate in the unipolar or bipolar mode,\n\nand others can be connected to operate in both.\n\nAnother important\n\ncharacteristic to consider is the speed in which the conversion is made\nand the settling time of the D/A.\n\nIf operation of the digital filter is\n\nin the low frequency range, not as much attention will have to be paid\nto this as if it were operating in a high frequency range.\n\nAs anyone\n\nmight speculate, the faster the conversion time and settling rate, the\nhigher the price.\n2.\n\nArithmetic Unit\nWhen designing the arithmetic unit of a digital filter, the first\n\nmajor decision that has to be made is that of parallel arithmetic operations\nor serial arithmetic operations.\n\nBoth schemes have been proposed and both\n\nhave their advantages and disadvantages.\n\nThe question as to which method\n\nis better can only be determined by the designer and his use for his filter.\n\n3-50\nIn general parallel arithmetic operations are used when there is a\ndesire for speed and serial arithmetic operations are desirable for a\nminimum of hardware.\n\nTechniques of performing parallel and serial arith-\n\nmetic operations will be considered in more detail shortly.\nThe second decision that must be made when designing an arithmetic\nunit of a digital filter is that of what type binary code to use.\n\nSince\n\nthe arithmetic unit must be able to add, subtract, multiply and perform\ntruncation, it would seem logical to use a signed one\'s complement or a\nsigned two\'s complement binary code.\n\nThe reason for using these codes\n\nover a straight signed magnitude code is because, with them, subtraction\ncan be performed by an addition process.\n\nAlso, multiplication can be\n\nperformed by a shifting and addition process using these codes.\n\nOf the\n\nsigned one\'s complement code and the signed two\'s complement code it\nseems that a majority of the time the two\'s complement code is used.\nNow an example of an arithmetic unit organized in a parallel and\nserial fashion will be presented, starting with a parallel organization.\nThis particular arithmetic unit is able to add, subtract, multiply,\nand perform truncation.\n\nThe code used by the computer is a signed\n\ntwo\'s complement code.\nThe arithmetic unit performs the addition and subtraction process\nof two n bit words, A and B, by use of n full-binary-adders (FBA)\nconnected in the configuration shown in Fig. 4.10.\n\nIf addition is to\n\nbe performed (A + B), A and B are applied to the input of the adder circuit\nand the output will be the sum of A and B.\n\nIf the arithmetic operation\n\n3-51\n\nSB\n\nSn_ 1\n\nS3\n\nS2\n\nA = ASB, An_-1\nB = BSB, Bn_ 1 "\'\n\nFig. 4.10.\n\nS1\n\n\' AO\nB0\n\nFull-binary-adders used for addition and\nsubtraction of two\'s complement binary\nnumbers.\n\nso\n\n3-52\n(A - B) is performed, then A is applied as it is to the input of the\nadder and B is two\'s complemented and applied to the input.\n\nThe resulting\n\noutput of the adder is (A - B).\nNow that it has been demonstrated that we can add and substract\ntwo words in two\'s complement code with FBA circuits, an accumulator will\nbe defined and it will be shown that with an accumulator and shift\nregisters, all the arithmetic operations of addition, subtraction and\nmultiplication can be performed.\nA binary accumulator consists of a register, which stores a binary\nnumber (the augend) in signed-magnitude form and upon receiving another\nbinary number (the addend) in the same form adds the second number to the\nfirst and then stores the sum in the register.\n\nThe logic diagram of a\n\nparallel binary accumulator is shown in Fig. 4.11.\nin the accumulator functions as a modulo 2 counter.\n\nEach flip-flop\nThe augend is\n\ninitially stored in the accumulator and, during the addition process,\neach flip-flop counts parallel incoming pulses representing the addend\nbits and generates a carry pulse to the next significant bit when the\nflip-flop changes its state from 1 to 0.\nTo illustrate the use of the parallel binary accumulator in performing\nthe multiplication and summation process, observe the difference equation\n\neo(kT) = aoei(kT) + clm(kT - T).\n\n(4-15)\n\n3-53\n\ns29\n\nLSB\n(Least Significant\nBit)\nnificant\n\nsO\n\n(Bit\n\nSB\n\n(Sign Bit)\n\nClear\n\nQ\nAccumulate\nPulse\nU2\n\nLSB\n\nFig. 4.11.\n\nU1\nMSB\n\nu\n\nParallel binary accumulator.\n\nSB\n\n3-54\n\nLet\n\na0\no= +1.25 = +(1.01)2\ncoefficient\ncl = +0.50 = +(0.10)2\n\n++\n-\n\n0\n\n-* 1\n\nei(kT) = +6. = +(110.)2\n\nvariable\nm(kT - T) = -3. = -(011.)2\n\nthen\n\n+110.\n+1.01\n+110.\n+ 00.0\n+ 1.10\n+ 111.10\n\naoei(kT) =\n\nclm(kT - T)\n\n=\n\nx\n\nand eo(kT) = + 111.10 - 001.10 = +110.0 = +(6.)10\n\nx\n\n-011.\n+0.10\n-000.\n- 01.1\n- 0.00\n-001.10\n\n.\n\nA simplified block diagram of a shift register and parallel binary\naccumulator implementation of the example difference equation is included\nin Fig. 4.12.\n\nThe following sequence of events is offered to explain\n\nthe operation of this implementation.\n\n1.\n\nSet the accumulator output to zero.\n\n2.\n\nLoad cl[+0.10] and m(kT - T)[-011.] into the two shift registers.\n\n3.\n\nApply an accumulate pulse, a shift pulse, another accumulate\npulse, a second shift pulse, and a third accumulate pulse\n(abbreviate this sequence by asasa; the output of the accumulator is now clm(kT - T)[-001.10].\n\n4.\n\nClear the shift registers.\n\n3-55\n\nShift Register\nLoad Coefficients\n(SB\n\nMSB\xc2\xb7SB\n\nLoad Variables\n\nB\n\n~MSB\n\nLSB\n\nLSB\n\nAccumulate Pulses\n\n(BP) Binary Point\n\nFig. 4.12.\n\nImplementation of an example difference equation.\n\n3-56\n\n5.\n\nLoad a0 [+l.01] and ei(kT)[+ll0.] into the two shift registers\nwhile leaving clm(kT - T) shared at the accumulator output.\n\n6.\n\nasasa; the accumulator output is now aoei(kT)\n+ c1 m(kT - T)[+110.00].\n\nThus the difference equation is implemented.\nThere are other designs for accumulator of Figs. 4.11 and\n4.12.\n\nOne design which is used frequently uses FBA\'s and clocked\n\nJ-K flip-flops.\n\nThis accumulator design is shown in Fig. 4.13.\n\nThis\n\naccumulator operates in the same manner as the previously discussed\ndesign when used in calculating difference equations.\n\nThe FBA\'s are\n\nused to add the new input to the accumulated sum already in the accumulator\n(stored at the output of the JK flip-flops) and once the new sum is\nobtained an accumulate pulse is applied which clocks the J-K flip-flops\nand causes the input bits of the flip-flops to be transferred to the\noutputs where it will be stored until the next accumulate pulse arrives.\nAs stated previously, serial arithmetic may also be used for digital\nfilter implementation.\n\nLet us consider it now.\n\nSerial arithmetic is used mainly for two reasons:\n\n1) there is\n\na savings in hardware which will be demonstrated shortly and 2) serial\narithmetic provides for an increased modularity and flexibility in the\ndigital circuit configurations.\n\n3-57\n\nInput to accumulator\n\n/\n\nA\n\nLSB\n\nMSB\n\n/\nOutput of accumulator.\n\nFig. 4.13.\n\nParallel binary accumulator\n- constructed from FBA\'s and\nclocked J-K flip-flops.\n\n3-58\nThe two\'s-complement representation of binary numbers is appropriate\nfor digital filter implementation using serial arithmetic because additions may proceed, starting with the least significant bits, with no\nadvance knowledge of the signs or relative magnitudes of the numbers\nbeing added.\nThe serial arithmetic unit, as the parallel implementation, must\nbe able to add, subtract, and multiply and truncate.\nThe block diagram of a serial adder (subtractor) is shown in\nFig. 4.14.\nBriefly it operates in the following manner.\n\n1.\n\nAll registers and the delay flip-flop are cleared.\n\n2.\n\nWords A and B, which are to be added or subtracted, are\nshifted into the shift registers to the left of the FBA\nwith their binary points aligned.\n\nThe shifting stops when\n\nthe LSB of A or B reaches the LSB position in the register.\nAll registers are now properly set to perform the addition\n(subtraction) process.\n3.\n\nNow shift all registers to the right and the delay flip-flop\nfor the carry at the same time.\n\nThe proper sum of difference\n\nwill be shifted into the register on the right.\n\nIf subtraction is being performed by the above circuit, the subtrahend will have to be two\'s complemented before it is loaded into its\nappropriate register.\n\nThis operation can be performed with a simple\n\n3-59\n\nAugend or Minuend\nRegister\n\nSB\n\nMSB\n\nLSB\n\n. 44\n___ad\na\n\nC\n\nCc\n(.on\n\nA s\n\n~-J~B0MSB\nFBA\n\nB\n\n=.SB\nM\n\n--\n\nSum\n\nSum (Difference)\nI L--LSB\\\n\niLSB--\n\nShift Register\nShift registers\nShift and add (sub) lead\nAddend or subtrahend register\n\nFig. 4.14.\n\nA serial adder (subtractor) circuit.\n\nK\n\n3-60\n\nsequential circuit which, for each input word, passes unchanged all\ninitial least significant bits up to and including the first "1" and\nthen inverts all succeeding bits.\n\nThe circuit that will do this is\n\ndepicted in Fig. 4.15 [18].\nA serial multiplier configuration is shown in Fig. 4.16.\n\nThis\n\nmultiplier will only multiply two positive two\'s complement numbers,\nwhich does not restrict it, since the circuit of Fig. 4.15 can be\nused to two\'s complement any negative number that must be used in the\nmultiplication process.\n\nAlso, one\'s complement numbers may be multiplied\n\nby this circuit since if they are negative they may be complemented by\na simple circuit to obtain the positive form.\n\nIf this is done for a\n\ntwo\'s complement or a one\'s complement code, the sign bit of the resulting\nproduct will have to be retained and if it is negative, the resulting\nproduct term will have to be complemented appropriately.\nBriefly, the serial multiplier of Fig. 4.16 works in the following\nmanner.\n\nThere are three shift registers Z, X, and Y, a serial adder\n\nand a half adder.\n\nThe sign bit leads the serial word as indicated by\n\nthe subscripts of the letters Z, X, and Y.\n\nAll bits except bits X1 and\n\nY1 of registers X and Y form a combined shift register.\n\nRegister Z is\n\nalso a circulating register.\nInitially, the multiplier is stored in register Y with the sign bit\nin Y1 .\n\nNext the multiplicand is serially transferred to register Z with\n\nthe sign bit in Z.\n1\n\nSign bits Z1 and Y1 do not move during the succeeding\n\nshifting of the contents of registers.\n\n3-61\nInput\n\np-u-t-- - Input\n\nQ\n\n>J\nClock\n\n--\n\n0\n\nC\nK\n\nReset\nFig. 4.15.\n\nSerial two\'s complementer.\n\nFrom two\'s\ncomplementer\ndevice which\nmakes sure\nnumbers are\npositive\n\nFig. 4.16.\n\nA serial multiplier.\n\n3-62\n\nThe value of 1 or 0 of the least significant bit of the multiplier\nin Y6 determines whether or not the number bits of the multiplicand\nare to be added; and the addition, if there is one, is carried out.\nAlso during this addition time, the circulating register Z restores its\noriginal contents and the partial product is serially inserted into\nregister X, occupying bits X2 to X6 .\nshifted\n\nThe combined register is then\n\n1-bit to the right; during this shift, any carry bit left in\n\nthe delay flip-flop is shifted into X2 , and the least significant bit\nis now at Y2 .\n\nThe next addition begins at X 6 but not at bit Y2 .\n\nAfter the right shift, the least significant bit of the multiplier\nin Y6 is lost, as Y1 is not a part of the combined register.\n\nY6 now\n\ncontains the second least significant bit of the multiplier, which\npossibly could initiate another multiplication.\nThis process of addition and right shifting continues until all\nmultiplier number bits are shifted out of the combined register.\n\nAfter\n\nthis, the product is available in the combined register with the most and\nthe least significant halves of the product being stored respectively\nin the X and Y registers.\nbe in Y1 .\n\nWhen there is no round-off the sign bit will\n\nWhen there is round-off, 1 is inserted into bit X6 , and the\n\nsign of the product is inserted into bit X1 .\n\nAfter this the number in\n\nregister X is in desired order [24].\nWhen using the multiplier described above, the product term addition\nrequired for the completion of the difference equation calculation process can be handled by the serial adder previously described.\n\n3-63\n\nThere are other techniques of parallel and serial multiplication\nthat will not be discussed here.\n\nOne technique which makes parallel\n\nmultiplication much faster is the Wallace technique described in [25].\nLikewise, there is a serial multiplication technique presented in [18]\nwhich makes serial multiplication much faster.\nUsually when a difference equation is calculated by a digital\nfilter, it is reduced in bit length before it is stored in memory or\nthe output register.\n\nIt is the purpose of the reduction logic of a\n\ndigital filter to do this.\n\nMost reduction logic either performs\n\ntruncation or round-off with most being truncation type.\nThe circuitry required for reduction logic is usually simple since\nit is usually a combinational logic circuit.\n\nShown in Fig. 3.14\n\nis reduction logic which will truncate a signed magnitude binary word\nwith 14 magnitude bits (8 to the left of the binary point and 6 to the\nright) such that it has 6 bits to the left of the binary point and 2\nto the right.\n\nThe lower four bits that are truncated are omitted from\n\nthe truncated word and anytime the untruncated word has a weighted bit\nin one of the two most significant bit positions, every bit of the\ntruncated word will be a weighted representation, i.e. the truncated\nword is saturated.\n3.\n\nMemory Design.\nThe memory of a digital filter is used for coefficient storage,\n\ninterval variable storage which performs time delay, input word storage\nand output word storage.\n\n3-64\n\nUn-Truncated\nInput Word.\n\nTruncated Output\nWord\nwith Saturation\n\nBP\nFig. 4.17.\n\n-\n\nBinary Point\nReduction Logic\n\n3-65\n\nThere are two types of memory that are usually used for digital\nfilter implementations:\n\n1) flip-flop type memories, and 2) read-only-\n\ntype memories (ROM).\nA majority of the time flip-flop type memories are used for internal\nvariable storage (m(kT - T), m(kT - 2T),\'--,m(kT - nT)) which is required\nfor the implementation of the time delays and also for the storage of\nthe input and output variables of the filter.\n\nShown in Fig. 4.18 is\n\na flip-flop type memory for the storage of m(kT - T) and m(kT - 2T)\nrequired for the realization of a second order D(z) in the modified\ncanonical programming form.\n\nThe word lengths in this figure are 8\n\nmagnitude bits with 6 of them to the left of the binary point.\n\nFor\n\nthis type memory construction a time delay is performed when the J-K\nflip-flops are clocked; m(kT) becomes m(kT - T) and simultaneously\nm(kT - T) becomes m(kT - 2T).\n\nThe input and output storage registers\n\nare constructed the same way as the first column of flip-flops in\nFig. 4.18.\nROM type memories are used mostly for coefficient storage.\n\nThe\n\ncoefficient values are loaded only once into the ROM and they are\nread out when they are needed for an arithmetic operation.\nDigital filters may also employ simple single pole, double throw\nswitches for coefficient storage.\n\nThis is usually done for versatility\n\nas the coefficients may be changed by a simple toggle of a switch.\nThe only disadvantage of this is that the coefficients cannot be changed\n\n3-66\nm(kT)\n\nm(kT-T)\n\nM(krT-21T)\n\nr/\'\n\nClock\n\nFig. 4.18.\n\nInternal variable storage for a second order\nD(z) in modified canonical programming form.\n\n3-67\n\nwhile the filter is operating in real-time.\nthen started over again.\n\nIt must be stopped and\n\nThe reason for this is that the switches\n\ncan\'t all be manually thrown during one sample period which would be\nrequired.\n4.\n\nController Design.\nThe controller of a digital filter ensures the proper calculation\n\nof the difference equations being realized by the filter.\n\nAs an\n\nexample of what a controller must do, let us consider the calculation\nof the difference equations of a second -order D(z) in the modified\ncanonical programming form.\n\nThe difference equations are shown below\n\neo(kT) = aoei(kT) + clm(kT - T) + c2 m(kT - 2T)\n\n(4-16)\n\nm(kT) = ei(kT) - blm(kT - T) - b 2 m(kT - 2T)\n\n(4-17)\n\nEq. (4-16) will be calculated first as it is desirable to obtain\nan output as soon as the input is available to the filter.\n\nOne\n\npossible operation sequence that the controller may assume for a\nparallel arithmetic realization using an accumulator is listed below:\n\n1.\n\nActivate the A/D so that ei(kT) can be obtained.\n\n2.\n\nShift the m(kT - T) and m(kT - 2T) storage registers to\nperform time delay.\n\n3.\n\nClear the accumulator and its associated shift registers.\n\n4.\n\nLoad cl and m(kT - T) into their respective shift registers.\n\n3-68\n\n5.\n\nPerform the multiplication clm(kT - T).\n\n6.\n\nClear the accumulator shift registers.\n\n7.\n\nLoad c2 and m(kT - 2T) into the shift registers.\n\n8.\n\nPerform the multiplication c2 m(kT - 2T).\n\n9.\n\nClear the shift registers.\n\n10.\n\nLoad ao and ei(kT) into the shift registers.\n\n11.\n\nMultiply aoei(kT).\n\nThe output of the accumulator now contains\n\naoei(kT) + clm(kT - T) + c2 m(kT = 2T) = eo(kT).\n12.\n\nClear the accumulator and the shift registers.\n\n13.\n\nLoad -bl and m(kT - T) into the shift registers.\n\n14.\n\nMultiply -bl m(kT - T).\n\n15.\n\nClear the shift registers.\n\n16.\n\nLoad -b2 and m(kT - 2T) into the shift registers.\n\n17.\n\nMultiply -b2 m(kT - 2T).\n\n18.\n\nClear the shift registers.\n\n19.\n\nLoad ei(kT) and 1.0 into the shift registers.\n\n20.\n\nMultiply l.Oei(kT).\n\nThe output of the accumulator is now\n\nei(kT) - blm(kT - T) - b2 m(kT - 2T)\n\n=\n\nm(kT).\n\nRepeat same process again.\n\nTo ensure the correct sequence of operations that must be performed\nand the correct transfer of data such that there will be data available\nwhen required, the controller is divided into two parts:\n\n1) the control\n\n3-69\n\nfunction generator and 2) the data transfer logic.\n\n\'Tet ccintrol\n,\n\nFlitlctllo\n\ngenerator is simply a logic circuit which has as its output a sequence\nof pulses (with their timing and spacing very important) which controls\nthe operation of the arithmetic unit, input/output, and memory.\n\nThe\n\ndata transfer logic is simply combinational logic circuitry which, under\ncommand from the control function generator, transfers data to and from\nthe memory, input/output, and arithmetic unit.\nThe first step in designing any controller is the selection of an\noperation sequence, such as the previous example.\n\nAfter this is\n\ncomplete, the control function generator and then the data transfer\nlogic may be designed.\nNow that an approach has been presented by which all the functional\ncomponents of a digital filter may be designed, the only remaining\ndesign consideration remaining is the interconnection of all four\nfunctional components such that they may function as a digital filter.\nThe interconnection of the functional units may be done in numerous\nways.\n\nNo specific approach will be given here since, in most cases,\n\neach designer of a digital filter has what he thinks is his own unique\nand novel way to interconnect the functional components.\n\nIn general,\n\nthe connections of Fig. 4.8 must be made in as modular fashion as\npossible for possible LSI realizations.\n\nIf they vary it will be because\n\nof programming form variation and order of D(z) variation.\n\n3-70\n\nNow that a basic SP computer implementation of a digital filter\nhas been discussed, it will be in order to discuss variations of the\nSP computer implementation.\n\nImplementation by Microprogrammable SP Computer.\nThe three previously discussed implementation techniques all\nhave limitations, the most noticable of these being that each type\nimplementation will realize a D(z) in only one programming form.\n\nIt\n\nwould be advantageous to design a digital filter which would realize\na D(z) in any of the eleven previously discussed programming forms.\nIn answer to the question that may arise as to why is one programming form better than the other; it can be shown, as discussed\npreviously, that for a particular D(z) with set coefficients and\nsampling rate, different programming forms have different quantization\nerrors.\n\nIn general, for a particular D(z) that must be realized, it\n\nis desirable to choose the programming form that introduces the least\nquantization error, therefore necessitating the need for a digital\nfilter implementation that can realize a D(z) in several programming\nforms.\n\nThe implementation approach taken to do this is a microprogram-\n\nmable design as discussed in [12].\nIt will be the purpose of this section to give a discussion of\nthe computer organization and not to go into too much detail about the\nlogic design since it is not necessary for an understanding of the operation of the microprogrammable implementation.\n\n3-71\n\nThe prime function of the SP computer is the realization of a\nsecond-order digital filter in a choice of digital filter programming\nforms.\n\nAs for the previous implementations this calls for the solution\n\nof the appropriate difference equations and entails the operations of\naddition, multiplication and time delay.\n\nThe SP computer is binary,\n\nsynchronous and parallel, with the calculations to be done using 2\'s\ncomplement arithmetic.\n\nIt is a stored program computer, i.e., the\n\nControl Unit looks up the sequence of instructions in the memory,\ninitiates arithmetic operations and causes operands or immediate results\nto be transferred between the Arithmetic Unit and the Memory, as\nrequired by the program instructions.\n\nSynchronization of the computer\n\noperations and generation of control pulses is the task of the Control\nUnit.\nTo increase the sampling rate of the filter, a high speed Wallace\nMultiplier is employed as described in [12].\n\nAlso a rapid-access\n\nmemory will be used for the program memory to aid in decreasing the\nworst case delay between the time of input sampling and its corresponding\noutput.\nA block diagram illustrating the four basic functional units of\na digital computer is as shown in Fig. 4.7.\n\nTherefore, just as for\n\nthe previously described digital implementation of a digital filter, the\norganization of the SP computer will be divided into these four units.\nA detailed block diagram of the SP computer is shown in Fig. 4.19.\nThe functional blocks in Fig. 4.19 which make up the Input/Output Unit\n\n3-72\n\nControl\nPulses\n\nei(t)\n\nFig. 4.19.\n\neo(t)\n\nBlock diagram of a microprogrammable digital\nfilter implementation.\n\n3-73\n\nof Fig. 4.7 are the A/D converter along with its input register and\nthe D/A converter and its output register.\n\nIn many cases the input\n\nregister is considered as a part of the A/D converter and is not shown\nseparately.\n\nThe Memory is composed of the coefficient storage, variable\n\nstorage, the program memory, and their appropriate memory address select\nand data select networks.\n\nThe coefficient register, variable register,\n\naccumulator register, high-speed multiplier and the reduction logic\nnetwork comprise the Arithmetic Unit.\n\nIncluded in the Control Unit\n\nare the sample clock, fundamental clock, binary counter, timing level\ngenerator, control matrix, the instruction register and its code\ntranslator.\n\nIn addition to the four basic functional units which are\n\ngenerally used to represent digital computers, the SP computer in\nFig. 4.19 employs an\n\nOperator Control Unit.\n\nThe Operator Control\n\nUnit is used to program the SP computer for a particular filter form\nand also allows the operator to load the coefficients of the transfer\nfunction, D(z), into the coefficient storage locations.\n\nThe Operator\n\nControl Unit can also be used in the testing and trouble shooting of\nthe computer.\nAt this point it would be desirable to present a brief description\nof the operation of the SP computer.\n\nFor purposes of illustration,\n\nassume that a known transfer function, D(z), is to be realized and that\na specific filter programming form has been selected.\n\nWith reference\n\nto the functional blocks of Fig. 4.19, the programming form is\nchosen by setting the 4 bits in the program register to the proper values\n\nK\n\n3-74\n\nas will be defined later.\n\nNext the constants for the difference equations\n\nare manually written into the coefficient storage after switching the\nmemory control switch, S, to the "1" position.\n\nAlso, to control the\n\noutput reduction logic, load the shift key words into the variable\nmemory which are called by the quantize instruction to provide the\ndesired number of accumulator bits to be input to the D/A or to be\nwritten into the variable storage.\n\nThe programming of the SP computer\n\nis complete and it now awaits the first input signal.\nNormal operation begins as the sample clock gates the translated\ncode of the program register into the program memory address register.\nThe memory address register contains the address of the first instruction\nneeded to calculate the difference equations for the filter programming\nform chosen.\n\nStored in the program memory in groups of consecutive\n\naddresses are the macro-instructions for all filter programming forms.\nFor example, the first seventeen locations in the program memory are\nthe macro-instructions for the direct programming form.\n\nUpon receiving\n\na read pulse, the program memory loads the 16-bit instruction register.\nThe instruction format is shown in Fig. 4.20.\n\nIts four MSB\'s comprise\n\nthe op code (operation code), the next six bits contain the first\noperand address (coefficient address), and the last six bits contain\nthe second operand address (variable address).\n\nAfter the op code is\n\ndecoded into one of the eleven available macro-operations, the Control\nUnit generates a corresponding sequence of micro-operations. Thus,\neach macro-operation is built up as a sequence of elementary microoperations.\n\n3-75\n\nOp Code\n\nSecond Operand Address\n\nFirst Operand Address\n\n16-Bit Instruction Register\n\nFig. 4.20.\n\nMacro-instruction format.\n\n3-76\n\nThe remainder of the discussion of a micro-programmable realizations\nwill be devoted to the specification and organization of the four basic\nfunctional blocks of Fig. 4.7.\n1.\n\nInput/Output Unit.\nAs before, it is the task of the input interface element to digitize\n\nthe input analog signal ei(t) in the A/D converter and furnish this\ndigital signal ei(kT) to the SP computer.\n\nThe output, eo(kT), is in\n\ndigital form and is converted to a discrete analog output, e0 (t), by\nthe output interface element, the D/A converter.\nA/D Conversion.\n\nThe A/D converter will be such that it may trans-\n\nform ei(t) into a two\'s complement binary representation and thus will\nbe used as the input interface of the SP computer.\n\nWord size may vary\n\nfrom several bits up to sixteen bits including the sign bit.\n\nIt is\n\nassumed that an A/D converter which meets the resolution and speed\nrequirements of the SP computer is available.\nA 16-bit input register is used to hold the A/D converter output.\nThe input register functions to maintain constant values for the input\ndata bits during the period they are used.\n\nAfter each conversion the\n\nA/D converter produces an end-of-conversion (EOC) signal which is used\nto load the new digital word into the input register.\n\nIf the A/D\n\nconverter wordlength is smaller than sixteen bits, the remaining bits\nof the input register must be filled in with the sign bit or zeroes.\nD/A Conversion.\n\nSince the D/A converter is the output interface\n\nelement, the selection of a D/A converter type is determined wholly by\n\n3-77\n\nthe requirements placed on its output signal by the external system.\nThis may lead to a D/A converter which converts a specified number of\nbits into either a unipolar or bipolar analog signal, the external\nsystem may require a pulse-width-modulated.voltagesignal, or even yet,\nmay require a digital input, whereby a D/A converter is not needed.\nFor the particular micro-programmable digital filter being discussed,\nthe D/A was chosen so that a bipolar analog voltage may be presented\nat the output.\n2.\n\nArithmetic Unit.\nIt is the purpose of the Arithmetic Unit.to perform the multiplica-\n\ntion and accumulation operations required to solve the difference\nequations of the digital filter programming forms described earlier.\nA major portion of the Arithmetic Unit consists of a high-speed\nmultiplier.\n\nInputs to the Arithmetic Unit are the multiplier (coefficient\n\nregister), the multiplicand (variable register), and the accumulator\nregister.\n\nBoth.multiplier and multiplicand inputs are 16-bits long\n\nand are coded using the two\'s complement representation.\n\nThe output\n\nof the high-speed multiplier is a 34 bit, two\'s complement number which\nis stored in the accumulator register.\nHigh Speed Multiplier and Accumulator Register.\n\nThe data input\n\nregisters (coefficient and variable) and the data output register\n(accumulator) are organized to permit the multiplication of the contents\nof the input\n\nregisters and to add the results to the contents of the\n\naccumulator.\n\nThe simplified block diagram of the Arithmetic Unit in\n\n3-78\n\nFig. 4.21 will be used to explain the calculation of an example\ndifference equation.\nis gated.\n\nNote that the input to the accumulator register\n\nThis is necessary since the output of this register is fed\n\ndirectly into the multiplier, i.e., the next state of the accumulator\nregister is dependent not only on the multiplier inputs but also its\nown present state.\n\nThus once the output of the multiplier reaches a\n\nstable state the data is gated into the accumulator register.\nThe example difference equation requires two multiplications,\nwith the results of each added to the contents of the accumulator\nregister.\nis cleared.\n\nBefore starting the calculations, the accumulator register\nIts initial contents are denoted by (Acc) 0 .\n\nNote that\n\nthe accumulator register supplies an input to each column (which corresponds to a Wallace multiplier tree) of the array.\n\nThis increases the\n\nsize of the multiplier structure slightly but eliminates the time delay\nof an adder network which would other wise be necessary to add the\ncontents of the accumulator and the multiplier output.\n\nAfter the\n\nfirst multiplication, the result is gated into the accumulator, becoming (Acc)l.\n\nFor accumulation of the second product, alei(kT - T),\n\n(Acc)1 and the partial products from the AND gate array are used as\ninputs to the free network.\n\nThe result,\n\ne0 (kT) = (12/16)(10/16) + (-10/16)(-6/16) = 180/256\n\nis gated into the accumulator register after sufficient time for the\ninputs to propogate through the Wallace multiplier trees.\n\n3-79\n\n!\n\nCoefficient Register\n\nI\n\nVariable Register\n\nI\nHigh-Speed\nMultiplier\n\n___t___\n\nAccumulator Register\n\nI_____\n\nGate Multiplier\nOutput to\nAccumulator Register\n\nV\nExample difference equation:\neo(kT) = aoei(kT) + alei(kT - T)\nlet\n\nao = 0.75 = 12/16 = 0.1100\nal = -0.625 = -10/16 = 1.0110\nei(kT) = 0.625 = 10/16 = 0.1010\nei(kT - T) = -0.375 = -6/16 = 1.1010\n\n0.1100\n0.1010\n0.00000000\n0.00000000\n0.0001100\n0.000000\n0.01100\n0.0000\n0.01111000\n\n(Acc)0\n\n(Acc)1\n\n= 120/256\n\nFig. 4.21.\n\n1.0110\n1.1010\n0.01111000\n0.00000000\n1.1110110\n0.000000\n1.10110\n0.1001\n1\n0.10110100\n\n(Acc)l\n\n(Acc)2 = 180/256\n\nCalcuation of example difference\nequation by Arithmetic Unit.\n\n3-80\n\nThe first multiplication and accumulation operation is straightforward since both the multiplier and multiplicand are positive numbers.\nHowever, this is not the case for the second operation.\n\nNote, that if\n\nthe product of the multiplier bit and the multiplicand sign bit is a\n"one", it must be repeated in the left-most positions.\n\nIn the first\n\noperations, this product was "zero" in all cases and thus the leftmost fill-in positions contain all zeroes.\n\nAlso, note that the\n\nnegative multiplier in the second operation requires that the last\npartial product term be the two\'s complement of the multiplicand.\nThis is accomplished by taking the one\'s complement of the multiplicand\nand forcing a "1" into the Wallace tree for the LSB of this partial\nproduct.\n\nThis procedure is taken care of by the partial product\n\ngeneration logic of the high-speed multiplier and is presented in\ndetail in [12].\nEvery phase of the multiplier has been demonstrated, except the\nestablishment of the length of the accumulator register.\n\nMultiplying\n\ntwo 16-bit, sign two\'s complement numbers yeilds a 31-bit product.\nSince the direct digital filter programming form requires the summation\nof the greatest number of products (5) in the solution of any single\ndifference equation, this sets the required length of the accumulator\nregister at 34 bits.\n\nThis means that if two maximum size 16-bit\n\nnumbers are multiplied and accumulated five times, a 34-bit register\nwould be required to express the sum.\n\n3-81\n\nReduction Logic.\n\nThere are several register lengths in the SP\n\ncomputer which need to be analyzed; the 34-bit accumulator register,\nthe 16-bit data locations in the variable storage, and the N-bit D/A\nconverter.\n\nThis is brought about by the use of the solution of the\n\ndifference equation in later calculations or as an output, eo(kT).\nIn the first case, this 34-bit solution must be stored in a 16-bit\nlocation.\n\nTherefore it is necessary to quantize the output of the\n\naccumulator register to 16 bits.\n\nIn the second case, it is also\n\nnecessary to reduce the word length, since a 34-bit D/A would be both\nexpensive and impractical.\n\nThus the need for reduction logic has been\n\nclearly established.\nThe number of bits in the output variable remaining after quantizing\nthe contents of the accumulator register is determined by the size of\nthe D/A converter.\n\nFor an N-bit D/A converter the reduction logic may\n\nselect the first N least significant bits (LSB\'s), the first N most\nsignificant bits (MSB\'s) or any intermediate group of N bits.\nThis selection of output data bits is accomplished by writing\ninto the variable storage\n\na shift key word for each quantize instruction\n\nin the program memory for a particular programming form.\n\nPart of the\n\ninstructions will contain the address of this shift key, which is read\nfrom storage and gated into the reduction shift register, the contents\nof which determine those bits of the accumulator register to be loaded\ninto the D/A or the variable storage.\n\nThis is done by choosing the\n\nappropriate bits of the accumulator and shifting these bits to the\nleft until they occupy those positions with output lines.\n\n3-82\n\n3.\n\nMemory.\nEach of the sections of memory shown in Fig. 4.19 provides\n\nstorage for a specific type of data.\n\nThe memory, as previously dis-\n\ncussed, is not only used for storage purposes, but is also utilized\nto perform the time delay operations which are required to calculate\nthe difference equations.\n\nIncluded in the description of the memory\n\nsections will be the memory address registers, the address select\nlogic and the data select logic.\nCoefficient Storage.\n\nUpon selection of a transfer function and\n\na filter programming form, it is necessary to store the proper filter\nconstants and coefficients to be used in the solution of the difference\nequations.\n\nThe coefficient storage is a high-speed Read/Write memory\n\nwhich is composed of two memory modules.\nstorage locations for sixteen 8-bit words.\n\nEach module has addressable\nProper connection of the\n\nmodules yields 9 (16 x 16)-bit memory as shown in Fig. 4.22.\nData is manually written into the coefficient storage locations\nthrough the use of the data register, the memory control switch, manual\naddress load pulse, and the manually controlled write pulse on the\ncontrol panel.\n\nThe address select logic in Fig. 4.22 is necessary\n\nto allow the coefficient address to be chosen from the control panel\naddress register when manually writing coefficeint values into memory\nor the first operand address portion of the instruction register when\nreading coefficient values from memory.\n\n3-83\n\nManual Address Register\nMemory Control Switch\n\nFirst Operand Address\n\nManual Address\nLoad Pulse\nData Input\n|\n\nI\n\n.I\n| Coefficient Register\n\nFig. 4.22.-\n\nI\n\nCoefficient storage.\n\n16 Bits\n\n3-84\n\nThe organization of the variable storage, as\n\nVariable storage.\n\nshown in Fig. 4.23, is similar to that of the coefficient storage.\nIts function is not only that of storing the input, ei(kT), and the\noutput, eo(kT), but also that of performing time delay.\n\nAssume the\n\ninput sample, ei(kT) and its previous value, ei(kT - T), are stored in\nmemory.\n\nAfter the last computation involving ei(kT - T) is completed,\n\nei(kT) is written into the memory location allocated to ei(kT - T)\nwhere it waits until the next sampling period to be used as ei(kT - T);\nthus the time delay operation is performed.\nNote that the variable storage requires both data and address\nselect logic.\n\nThis is due to the variable storage being used by a\n\nmultiple of sinks and sources.\n\nData inputs to the variable storage\n\nmay come from either the control panel, A/D, variable register or the\nreduction logic.\n\nThe variable storage may be addressed by the control\n\npanel or the second operand address portion of the instruction register.\nProgram memory.\n\nThe program memory functions as a storage location\n\nfor the macro-instruction necessary to solve the difference equations\nof the various filter programming forms.\n\nThese instructions are\n\norganized into groups; each group corresponds to a particular\nprogramming form.\n\nWithin each group, the macro-instructions are\n\nsequentially arranged as needed in the solution of the particular set\nof difference equations.\nThe program memory is a high-speed ROM that has 256 words of 16\nbits each.\n\nEach word (macro-instruction) is broken into three sections:\n\n3-85\nMemory Control Switch\nSecond Operand Address\nManual Address Register\n\nI\n\nManual Data\nA/D\nVariable Register\nI\n\nAddress Load\nPulse\n\nFig. 4.23.\n\nVariable storage.\n\nReduction Logic\nf\n\n3-86\n\nthe op code (4 bits), the first operand address (6 bits) and the second\noperand address (6 bits).\n\nThe op code specifies the operation to be\n\nperformed on the operands in the memory located at the addresses\nspecified by the two address fields.\n\nFor op codes which require only\n\none operand (or no operand), that portion of the program memory is\nblank, i.e., contains any combination of zeroes and ones.\nis the store input instruction.\n\nAn example\n\nHere the op code is "0101", the coef-\n\nficient address is blank and the variable address contains a 6-bit code\nspecifying the storage location in the variable storage which is to\nreceive the input data.\nSince the program memory contains groups of macro-instructions\nfor all filter programming forms, it is necessary to be able to locate\nthe first address in each group once a form has been chosen.\n\nThe\n\ncontents of the program register is translated into the program memory\naddress of the first macro-instruction for each filter programming\nform.\n\nNext, the sample clock gates the translated code into the program\n\nmemory address register (PMAR) and the first macro-instruction is\naccessed by a memory read pulse.\n\nFirst and last in every sequence of\n\nmacro-instructions is a start A/D instruction.\n\nThis is necessitated\n\nby the overlapping of the instruction and execution cycle.\nIn order to program the SP computer, it is necessary to code\neach digital filter form using 4 bits.\n\nTable 4.1 presents the coding\n\nscheme for the filter being described in [12].\n\n3-87\n\nTABLE 4.1.\n\nProgram Register Contents\n\nProgram Code\n\nDigital Filter Programming Form\n\n0000\n\nDirect\n\n0001\n\nModified Direct\n\n0010\n\nStandard\n\n0011\n\nModified Standard\n\n0100\n\nCanonical\n\n0101\n\nModified Canonical\n\n0110\n\nParallel\n\n0111\n\nCascade\n\n1000\n\nModified Cascade\n\n1001\n\nStructure XI\n\n1010\n\nStructure XII\n\n1111\n\nTest Mode\n\n3-88\n\nAs an example of the sequence of macro-instructions in the program\nmemory, consider the direct programming form with the difference\nequation,\n\neo(kT) = aOei(kT) + alei(kT - T) + a2 ei(kT - 2T)\n\n(4-18)\n-bleO(kT - T) - b2 eo(kT - 2T)\n\nTable 4.2 presents a word description of the macro-programming\ninstructions for this form.\nNotice that during the time the A/D is converting the analog\nvoltage signal to a digital signal\n\nthe computer is calculating the\n\nportions of the difference equations that do not require the digitized\ninput, ei(kT).\n\nThis is intended to minimize the time delay between the\n\ninput sample and its output response.\nThe macro-instruction "Store (variable)" means load the contents\n(where "contents" is denoted by the enclosing parentheses) of the\nvariable register into the specified address of the variable storage.\nStore (reduction logic) means to write the output of the reduction\nlogic into the variable storage.\n\nIn order to permit the use of A/D\n\nconverters with different conversion rates, a "wait on A/D" instruction\nis incorporated.\n\nIf the converison is complete at the time that this\n\ninstruction is reached, the next instruction "Store input" is executed;\notherwise, the computer idles until it receives the end of conversion\nsignal form the A/D.\n\nHowever, there may be instances when a high-speed\n\n3-89\n\nTABLE 4.2.\n\nOp Code\n\nProgram Memory Contents for Direct Form\n\nFirst Operand Address\n\nSecond Operand Address\n\nStart A/D\nClear Accumulator\nMultiply & Accumulate\n\na2\n\ne i (kT - 2T)\n\nMultiply & Accumulate\n\na1\n\nei(kT - T)\n\nStore (variable)\n\nei (kT - 2T)\n\nMultiply & Accumulate\n\ne0 (kT - 2T)\n\nMultiply & Accumulate\n\neo(kT - T)\n\nStore (variable)\n\ne0 (kT - 2T)\n\nWait on A/D\nStore Input\nMultiply & Accumulate\n\nei(kT - T)\na0\n\nei(kT - T)\n\nQuantize (Acc)\n\nShift Key\n\nStore (Reduction Logic)\n\ne0 (kT - T)\n\nQuantize (Acc)\n\nShift Key\n\nLoad D/A\nReset RMAR & Halt\nStart A/D\n\n3-90\n\nA/D converter is employed.\n\nIn this case the input ei(kT) is stored\n\nand not utilized in the calculations until all other multiplications\nare performed.\n\nThis may also be an unnecessary delay in the response\n\nof the filter.\n\nThus, it may be appropriate to postpone the "Start\n\nA/D" instruction in the program to ensure that the "Wait on A/D"\ninstruction places the computer in an idle state.\nThe codes for the macro-instructions are listed in Table 4.3.\nThis table is used to generate Table 4.4, which presents the actual\ncontents of the program memory, the variable storage and coefficient\nstorage for the direct filter programming form.\n\nAddresses for the program\n\nmemory are encoded in octal in Table 4.4.\nNote that in Table 4.4 the first and second operand address\nfields are 6-bits, while the actual memories (coefficient storage and\nvariable storage) have only 4-bit addresses.\n\nThus after loading\n\nthe instruction register only the four right-most bits of each of the\naddress fields are used as addresses for reading the contents of the\ncoefficient and variable storages.\n\nNotice, also, the shift key word\n\nin the variable storage at location, 0100.\n\nThis shift key is requested\n\nby the quantize op code, 0111, and is loaded into the reduction shift\nregister.\n\nA shift key word exists for each quantize instruction.\n\nSpace is allocated in the program memory for the remaining programming forms.\n\nThey will not be illustrated since one may get an\n\nidea of their structure from observing the direct programming example.\n\n3-91\n\nTABLE 4.3.\n\nMacro-Instruction Op Codes\n\nOperation\n\nOp Code\n0000\n\nStart A/D\n\n0001\n\nClear Accumulator\n\n0010\n\nMultiply & Accumulate\n\n0011\n\nStore (variable)\n\n0100\n\nWait on A/D\n\n0101\n\nStore Input\n\n0110\n\nLoad D/A\n\n0111\n\nQuantize (Accumulator)\n\n1000\n\nStore (Reduction Logic)\n\n1001\n\nHalt\n\n1010\n\nReset PMAR and Halt\n\nNot Used\n\n1111\n\n3-92\n\nTABLE 4.4.\n\nMemory Contents for Direct Form.\n\nProgram Memory\n\n(Address)\n8\n\nOp Code\n\n000\n001\n002\n003\n004\n005\n006\n007\n010\n011\n012\n013\n014\n015\n016\n017\n020\n\nAddress\n0000\n0001\n0010\n0011\n0100\n0101\n\n0000\n0001\n0010\n0010\n0011\n\nFirst Operand\nAddress\n\nSecond Operand\nAddress\n\n000000\n000001\n\n000000\n000001\n000000\n\n0010\n0010\n0011\n0100\n0101\n\n000010\n000011\n\n000010\n000011\n000010\n\n0010\n0111\n1000\n0111\n0110\n1010\n0000\n\n000100\n\n000001\n000001\n000100\n000011\n000101\n\nCoefficient Storage\n\nAddress\n\na2\nal\n-b2\n-ba\nao\n0\n\n0000\n0001\n0010\n0011\n0100\n0101\n\nVariable Storage\nei(kT\nei(kT\neo(kT\ne0 (kT\nShift\nShift\n\n- 2T)\n- T)\n- 2T)\n- T)\nKey\nKey\n\n3-93\nProgram Memory Address Register.\n\nMacro-instructions retrieved\n\nfrom the program memory are read from memory addresses contained in\nthe program memory address register (PMAR).\nthe code translator and the control unit.\n\nInputs to PMAR came from\nData from the code translator\n\nis loaded into PMAR by the sample clock, whereas, upon receiving the\ncontrol pulse, "up date PMAR", PMAR performs the operation\n\n(PMAR) + 1\n\n-\n\nPMAR,\n\nand now contains the address for the next macro-instruction. Since\nthere are 240 addresses in the program memory, the PMAR must be eight\nbits long.\n4.\n\nControl Unit\nThe program for a digital computer consists of a set of machine\n\noperations such as addition and multiplication.\n\nInstructions for these\n\noperations have been referred to as macro-operations. Inside the SP\ncomputer these operations are further decomposed into a set of elementary\noperations called micro-operations.\n\nCount, shift, gate the memory\n\naddress register, are examples of micro-operations. Normally, in\ngeneral purpose computers, macro-instructions are at the programmer\'s\ndisposal and may be readily changed by re-writing the program.\n\nHowever,\n\nas previously described, in this SP computer the macro-instructions are\npre-programmed in the program memory.\n\nThese instructions are sequentially\n\nread from the program memory and loaded into the instruction register.\nIn the op code portion of the instruction register is a 4-bit code\n\n3-94\nwhich specified the macro-instruction to be executed.\n\nThis op code\n\nis fed to the control unit of the SP computer and a sequence of microoperations is performed for each op code.\nIt is the purpose of the Control Unit to translate the op codes\nand supply all synchronization and control pulses to the rest of the\nSP computer.\n\nA block diagram of the Control Unit elements is shown in\n\nFig. 4.24.\n\nThese elements include the instruction register, a decoder,\n\na four-state counter, a timing level generator, a control matrix, a\nThe organization and\n\nfundamental clock and an operation flip-flop, D.\n\nfunction of each of these elements is discussed below.\nInstruction register and decoder.\n\nAs macro-instructions are re-\n\ntrieved from the program memory, they are stored in the instruction\nregister until the instruction is executed and a new one is retrieved.\nOnly the op code portion of the instruction register is employed by the\nrest of the Control Unit, while the operand addresses are sent to the\ncoefficient and variable memory address registers.\nThere are eleven macro-instructions which are used by the SP computer\nto solve the difference equations of the various digital filter programming\nforms.\n\nEach op code portion of a macro-instruction is translated by\n\nthe decoder into one of the macro-operations, fi, i = 0, 1, 2, "\'\n\n10.\n\nFor each op code, the decoder activates one and only one output line.\nFundamental clock.\n\nEmployed as a fundamental clock is a free-\n\nrunning multivibrator whose frequency is obtained after determining a\nbasic timing cycle for the SP computer, which can be done after the\n\n3-95\n\nU(\n0\n\n0\nU)\n\'a\n\n~~ ~\n\n4,10\n\n~\n\n~\n\nV\n*,\n\n~ ~\n\n4.\'\n(4.4\n\nla\n\n4\n\nN\n-4\n\nc\n*0\n\ne\n\na\n\n-4\n\n0\n\nl\n\nu\n0\n\n-4\n0\n\n4o\n\nC\n0\n\ns-\n\n0\n\nv\n4\nv,\n\nco\n\n4,\n\nO\n\n0\nCu\n\n-4\n\ncur\n\nC\n\n0\n\nCu\n\n\'\xc2\xa5\n\nU\n\n.,,\n\n\'a\n\nC:\n\nCu\n,-0.\n-4\n4-\'\n,--\n\no\n\nlIw\nC4-4\n\nI4-i\n\nI -4\nco\nIx\n\no\n\n3-96\n\ndetailed logic design of each functional unit is complete.\n\nNote that\n\na clock signal that is 1/4 as fast as the fundamental clock is used as\ninput to the Control Unit.\n\nThis is because one of the micro-operations,\n\nquantize (Acc), is a serial operation and requires these high frequency\npulses in order to avoid slowing the operation of the SP computer.\nFour stage counter and timing level generator.\n\nThere are fourteen\n\nmicro-operations (from which eleven sequences of micro-operations are\nformed for the eleven macro-operations), and the control signals for\nthese micro-operations are designated as mj, j = 0, 1, -\'\', 13.\n\nThe\n\nnumber of timing levels for the execution of the macro-instructions\nwill require six states of the four stage counter, L, for all instructions\nexcept multiply and accumulate, whose last micro-operation is performed\non state fourteen.\n\nThus this macro-instruction is used as a control\n\ninput to the counter.\nControl matrix.\n\nBasically, the function of the control matrix\n\nis to provide the proper combination of op codes and timing levels.\nEssentially, the control matrix is an AND-OR switching matrix.\nOperation flip-flop.\n\nControlling the operation of the SP computer\n\nis the operation flip-flop, D.\n\nIf D is set, clock pulses enter the\n\nfour stage counter under normal operations.\n\nFlip-flop D may be set by\n\nthree signals, the manual start, the end-of-conversion (EOC) signal\nfrom the A/D and the sample clock pulse.\n\nIt may be reset by the control\n\npulses, wait on A/D, halt, and reset PMAR and the manual stop button.\nNote that when the computer is halted by reset PMAR, the first instruction\n\n3-97\n\nto be executed is already in the N register ready for execution.\n\nCare\n\nmust be taken to ensure that EOC does not occur before the wait on\nA/D signal during the execution of a given filter programming form.\n5.\n\nOperator Control Unit.\nAs mentioned earlier, the Operator Control Unit is used in manually\n\nprogramming the SP computer to realize the transfer function, D(z), in\nthe selected digital filter programming form.\n\nReferring to Fig. 4.19,\n\nthe elements comprising the Operator Control Unit are the manual data\nregister, the manual memory address register, two write pulse circuits\n(one each for the coefficient and variable storage), a program register,\na memory control switch, a manual start pulse circuit, a manual stop\npulse circuit, two pulse circuits for manually loading the coefficient\nand variable memory address registers, a row of sixteen indicator lights\nand a monitor switch, and a sample clock enable switch.\nEach of the registers may be implemented with single pole-doublethrow (SPDT) switches for ease of setting and resetting.\nData may be entered into either the coefficient or variable storage\nby setting the manual memory control switch to the "1" position, setting\nthe switches of the manual data register to the binary data value,\nsetting the address into the manual address register, loading this\nvalue into the coefficient or variable memory address register, and\npressing the coefficient or variable write button.\n\nNotice the data\n\nregister and memory address register are common to both the variable\nand coefficient storage since each employs a separate load address and\nwrite pulse.\n\n3-98\n\nPrevious discussion has described the functions of the program\nregister, manual start button and the manual stop button.\n\nThe indicator\n\nlights and monitor switch function in checking the operational status\nof the SP computer.\nFrom the above discussion, one should understand the basics of a\nmicroprogrammable digital filter implementation.\n\nIf further details\n\nare desired, one may refer to [12].\nNext to be discussed will be the aspect of timesharing digital\nfilter implementations.\n\nTime-Sharing of a Digital Filter Implementation\nFor many applications of a digital filter it is sometimes necessary\nto provide discrete-time filtering for several independent loops or\nchannels with examples being in control systems and digital communications\nsystems.\n\nThis may be accomplished with several digital filters but it\n\nwould be more practical, more reliable, and more economical to provide\nthis filtering with a special-purpose computer organized and programmed\nas a time-shared digital filter [ 9].\nThere are two basic ways in which a digital filter may be timeshared.\n\nThe first of these is in multi-loops and the second is for\n\nhigher order D(z) realization.\nFig. 4.25 illustrates a digital filter being time-shared in N\ncontrol loops.\n\nIt is seen that there is an enormous amount of hardware\n\nsaved by doing this.\n\nFirst there is only one computing element (SP\n\n3-99\n\nv (t)\n2\n\neol(t)\n(t)\nltiplexed\nA/D\n\n__\n\nTime-Shared\nSpecial-Purpose\nComputer\n\nD/A\n\n(t)\n\ne\n02\n\nvn(t)\n\nD/A\n\ne~n(t)\nn\n\nFig. 4.25.\n\nBlock diagram of a digital filter being\ntime-shared in multiple loops.\n\n3-100\n\ncomputer) required and second only one A/D is required, whereas if\ntime-sharing weren\'t used N computing elements and N A/D converters\nwould have been required.\nFig. 4.26 illustrates a digital filter being time shared for\na higher order D(z) realization.\n\nThis can be done by cascade or\n\nparallel methods as shown in parts (a) and (b) of the figure.\n\nCascading\n\nor paralleling filters to obtain a higher order realization is preferred\nbecause of the word length problem (resolution) that is encountered\nfor a single high order D(z) realization.\n\nTime sharing of a single filter\n\nas shown in Fig. 4,26 can be easily accomplished if the assumption is\nmade that n is even and that D(z) can be factored into n/2 second-order\nfilters Fl(Z),\n\n\'\', Fn/2(z) and partial fractional into n/2 second-\n\norder filters Pl(z), P 2 (z), "\'Pn/2(z).\n\nEach of the second-order filters\n\nmay be realized by any of the available programming forms.\n\nThe total\n\ntransfer function D(z) still has the same memory requirements, the same\nnumber of memory locations in each filter storage unit, but now the\nvariables and coefficients of each second-order filter are the signals\nstored.\n\nSince each second-order filter has the same program, the\n\ncontrol sequence generator just repeats the same sequence n/2 times\nduring each sampling interval.\nBoth cascade and parallel techniques are desirable methods for\nrealizing higher order D(z)\'s, but the additional summing junction\nnecessary with the parallel method makes the cascade method better\nsuited for a modular realization.\n\n3-101\n\neo(kT)\n\nI,\n\nD(z)\n\n(a)\neo(kT)\n\n(b)\nFig. 4.26.\n\n(a) Cascade and (b) parallel methods of\nrealizing a higher order D(z).\n\n3-102\n\nFor a better understanding of time-shared digital filter implementations, let us look at the organization of a SP computer realization\nof a 2nd order time shared digital filter which may be time shared in\ntwo loops or cascaded or paralleled for the realization of a 4th order\nD(z).\nFig. 4.27 is a functional block diagram of the SP computer\nrealization of two digital filters.\n\nLike previous SP computer realiza-\n\ntions, the computer functions are still divided into four main categories;\ninput-output equipment, memory unit, arithemtic unit, and control unit.\nThe input-output equipment provides the interface between the analog\nsystem and the digital computer.\n\nThe analog inputs are sampled via the\n\nmultiplexer and A/D, and the output samples are demultiplexed and shared\nin the buffered D/A\'s.\nThe arithmetic unityfor this type realization, also must be able\nto perform multiplication, addition and subtraction.\n\nThe selection\n\nof an arithmetic unit must first entail the selection of serial or\nparallel type arithmetic operation.\n\nUsually this choice means a decision\n\nmust be made between computational speed and hardware economy.\n\nThe choice\n\nmade here was a reasonable fast and economical method where a parallel\nbinary accumulator and two shift registers are used to add up the product terms as previously discussed.\n\nSerial arithmetic is used for\n\npartial product multiplication in the implementation discussed in [18].\nAs before, the reduction logic truncates to maintain word-length\ncompatibility, and saturation logic sets the output word to maximum\nvalue when its range is exceeded.\n\n3-103\n\nThe memory is divided into two identical storage units for filter-1\nstorage and filter-2 storage.\n\nCorresponding filter signals such as\n\nel (k - 1) (short notation for el(kT - T)) and e2 (k - 1) have the same\nrelative storage locations in their respective storage units and have\nthe same addresses within their respective modules.\n\nThe memory is\n\ncontrolled by Read and Write commands (not shown) and two address\ncommands:\n\na filter address to select the proper storage unit, and a\n\nsignal address to select the proper signal location in a storage unit.\nThe filter selection logic is controlled by the filter address and\ndetermines which storage unit is addressed by the signal address.\n\nThis\n\nmodular arrangement of the memory is .ideally suited for the "wired-OR"\nfeature of integrated circuit memory.\n\nWith this, feature storage for\n\nadditional filters can be added by hard-wiring inputs and outputs\nof the storage units and simple modification of addressing.\nThe control unit contains the master control unit that provides\nthe multiplexing by means of the filter address and determines the\nsampling rate for each filter by controlling the start of each differenceequation computation.\n\nThe control unit also contains the control-sequence\n\ngenerator which provides a sequence of instructions, initiated by a\npulse from the master controller, to control\ncomputations.\n\nthe difference-equation\n\nThus the hard-wired program of the control-sequence\n\ngenerator determines the programming forms of the filters.\n\nTo maintain\n\nsimplicity, the same programming form is chosen for each filter, and\nhence the same sequence is generated each time regardless of which filter\n\n3-104\n\nFilter I Storage\n\nel(k)\n\nE.:Y\nU:;IT\n\n;\ni\'0\n\n|el(k-)\n\n21\n\nvlgk)\nI\n\nJnl\n\ne.2 (k)\n\nbb!2\nl|\n\n"1\n\ne j(k)\n\ne2(k-,n)\n\n-b21\n-bfl |\n\n_\n\n_\n\nIL-L -\n\nFig. 4.27.\n\n_\n\n12\n\nV2!b)\n22\n\n|\n\na22\nan2\n\nb22\n|\n\n_\n\nAddress\n_I_ _\n\nI\n\n2\n\ne2(k)\n\n!\n\n.\n\nI\n\nFitecr 2 Storaeg\n\n_\n\n_\n\n_\n\nCPTE~L\'\n- raivm\n-,\n\ni\n\n.\n_I__\n\nAddress\n_\n__\n\nJii~iiL\n\n.i\'1~~:\n\n_\n\n___\n\n_J\n\nI\n\n-UE:\n\nBlock diagram of the time-shared\nrealization of two digital filters.\n\n3-105\n\nis being addressed and regardless of the number of filters being realized.\nA priority system is provided so that once the computation of a difference\nequation has begun, it is carried to completion even if it means that a\nsample for another filter must be omitted.\n\nRange Switching Digital Filter Implementation\nFor many applications of digital filtering, it is desirable to have\na very fast sampling rate.\n\nAn example of this is when very high frequency\n\nsignals are being filteredland since the sample rate must be at least\ntwice the highest signal frequency, it is seen that very high rates\ncan be required.\n\nThe limiting factors for a filter\'s sampling rate\n\nare the conversion speed of the A/D and D/A input-output system and\nthe time required in the arithmetic calculations of the difference\nequations being realized.\n\nThe most important factor which limits the\n\nspeed of arithmetic calculations is the bit-lengths of the data processed\ninternally by the filter.\n\nThis includes the length of the input word to\n\nthe filter, the internal variable wordlength (length of ei(kT - nT),\nm(kT - nT), etc.) and the output wordlength.\n\nIn general, the shorter\n\nthese wordlengths are the faster the arithmetic calculations can be\nperformed, with this being true for serial and parallel arithmetic\ntype filters.\n\nAlso, as a result of shorter wordlengths, the filter\n\nhardware is reduced.\nBecause of the quantization error introduced by shorter wordlengths,\na scheme must be devised to eliminate the larger quantization errors\n\n3-106\n\nintroduced by the shortened words.\n\nA method was devised to do this\n\nand the resulting filter was called a "Range-Switching" digital filter\nas described in [8].\n\nIn short, the filter with reduced wordlength\n\nperformed as if it had a much longer wordlength, under certain conditions.\nA block diagram of the filter described in [81 is shown in Fig.\n4.28.\n\nIts operation will now be described.\n\nThe A/D converter converts the analog input signal into an 8-bit\ndigital approximation allowing the digital output of the converter to\nhave an integer value ranging from 0 to 255.\n\nThe input select logic\n\nselects either the four MSB\'s or the four LSB\'s of the A/D output to\nbe the input to the SP computer.\n\nThe scheme used is to select the four\n\nLSB\'s if all the four MSB\'s are logic "0" or the four MSB\'s if either\none is a logic "1" for a signed magnitude code.\n\nIt is seen from this\n\nthat the input to the filter is in either of one or two ranges.\n\nIf\n\nthe input to the SP computer is from the lower range (four LSB\'s),\nit may have any integer value between 0 and 15 in steps of 1.\n\nIf\n\nthe input is in the higher range (four MSB\'s) it may have any integer\nvalue between 16 and 255 in steps of 16.\n\nOf the two ranges, it is\n\nseen that the higher range has the larger quantization step h and it\nis 16 times that of the lower quantization step.\n\nThis also means\n\nthat a four bit combination coming from the higher range input would\nrepresent a magnitude 16 times the same bit combination combing from\nthe lower range input as shown in Fig. 4.29.\n\nBecause of this,\n\nbefore the special purpose computer can process its four bit input each\nsample period, it must know the range from which it comes to properly reweight\n\n3-107\n\n(8 bits)\n\n(8 bits)\n\n(4 bits)\n\n(12 bits)\n\nei(t)\n\nFig. 4.28.\n\nBlock diagram of a "range-switching"\ndigital filter.\n\n-01\n\nO0\n\n-\n\n0\nO0\n1\n\n-\n\n0\n\nA/D\n\n0\n\nFig. 4.29.\n\nThe two magnitudes ranges.\n\nThis four bit combination\nhas a magnitude of 128 or\n16 times that of the lower\nfour bit combination.\nThis four bit combination\nhas a magnitude of 8.\n\n3-108\n\nthe internal variables if a range change is seen from the last sample\nperiod.\n\nThe internal variables of the SP computer are 8-bits long and\n\ncan possibly be multiplied or divided by a factor of 16 each sample\nperiod before calculation of the difference equations start or their\nweight may remain the same.\n\nIf there is a change in the input from the\n\nhigher to lower range the internal variables are multiplied by a factor\nof 16\n\n(the relative magnitude change of the input).\n\nIt must be\n\nremembered that this is being done to keep the weight of the input\nrelative to the internal variables.\n\nIf the input represents a small\n\nvalue, the internal variables must be increased to make the input appear\nsmall.\n\nThe opposite of this takes place when there is a change from\n\nthe lower to the higher range.\n\nIn this case the internal variables\n\nmust be made to appear small to the large input, so they are divided\nby a factor of 16.\n\nIf there is no range change between sample periods,\n\nthe weight of the internal variables remains the same.\nlogic in Fig.\nof the filter.\n\n4.28\n\nThe output select\n\nis employed to properly weight the 12-bit output\n\nThe output will have its greatest weight when the input\n\nfor a particular sample period was in the higher range.\n\nIf the input\n\nfor a particular sample period is in the lower range, the output select\nlogic weights the output bit configuration such that its analog voltage\nlevel representation is 1/16 of that of the same bit configuration with\nthe input in the higher range.\nIt was mentioned earlier that the "range-switching" scheme was\nused to reduce wordlengths but at the same time obtain the accuracy at\n\n3-109\n\nthe filter output of a much longer wordlength filter.\n\nAlso, the "range-\n\nswitching" filter might be thought of as a technique by which for a\nfixed wordlength input, the quantization errors are reduced by the\nrange-switching process.\nFilters of the "range-switching" design seem to have a bright\nfuture, especially with the advent of LSI.\n\nUsing a reduced wordlength\n\nmodular filter design, it would be very easy to have a digital filter\ncomposed of four or five LSI chips.\nOne application of "range-switching" filters that has great promise\nis that of filtering in nulling type control loops.\n\nFirst, the design\n\nsaves hardware because of the reduced wordlengths, it is fast and because\nof the design of the filter, the quantization step length decreases as\nthe loop error is nulled toward zero giving the loop finer granularity\ncontrol to keep the error signal closer to zero.\nA block diagram illustrating the components of the SP computer\nrealization of the "range-switching" digital filter is shown in Fig.\n4.30.\nThe input/output unit consists of a successive approximation type\nA/D converter, a 12-bit D/A converter and the input/output select logic\nwhich are combinational circuits that select the input word and insert\nthe output word into the proper bit position of the D/A.\nmentioned the output of the filter is an 8-bit word.\n\nAs previously\n\nThe 12-bit D/A\n\nconverter is required such that the 8 output bits can be inserted into\nthe 8 MSB positions of the D/A when the input is in the higher range\n\n3-110\n\n_ _\n\n_ _\n\nMemory\n\n__\n\n_\n\n_\n\n_,_\n\n__\n\nCoefficient\nStorage\n\n_\n\n_ _\n\n-IS\n\n_ _ _\n\n_ __\n\n_._\n\nm(kT-T) _-_m(kT-2T) _\n\n_ _ _\n\ne o (kT)\n\nlS\nI ao, - r -bA\n\nControll\nUnit\n\nData\nTransfer\nLogic\n\nShift\nRegisters\n\nArith.\nUnit\n\nLF_-\n\nInput\n\n_-\n\n-\n\n-\n\n-\n\nA/D\n\n1\xe2\x80\xa61J\nI\n1\nj\nII\n\nI\n\nB\n\n-\n\nParallel\nI\nBnary\nAccumulator\n-\n\nFig. 4.30.\n\n-\n\n- -\n\n-\n\n-\n\n-\n\n-\n\nSelect\nLogic\n.\n.......\nI ev\n\n_\n\nControl\nFunction\nGenerator\n\n-----Il I\n\n-\n\nj\n\n-\n\n-\n\n1\n\nReductior\nLogic\n\nI\n\n-\n\nSelect\nLogic\n.\nI_\n\nut\nOutRt\n\nD/A\n.....\n\n.\n\nI\n\n, _ _ _ _ _ _~~~~~~~~~~~\nFunctional diagram of a range adaptive filter.\n\nF\n\nI\n\nI~~~~~~~~~~\n\n3-111\n\nand in the lower 8-bit positions of the D/A when the input of the filter\nis in the lower range.\n\nIn effect, a particular bit combination that\n\nis inserted into the lower 8-bit positions will have an analog voltage\nlevel 1/16 of what it would have been if inserted into the 8 MSB\npositions.\n\nIt should be noted that the weighting factor of the output\n\nis the same as the input.\nParallel fixed point arithmetic is used in the arithemtic unit\nin conjunction with shift registers, an accumulator, and the reduction\nlogic to calculate the difference equations.\nThe memory is composed of the coefficient storage (SPDT Switches),\ninternal variable storage (flip-flop registers), and the output storage\n(flip-flop register).\n\nThe weighting of the internal variable memory is\n\nunder command of the controller.\n\nSince fixed point arithmetic is used,\n\nmultiplication or division by 16 is easily accomplished by shifting the\ninternal variables left or right respectively four places relative to\nthe binary point.\n\nIf a weighted bit is lost when left shifting, provisions\n\nare made for the 8-bit internal variable to be saturated (all one\'s for\na signed magnitude code).\nIn concluding the discussion of the hardware implementation, the\ncontrol unit is composed of the control function generator and the data\ntransfer logic.\n\nIn addition to the normal tasks of the control function\n\ngenerator, it has the duty of deciding how the internal variables must\nbe weighted each sample period.\n\n3-112\n\nA common question arises as to what is the limit on bit-length\nreduction.\n\nThe most general answer to this is it depends on the appli-\n\ncation of the filter.\n\nThe method of determining the minimum bit length\n\nis the trial and error technique\nconfiguration it is to be used.\n\nof simulating the filter in whatever\nAs an example, the above described\n\n"range-switching" filter was used in a pendulous integrating gyroscopic\naccelerometer control loop.\n\nTime simulations in FORTRAN of the loop\n\nwith the filter inserted demonstrated that the loop could be stabilized\nfor pulse inputs to the loop with the filter having a 4 magnitude bit\ninput, 6 magnitude bit internal variables and a 8 magnitude bit output.\nFrom these bit lengths it is seen that an LSI realization of the filter\nwould be quite small and simple.\nAt the present time further work is being done to investigate the\npossibility of further bit length and hardware reductions of a digital\nfilter such that LSI implementations will even be more attractive.\n\nLSI Circuit Digital Filter Implementation\nThe first complete LSI implementation of a digital filter was\ndesigned and built by Autonetics Division of North American Rockwell,\nAnaheim, California.\n\nConcerning the state-of-the-art of digital filter\n\nimplementation techniques, the design was the ultimate.\n\nThe design\n\nis completely modular and therefore it is easily adaptable for an\nLSI realization.\n\nThere are 2 main chips, a serial-parallel multiplier\n\nchip and a shift register chip.\n\n3-113\n\nThe serial-parallel multiplier chip is arranged such that it can\nperform all arithmetic functions required for a digital filter implementation:\n\naddition, subtraction, and multiplication.\n\nThe shift register\n\nchip contains shift register memories for the internal variables, input\nword, output word and also the control circuitry of the filter.\nThe interface elements of the LSI implementation are external to\nthe filter.\n\nThe A/D output is fed into the filter in a serial manner\n\nand the filter output is also serial.\n\nThe binary code used by the\n\nfilter is two\'s complement.\nThe internal wordlengths of the filter are adjustable.\n\nTo change\n\nthem, all one has to do is to make connection changes on the chips.\nThe filter as designed has fixed length coefficients and each being\n16-bits long.\n\nThey are easily set by single pole double throw switches.\n\nThe filter realizes any first, second, or third order D(z) which\nhas real poles in the parallel programming form.\n\nCommercial Digital Filters\nNow that several digital filter implementation techniques have\nbeen presented, a short discussion of digital filters available on the\ncommercial market will be in order.\n\nBecause of the relative newness\n\nof the area and the recent advent of LSI circuitry, there are few\ncommercial builders around, two of which are listed below.\nOne of the first builders of a digital filter for the commercial\nmarket was the Rockland Systems Corporation of Blauvelt, New York.\n\nThey\n\n3-114\nnow produce a line of programmable recursive digital filters which meet\na wide variety of signal processing requirements.\n\nAll of their filters\n\nare composed of four basic components - adders, multipliers, shiftregister delays, and memory with a modular approach being adapted to\nprovide the greatest possible flexibility and efficiency as is described\nin [18,19].\n\nThe basic components are usually combined into second-order\n\nbuilding blocks (two poles and/or two zeroes) and these blocks are then\ncombined or multiplexed to realize any number of filters of any desired\norder.\n\nProgrammability is achieved by employing a read/write coefficient\n\nmemory.\n\nFixed filter characteristics may be obtained with a read-only\n\nmemory.\n\nRockland produces a series of filters designed in the above\n\nmanner.\nRockland also produces a programmable tenth-order recursive digital\nfilter which can realize arbitrary "all pole" designs such as Butterworth,\nBessel, or Chebyshew low-pass, high-pass, or band-pass filters.\n\nUp to\n\n10 pole positions can be programmed through ten 12-bit filter coefficients;\nwhile up to 10 zero\'s can be positioned at DC or the Nyquist frequency,\nor can be deleted altogether.\n\nSampling rates of up to 100 KHz can be\n\nachieved.\nElectronic Communications, Inc. (ECI) is the second commercial\nbuilder\n\nof digital filters to be discussed [26].\n\nThe filters produced by ECI are the nonrecursive type and they are\nactually signal-processing instruments that perform the convolution\nintegral in order to effect a filtering function.\n\nThis means the filters\n\n3\n\n3-115\n\nwork strictly in the time domain and is represented by its impulse\nresponse and not by its amplitude and phase characteristics as are some\nnonrecursive filters.\n\nThe analog input signal to this filter must be band-\n\nlimited, and is accomplished by using a low-pass analog prefilter.\n\nThe\n\ninput bandwidth is restricted to less than half the sampling rate.\n\nOne\n\nof their more common filters with a sample rate of 10 KHz has a preamplifier cut-off at 2.5 KHz.\nThe ECI digital filter has two identical shift register memories.\nOne stores samples of the band-limited input signal, while the other\ncontains samples of the impulse function of the filter that is desired.\nThe sampled impulse response is represented by coefficients that are\nthe various amplitudes of samples spaced equidistant along the time axis.\nThe coefficients are obtained by using computer software available\nfrom the company and programmed into the digital filter via a paper\ntape.\n\nThe tapes are set up so that the programs can be put on a time-\n\nshared computer system.\n\nUp to 200 coefficients can be stored in the\n\nsampled-impulse-responsememory.\nThe contents of the two memories are fed into a single multiplier\nsection that forms the product of corresponding samples form each\nmemory.\n\nThe output of the multiplier goes to an accumulator that puts\n\nout the digitized filtered version of the input waveform.\nThe digital filter designed in this manner can only simulate zeroes\nfor the filter transfer function because of its nonrecursive nature.\n\nThis\n\ndoes not seem to limit its use though, as any filter can be represented\n\n3-116\n\nby its impulse response.\n\nThere are few recursive filters whose impulse\n\nresponse cannot be satisfactorily represented by ECI\'s nonrecursive filter.\nThis concludes the discussion on SP computer implementations. FFT\nhardware realizations will now be discussed.\n\nV.\n\nFFT HARDWARE\n\nWe have previously seen that a digital transfer function, D(z),\nmay be calculated by the FFT.\n\nThe theory behind this was discussed,\n\nenabling the discussion of FFT hardware to now follow.\n\nThree area\'s\n\nof FFT hardware will be discussed starting with commercial FFT processors that are available and concluding with a discussion of the\nMIT fast digital processor.\nCommercial Equipment\nThere are several manufacturers of FFT processors.\nis the Raytheon Computer Co. of Santa Ana, California.\nmanufactures\n\nOne of these\nThis company\n\nwhat it calls an "Array Transform Processor." This\n\nprocessor has several capabilities, among them FFT and Inverse FFT\nprocessing, convolution integral processing, complex.multiplying,\ncomplex spectral magnitude processing, real multiplying, read add/\nsubtract processing, scanning of arrays and array movement.\nThe Raytheon array transform processor is an auxiliary processor\nof the Raythean 700-Series computers and is a hardware array processor.\nIt comes in several models with the models differing in the number of\ndata points that can be handled (256 - 16384) and the wordlengths available.\nAnother company which manufactures a hardware FFT processor is\nthe Elsyter Co. of Syosset, New York.\nthe 306/HFFT.\n\nTheir processor is labeled as\n\nIt will calculate the direct or inverse FFT and it is\n\ncontained within the mainframe of its host NOVA 800 computer for more\n3-117\n\nI\n\n3-118\n\nefficiency and small size.\n\nIt has a core memory of 4096-16 bit words\n\nexpandable to 32768 words.\n\nIt has the features of hardware multiply\n\nand divide, teletype interface, array complex coordinate converter to\nperform Cartesian to polar and vice-versa conversions without program\nintervention and an hardware FFT interface-subroutine to control the\noperation of the hardware FFT.\nIt can be used in three modes:\n\n1)\n\nStand\n\nalone peripheral FFT\n\nprocessor, 2) part of a free standing spectrum analyzer system, and\nas 3) a free standing computer.\nThe last commercial FFT processor to be discussed will be the "FFT\n256 FAST FOURIER TRANSFORM ANALYZER," a "stand-alone" processor manufactured by Unigon Industries, Inc., Plainview, Long Island, New York.\nIt is a smaller processor than the one previously discussed in that it\nhas a capacity of 256, 1024, or 4096 real points and a wordlength of\n8-bits.\nThis processor performs the functions of the direct and inverse\nFFT, power spectrum and cross power spectrum analysis, square spectrum\nanalysis, auto correlation, cross correlation, convolution, convolution\nspectrum and auto correlation.\nselectable.\n\nEach one of these functions is switch\n\nLet us now discuss a fast digital processor designed\n\nby MIT.\nMIT Fast Digital Processor [27]\nThere are many techniques by which GP digital computers may be\nmodified or enlarged to increase the operating speed.\n\nAs an example,\n\n3-119\n\nspeed savings may be attained by attaching fast multiply and divide\nhardware or by having separately addressable memory modules so that\ninstruction cycles and data cycles may be overlapped.\n\nIncreases in\n\nspeed results from attaching arithmetic hardware which performs high\nspeed special operations such as digital filtering and discrete spectrum\nanalysis.\n\nIf this arithmetic hardware is added in addition to a high\n\nspeed memory, speed increases on the order of 40 to 100 can be attained\nin performing operation such as the FFT.\n\nThis technique has a disadvantage\n\nin that it requires programming that is not easily structured, which\nin turn decreases the speed advantage of the special hardware.\nIt was because of this that emphasis was directed toward incorporating\nmore general purpose features into a signal processing computer structure.\nWhat resulted was the MIT fast digital processor which is a general\npurpose digital attachment to a UNIVAC 1219 computer.\n\nThe architectual\n\nchanges required to increase the speed of repetitive arithmetic operations\nfor signal processing can be classified as 1) the use of scratch pad\nmemories, 2) pipeline schemes, and 3) parallel processing.\nThe fast digital processor (FDP), designed with the above architectural\nchanges in mind, is able to perform signal processing simulations close\nto two orders of magnitudes faster than present conventional digital\ncomputers.\n\nAs an example, a vocoder simulation which normally requires\n\nabout 200 times real time on a standard computer, could be programmed\nto operate close to real time on the FDP.\n\n3-120\n\nThe main applications of the FDP are in the areas of communication,\nradar, speech processing, biology, medicine, and sonar.\nFast commercial integrated circuit elements and a logical structure\nwhich permits each main unit of the machine to operate at maximum\nspeed enables the FDP to obtain a speed advantage.\nThe arithmetic section is designed to perform efficiently the\nsum-of-products operations which are most important to recursive\ndigital filtering, the FFT and correlation operations.\nThe data memories are structured so as to exchange data with the\narithmetic section at maximum efficiency.\nIts\n\nThe control uses a separate memory for storing instructions.\nstructure allows the data memories to operate at maximum speed.\nLet us briefly describe some of the main features of the FDP\nstructure.\nFDP structure.\npaths of the FDP.\n\nFig. 5.1\n\nshows the most important data transfer\n\nPrograms are run from the memory M C , which controls\n\nthe main data flow from memories Ma and Mb to all elements shown in\nSince Mc is intended to be a small memory, longer programs\n\nFig. 5.1.\n\ncan be stored in Ma and Mb and block transferred to Mc when needed.\nMa, Mb, and Mc are addressed independently and can therefore be operated\nin parallel.\n\nExcept for block transfers from Ma and Mb, the control\n\nmemory Mc cannot be written into making the programs run by the FDP\nalmost non-self-modifying.\n\n3-1.21\n\nAeD\n\nMD\n\n1024\n\n|\nE\n\n18\n\nFig. 5.1.\n\n>\n\n,\n\n1024\n\nRead or\nWrite\n140 ms\n\nData\naddrs\n\n18\n\nStructure of the MIT fast digital processor.\n\n3-122\n\nThe parallelism inherent in the FDP is partially indicated in\nFig. 5.1.\n\nListed below are a few special features incorporated for\n\nspeed:\n1)\n\nfour arithmetic units, each including a\'multiplier which can\noperate in parallel with the main arithmetic registers;\n\n2)\n\ntwo independently addressable integrated circuit memories,\nMa and Mb with read and write times of 140 ns;\n\n3)\n\na separate instruction memory, which allows overlap of instructions\nand data cycles;\n\n4)\n\na double length instruction word which enables two instructions\nto be simultaneously executed on the FDP.\n\nThe size of Ma and Mb is 1024 words and is addressed by a 10-bit\nword.\n\nAddressing is indirect, through Md, a 16-register 24-bit integrated-\n\ncircuit memory, as is shown in Fig. 5.2.\n\nThe indexed address for Ma\n\nand Mb is formed by adding the contents of the two 12-bit portions of\nMd to Xa and Xb.\n\nWriting into Md requires no special instructions\n\nbecause addresses 0 through 15 of Ma and Mb are wired to control Md\nas well as the data memories on a write cycle.\nThe I/O capabilities of the FDP were minimized deliberately since\nthe UNIVAC 1219 already supplied most of the necessary I/O control.\n\nAn\n\nA/D and D/A converter were applied to make the FDP applicable for realtime processes.\n\nThe only other I/O path is to and from the mother\n\ncomputer, the UNIVAC 1219, which will be needed for assemblying and\nediting FDP programs and supplying medium size core storage.\n\n3-123\n\n6\n\n4\n\n4\n\n4\n\nt--\n\nS--Data\nFig. 5.2.\n\nMemory addressing.\n\nI ns t ruc t i on\nfrom\nMc (left)\n\n3-124\n\nThe FDP is an 18-bit fixed point processor.\n\nFloating point routines\n\nmay be used but must be programmed, as must multiple processor arithmetic.\n\n.\n\n3-125\n\nREFERENCES\n\n[1]\n\nS. A. White and T. Mitsutomi, "The IC Digital Filter: A Low Cost\nSignal-Processing Tool," Control Engineering, pp. 58-68, June 1970.\n\n[2]\n\nS. C. Silver, "The Digital Filter: Potent Processing Tool,"\nElectronic Products, pp. 32-37, March 1970.\n\n[3]\n\nL. B. Jackson, J. K. Kaiser, and H. S. McDonald, "Implementation\nof Digital Filters," IEEE International Convention - 68, New York,\nN. Y., March 1968.\n\n[4]\n\nD. J. Gawlowicz, "A Programmable Digital Compensator for SingleLoop High Performance Digital Servomechanisms," Grant NsG-36-60,\nCase Institute of Technology, Cleveland, Ohio, 1965.\n\n[5]\n\nA. Deerfield, "Canonical Digital Filters," NEREM - 67, Boston,\nMass., Novenber, 1967.\n\n[6]\n\nC. C. Carroll, H. T. Nagle, Jr., and H. H. Hull, "On the Realization of a Generalized Second Order Digital Compensator," Record of\nthe IEEE 1968 Region 3 Convention, pp. 13.1.1-13.1.5, April, 1968.\n\n[7]\n\nH. T. Nagle, Jr., and C. C. Carroll, "Organizing a Special Purpose\nComputer to Realize Digital Filters for-Sampled-Data Systems,"\nIEEE Transactions on Audio and Electroacoustics (Special Issue on\nDigital Filters), Vol. AU-16, No. 3, September, 1968.\n\n[8]\n\nC. C. Carroll, J. R. Heath, and H. T. Nagle, Jr., "Reducing Quantizer Deadband with a Range Switching Digital Filter," 1969 Computer Group Conference Digest, pp. 68-81.\n\n[9]\n\nC. C. Carroll and J. W. Jones, Jr., "A Time-Shared Digital Filter\nRealization," 1969 Computer Group Conference Digest, pp. 74-77.\n\n[10]\n\nH. T. Nagle, Jr., C. C. Carroll, and J. W. Jones, "A Hybrid\nRealization for Sampled-Data Controllers," IEEE Transactions on\nEducation, Vol. E-13, No. 1, pp. 31-37; July, 1970.\n\n[11]\n\nH. T. Nagle, "Digital Filter Implementations for Sampled-Data\nControl Systems," (INVITED PAPER) Proceedings of the 15th Midwest Symposium on Circuit Theory, Denver, Colorado; 6-7 May, 1971.\n\n[12]\n\nR. White and H. T. Nagle, Jr., "Digital Filter Realizations\nUsing a Special-Purpose Stored-Program Computer," IEEE Transactions\non Audio and Electroacoustics, Vol. AU-20, October, 1972, pp. 289294.\n\n3-126\n\n[13]\n\nH. T. Nagle, Jr., and C. C. Carroll, "Memory Sizing for Digital\nFilters," Proceedings of the IFIP Congress 71, Ljubljana,\nYugoslavia; 23-25 August, 1971.\n\n[14]\n\nH. T. Nagle, Jr., and M. M. Edgeworth, "Computer Aided Design of\nFilters," TR#14, NAS8-20163, Marshall Space Flight Center,\nSeptember, 1971.\n\n[15]\n\nF. Kuo and J. F. Kaiser, System Analysis by Digital Computer, New\nYork, N. Y., John Wiley and Sons, Inc., 1966.\n\n[16]\n\nH. T. Nagle, Jr. and C. C. Carroll, "Signal Amplitude Quantization\nin Digital Filters," Second Hawaii International Conference on\nSystem Sciences, Honolulu, Hawaii; 22-24 January 1969.\n\n[17]\n\nH. T. Nagle, Jr., "An Introduction to Digital Filtering," Course\nNotes, Auburn University Short Course on Digital Systems and\nFilters, Huntsville, Alabama; 8-12 September 1969.\n\n[18]\n\nL. B. Jackson, J. F. Kaiser, and H. S. McDonald, "An Approach to\nthe Implementation of Digital Filters," IEEETAU, Vol. AU-16, No. 3,\nSeptember, 1968, pp. 413-421.\n\n[19]\n\nSeries 4000 Programmable Digital Filters Specifications Sheet,\nRockland Systems Corp., Blauvelt, N. Y.\n\n[20]\n\nArray Transform Processor Specifications Sheet, Raytheon Computer,\nSanta Ana, California.\n\n[21]\n\nFast Fourier Transform Analyzer Specifications Sheet, Unigon\nIndustries, Plainview, N. Y.\n\n[22]\n\nD. B. Kimsey and H. T. Nagle, Jr., "Digital Filter Implementation\nby Minicomputer," Proc. IEEE Region 3 Convention, April 10-12,\n1972, pp. C3-1, C3-4.\n\n[23]\n\nB. C. Kuo, Analysis and Synthesis of Sampled-Data Control Systems.\nEnglewood Cliffs, N. J.: Prentice Hall, 1963.\n\n[24]\n\nY. Chu, Digital Computer Design Fundamentals, New York, N. Y.,\nMcGraw-Hill Book Company, Inc., 1962.\n\n[25]\n\nC. S. Wallace, "A Suggestion for a Fast Multiplier," IEEE Transactions on Electronic Computers, Vol. EC-13, January 1965,\npp. 14-17.\n\n3-127\n\n[26]\n\nL. Mattera, "Digital Filters with LSI Promise; A New World of\nApplications," Electronic Design, January 1971, pp. 24-26.\n\n[27]\n\nB. Gold, I. L. Lebow, P. G. McHugh, C. M. Rader, "The FDP, a\nGast Programmable Digital Processor," IEEE Transactions\non Computers, Vol. C-20, January, 1971, pp. 33-38.\n\n'