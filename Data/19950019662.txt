b'NASA Contractor Report 4666\n\nParticle-Mesh Techniques\n\nPeter MacNeice\nHughes STX\nLanham, Maryland\n\nPrepared for\nGoddard Space Flight Center\nunder Contract NAS5-32350\n\nNational Aeronautics\nand Space Administration\nScientific and Technical\nInformation Branch\n1995\n\nThis publication is available from the NASA Center for Aerospace Information,\n800 Elkridge Landing Road, Linthicum Heights, MD 21090-2934, (301) 621-0390.\n\nParticle-Mesh Techniques\nP. MacNeice\nHughes S T X , Goddard Space Flight Center\nGreenbelt, M D 20771\nmacneice@alfven.gsfc.nasa.gov\nSeptember 16, 1994\n\ni\n\nAbstract\nThis is an introduction to numerical Particle-Mesh techniques, which\nare commonly used to model plasmas, gravitational N-body systems\nand both compressible and incompressible fluids. The theory behind\nthis approach is presented, and its practical implementation, both for\nserial and parallel machines, is discussed. This document is based on a\nfour hour lecture course presented by the author at the NASA Summer\nSchool for High Performance Computational Physics, held at Goddard\nSpace Flight Center.\n\niii\n\nP\n\nx\n\n1\n\nIntroduction\n\nParticle simulation techniques attempt t o model many-body systems by\nsolving the equations of motion of a set of particles or pseudo-particles which\nare used t o represent the system. Particle-mesh( PM) techniques represent a\npopular variant on this theme, in which a numerical mesh is added t o more\neffectively compute the forces acting on these model particles.\nOtherwise known as Particle-In-cell(PIC), particle-mesh codes come in\ntwo basic flavors, pure particle-mesh, and a combination of particle-mesh\nand particle-particle known as P3M [l].In this chapter we will only consider\npure particle-mesh.\nThe particle-mesh technique was originally invented at Los Alamos(circa\n1955 - Harlow et a1) for application in compressible fluid flows [4].It was\nessentially reinvented in the mid sixties by Buneman and by Hockney for\napplication t o plasmas. Since then it has been applied most often to plasmas, but also t o gravitational N-body systems, in solid state device design,\ncompressible fluid flow, incompressible fluid flow(the vortex method), and\nMHD.\nIn the fluid and MHD applications, the particles are introduced as a\nnumerical artifice t o add an appealing lagrangian character to the model.\nWe will mention these applications only briefly. Our main focus will be\nthose applications which model true particle systems. More specifically we\nwill concentrate on those in which the systems particles interact with each\nother through long range forces. In what follows, unless otherwise stated, we\nshall assume that our objective is to model a system of particles interacting\nthrough long range forces.\nP3M is used almost exclusively for modelling gravitational N-body systems.\nTracking particle trajectories enables us to explore physical effects which\nare inaccessible t o other modelling techniques. For example the more common fluid modelling approaches average over the velocity distributions of\nthe particles in the system. To do this they implicitly assume a form for\nthis distribution, which is invariably close t o equilibrium. But much of\nthe interesting physics of these systems is associated with timescales before\nthese equilibrium velocity distributions can be established. To explore this\nphysics we need t o resolve this structure in velocity space. There are two\ntechniques we might use t o study structure in velocity space, a continuum\napproximation based on a Boltzmann or Vlasov type equation, or a particle\ncode. Particle codes are\n\n1\n\ni\n\n0\n\nsimpler t o code up and analyze than Vlasov solvers\n\n0\n\nmore robust\n\n0\n\nhave a lagrangian character which lends them a certain economy\n\n0\n\ncan be applied in 2 or 3D where Vlasov solvers are generally impractical.\n\nHowever they are generally noisier than Vlasov solvers.\nThe appeal of particle codes is rather obvious. By modelling a system,\nsuch as a plasma using particles, we are automatically incorporating much\nof the systems structure in our model. This has many computational advantages. Instead of working with a complex set of fluid equations, or a\nFokker-Planck equation, we get to work with the equations of motion of\nthe particles which are simple ODES. We never have t o worry about negative mass or energy densities developing. Also particle codes have a natural\nadaptivity in the sense that computational effort is automatically focussed\nwhere the particles accumulate.\nOf course particle codes have limitations of their own. For example in\nmost cases the real physical system has many more particles than we can\ncope with on a computer. As a result the forces acting on the individual\nparticles in a numerical model are much noisier\' than in the real physical\nsystem and we are required to take measures t o limit this noise. For systems\nin which the particles interact through long-range forces, the most naive\nimplementation scales in a prohibitively expensive way and we are forced\nt o develop more efficient algorithms. These problems limit the range of\nphysical systems t o which particle methods can be successfully applied.\nThe best way t o understand the limitations which apply t o particlemesh codes is by trying t o construct a code. So let us consider how we\nwould construct a particle model of an electrostatic plasma.\n\n1.1\n\nParticle-Particle\n\nLet us assume that at time t we have N p particles in our system located at\nq ( t ) with velocities v;(t), where 1 5 i 5 Np.2 The force on particle i is\n\'Noisier in the sense that statistical fluctuations become more significant. We will\nexplore this point in more detail later.\n2Unless otherwise stated, variables subscripted with the letters i or J\' will be assumed\nto identify properties associated with particles.\n\n2\n\nLoad initial particle\npositions and velocities\n\nCalculate force on each\nparticle using equation (1)\n\nof motion through At\n\nFigure 1: Flowchart for a Particle-Particle code.\ngiven by\n\nF; = qi\n\n2;- x j\n\nNp\n\nqj\n\nj=l,#i\n\n- xj13\n\nwhere q; is the charge on particle i . The equations of motion for particle i ,\nwith mass mi are\n\ndx;\n\n- = 21;\n\ndt\n\ndv; F ;\n- -- dt\nmi\nThe flowchart for a simple program t o model this electrostatic system might\nlook like figure 1. The force on particle i is computed by summing its interaction with every other particle. We would describe this as a particle-particle\ncode. It has a severe limitation. The number of arithmetic operations required in the force evaluation scales as N;. In a 3D simulation the interaction between 2 particles requires approximately 10 floating point operations.\nFor a code running for Nt timesteps the force evaluation requires roughly\n10 x Nt x N ; / 2 floating point operations. On one processor of a YMP C-90\nwhich has a clock speed of 4 nanoseconds, a 1000 timestep calculation using\na modest 106 particles would last more than 200 days.\n\n3\n\n\xe2\x80\x99\n\nSo the particle-particle approach will only allow us t o model a system\nin which the physics is determined by the interaction of a small(< l o 5 )\nnumber of particles. For any other system we need t o reduce the scaling of\nthe operation count in the force evaluation below order N;. This is done\nin one of two ways, by using a particle-mesh technique, or by using a tree\ncode. Particle-mesh techniques are most effective when the particle density\ndistribution is relatively uniform. Tree codes are favored in systems with\nlarge density contrast.\n\n1.2\n\nParticle-Mesh\n\nIn the particle-mesh approach we replace the force equation (1) with an\nevaluation based on continuum representations of the charge density and\nelectric field. Poisson\xe2\x80\x99s equation\n\nv2(b - p ( z )\n=\nrelates the charge density p ( z ) t o the electric potential\nis\nE ( z ) = -v(b\nand the force on particle i is\n\n4. The electric field\n(3)\n\nWe then use finite difference approximations on a mesh t o solve equations\n(2)-(4). The steps in this process are\n0\n\ncalculate p at each mesh point using the particle position information.\nThis operation scales as order Np.\n\ne\n\nsolve equation (2). If we use Fast Fourier Transforms(FFT\xe2\x80\x99s) this\nscales as NglnNg where Ng is the number of mesh points.\n\ne\n\nevaluate E at each mesh point using equation (3). This operation\nscales as Ng.\n\ne\n\nuse interpolation and equation (4)t o evaluate the force on each particle. This scales as N p .\n\nCombining these steps we can see that the number of floating point operations for the complete scheme scales as\n\n4\n\nLoad initial particle\npositions and velocities\n\n1\nDeposit particle charge\non mesh\n- a scatter operation\n\nSolve particle\nequations of motion\n\non the mesh\n\nInterpolate E to particle\npositions and compute F\nI - a gather operution\n\nI\n\nFigure 2: Flowchart for a Particle-Mesh code.\nwhere a,P and y are constants. The flow chart for this scheme is shown in\nfigure 2.\n\n1.3\n\nFor Which Systems Does PM Work?\n\nObviously when we introduced the mesh which we shall assume is uniformly\nspaced with cell size A, we sacrificed our ability to accurately model phenomena at length scales shorter than A.\nConsider the field produced by a single charge. When we assign this\ncharge to the mesh we must decide which mesh points in the particles vicinity\nacquire charge. The mesh spacing A is the length scale characterizing the\ncoarseness of this assignment. In a sense the particle has acquired a finite\nsize. The force field produced by this \xe2\x80\x98finite-sized\xe2\x80\x99 particle will accurately\nreproduce that of a point particle at distances from the particle which are\nlarge compared with A. But no matter how we assign the charge, the force\nwill become more inaccurate as the distance from the particle decreases\ntoward A.\n5\n\nNow let us pose the question, for what types of physical system are\nparticle-mesh codes appropriate?\nSuppose that the nature of the system is such that the force on any particle is dominated by the contributions from its nearest neighbors. Systems\nof this type are often called \xe2\x80\x98collisional\xe2\x80\x99 or \xe2\x80\x98correlated\xe2\x80\x99. In these cases the\nforce evaluation will be inaccurate unless we make A small compared with\nthe typical distance of closest approach. Let us make the rather conservative\nassumption that the distance of closest approach (Zczose) is as large as l / l O t h\nof the average inter-particle spacing. Then this accuracy criterion requires\n\nwhere L = ( NgA3)l13 the characteristic physical dimension of the system.\nis\n\nFor example if\n\n($)*I3 5 .01\nwhich produces a force error of\n\n5 1%at closest approach, then\n\nie. more than lo6 grid cells per particle are required. Clearly particle-mesh\nis an expensive way t o achieve even modestly accurate individual particle\ntrajectories in a \xe2\x80\x98collisional\xe2\x80\x99 system, both in terms of the necessary flops and\nstorage. It should come as no surprise that there are more efficient ways t o\nachieve this accuracy (eg. tree codes which scale as NJnN,).\nNow consider the opposite case, where close neighbors contribute very\nlittle t o the force on a particle which is dominated by the sum of its interactions with distant particles. This type of system is called \xe2\x80\x98collisionless\xe2\x80\x99\nor \xe2\x80\x98uncorrelated\xe2\x80\x99. At first sight this may seem an unlikely circumstance for\na force law which falls off as T - ~ ,but in fact plasmas, and most N-body\ngravitational systems fit this description. In these cases the important contributions t o the force are accurately represented, since from the point of\nview of any particle most of the other particles in the system will be many\ncells away. So particle-mesh should be appropriate for collisionless, not collisional, systems.\n\n6\n\n1.4\n\nNoise Reduction\n\nThe granularity of a particle representation inevitably introduces short-scale\nfluctuations into the force field, which are superimposed upon a more slowly\nvarying component. The mean amplitude of these fluctuations is proporwhere n is the particle number density. The ratio of the mean\ntional t o\namplitudes of the fluctuations to the slowly varying component varies as\nl/&. In the real system these fluctuations cause particles t o be scattered\nat a frequency which we call the collision frequency. Because our numerical\nmodel typically uses far fewer particles than are present in reality, the effect\nof these fluctuations is greatly enhanced. This produces anomalously large\ncollision frequencies.\nWe need to take steps to reduce the significance of these fluctuations.\nFortunately we do not need to reduce the fluctuation amplitudes t o their\ncorrect values, but merely to levels at which they no longer dominate the\nforces on the particles, or influence the particles significantly during the\ncourse of our simulation. Remember, each particle contributes charge t o\nsome number, C, of cells in its neighborhood. The average value of C\ndepends on the number of spatial dimensions, d , in the model, and on the\nshape and size of the particles. Suppose each particle is a uniform charge\ncloud of length w in each dimension. Then for w = A in 1D we get C = 2,\nin 2D we get C = 22 = 4 and in 3D we get C = 23 = 8. In general we have\n\nJn,\n\nC = ( l + , w. d\n)\nThe granularity with which the particles represent the charge distribution\nis reduced by increasing the number of charge contributions which are made\nin each cell. This requirement is expressed in the inequality\n\nC N , >> N,,\nor\n\n%>>\nNg\n\nw\nA\n\n1\n-=\n\n(I+-)\n\nC\n\n-d\n\n*\n\nWe can see that there are two ways t o satisfy this inequality and reduce the\nimportance of the statistical fluctuations, either by increasing N,/N,, the\nnumber of particles per cell, or by increasing w / A , the ratio of the particle\nsize t o the grid size.\n7\n\n* * *\n\n*\n\nI\n\nI\n\nI\nI\n\n-\n\nI\n\nI\n\nI\n\nI\nI\n\nI\nI\n\nI\n\n1\n\nI\nI\nI\n\nI\n\nt\n\n-\n\nI\nI\nI\n\n1\n\nI\nI\n\n:\n\nI\n\nI\nI\n\nxg\n\nI\n\nI\n\nI\n\n:\n\nb\n\nx\n\nFigure 3: Charge assignment for NGP in 1D. The asterisks denote the location of particles, and the filled circles the mesh cell centers. The arrows\nassociated with each particle indicate t o which grid cell all the particles\ncharge is t o be assigned. The dashed vertical lines denote mesh cell boundaries.\n\n1.5\n\nCharge Assignment and Force Interpolation\n\nOnce we introduce the grid we can no longer view the particles as point\nparticles. We have to specify how the particle charges are assigned t o the\nmesh and how the electric field at the mesh points is used t o determine the\nfield acting on each particle. As we noted this leads naturally t o the idea of\na finite sized particle.\nThe simplest charge assignment is called \xe2\x80\x98(Nearest Grid Point\xe2\x80\x9d (NGP).\nAs the name suggests this associates a particles charge with the grid point\nnearest t o the particle, as shown in figure 3. The most popular scheme\nis \xe2\x80\x9cCloud-in-Cell\xe2\x80\x9d(C1C). In this case each particle can be regarded as a\nuniform distribution of charge, of width w, centered about the particle\xe2\x80\x99s\n.\nnominal location, as shown in figure 4 Usually w is set equal t o the mesh\ncell size A. CIC is slightly more expensive computationally than NGP, but\nhas better numerical properties.\n\n8\n\nFigure 4: Charge assignment for CIC in 1D. As in the previous figure the\nasterisk denotes the particle location. The different hatched areas indicate\nthe relative proportions of the particle\xe2\x80\x99s charge assigned t o mesh cells g and\ng+1.\n\n1.6\n\nN-body Theory\n\nBefore we can understand why PM works for plasmas and some gravitational\nN-body systems, we need t o understand a little about how these systems\nbehave .\nOur goal in N-body theory is t o understand the evolution of macroscopic\nproperties of the system. We can specify the initial conditions and boundary\nconditions for the macroscopic variables which interest us. The evolution of\nthese macroscopic quantities depends on the behavior of the N bodies, but\nthe macroscopic quantities do not uniquely identify the micro-state of the\nsystem. By micro-state we mean that the state of the system is defined in\nterms of the positions and momenta of every particle. There are an infinite\nnumber of micro-states which are consistent with a given macro-state.\nThe theory for this problem is a statistical one. It imagines an infinite number of copies(micro-states) of the system, each consistent with the\nmacro-state. The description of a typical micro-state is based on an expansion about the average of this \xe2\x80\x98ensemble\xe2\x80\x99 of micro-states, the so called\nensemble average. The ensemble average defines a smoothed continuous\n9\n\nforce field. A true micro-state has the granularity that accompanies a distribution of discrete charges. It can be represented by the sum of the average\nforce field, and a term describing fluctuations in the force field. If F M is the\nforce field of the micro-state, and we use ( ) t o denote ensemble averaged\nquantities, then\n\nF M = (FM)+SFM\nwhere SFM are the fluctuations in the micro-state. We are not interested\nin the exact form of 6 F M for any particular micro-state. We are interested\nin the statistical properties of these fluctuations in the ensemble of microstates.\n)\nLet z = ( ~ , pdenote a point in 6 dimensional phase space, and 2; =\n(.;,pi) the ith particles coordinates. The micro-state is given by X =\n( ~ 1 , 2 2 , . . ,Z N ) . The distribution function for X is denoted by f N ( X , t ) ,\n.\nwhere f ~ ( X , t ) d Xis the probability that the system is in a micro-state in\nthe element d X at X . It evolves according t o the Liouville equation\nafN\n\n- at\n+\n\nc-*;(\n\nPi\n\nafN\n\n+Fi.\n\n"-)\n\n= 0.\n\ndPi\n\nl<ijN\n\nThe microscopic phase density is\n\nNM(Z,t) =\n\nS(Z -\n\n1\n\n<isN\n\nz;(t)).\n\nwhere V is the systems volume in r-space. This equation defines f ~ ( z , t )\nwhich is the probability that a particle will be found at z at time t , regardless\nof where all the other particles in the system are located. Many of the\ninteresting macroscopic properties of the system can be determined from f 1 ,\nso we would like t o solve for it. Since d N M / d t = 0 we get\n\nd N M +-.- d N M +-.- d N M\ndr\nap\nat\nat\nar at\n\nap = o\n\n10\n\nor\n\ndNM\n+ w e -\n\ndt\n\ndNM\ndNM\n+F*-=O.\ndr\ndP\n\nTaking the ensemble average of this equation gives\n\nd\n-(NM)\n\ndt\n\ndN\n+ w . -ddrN M ) + (3\'. dPM ) = 0.\n(\n-\n\n+\n\nWriting F M = ( F M ) SFM and ( N M )= ( N / V ) gives\nfi\n\ndfl\nat\n\n+\n\ndfl\nM\nafl\n-+ ( F ) . - = -(SFM\ndT\n\nd\n--SNM).\ndP\n\ndP\n\nIf we set the right hand side of this equation, which is often referred t o as the\ncollision integral, t o zero we get the Vlasov equation. This depends only on\nIt\nthe ensemble averaged field ( F M ) . does not incorporate any effects due t o\nthe granularity which is present in any real micro-state. If we wish to include\neffects due t o this granularity, we need to include the right hand side term\nwhich depends on the ensemble average of correlations in the fluctuations\nof the micro-states. For plasmas the simplest non-zero approximation for\nthis term produces the Landau equation. A yet more detailed approximation leads t o the Balescu-Lenard equation which incorporates the dynamic\npolarization effect known as Debye shielding.\nWhat does SFM look like for systems in which the particles interact\nthrough an r - 2 force?\nConsider a distribution of stars, all of mass m, with number density\nn ( r ,0, (6) in spherical coordinates. Let us assume for simplicity that n has\nno significant T dependence. The gravitational force on a star at r = 0 due\nt o the stars in a volume element r 2 d r sin OdOd(6 at r is\n1\nm2Gn(0,\nq5)r2dr sin SdOdqh .. i\nr2\n\n\'\n\nwhere .. is a unit vector in the radial direction. T h e force on the star due\ni\nt o all the stars in a shell of thickness d r and radius r is\n\nd ( F M >= d r\n\n[/\n\n1\n\nn(8,q5).i. sin OdOdq5 m2G.\n\nThe term in the square brackets has no dependence on distance. So in this\nidealized case all shells of a given thickness contribute equally t o the force on\n\n11\n\nthe star at r = 0. Since there are more shells at large r than small r , distant\ninteractions will dominate ( F M ) .It is this behavior which introduces long\nwavelength collective modes t o the system.\nThe fluctuation term SFM has a different T dependence. The contribution t o the fluctuations from a given volume element scales as the square\nroot of the number of stars in that volume element,\nrn2GJn(8, 4)r2dr sin 8ded4 ..\ni\n\n[\n\n1\n-,\nr2\n\n= rn2Gl----/ n(8,4)sin8dBd4 ..] i\n~\n\n.\n\nSumming over 8 and 4 gives the contribution d(SF\')\nfrom the shell d r at\nr . Once again the factor in the square brackets is independent of distance\nso\n\nd(SF\')\n\nK\n\nfi\n\n-.\n\nr\n\nIn a plasma close to equilibrium, Debye shielding adds an exponential factor\ne-\'lXd t o this behavior causing even faster fall off with distance. A d is the\nDebye length.\nOur conclusion from this crude hand-waving argument is that fluctuations in the force are a short scale phenomena. If they are significant then\nwe will need to model them accurately in our simulation. This means resolving the interactions between close neighbors. We have seen however that\nPM codes are very inefficient for this type of system. On the other hand,\nif fluctuations in the force are not important, the system evolves under the\ninfluence of the long range ensemble average ( F M ) . PM codes accurately\nrepresent forces on scales greater than a few mesh cell sizes, so we can expect\nthem t o do a good job for these \'collisionless\' systems, for which they can\nbe used to study collective modes in the wavelength range A 5 X 5 Nll\'A.\nHow do we determine whether force fluctuations are important in a real\nplasma or gravitational N-body system? The more particles that are involved in representing a given mass or charge distribution, the less granular\nthe resulting force field will be and the less significant SFM will become.\nIn an homogeneous plasma only those particles within a distance Ad contribute t o SFIM.Therefore as Nd = nAi increases, the plasma becomes more\ncollisionless. We can assume\nNa >> 1\nas a condition for collisionless behavior.\n12\n\nFor gravitational N-body systems the criterion is less clear-cut. Imagine\na galaxy of radius R with N stars. For simplicity let us assume that within\nthe galaxy the distribution of stars is uniform. For a star at the center of\nthe galaxy the mean force (ie. the ensemble averaged force) is zero. Only\nthe fluctuations persist. If we sum the individual binary interactions of a\ntest star with all the stars in the galaxy, assuming each interaction can be\nconsidered independent, we can derive an expression for the rate at which\nthe test star is deflected from its trajectory. Using Rutherford scattering\ntheory we derive an expression for A v i due to a typical interaction with a\nfield star. Here I denotes the plane perpendicular t o the initial velocity of\nthe test star. Then we sum this expression over all possible interactions as\nthe star crosses the galaxy once.3 The result is\n\nAv2 ,\nI 81nN.\n.\n.\n.\n.\n.\nN\n\n02\n\nSo the condition we need t o satisfy is that Avf/v2\n81n N\n\nN\n\n<< 1, ie.\n\n<< 1.\n\nThis is a very crude and idealized estimate, but it gives us a more quantitative criterion for deciding if P M is an appropriate technique for a gravitational N-body p r ~ b l e m . ~\nFrom this expression it is clear that galaxies\n( N 10\xe2\x80\x9d) are collisionless, globular clusters ( N lo4 - lo6) are marginally\ncollisionless, while stellar clusters ( N l o 2 ) are dominated by fluctuations.\nThere is a caveat t o bear in mind with regard t o gravitational systems. It\nis possible that some stars in the system might be unaffected by fluctuations\nwhile at the same instant other stars are being dominated by fluctuations.\nThis occurs because of spatial clustering which tends t o develop in gravitational systems but not in plasmas. Conditions in the neighborhoods of two\nspatially separated stars can be significantly different.\nIn both plasmas and gravitational N-body systems the particles interact with each other through a r-2 force, and so we might expect them t o\nbe similar in nature. However in the gravitational case the forces are always attractive. In plasmas the interactions between particles can be either\nattractive or repulsive. As a result these systems have major differences.\nN\n\nN\n\nN\n\n3This of course violates our assumption that the star is at the center of the galaxy, but\nour purpose here is to derive a crude estimate for which we consider this approximation\nacceptable.\n*Less crude criteria can be developed from the Landau equation. See chapter 8 of\nBinney and Tremaine [8].\n\n13\n\nGravitational N-body systems are subject t o a collapse instability which\ndoes not occur for plasmas. When a density perturbation develops, the\nresult is t o more strongly attract surrounding material toward the denser\nregion, reinforcing the original perturbation. Plasma density perturbations\ndo not feed themselves in this way.\nThe result is that gravitational systems generally possess very high density contrast, with particles distributed in clusters which may be part of a\nhierarchy of clusters. Plasmas tend t o be much more uniform.\nIn addition plasmas are affected by a phenomenon known as Debye\nshielding. Every particle in the plasma has a tendency t o attract those\nparticles in its neighborhood with charge of opposite sign and repel particles with charge of similar sign. This weakly correlated behavior has the\neffect of screening fluctuations on scales longer than the Debye length Ad.\n\n1.7\n\nKey features of a collisionless plasma\n\nMost of the pioneering studies of the properties of PM schemes were collisionless plasma calculations. So let us take a moment here t o review some\nof the key features of collisionless plasmas in light of the points made in the\nprevious section.\nThe interaction force\\ T - ~ )is a long range force. In principle and in\npractice the system supports long wavelength modes involving coherent collective motion of many particles.\nThe highest characteristic frequency associated with collective modes is\nthe electron plasma frequency\n%e\n\n=\n\n4nne2\n(->m e\n\n.\n\nThe range of wavelengths of these coherent modes is bounded at the lower\nend by the Debye length Ad. Electron thermal motions dampen modes with\nX < Xd so strongly (Landau Damping), that we can say collective modes\nonly exist for X > Ad.\nParticles separated by less than Ad see each other as individuals - particles separated by more than Xd interact with each other as participants in\ncollective wave modes.\nThe collision frequency v c ( r T ~ ~ IisI the rate at which sub-Ad fluctuations\n)\nscatter a Darticle.\n2nvc nlnA\n1\n-0: -0: -1nNd\nupe\n& Nd\n14\n\nie .\n2TUC\n\n-<< 1 + Nd >> 1\nW.\nP\n\nIn collisionless systems, the X > Ad coherent collective modes are the objects\nwe wish t o study.\nA collisionless system has a very large number of particles in a Debye\nsphere. A collisional (fluctuation affected) system has relatively few. To\nproperly model a collisional system we must faithfully reproduce Nd. For\na collisionless system we can ignore sub-Ad fields. The challenge there is t o\nmake sure that while grossly under-representing the number of particles we\nmaintain\n2T u,\n7\n-<< 1.\nWP.\n\nOne wary t o achieve this is by filtering out the sub-& scale components of\nthe electric field. Since Amin = 2 4 is the shortest wavelength which the grid\nwill support, we can arrange this if we set A Ad.\nN\n\n2\n\nES1\n\n-A\n\nID Electrostatic Code\n\nES1 is a well-known and documented 1D electrostatic code, written by Birdsall and Langdon, which is widely used as a teaching aid. For a more complete description of the code and a variety of simulation projects to which it\ncan be applied, the reader is referred to reference [a]. At this point we shall\npresent a quick outline.\n\n2.1\n\nIntegration of the Equations of Motion\n\nThe particle equations of motion are\n\nd ;\n-x- - v;\ndt\ndv;\ndt\n\nF;\nm;\n\n_ _ -\n\nESl uses a leap-frog scheme which is second order accurate in time for\nconstant integration timestep At. The discrete equations are\n\n15\n\ntotAt/2\n\ntot3At/2\n\nt\'\n\n1\n\nI\nI\nI\n\nI\nI\n\nI\nI\n\nto\n\nI\n\nb-\n\nt,tAt\n\ntot2At\n\nt\n\nFigure 5: The leap frog time integration. Note that the times at which v\nand 2 are known are offset by At/2.\n\nwhere superscripts denote time levels. This is illustrated in figure 5. Note\nthat the times at which a particles position are known are offset by At/2\nfrom the times at which its velocity is known. For an harmonic oscillator (ie.\nF cx x - xo) of frequency wo, leap frog has no amplitude error for woAt 5 2,\nand has second order phase errors. Thus choosing A t t o satisfy wont = .3\ngives reasonable accuracy provided we do not run the integration beyond\nroughly 100At.\n\n2.2\n\nIntegration of the Field Equations\n\nES1 solves t h e 1D form of Poisson\'s equation\n\nand computes the electric field using\n\nE x ----. 34\ndX\n16\n\nTaking fourier transforms of these equations gives\n\nand\n\n@ ( k )= -ik$(k).\nFor a discrete periodic system of length L , the fourier transform and inverse\ntransform are defined by\nNg-1\n\nf \xe2\x80\x9d ( k l )= AX\n\nE f(xg)e-iklzg\n\ng=o\n\nwhere kl = 2 ~ 1 / L .We could form the discrete transform P(k1) and then use\nequations (12) and (13) t o find @ ( k l ) ,and finally apply the inverse transform\nt o get E(x,). However this mixes discrete representations P(k1) and @ ( k l )\nwith continuous representations k 2 and -ik for the operators d2/dx2 and\nSo instead we replace equations (10) and (11) with their finite\nd/dx.\ndifference approximations\n4g+l\nand\n\n-\n\n24g\nAx2\n\n+\n\n4g-1\n\n= -pg.\n\nEg = - @g+l- @g-1\n\n242\nwhere Ax is the mesh cell size. Taking discrete transforms of these finite\ndifference approximations gives\n\nwhere\n\nand\nKl\n\n= k l ( sin iklAx\n$klAX\n\n5We will explore this further in section 4.7\n\n17\n\n).\n2\n\nFigure 6: The variation of charge from particle i assigned t o mesh point g\nas the particle\'s location x; is altered, for NGP assignment.\n\n2.3\n\nAssignment of Particle Charge to the Mesh\n\nWe are given two choices by ES1. The lowest order and least expensive is\nNearest Grid Point(NGP). In this case all of a particles charge is assigned\nt o the mesh cell in which the particle is located, as shown in figure 6. As a\nparticle crosses the boundary between two cells, the charge assigned t o both\ncells jumps discontinuously. This produces some noise in both p and E .\nThe higher order alternative provided is Cloud-In-Cell(C1C). In this case\neach particle can be considered as a uniform charge cloud of width w. The\ndefault width is w = Ax, the mesh spacing. This scheme is illustrated in\nfigure 7. If the particle position 2; is located between the two mesh cell\ncenters xg-l and xg, the charge assigned t o each of these cells is given by\n\nand\npg-1\n\n=q\ni\n.\n\nxg - 2;\n\n18\n\nAx\n\nI\n\nI\n\nI\nI\nI\nI\nI\nI\n\n-\n\n.\ni\n\n-\n\n.\n\nxg\n\ni\n\nb\n-\n\n:\n\nx,\n\nFigure 7: The variation of charge from particle i assigned t o mesh point g\nas the particle\xe2\x80\x99s location x; is altered, for CIC assignment.\n2.4\n\nForce Evaluation\n\nTo ensure momentum conservation the same interpolation scheme is used t o\ncompute the force on a particle as was used t o perform the assignment of the\nparticles charge t o the mesh. If we used NGP then the force on the particle\nis simply the force evaluated at the mesh point nearest t o the particle. For\nCIC it is given by the formula\n\nfor x g - l 5 2; 5 xg. There is also an energy conserving option which uses\nCIC charge assignment and NGP force evaluation. However this does not\nconserve momentum.\n\n3\n\nTime Integration Schemes\n\nIn this section we provide a brief review of the properties of a number of the\nmost frequently used time integration algorithms. We have followed the format adopted in chapter 4 of Hockney and Eastwood [l]t o which the reader\n\n19\n\nis referred for more detail. We will consider three algorithms of different\norder, the first order Euler scheme, also known as upwind differencing, the\nsecond order leapfrog algorithm which we have already encountered, and a\nfourth order Runge-Kutta algorithm. For the sake of brevity we will attempt\nt o derive the properties of the leapfrog scheme only, while simply quoting\nthe corresponding results for the two other schemes.\nApplying the Euler scheme t o the particle equations of motion gives\n\nThe leapfrog scheme was given in equations (8) and (9). The fourth order\nRunge-Kutta scheme for a system of equations given by\n\n- = f(z,t)\ndz\ndt\nwhere z and f are vectors (ie. in our case z = ( 2 , ~), is defined as\n)\n\nwith\n\n1\n\nkq = A t f ( 3+ s k 3 , t n\n\n+ s1a t ) .\n\nThere are four important criteria t o be considered when choosing an\nalgorithm t o integrate the particle equations of motion,\ne\n\nconvergence\n\ne\n\naccuracy\n\ne\n\nstability\n\ne\n\nefficiency.\n\n20\n\n301\n..\n\nConvergence\n\nBy convergence we mean that the numerical solution converges t o the exact\nsolution of the differential equation in the limits as At and Ax tend t o\nzero. For linear schemes Lax has shown that consistency and stability are\nnecessary and sufficient conditions for convergence. Consistency requires\nthat the difference approximation should reduce t o the differential equations\nin the limit as At + 0. It should also preserve time reversibility. All\nthree schemes reduce t o the correct differential equations in the limit as\nAt -+ 0. However of the three only leapfrog satisfies the time reversibility\nrequirement.\n3.0.2\n\nAccuracy\n\nBy accuracy we mean the truncation error associated with approximating\nderivatives with differences. If we combine the two equations defining the\nleapfrog approximation into one we get\nxn+l - 2xn + xn-1\n- F(X" 1\n(18)\nAt2\nm\nwhich can be compared with the exact expression\n\nd2 -x - F\nat2\nr\'\nn\nUsing Taylor series expansions for xn+l and xn-\'\nxn+l = xn\n\nabout xn gives\n\n+\n\nxn-l\n\nCombining these gives\n\nThus leapfrog is second order accurate in time.\nIt is relatively trivial t o show that the Euler scheme is only first order.\nShowing that the Runge-Kutta scheme is fourth order is a very long and\ntedious task which will be left up t o those readers with superhuman patience.\nIt should be pointed out that order is generally but not always a reliable\nguide t o the accuracy of a scheme. Each scheme has its complement of\npathological applications which can cause it t o break down.\n21\n\n3.0.3\n\nStability\n\nDo errors grow in time? If round-off error (ie the error introduced because\nthe computer only stores numbers up t o a certain precision) grows in time\nthen the scheme is unstable.\nConsider again the leapfrog equation (18). Let zn be the numerical\nsolution at time tn, and X" be the exact solution (ie no round-off error) t o\nthis difference equation. We shall denote the numerical error at time tn by\n\nUsing equation (19) to replace z in equation (18) gives us an equation for\nthe evolution of the error E with time,\n\nIf we assume that we are looking at bounded oscillatory solutions of the\nform\nE = (X)n = (,xw*t)n\n"\nthen by substituting this into the previous equation we get\n\nX2\n\n- 2X\n\n+ 1 = -(RAt)2X\n\nwhere we assume also that\n\nThis has solutions\n\nand the general solution is\nE\n"\n\n= aXT\n\n+ bX?.\n\nThe scheme will be stable provided 1Xj-l 5 1. Figure 8 shows how X k varies\nas a function of RAt. When RAt < 2,\nhas an imaginary part, but\n22\n\nh\n\n1\n\nP-\n\nFigure 8: The roots of equation (22) as a function of RAt. For RAt < 2\nboth roots have an imaginary component, but both have magnitude 1x1 = 1.\nFor R A t > 2, both roots are real and 1A-l > 1.\nfor RAt 2 2 both solutions are real. For RAt\n2 we can easily show\nthat IX*l = 1. This means that not only is the leapfrog scheme stable for\nR A t 5 2, but it has the additional advantage that it suffers no amplitude\ndissipation. When RAt > 2 however 1A-I > 1. Therefore to guarantee\nstability, we can calculate the largest value of Im-\'dF/dX1 and then set A t\nsuch that\n\nThe stability equation for the Euler scheme as given in equations (16)\nand (17) has roots\nA* = 1 f iRAt\nwhich has IA*( 2 1, and so is unconditionally unstable. The Runge-Kutta\nscheme is also unstable when applied t o the particle equations of motion.\n23\n\nWhen we make the simplifying assumption that the force acting on the\nparticle is proportional t o the particle displacement , ie simple harmonic\noscillation, the stability equation has solution\n\n5\nX,t = 1 - -(QAt)2\n12\n\n1\n+ -48( Q ~ l t >f R A t ( 1 ~\n\n3\n-(Qat)\').\n24\n\nIn this case IX*l > 1for all RAt except in a short interval about RAt N- 2.5.\nThis analysis is more complicated for non-oscillatory solutions. There\nwe need t o check that en grows more slowly than Xn.\nThere is of course another requirement for accurate stable integration of\nthe equations of motion. No particle can be allowed t o move more than one\nmesh cell in distance during one timestep.\n304\n..\n\nEfficiency\n\nThis is a critical consideration since whatever scheme we choose will be used\nfor each particle at each timestep, a total which can be comfortably in excess\nof 10" times. It is generally true of a lower order scheme that they\n0\n\ninvolve fewer intermediate time levels per timestep\n\n0\n\nrequire fewer stored intermediate values\n\n0\n\nrequire fewer floating point operations per timestep\n\n0\n\nhave a greater stability range\n\n0\n\nrequire finer timesteps to achieve the same accuracy\n\ncompared with a higher order scheme. The conventional wisdom is that the\nsimple second order leapfrog achieves the best balance between accuracy,\nstability and efficiency. In ES1, with normalizations such that At = 1, and\nwith the variable replacements\n\nX\n\nx--.fZ=Ax\nthen\n\n24\n\nwhere\nA(Z0Zd) = aE(ZoZd)\n\nwith a = qAt2/mAx. This implementation is remarkably efficient requiring\njust 4 fetches from memory, 2 additions, and 2 stores in memory per particle\nper timestep.\n\n4\n\nSpatial Discretization\n\nThe mesh is involved in three separate stages of the code,\n1. assignment of the particle charge t o the mesh\n\n2. solving the field equations\n3. interpolating mesh defined forces t o the particle positions.\nAll three steps introduce some error. However we should remember that the\nonly error that matters is the final combination of errors. As we shall see it\nis sometimes possible t o manage these individual errors in ways which allow\nthem t o partially cancel each other and so produce a superior result.\nIn discussing the ramifications of spatial discretization we will limit ourselves t o the two schemes which we have seen already, NGP and CIC. To\nillustrate discretization errors we will consider a simple test problem, a periodic 1D electrostatic system with boundaries at x = fL/2, and a uniform\nmesh with cell size Ax, and we shall assume the following discretized field\neauations,\n\nEg = - 4g+1 - 4g-1\n2Ax\nBefore we proceed there are a couple of function definitions which we need\nt o establish. We define a cloud shape function S(x) which gives the charge\ndensity associated with a finite-sized particle at x = 0. For NGP this is\n\nand for CIC,\n1\n\nS(X)\n\n= \'n(x).\nAx\n\n25\n\nLL-\n\nAx\n\nX\n\n"\n\nk\n\nFigure 9: The weighting functions and their fourier transforms used in NGP\nand CIC schemes. The NGP weighting function W p ~ ~ p is z ) hat function\n( the\nII(z), and the CIC weighting function Wc~c(tc)the triangle function A(.).\nis\n\n1 is the hat function shown in figure 9 and defined in equation (23). The\n1\ncharge assigned t o mesh point g from particle i located at z; is computed\nfrom the weighting function\n\n1\n\n9\n\n"S+\n\nW(5;- 5g ) =\n\npg-\n\naz\n\nThe total charge at g is\n\n26\n\nS ( 5 ; - 5\')dz\'.\n\nIf the number density of particle centers is\nNIJ\n\nn ( z )= Z S ( z - z )\n;\ni=l\n\nwe can define a continuous charge density pc by analogy\np c --\n\nq L I 2 W(x\xe2\x80\x98- z)n(z\xe2\x80\x99)dz\xe2\x80\x99.\nAx -LIZ\n\nNote that pg = p c ( z g ) ,in other words pg can be obtained by sampling the\ncontinuous density distribution pc.\n\n4.1\n\nNGP\n\nFor NGP the weighting function is given by\n\nI x I<\n-\n\nAx/2\n\nI>\n\nAz/2.\n\nW(x) = rI(z) =\n12\n\n(23)\n\nThe mesh charge density is obtained by sampling the continuous density\n\nwhere\nNJ\nI\n\nn(x) = Z S ( z\n\n- x;).\n\ni=l\n\nThe force on particle i is given by the force evaluated at the nearest cell\ncenter. If xg - Ax/2 5 z;5 xg A z / 2 then\n\n+\n\nNow, consider our simple test problem, a 1D electrostatic system with\nperiodic boundary conditions imposed at IC = f L / 2 , and just two particles,\na charge - q at X I , and a charge $4 at 2 2 , with 2 2 > 21. We shall label\n27\n\n.44\n\n-\n\nF\n\nJ\n\ninkrpdcle\nseparation\n\n16A.X\n\n-4\n.4\n\n.39\n\nF\n\n739\n\nFigure 10: The mutual force between a positive and negative charge as\na function of their spatial separation, in a periodic system of length 16Az,\nusing NGP(top frame) and CIC(bottom frame) charge assignment and force\nevaluation.\n28\n\nthe center of the mesh cell containing the negative charge 4, the cenand\nter of the mesh cell containing the positive charge T2. Using NGP charge\nassignment, the potential at the mesh points is given by\n\nwhere O = q(T1 - :2)/L. The force on the particle at\n,\n\n52\n\nis\n\nThis is plotted in figure 10 as a function of the inter-particle separation\n22 - X I , for a system with L = 1 6 0 2 . Note that as we vary the particle\nseparation the force jumps discontinuously when one of the particles crosses\na mesh cell boundary. Another unfortunate property of this force is that\nit is not invariant under spatial translation. Suppose we vary (XI x 2 ) / 2 ,\nthe mean position of the particles, while keeping their separation 22 - 2\n1\nfixed. Then the force fluctuates with period Ax, as shown schematically\nin figure 11. This fluctuation in the inter-particle force is greatest at small\nseparations. Minimizing this loss of displacement invariance is one of the\nmost significant improvements we can make in designing a particle code. As\nwe shall now see using a higher order scheme, such as CIC, achieves this.\n\n+\n\n4.2\n\nCIC\n\nWith NGP we used one mesh point t o define charge assignment and force\ninterpolation, and we got a noisy result. CIC uses two mesh points. In this\ncase the particle can be viewed as a uniform density charge cloud of width\nAx. The charge assignment scheme is illustrated in figure 7. The weighting\nfunction is given by\n1-\n\nW ( x ;- Zg)= A(2i - X g ) =\n\nio\n\nI 2; - I C g I /A2\n\nI 2; - xg 1 Ax\n5\notherwise\n(26)\n\nA ( x ) is called the triangle function, and is shown in figure 9. We will leave\nit as an exercise t o the reader to derive the electric potential on the mesh\n29\n\nE\n\nt\n\nFigure 11: Variation of the mutual force between a positive and negative\ncharge as a function of their mean position but with fixed separation, for the\ntwo distinct cases when the particles are separated by more than Az(top),\nand by less than Az(bottom). NGP charge assignment and force evaluation\nare used.\nfor the test problem considered in the previous section. An example of the\ninteraction force between the two particle is plotted in the bottom frame of\nfigure 10 as a function of the inter-particle separation. For 2 2 - 2 1 2 2A2 we\nrecover the exact analytic answer for the 1 D problem.6 For 22 - 2 1 < 2A2,\nthe inter-particle force depends also on the positions of the particles with\nrespect t o the mesh. Figure 10 illustrates one example, where 2 1 was chosen\nt o be -0.4Az from cell center while 22 was varied.\nIt is immediately apparent by comparing the two frames in figure 10\nthat CIC gives much smoother forces than NGP. It is also obvious that\nthe dependence of the inter-particle force on the mean particle position\n(21\n22)/2 is much weaker than in NGP. Finally, the errors that do remain\nare more localized spatially than for NGP.\n\n+\n\n\'This occurs because in 1 D the exact analytic solution for the potential is piece-wise\nlinear in x. CIC interpolation is capable of representing this variation exactly. This\nfortuitous match does not occur in 2 or 3D.\n\n30\n\n4.3\n\nMomentum Conservation\n\nBoth the NGP and CIC examples above conserve momentum. We can in\nfact show that momentum will be conserved provided we use centered differencing, and we use the same weighting function W for charge assignment\nand force interpolation. To prove that a scheme conserves momentum we\nneed t o demonstrate\n0\n\n0\n\nthat no self-forces act on the particles\nthat interacting particles impose equal but opposite forces upon each\nother.\n\nIt is convenient at this point to develop some additional array notation.\nLet us consider a vector p such that each element of p is the charge density\nat a different mesh point, ie p = ( P I , , . . ,pg,. . . , p ~ , ) . Let us also construct\nsimilar vectors @ and E from the mesh values of the electric potential and\nelectric field. Differencing Poisson\'s equation leads t o a matrix equation\n\nA @= - p\nwhere A is an N g x Ng matrix. Similarly, differencing\n\nproduces\nE\n\n1\n\n-B@\n\nwhere B is another N g x N g matrix. Combining these equations gives\n= BA-\'~\n\nE\n\n= cp.\nChoosing the differencing scheme\n\n9\n\n4 g + 1 - 249\n\n+\n\n+\n\n49-1\n\nAx2\n\n8x2\n\nmeans A and A-l are symmetric. With\n\nw\n\n-\n\n4g+1\n\n---f\n\n- 4g-1\n\n242\n\ndX\n\nB is anti-symmetric. Therefore C = BA-I is also anti-symmetric.\n31\n\n431\n..\n\nSelf-Forces\n\nConsider a charge q a t x . Let us assume for now that the weighting functions\nused t o assign charge t o the grid and t o interpolate the force from the mesh\nt o the particles are not necessarily the same, and we will denote them by\nW p , and W F respectively. Consider a charge q at x . The force on this\nparticle is\nF(x) = q C W F ( x - ig)E(zg).\n9\n\nThe field E is given by\n\n9\xe2\x80\x99\n\nand so therefore\n\nF ( 4 = X X q W F ( x - .g)Cgg\xe2\x80\x98P(xg\xe2\x80\x99>*\ny\n\n9\xe2\x80\x99\n\nTo calculate the self-force on the particle we consider only the contribution\nt o p from the particle itself, ie.\n\np(xg\xe2\x80\x99)+ S p ( Z g \xe2\x80\x99 ) = qW\xe2\x80\x9dx\n\n- Zg\xe2\x80\x99).\n\nThe resulting self-force is\n\nq2CggrWF(x - x g ) W Q ( x xg\xe2\x80\x99).\n-\n\nFse1f(Z)=\n9\n\n9\xe2\x80\x99\n\nNow if we assume that W F z WP, then the necessary condition t o achieve\n= 0 is that C be anti-symmetric. We saw above that centered differencing makes C anti-symmetric. Thus we have established that the combination of centered differencing and W F Wp eliminates self-forces.\nFselj\n\n=\n\n4.3.2\n\nMixed NGP/CIC schemes\n\n+\n\nWhat if W F WP? Consider a single particle with charge q located a t\nx in an infinite 1D system. Let us use x , t o denote the center of the cell\ncontaining the particle.\nFirst consider the combination of NGP charge assignment and CIC force\ninterpolation. We can easily show that the potential 2.t mesh point g is\n\nQ\n(bg = --lxg - xml.\n2\n\n32\n\nUsing CIC for the force evaluation we get\n\nq 2 ( x- x , ) / ~ A x\n\n5\n,\n\n- - Q ~ ( x ,-\n\nX\n,\n\n- Ax12\n\n+ Ax12\n\n< x < X,\n\nF={\nx)/~Ax\n\n< x < 2.\n,\n\nThe self-force acts t o drive the particle away from the cell center. This can\nproduce instability.\nNow consider CIC charge assignment and NGP force interpolation. Let\nus further assume that the particle lies in grid cell m as before and is located\nA x / 2 . Using CIC charge assignment gives\nbetween x , and x ,\n\n+\n\n,\nSetting 6 = 0 and assuming equal but opposite potential gradients at koa,\nie .\nas g -+\n\n00,\n\nwe can easily show that\n&+l\n\nQ\n= --(2(xm - x )\n\n2\n\nwhich implies that\n\nF=-q\n\n2\n\n+Ax)\n\n(-).\n\nX - X m\n\n242\nThe particle will perform simple harmonic oscillation about the nearest grid\npoint with frequency\nW&\n\n=\n\n{E.\n2mAx\n\nFrom our analysis of the leap-frog scheme we know this will be stable provided the timestep is chosen t o satisfy w,,ljAt < 2.\nThe general principle here is that we should always use a force interpolation scheme which is of no higher order that the scheme used for charge\nassignment.\n\n33\n\n4.3.3\n\nEqual but opposite forces\n\nNow consider two particles, q1 at\ndue t o particle 2 is\n\n21\n\nand q2 at\n\n22.\n\nThe force on particle 1\n\nSimilarly, the force on particle 2 due t o particle 1 is\n\nBut since C is anti-symmetric\n\nNote, this result again depend on C being anti-symmetric, and W F E WP G\n\nW.\nWe have highlighted three properties which we would like the charge\nassignment /force interpolation scheme t o satisfy,\n0\n\n0\n\n0\n\nat particle separations large compared with the mesh spacing, the\nfluctuations in the inter-particle force due to displacement relative t o\nthe mesh should become negligible\n\nas a particle moves across the mesh the force it experiences and the\ncharge it assigns t o the mesh should change smoothly\nmomentum should be conserved.\n\nHockney and Eastwood [l]describe a hierarchy of schemes constructed on\nthese principles (NGP, CIC, TSC, ...). Alternatively, schemes can be developed by using multi-pole expansions of the charge distribution of each finite\nsized particle about the nearest grid point t o the particle. To zeroth order\nthis approach returns NGP. Including the dipole gives a scheme similar t o\nCIC.\n\n34\n\n4.4\n\nAliasing\n\nThe spurious fluctuations which appear as a result of the loss of displacement\ninvariance, manifest themselves in k-space as non-physical mode couplings,\nknown as \xe2\x80\x9caliasing\xe2\x80\x9d. Spatial structure on scales finer than Ax cannot be\nrepresented on the mesh. Some of the power in this fine scale is erroneously\ninterpreted by the mesh as belonging to longer wavelength modes which the\ngrid does support.\nFor a 1D infinite system the density of particle centers is given by\n&\n\nn(x) =\n\nC6(x- x;)\n\nand the continuous charge density is\n\nThis has fourier transform\n\nwith\n\nBy introducing a mesh we reduced our representation of p ( x ) from a continuous representation p c ( z ) to a sampled representation p s ( x g ) . The only\nwavelengths which can be represented on the mesh are X 2 2 4 2 , ie k 5\n7r/Ax = kg,;,j/2. The appropriate transform for this discrete representation\nis the discrete fourier transform\n\ng=--00\n\nEquation (32) expresses p s ( x g ) in terms of the transform of the sampled\ncharge density, but we can also write it in terms gf the transform of the\ncontinuous charge density (equation (31)),\n\n1\n/\n\np s ( x g ) = p c ( x g ) = 2n\n\n35\n\n00\n\n-00\n\ndk ,&(k)eikzg.\n\n(33)\n\nBreaking up this integral into intervals of length kgTid, and setting k =\n1 nkgT;d,\n\xe2\x80\x98\nequation (33) becomes\n\n+\n\nComparing equations (34) and (32) implies,\nM\n\nn=-m\n\nBut inkgTidxg 2ning and therefore\n=\n\nWhen we represent p on a discrete mesh instead of a continuum, we limit the\nindependent wavelengths which the solution can contain t o the wavenumber\nrange -kgT;d/2 5 k 5 kgTid/2,called the \xe2\x80\x9cprincipal zone\xe2\x80\x9d or \xe2\x80\x9cfirst Brillouin\nzone\xe2\x80\x9d. The transform of the charge density in our discrete representation is\ngiven by the sum of copies of the transform of the continuous charge density,\neach offset by a different multiple of kgTid. This means of course that F S ( k )\nis periodic in k with period k g T i d . The extra contributions (from I 1 > 0) t o\nn\np^,(k) inside the principal zone are called aliases.\nNote that for the infinitely long mesh there is a continuum of allowed\nwavenumbers in the principal zone. In the case of a finite sized mesh\nthere would be just a discrete set of wavenumbers given by k = 0 and\nk = 2n/(nAx)where 1 5 n 5 Ng.\nAliasing is generally worse for k N kgT;d/2than for k 0, ie the shortest\nwavelengths are most affected. Also, the smoother p is, ie the less power\nthere is at short wavelengths, the less likely it is that aliasing will be a\nproblem. Recall that\n\n-\n\nwhich implies by the convolution theorem that\n\nP^,(lc) = -&(k)Jv(k).\n?\n!\nAx\n36\n\nL\n\nPrincipal Zone\nFigure 12: The fourier transform of the discrete sampled charge density,\nwhich is the sum of copies of the transform of the continuous charge density,\neach offset by a different integer multiple of kgrid.\nEquation (35) implies that the narrower i C ( k )is in k-space, the less aliasing\nwill occur.\' But from equation (36) we can see that we can narrow b c ( k )\nby narrowing m ( k ) , or in other words by making the charge assignment\nsmoother.\nFor NGP we saw that W N G ~ (= I) ( x ) , the hat function defined by\nX I\nequation ( 2 3 ) and shown in figure 9. The fourier transform of this is\n\nk N G p ( k ) = A X six(,)\n\nkAx\n\n=Ax\n\nsin( k A x / 2 )\nkAxc/2\n\nIf we use CIC then W C I C ( X )A ( x ) , the triangle function defined in equa=\n\'If j , ( k ) is band limited, in other words, a critical wavenumber k, exists such that\n,.\nj c ( k ) = 0 for Ikl 2 k,,, then no aliasing will occur provided k y r t d 2 k, Under these\n,.\ncircumstances p can be represented exactly on the mesh. When k y r , d = k,, the mesh\nsamples p at the Nyquist frequency.\n\n37\n\ntion (26), and also illustrated in figure 9. This has a transform\n\nIn figure 9 we can see that I/i\xe2\x80\x99clc(k) narrower than I/i\xe2\x80\x99jvGp(k) and so less\nis\nsusceptible t o aliasing.\nAliasing will be produced by any mechanism which introduces wavelengths shorter than 2 4 2 . Low order charge assignment schemes do this.\nSetting the particle size w smaller than the mesh size will also do this. We\nget strong aliasing if we set w < Ax/lO. This limits the dynamic range\nof wavelengths which we can include in our model t o X < 10Ngw. This\nlower limit on w can be relaxed somewhat by using higher order assignment\nschemes.\nSetting the Debye length much less than Ax will also introduce wavelengths shorter than 2Ax. In a mono-energetic beam for example, T = 0\nand so Ad = 0. If we use ES1 t o model this system we find that the beam\nbegins to spread in v space. Aliasing is feeding energy from X < Ax modes\ninto modes supported by the mesh, heating the beam and thereby raising\nboth T and Ad. This thermal instability persists until A d grows sufficiently\nthat the condition Xd << Ax is no longer satisfied.\n4.5\n\nThe Field Solver\n\nFormally, we can write\n00\n\n4s(xg) = A X\nG,(xg\ng\xe2\x80\x99=--oo\n\n- xgl)ps(zgO\n\nor in k-space,\n\n&(k) = GS(k)bS(k).\n(37)\nThe function G, is a discrete representation of the Greens function for Poisson\xe2\x80\x99s equation. The transform of the exact(continuous) Greens function for\n0\xe2\x80\x996 = -p is l/k\xe2\x80\x99. If we use G , ( k ) = 1/k2 then\n\n38\n\nFor g\' = g this gives G, = 0. When g\' # g we have G,(xg//) = -Gs(-xgtt) #\n0, ie in x-space l/b2 appears as a very non-local operator. If instead we use\nthe finite difference approximation for V2#,\n4g+1 - 24g\n\n+\n\n4g-1\n\n= -&AX\n\n2\n\nwhich transforms t o\n\nwhere\n\n00\n\n$,(k) = Ax\n\nq5s(xg)e-ikzg,\n\ngiving\n\nDifferencing exaggerates the amplitude of higher k modes.\n4.6\n\nForce Evaluation\n\nRecall that\n\nE = - - 04\ndX\nand\n\nk(L)= - i k $ ( k ) .\nAgain we can choose t o use these exact operators, or finite difference approximations to them. If we replace dq5/ax with\n4g+1 - 4g-1\n2Ax\nwe replace the exact operator, -iL, in fourier space, with -ik sin (kAx)/kAx,\n\nk(k)\n\n= -ik sinc(LAx) $ ( k )\n= -in(kAx) & k ) .\n\nIn this case differencing acts t o dampen high k modes.\n\n39\n\n(40)\n(41)\n\n0\n\nn/Ax\n\nn/Ax\n\n0\n\nk\nFigure 13: The function K, is introduced into the evaluation of the electric\nfield when we use the centered second order finite difference approximation. It acts to dampen the higher k modes. Similarly k 2 / K appears in the\npotential solver but this exaggerates the higher k modes.\n\n4.7\n\nOptimal Schemes\n\nIn the last few sections we have used fourier analysis t o study why spatial\ndiscretization errors occur and how they may affect the overall solution. We\ncan use this type of analysis to develop improved schemes in which the errors\nintroduced at each stage can partially cancel each other.\nAll four stages in our code which contribute t o the evaluation of the force\non the particles, ie charge assignment t o the mesh, solving Poisson\xe2\x80\x99s equation\nt o obtain the potential, differencing the potential t o derive the electric field\nand force values at the mesh points, and finally interpolating force values at\nthe particle positions from the mesh values, can be expressed as convolutions\n\n40\n\nin x-space and so as products in k-space. For example, rewriting\n\nEg = - dg+l - dg-1\n2Ax\n\nas\n\nwhere the operator D is defined as\n\nThe fourier transform of equation (43) is\n\nk(k)= - B ( k ) J ( k ) .\nSimilarly interpolation of force t o particle positions is given by\n\nwhich has transform\n\nF(k)= - L @ ( k ) k ( k ) .\nAx2\n\n(44)\n\nCombining equations (43),(44),(36)and (37), gives\n\nF ( k ) = -L @ ( k ) f i ( k ) G ( k ) & (\nAx2\nAx\n\nk)@( k )\n\nand so\n\nThis equation enables us t o combine the separate numerical steps in a way\nwhich produces the most \xe2\x80\x9caccurate\xe2\x80\x9d expression for F ( z).\n41\n\n5\n\nEnergy Conservation and Collision Times\n\nThe schemes we have considered so far are momentum conserving, but do\nnot conserve energy, although the misconservation can be kept t o acceptably\nsmall levels. Energy conserving schemes do exist, but they fail t o conserve\nmomentum. The crucial difference in energy conserving schemes is that VW\nor VS, when needed, are evaluated analytically rather than numerically. We\nshould point o u t that the term \xe2\x80\x9cenergy conserving\xe2\x80\x9d is a little misleadingleading in this context, since these schemes only conserve energy in the\nlimit as At + 0. In other words, introducing time discretization breaks this\nconservation.\nIn momentum conserving schemes the misconservation of energy is due\nto aliasing. Likewise in energy conserving schemes the misconservation of\nmomentum is also due to aliasing. If we eliminate aliasing by using a band\nlimited cloud shape function S(z), we can conserve both energy and momentum. However this is an expensive option because it generally requires\na. very high order weighting function W ( z )which is highly non-local, and\nfor this reason is almost never used.\nWe have seen that aliasing is less of a problem for schemes which use\nhigher order charge assignment. Therefore, for the momentum conserving\nschemes which we have considered, we would expect the GIG scheme to\nconserve energy better than the NGP scheme.\n\n5.1\n\nHeating Time\n\nThe principal symptom of energy misconservation is particle heating. Hockney [6] made a systematic study of heating and collision times for a 2D\nelectrostatic plasma. Let hi(t) be the deviation of the kinetic energy of the\nithparticle from its initial value. The average over all particles is\n\nWe define a heating time TH such that\n\nHow do we determine (~(TH)) Assume that the errors in our model con?\ntribute t o a stochastic error field S E . For simplicity we shall assume SE is\n42\n\nconstant in magnitude but varies randomly in direction. For one timestep\nit introduces a momentum change\n\nm6v = qSEAt\nfor each particle. Each particle describes a random walk in v-space from its\ninitial velocity vo. Writing Av = w - W O , after n timesteps\n\n( A v )= 0\nq2At2\n(lAv12) = n----l6El2\nm2\n\nwhich implies\n\nStochastic heating increases linearly with the number of timesteps.\nHockney has shown that the heating timescale is a complicated function\nof both Ax/Xd and w,,At. The heating rate increases as either Ax/Xd or\nupeAtincreases. CIC has a slower heating rate than NGP, by a factor of\nroughly 20.\n\n5.2\n\nCollision Times\n\nThe effective collision frequency v, was determined in the 2D model by\nmeasuring the deflection cp;(t)of particles from their original direction.\n\nThe collision time rc is defined by\n\nHockney [6] found the relationship\n\n43\n\nRecall that rpeis the shortest timescale associated with collective modes in\nthe system. As the particle size w 40 the collisionality of the system is\ndetermined by the number of particles in the Debye circle (nX2). The finite\nsize of particles helps to increase the collision time. Note that since w is the\nsame for both NGP and CIC, both schemes have the same collisionality.\n\nHigher Dimensions\n\n6\n\nThe basic steps are the same. However everything is more complicated\nto program and more costly t o run. Anisotropies appear due to the use\nof square or cubic particle shapes, and due t o directional dependencies in\ntruncation errors of finite difference approximations. Waves propagate with\ndiffering ease in directions aligned with or between axes. These anisotropies\ncan be reduced by using smoother cloud shape and assignment functions.\nVisualization of results is considerably more difficult, particularly in 3D.\n\nPIC for Compressible Fluid Flow\n\n7\n\nPIC was originally invented by Harlow for this application. His motivation\nwas t o develop a tool to study highly distorted or sheared flows, or strongly\nshocked flows involving material interfaces and contact discontinuities in 2\nor 3D. Here the particles represent fluid elements. In time the approach was\ndropped in favor of improved fluid codes because it was\ne\n\ntoo noisy\n\n0\n\nhad high numerical viscosity (momentum diffusion)\n\n0\n\nsuffered from large heat conduction (energy diffusion).\n\nIt has been revived recently in codes like FLIP which are low dissipation\nPIC codes.\nIn its original incarnation, fluid PIC was a partially lagrangian method\n(mass was the only lagrangian variable). Modern fluid PIC is a fully lagrangian method (ie. mass, momentum and energy are all lagrangian variables).\nFor old style fluid PIC a typical timestep would be as follows:\n1. load N p particles, each with mass m;, velocity u; and internal energy\ne;.\n\n44\n\n2. set up a mesh.\n3. construct mesh point values of the fluid density pg, velocity U , and\ninternal energy Eg by assigning the equivalent particle quantities t o\nthe mesh using eg.\n1\np --\n\nrn;W(x;- zg).\np\n\n"-v,\n\n4. solve the navier stokes equations on the mesh using finite differences.\n\n5. construct a velocity and energy for each particle using eg.\nu;\n\n=-\n\np , W ( X ,\n\n- Xi).\n\n9\n\n6. move the particles by solving\n\ndx;\n- = u;.\ndt\n7. begin cycle again at 1.\nMass density information goes just one way, from the particles to the\ngrid. Mass is a lagrangian variable so mass diffusion is eliminated. However\nparticle momentum and energy information is replaced every timestep by\nnew values interpolated from the grid solution of the fluid equations. It\nis this transfer backwards and forwards which made the old style PIC so\ndiffusive.\nModern fluid PIC, like FLIP, makes momentum and energy lagrangian\nvariables also. This is achieved by solving fluid equations on a lagrangian\ngrid, (ie. a grid which moves with the local fluid velocity), then updating rather than replacing the particle velocities and energies using the grid\nsolution.\nFLIP( Fluid Implicit Particle, Brackbill et al) is an implicit lagrangian\ncode with adaptive re-zoning. It is less accurate than finite difference methods where they apply. It is also more expensive. However it still retains its\nadvantage over finite difference methods where contact discontinuities and\nmaterial interfaces are important.\nEPIC(Ephemera1 PIC, Eastwood) is an alternative low dissipation fluid\nPIC approach. It is finite element based, using an anti-diffusion step t o\nremove momentum and energy diffusion.\n45\n\n8\n\nApplication to Incompressible Fluid Flow - the\nVortex Method\n\nBy definition, for incompressible flow\n\nvp=o\nand by implication from the mass continuity equation\n\nv.u=o.\nWe can therefore write u as\n\nu=vx+\nwhere\n\n+ is the stream function. The momentum equation is\ndu\np- = v p\ndt\n\nand the vorticity is defined as\n\nw=vxu\nWe can show that vorticity is a conserved property of a fluid element in\nincompressible flow, ie.\nd\n-v\n\ndt\n\nand since V\n\nx\n\n1\nx u = --v x v p\nP\n\nV f = 0 for any function f we have\n\nw\n-d = o\ndt\n\nIn 2D, u, = 0 and d / d z z 0, so\n\nw = -V2(&k).\n\nSo for an element of fluid to which we assign a vorticity uzk)the equations\nof motion are\ndx\n-= u\ndt\n46\n\nu=\n\nv x ($&)\n\n02&= -w,\nie. the same equation structure as for a 2D electrostatic plasma. 3D vortex\nmethods involve tracking vortex tubes. This is not a simple generalization\nof the 2D method. A comprehensive review of the vortex method is given\nby Leonard [5].\n\nParallel PIC\n\n9\n\nTo understand how t o optimize a PIC code for a particular architecture we\nneed to understand how the processors and data memory are configured.\nWe can think of our parallel computer as a system of data memory which\nfeeds data to and receives data from a set of arithmetic processing units.\nThis memory system can include\n0\n\nregisters on processor\n\ne\n\ncache on processor\n\n0\n\ndistributed RAM.\n\nEach of these components have significantly different access times. The\ntrick in optimizing the code is to tailor the algorithm and data layout to the\nmachine in such a way as t o keep the largest possible number of processors\nworking effectively while reducing their access times to the data which they\nneed.\nIn the most abstract sense, we have an algorithm and a data structure\nto map t o the architecture. The four basic steps in a PIC algorithm are\n1. assign particle charge(mass) to the mesh\n2. solve for the force field on the mesh\n\n3. interpolate force from the mesh t o the particle positions\n4. push the particles.\nIn combination these four steps involve computation and communication\nbetween two different data structures. The field data has the qualities of\nan ordered array in the sense that each element has specific neighbors. The\nparticle data has the qualities of a randomly ordered vector, in which element\n47\n\ni refers t o particle i , and no element has any special relationship t o its\nneighbors in the vector.\n\nSteps 2 and 4 are parallelizable in rather obvious ways, since they involve\nonly simple and completely predictable d a t a dependencies, and do not couple\nthe two data structures. Steps 1 and 3 however do couple the two data\nstructures, with complicated and unpredictable data dependencies which\nevolve during the simulation. It is these steps which invariably dominate\nthe execution times of parallel PIC codes.\nThere is one further observation t o make before we discuss implementation specifics. On a serial machine our code will execute its computational\nworkload in a time which is independent of any correlations in the spatial\nlocations of the particles. This is not true on parallel machines. Particle\nclustering can create communication and/or computational hot-spots which\nimpair performance. For example, an algorithm which works very efficiently\nfor an homogeneous plasma application may be very inefficient for a highly\nclustered gravitational N-body problem. This can be an important factor\nin choosing an algorithm.\nOn a specific parallel machine, many factors will influence a codes performance. Machine architecture is unquestionably the most important. It\nwould be appealing to present the best techniques for each of the broad architecture classes, such as SIMD (Single Instruction Multiple Data), MIMD\n(Multiple Instruction Multiple Data) with distributed memory and MIMD\nwith shared memory. However such a clean presentation would be misleading. Even within these broad classes there are more minor architectural\ndifferences which can cause us t o favor different algorithms. Differences in\ncompilers, both in terms of functionality and maturity,8 add t o the problem.\n\nSo we will not attempt such a general description of preferred parallel\nalgorithms. Instead we will examine how PIC codes have been parallelized\non three specific machines. These techniques should serve as suggestions\nrather than rules, when we approach other machines. The three parallelization tasks we will consider are vectorizing the code for a powerful vector\nmachine such as the Cray YMP series, implementing it on the MasPar\xe2\x80\x99s\nSIMD architecture, and on a distributed memory MIMD machine, Intel\nTouchstone Delta.\n8Many of the parallel machines and parallel compilers are in the earliest stages of their\ndevelopment.\n\n48\n\n9.1\n\nVeetorization\n\nThree of the four basic steps of a PIC code (steps 2,3 and 4 above) are\nalmost trivially vectorizable.\nThe particles are pushed by looping over the particles in sequence and\npushing each in turn. Since each particle push is independent these loops\nvectorize. Highly efficient vectorized field solvers (eg fft\xe2\x80\x99s or multigrid) exist\nin various libraries(eg IMSL,NAG,ELLPACK) and we need not consider this\nany further. The force interpolation is a gather operation. Again we can\nloop over the list of particles, fetching the force values for the cells in the\nneighborhood of the particle. There are no data dependencies which will\ninhibit vectorization.\nThe only PIC code step which is not easily vectorizable is the charge\ndeposition step. In a serial implementation we would loop over the list of\nparticles in turn, determining where each particle is located in the mesh and\nthen distributing contributions t o the charge density of the mesh cells in the\nimmediate vicinity of the particle. This loop will not vectorize automatically\nbecause it is possible that two particles might try to add charge to the same\nmesh cell at the same time. We can solve this problem in more than one way.\nThe choice of solution depends on how often particles in the vector pipeline\nwill try to add charge t o the same mesh cell. If this is a regular occurrence\nthen the best solution is to pre-sort the particles. We will illustrate one\nway t o do this using an algorithm devised by Horowitz [7]. However if it is\na rare occurrence then we can test for when this happens and only inhibit\nvectorization when those particles are in the vector pipeline.\n9.1.1\n\nPre-sorting\n\nFor simplicity consider a 1D model using NGP weighting. Assume there\nare ng particles in mesh cell number g . In this approach these particles are\nnumbered from 1 t o ng. This is repeated for all the cells in the mesh. From\nthese lists we form a number of particle groups. Group one contains all the\nparticles numbered 1, group two all the particles numbered 2, and so on.\nNow loop over particle group 1. Each particle deposits its charge into the\nmesh cell t o which it belongs. Since no two particles in group 1 belong in the\nsame mesh cell we know that two particles can never be writing to the same\narray element at the same time, and we can force the loop over particles in\nthe group t o vectorize. We repeat this process in turn for all the particle\ngroups. The only part of this algorithm which does not vectorize is the\n49\n\noriginal grouping of the particles. It is easily extended t o higher dimensions\nand to cope with higher order charge assignment schemes such as CIC.\nAdditional memory space is required to store the particle lists (and ext r a charge density arrays in the case of higher order charge assignment).\nHorowitz discusses how the algorithm can be tuned to trade-off between\nCPU performance and memory usage as resources might dictate.\n9.1.2\n\nDependency Testing\n\nAgain, let us consider the 1D model with NGP weighting.\ndeposition would be achieved by the Fortran 77 loop\ndo i=l,npart\nrho(index(i))\nenddo\n\n= rho(index(i))\n\nThe charge\n\n+ q(i)\n\nwhere index(i) is the mesh cell to which particle i with charge q(i) belongs, and rho ( j ) is the charge in mesh cell j . This scatter-with-add loop\nhas potential vector dependencies, since index is not known until run time\nand changes during the simulation.\nWe break this loop into sections of length nblock.\ndo il=l,npart-nblock+l,nblock\ndo i=il,il+nblock\nrho(index(i)) = rho(index(i))\nenddo\nenddo\n\n+ q(i)\n\nNow consider one of these short inner blocks. We will test this inner loop\nt o see if any vector dependencies actually occur within it. First we set up\na temporary integer array, itemp, which has as many elements as there are\ncells in the mesh. We also set up two temporary integer arrays ia and ib of\nlength nblock, with ia(i)=i. Now we use the appropriate nblock elements\nof index to scatter ia into itemp, forcing vectorization and accepting any\noverwrites which might occur.\ncdir$ivdep\ndo i=il,il+nblock\nitemp(index(i)) = ia(i>\nenddo\n50\n\nThen we t r y t o reverse the scatter operation, by gathering elements of itemp\ninto ib, under the influence of index.\ndo i=il,il+nblock\nib (i)=it emp ( index ( i) )\nenddo\nIf no overwrites occured during the scatter operation, then the gather step\nexactly reverses the scatter and so ia and ib should be identical. In that\ncase, for this particular block, we can safely force the inner loop to vectorize\nusing a compiler directive. However if ia and ib differ anywhere, we have\ndetected a vector dependency and the inner loop must execute in serial\norder.\nBased on the success or failure of this test for each block, we can branch\nt o a copy of the inner loop either with or without a preceeding compiler\ndirective t o ignore vector dependencies.\nThis scheme enables us t o vectorize the charge deposition task over vector\nlengths nblock. Of course the test involves an additional overhead of a\nvectorized scatter and a gather step, so we would only consider it if we\nexpected the test t o find no vector dependencies much of the time. For a\nrandom spatial distribution of particles the chance that two or more particles\nin the same block will t r y to add charge t o the same cell is determined by\nthe ratio of nblock t o the number of cells in the mesh. For performance\nreasons we would like t o keep ia and ib in vector registers which means\nthat nblock will be set t o the vector pipeline length. On the Cray C90 that\nmeans nblock = 128. nblock will be much smaller than the number of mesh\npoints for all but the smallest 1D meshes. As a consequence, the benefit of\nthe improved vectorization should far outweigh the overhead associated with\nthe test.\nN o changes are required t o apply this scheme in 2 or 3D, and minor\nmodifications can accomodate higher order charge assignment schemes.\n\n9.2\n\nSIMD Implementation\n\nThe MasPar MP-2 has a SIMD architecture with up t o 16384 (128 x 128) processors. The nominal peak performance of a 128 x 128 machine is 6.2Gflops.\nEach processor has 64Kb of dedicaked data memory. The processors are arranged in a 2D array with dimensions which are integer increments of 32, ie\nCray\xe2\x80\x99s cf77 compilation system automatically implements this solution.\n\n51\n\n32, 64, 96 or 128. Straightline connections, known collectively as the X-net,\nexist between processors in the north, south, east, west, north-east, southeast, south-west and north-west directions. At the edges of the processor\narray the X-net wraps around so that the array has the same topology as\nthe surface of a torus. Inter-processor communications can be achieved in\none of two ways. The global router can be used for more complex patterns\nor for communication between widely spaced processors, while for regular\npatterns over short distances the X-net communications are much more efficient. The MasPar series broadens the definition of SIMD in at least one\nimportant way. It enables indirect addressing within a processor memory.\nOn a distributed memory machine, such as the MasPar, data layout\nacross processor memory is an integral part of algorithm design. For PIC\ncodes, once we have chosen the layout of the field arrays and particle data\nwe have essentially set the computational and communication workload for\neach processor during each of the steps of the code. The challenge on the\nMasPar is to spread the computation and communication workload as evenly\nas possible, while minimizing the amount of global router communication\nrequired."\nThe field arrays will be laid out so as t o optimize the field solver routine.\nWe do not need to consider the fine details of this layout, which will vary\ndepending on the exact size of the physical mesh and the size of the processor\narray. All we really need to recognize is that any acceptable layout will\nestablish a mapping between physical mesh points and the processor array\nso that it includes most if not all the processors, and that nearest neighbor\nmesh cells will map t o processors which are no further apart than nearest\nneighbors. For example if we have a 3D mesh of size 128 x 128 x 128 and\na processor array of size N,,,, = 128 x 128, we could map cell ( i , j , k ) into\nprocessor (i, j ) .\nThe major design question which faces us is how to distribute the particle\ndata. There are some obvious choices which focus either on computational\nload balance or on efficient interprocessor communication [9].\n\n9.2.1\n\nUniform Load Balance\n\n- with communication\n\nhotspots\n\nThe first option is t o parcel the particles out evenly amongst the processors, paying no attention to their physical locations. This achieves the best\n\'\'A plural floating point multiply takes 40 clocks on the MP-2, an X-net operation sending a real number a distance of 1 processor takes 41 clocks, and a random communication\npattern using the global router, with all processors participating takes N 5000 clocks.\n\n52\n\ncomputational load balance during the particle push and during the purely\ncomputational parts of the charge deposition and force interpolation tasks.\nIt also makes memory management easier, since we know exactly how much\nmemory we will need in every processor.\nHowever it makes very heavy use of the global router for interprocessor\ncommunication. Any given particle can potentially seek t o deposit charge\non any of the processors in the processor array. For example if we use CIC\nfor a 3D model, each particle has 8 charge contributions t o distribute to a\n2 x 2 x 2 block of elements somewhere in the charge array. We can pack these\n8 components into a message which is then sent by the global router t o one\nof the processors storing the 2 x 2 x 2 block, which then distributes them,\nas required among its neighboring processors using the X-net. Similarily\nduring the force interpolation the particle needs t o interpolate between the\n24 field components associated with the same 2 x 2 x 2 block (ie 8 components\nfor each of the z,y and z directions). It is actually more efficient t o pack\nthe particles coordinates into a message, send them to a processor in the\n2 x 2 x 2 block, collect the 24 components there using X-net, compute the\ninterpolated field values, pack them into a reply, and send the reply back to\nthe originating processor using the router.\nThis scheme is slow because of its extensive use of the global router,\nand it scales poorly in situations where clustering occurs. Communication\nhotspots occur when a large number of messages are being sent to the same\nprocessor at the same time. The processors can only process one router\nmessage at a time. If n p particles are actually located in cells which map\ninto processor p , then processor p will need to receive n p messages during\nthat timesteps charge deposition. This algorithm therefore will scale as\nng,,,, the maximum value of n p across the processor array.\n9.2.2\n\nUniform Load Balance\n\n- without communication hotspots\n\nWe can improve the scaling of the charge deposition by using a combination\nof a sort and a segmented vector scan-add [3]. This approach is easiest t o\nexplain for NGP charge assignment in the simple case where we have N p\nparticles and an equal number of processors.\nBefore we start we associate a unique id number ID;j = i ( j - l)Nx\n( k - l ) N , N , with each mesh cell ( i , j , k ) ,where N , and N y are the z and\ny dimensions of the mesh. As before we distribute the particles uniformly\namongst the processors, with, in this example, one particle per processor.\nWe think of the particle data and the mesh as long vectors. We set up\n\n+\n\n53\n\n+\n\nanother integer vector containing the particle cell ids. This vector is sorted in\norder of increasing cell id and the permutation required to sort it is recorded.\nNow we apply the same permutation to the remaining particle data vectors.\nThis permutation rearranges the one to one association between particles\nand processors. Because this is a permute operation it has no communication\nhotspots. It is performed using the global router. When this step is complete\nthe particle vector is composed of a sequence of blocks of varying lengths.\nEach block is a contiguous list of particles whose spatial locations map to the\nsame mesh cell, and each block is stored in a contiguous block of processors\nwith one particle per processor. We can now use a segmented vector scanadd operation t o sum the charge in each block. This can be written using\nX-net calls, and scales as In nkax.One possible choice of sorting algorithm\nis a split-radix sort which scales as In N p .\nThe force interpolation can also be handled using a scan function t o\nachieve a scaling with lnnkax. The first particle in each block fetches the\ncomponents of the field from the mesh cell corresponding t o the particle\xe2\x80\x99s\ncell id. Since no processor receives more than one request there are no\ncommunication hotspots during this fetch. Then we use a segmented scancopy t o copy these values t o every other particle in that particle block.\nThis scheme is easily extended t o accomodate higher order charge deposition algorithms and cases where the number of particles and processors\ndiffer. It is slow because the sort operation is slow, and will not compete\nwith the other schemes outlined here for simulations without severe particle\nclustering. However in cases with severe clustering it may be the only viable\nchoice because it is free of communication hotspots.\n9.2.3\n\nA Particle Migration Strategy\n\nThe schemes we have outlined so far all make heavy use of the global router.\nWe can avoid the router completely if we distribute the particles amongst\nthe processors according to the same mapping used for the field arrays.\nIf a particle lies in cell ( i , j , k ) we store it on the processor t o which we\nmapped cell ( i , j , k ) . During the charge deposition, no particle will need\nt o send charge any further than t o a neighboring processor, and during\nthe force interpolation the mesh field values which the particle needs are\neither on processor or stored by a neighbor. This enables us t o use X-net\ncommunications exclusively. To maintain this locality we are required t o\nmigrate particle information from processor t o processor as the particles\nmove between mesh cells. Since our timestep constraint limits the distance\n\n54\n\nany particle can travel during that timestep to less than one mesh cell width,\nthe migration can be achieved efficiently using the X-net.\nThere are of course drawbacks associated with this scheme. The additional code needed t o perform the particle data migration makes the algorithm more difficult t o program and debug. It also suffers from load\nimbalance as particle clustering develops. In this case both the communicaProcessor memory management\ntion and computation costs scale as nLaz.\nis tricky. We have t o allocate enough memory that the most heavily populated processor does not run out of memory. However this means that a lot\nof memory space in other processors will be allocated and never used.\nThis scheme has proven t o be significantly faster than the others we have\ndescribed when applied to relatively uniform spatial particle distributions.\nThis is a testament to the relative efficiency of X-net communications when\ncompared t o global router communications.\nOne possible solution t o its memory management weakness is to supplement this scheme with a ba,ckup routine similar to that used in the uniformly\nload balanced technique above. Two distinct particle populations are identified, those which have been migrated successfully(popu1ation I) and those\nwhich have not(popu1ation 11). We use the migration strategy wherever possible. Any population I particle which tried t o migrate to a processor whose\nmemory was already full is left where it is and relabelled as population 11.\nAfter we deposit the population I charge t o the mesh, we use the global\nrouter t o deposit charge from any population I1 particles. Similarily, when\nwe have completed force interpolation for the population I particles we use\nthe router approach to find the forces for any population I1 particles. At\nregular intervals (ie every 10 timesteps perhaps), we test t o see if the popT\nulation I paxticles might now be placed into the correct processors and so\ntransferred back to population I. This hybrid scheme enables us to minimize\nmemory wastage.\n\n9.3\n\nMIMD Implementation\n\nMIMD systems present us with a more coarse grained parallelism. A MIMD\nmachine will have somewhere in the range of 10 t o 2000 microprocessors,\neach capable of running their own instruction stream. In principle, this introduces the possibility of using control decomposition(ie farming out separate\ntasks t o different processors) as well as data decomposition. In practice the\nmandatory time ordering of the separate tasks in a PIC code leaves too little\nflexibility t o make much use of control decomposition. However the separate\n\n55\n\ninstruction streams do enable SPMD (Single Program Multiple Data) style\nprogramming, and as we shall see this can prove useful in optimizing load\nb alan ce .\nAs with SIMD, our design goals are t o keep as many of the processors\nas we can busy doing productive work. In choosing a data decomposition\nstrategy we must bear in mind that we have many fewer processors than\nin the SIMD machine, that these processors ,are considerably more powerful\ncomputationally than their SIMD counterparts, and come with larger local\nmemory banks. As a result, the balance between computation and interprocessor communication implied by our data decomposition must be struck\nsomewhat differently than in our SIMD approach. It may also be necessary\nt o dynamically reconfigure the data decomposition as the solution evolves,\nsince the load imbalance penalty for a poorly fitting decomposition becomes\nmore severe when we are working with fewer processors. The overhead for\nthis dynamic load balancing must be factored into our analysis.\nWe will discuss two specific MIMD PIC implementations. The first uses\nseparate domain decompositions for particle related operations and for the\nfield solver, at the cost of the extra communication required t o share data\nbetween the two. The second approach uses only one decomposition. Both\ncan perform dynamic load balancing.\n9.3.1\n\nA Dual Decomposition\n\nThe first example we have chosen to study is the \xe2\x80\x98General Concurrent PIC\ncode\xe2\x80\x99 developed by Liewer et a1 [lo] [ll]. They have run this code on a\nnumber of machines including the Intel Touchstone Delta.\nThe Delta features 576 i860 microprocessors, each placed at a node of\na regular 2D communication grid. Each processor has 16Mbytes of RAM,\nand in principle can achieve a peak of 80Mflops (single precision), although\nsustained performance is typically 5 10% of peak.\nInterprocessor communication is achieved by exchanging packaged messages. This message passing can be either synchronized amongst the processors or asynchronous. It is not significantly more expensive for a processor\nt o communicate with a distant processor than with a near neighbor. There\nis a significant setup overhead associated with each message sent, so it is\nbetter t o send a few long messages than a lot of short ones. The cost of a\nmessage increases with the length of the message, so it is advantageous t o\neliminate any unnecessary communication.\nFor simplicity, we will assume a 2D model covering a rectangular physical\n56\n\ndomain. The algorithm uses two separate data decompositions, the first t o\nensure that the particle push, charge deposition and force interpolation tasks\nare effectively load balanced, and the second to ensure that the field solve\nis load balanced.\nFirst the physical domain is divided into sub-domains, with one subdomain assigned t o each processor, and with roughly equal nurnbers of particles in each sub-domain. For non-uniform particle distributions these subdomains will not have equal areas and will contain different numbers of\nmesh points. Each processor is responsible for storing the data describing\nthe particles in its sub-domain and for integrating their equations of motion.\nWhen a particle moves from one sub-domain t o another the information describing the particle must be migrated to the appropriate processor. The\nprocessors also store the values of the electric field and charge density at the\nmesh points in their sub-domain, including any guard cells immediately outside the sub-domain boundaries which may be required. As the simulation\nevolves and particles move between sub-domains the sub-domain boundaries\nare adjusted at regular intervals t o maintain roughly equal numbers of particles in each sub-domain. This guarantees that the particle push will be\nvery evenly load balanced.\nBecause we are almost certain t o have unequal numbers of mesh points\nin each of these \xe2\x80\x98primary\xe2\x80\x99 sub-domains, the field solver will not be load\nbalanced. Therefore a secondary domain decomposition is established t o\nsuit the requirements of the field solver. Exactly what this decomposition is\nwill depend on the technique used t o solve Poisson\xe2\x80\x99s equation, but it is most\nlikely t o divide the mesh cells equally amongst the processors, in contiguous\nblocks. These \xe2\x80\x98secondary\xe2\x80\x99 sub-domains do not change during the simulation.\nAt the start of a timestep the particles deposit their charge to the mesh\ncells in their primary sub-domain. Because this information is needed by\nthe field solver, and because the primary and secondary sub-domains do not\nnecessarily coincide, this requires some inter-processor communication. The\ndata which processor A must send t o processor B is packaged together into a\nsend buffer at A and then sent to B. When it arrives it is unpacked and stored\nappropriately. The field solver executes and the electric field is determined\nfor every mesh point in the secondary sub-domains. The communication\npattern is then reversed and the field values for the mesh points in the\nprimary sub-domains are updated. At this point the forces on the particles\ncan be evaluated by interpolation from the mesh using local data only. Then\nthe particles are pushed. The final step is to transfer particle information\nbetween processors for any particles which have moved to a different primary\n57\n\nsub-domain. Again this is done by packaging the information into longer\nmessages in order to amortize the message start-up costs.\nThe selection of sub-domains is chosen so as to minimize inter-processor\ncommunication. There is no unique preferred solution t o this problem. It\nis model dependent. Generally speaking, increasing the ratio of sub-domain\narea t o boundary length (or volume to surface area in 3D) minimizes the\npercentage of particles which migrate between sub-domains. However if this\nreduces the overlap between primary and secondary sub-domains there will\nbe more data to be communicated before and after the field solver is called.\nIn some simulations particles tend to move in a preferred direction and then\nit pays to use long thin sub-domains aligned along this direction.\nThe details of the dynamic adjustment of the primary sub-domain boundaries obviously depend on how we choose t o configure the sub-domains. Because there is an overhead associated with this adjustment, it should only be\ndone when the load imbalance has exceeded some threshold value for which\nthe resulting gain in performance outweighs the overhead. Since we wish t o\nequalize the number of particles in each domain we need t o know how many\nparticles are in each sub-domain. This information can be accumulated with\nalmost no extra effort during the charge deposition task, by also calculating\nthe particle number densities at the mesh points. Then each processor sums\nthe number densities over all the mesh cells in its sub-domain.\n9.3.2\n\nA Single Decomposition\n\nIn this case a single domain decomposition is used for both particles and\nfields [12]. Since we do not keep a second copy of the field information this\nrequires less memory than the dual decomposition strategy. Inter-processor\ncommunication is required to migrate particle information when particles\nchange sub-domains, t o exchange guard cell field and charge density values,\nand in the exchange of any field information required within t h e field solver.\nThe load on processor i is assumed to be of the form\n\nL; = AN; + BM;\nwhere N; is the number of particles and M; is the number of mesh points\nin sub-domain i. A and B are constants which reflect the relative computational and communication costs of particle related and mesh related\noperations respectively. The objective is to adjust the sub-domain boundl\naries, and therefore N; and M; so that L; is the same for al sub-domains. It\n\n58\n\nis possible therefore that one processor might spend more of its time on particles and less on mesh points than another processor, and yet both would\ncomplete the timestep at the same time. This benefit cannot be realised\nfor electrostatic codes because the global character of Poisson\xe2\x80\x99s equation\nrequires that the processors be synchronized both at the start and finish of\nthe field solver calculation. However electromagnetic codes can be built that\nsolve just local finite difference approximations t o Maxwell\xe2\x80\x99s equations \xe2\x80\x98I.\nThese require less stringent processor synchronization. For example, at the\nbeginning of a timestep processors could accumulate current density (current density not charge density is required), update interior field values and\npush interior particles without requiring any information from neighboring\nsub-domains. Since this accounts for most of the computational effort a\ncombined field/particle load balance is effectively possible. The remaining\nsteps, exchanging guard cell information, updating sub-domain boundary\nfield values and particles, and migrating particles would require some synchronization between processors.\n\nElectromagnetic models can be set up so that the solutions to cV x E = -aB/at and\nCV x B = aB/at + J satisfy both Poisson\xe2\x80\x99s equation and V . B = 0, provided both are\nsatisfied by the inital fields [13].\n\n59\n\n10\n\nAcknowledgements\n\nThanks are due t o a number of people who took the time t o review the\nmanuscript and suggest improvements. These include Dr. Farzad Kaziminezhad.\n\n60\n\nReferences\n[l] R.W. Hockney and J.W. Eastwood, Computer Simulation Using\nParticles, Institute of Physics, (1988).\n\n[2] C.K. Birdsall and A.B. Langdon, Plasma Physics via Computer\nSimulation, McGraw-Hill Inc., (1981).\n[3] G.E. Blelloch, Vector Models for Data-Parallel Computing, MIT\nPress, (1990).\n\n[4] F.H. Harlow, \xe2\x80\x9cThe Particle-in-cell Computing Method in Fluid\nDynamics\xe2\x80\x9d, Methods Comput. Phys., 3,319, (1964).\n[5] A. Leonard, \xe2\x80\x9cVortex Methods for Flow Simulation\xe2\x80\x9d, Journal of\nComputational Physics, 37,289, (1980).\n[6] R.W. Hockney, \xe2\x80\x9cMeasurement of Collision and Heating Times in a\nTwo-Dimensional Thermal Computer Plasma\xe2\x80\x9d, Journal of Computational Physics, 8 , 19, (1971).\n[7] E.J. Horowitz, \xe2\x80\x9cVectorizing the Interpolation Routines of Particlein-Cell Codes, Journal of Computational Physics, 68, 56, (1987).\n[SI J. Binney and S . Tremaine, Galactic Dynamics, Princeton University Press, (1987).\n191 D. W. Walker, \xe2\x80\x9cCharacterizing the parallel performance of a\nlarge scale, particle-in-cell plasma simulation code\xe2\x80\x9d, Concurrency:\nPractice and Experience, 2, 257, (1990).\n\n[lo] P. C. Liewer and V. K. Decyk, \xe2\x80\x9cA General Concurrent Algorithm\nfor Plasma Particle-in-Cell Simulation Codes\xe2\x80\x9d, Journal of Computational Physics, 8 5 , 302, (1989).\n[ll] R. D. Ferraro, P. C. Liewer and V. K. Decyk, \xe2\x80\x9cDynamic Load\nBalancing for a 2D Concurrent Plasma PIC Code\xe2\x80\x9d, Journal of\nComputational Physics, 109, 329, (1993).\n\n[la] P. M.\n\nCampbell, E. A. Carmona and D. W. Walker, \xe2\x80\x9cHierarchial\nDomain Decomposition With Unitary Load Balancing For Electromagnetic Particle-In-Cell Codes\xe2\x80\x9d, Proceedings of the Fifth Distributed Memory Computing Conference, 943, (1990).\n61\n\n[13] J. Villasenor and 0. Buneman, \xe2\x80\x9cRigorous charge conservation for\nlocal electromagnetic field solver\xe2\x80\x9d, Computer Physics Communications, 69, 306, (1992).\n\n62\n\nI\n\nI\n\nREPORT DOCUMENTATION PAGE\n\nForm Approved\nOMB NO.0704-0188\n\nPublic reporting burden for this collection of information is estimated to average 1 hour per response, includingthe time for reviewing instructions. searching existing data sources, gathering\nand maintainingthe data needed, and completing and reviewing the collection of information. Send comments regardingthis burden estimate or any other aspect of this collection of\ninformation. including suggestions for reducing this burden, to Washington Headquarters Services. Directoratefor InformationOperations and Reports, 1215 Jefferson Davis Highway, Suite\n1204, Arlington, VA 22202-4302, and to the Office of Management and Budget, Paperwork Reduction Project (0704-0188). Washington, M 20503\n:\n\n1. AGENCY USE ONLY (Leave blank)\n\n3. REPORT TYPE AND DATES COVERED\n\nContractor R\nc\n4. TITLE AND SUBTITLE\n\nI\n\nOrt\n5. FUNDING NUMBERS\n\nCode 934\n\nParticle-Mesh Techniques\n\nI\n\n6. AUTHOR(S)\n\nPeter MacNeice\n\n7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES)\n\n3.\n\nPERFORMING ORGANIZATION\nREPORT NUMBER\n\nHughes STX\n4400 Forbes Boulevard\nLanham, Maryland 20706\n\n95B00072\nIO. SPONSORINGIMONITORING\n\n9. SPONSORlNGIMONlTORlNGAGENCY NAME(S) AND ADDRESS(ES)\n\nAGENCY REPORT NUMBER\n\nGoddard Space Flight Center\nNational Aeronautics and Space Administration\nWashington , DC 20546-0001\n\nCR-4666\n\n11. SUPPLEMENTARY NOTES\n\nPeter MacNeice: Hughes STX, Lanham, Maryland\n\n13. ABSTRACT (Wrnurn200 words)\n\nThis is an introduction to numerical Particle-Mesh techniques, which are commonly used\nto model plasmas, gravitational N-body systems, and both compressible and incompressible\nfluids. The theory behind this approach is presented, and its practicle implementation, both\nfor serial and parallel machines, is discussed. This document is based on a 4-hour lecture\ncourse presented by the author at the NASA Summer School for High Performance\nComputational Physics, held at Goddard Space Flight Center.\n\n5. NUMBER OF PAGES\n\n14. SUBJECT TERMS\n\nI\n\nComputational Techniques, Particle Methods, Plasmas\n\nI\n16. PRICE CODE\n\nI\n\n17. SECURITY CLASSIFICATION\n\nOF REPORT\n\nUnclassified\nNSN 7540-01-280-5500\n\n8. SECURITY CLASSIFICATION\n\nOF THIS PAGE\n\n\'\n\nUnclassified\n\n19. SECURITY CLASSIFICATION\n\nI\n\nOF ABSTRACT\n\nUnclassified\n\n20. LlMl7\'ATlON OF ABSTRACT\n\nI\n\nUnlimited\n\n._____\n\n..\n\nStandard Form 298 (Rev. 2-89)\ntL_IIL_\n\nI...\n,..*\n\n---a-\n\n....#.---\n\n'