b'On Estimating\n\nEnd-to-End\n\nNetwork\n\nMark Allman\nNASA Glenn Research\n\nPath Properties*\nVem Paxson\n\nCenter\n\nAT&T Center for Internet\n\nand\nGTE Internetworking\n\niman@grc,\n\nnasa.\n\nBerkeley,\n\ngov\n\nabout current\n\nnetwork\n\nconditions\n\navailable\n\nto\n\na transport protocol,\nthe more efficiently\nit can use the network to\ntransfer its data. In networks such as the Internet, the transport protocol must often form its own estimates of network properties based on\nmeasurements\nperformed\nby the connection\nendpoints.\nWe consider\ntwo basic transport estimation\nproblems:\ndetermining\nthe setting of\nthe retransmission\ntimer (RTO) for a reliable protocol, and estimating\nthe bandwidth\navailable to a connection\nas it begins. We look at both\nof these problems in the context of TCP, using a large TCP measurement set [Pax97b] for trace-driven\nsimulations.\nFor RTO estimation,\nwe evaluate a number of different algorithms, finding that the performance of the estimators is dominated by their minimum\nvalues, and\nto a lesser extent, the timer granularity,\nwhile being virtually unaffected by how often round-trip\ntime measurements\nare made or the\nsettings of the parameters\nin the exponentially-weighted\nmoving average estimators\ncommonly used. For bandwidth estimation,\nwe explore techniques previously sketched in the literature [Hoe96, AD98]\nand find that in practice they perform less well than anticipated.\nWe\nthen develop\na receiver-side\nalgorithm\nthat performs\nsignificantly\nbetter.\n\n1\n\nIntroduction\n\nWhen operating in a heterogeneous\nenvironment,\nthe more information about current network conditions available to a transport protocol, the more efficiently it can use the network to transfer its data.\nAcquiring\nsuch information\nis particularly\nimportant for operation in\nwide-area\nnetworks, where a strong tension exists between needing\nto keep a large amount of data in flight in order to fill the bandwidthdelay product "pipe," versus having to wait lengthy periods of time\nto attain feedback regarding changing network conditions,\nespecially\nthe onset of congestion.\nIn a wide-area\nnetwork, such as the Internet, that does not provide any explicit information\nabout the network path, it is up to the\ntransport protocol to torrn its own estimates of current network conditions, and then to use them to adapt as efficiently as possible.\nA\nclassic example of such estimation\nand adaptation\nis how TCP infers the presence of congestion\nalong an Internet path by observing\npacket losses, and either cuts its sending rate in the presence of congestion, or increases it in the absence [Jac88].\nIn this paper we examine two other basic transport estimation\nproblems:\ndetermining\nthe setting of the retransmission\ntimer (RTO),\nand estimating\nthe bandwidth available to a connection\nas it begins.\nWe look at both problems in the context of TCP, using trace-based\nanalysis of a large collection\nof TCP packet traces. The appeal of\n*This paper appears in ACM SIGCOMM \'99.\n\nCA 94704-1198\n\nvern@aciri,\n\nAbstract\nThe more information\n\nat ICSI\n\nLawrence Berkeley National Labratory\n1947 Center Street, Suite 600\n\n21000 Brookpark Rd. MS 54-2\nCleveland, OH 44135\nmal\n\nResearch\n\nand\n\norg\n\nanalyzing TCP in particular is that it is the dominant\nprotocol in use\nin the Internet today [TMW97].\nHowever, analyzing the behavior of\nactual TCP implementations\nalso introduces\ncomplications,\nbecause\nthere are a variety of different TCP implementations\nthat behave in\na variety of different\nways [Pax97a].\nConsequently,\nin our analysis we endeavor to distinguish\nbetween\nfindings that are specific to\nhow different TCPs are implemented\ntoday, versus those that apply\nto general TCP properties, versus those that apply to general reliable\ntransport protocols.\nOur analysis is based on the Af_ subset of TCP trace data collected in 1995 [Pax97b].\nThis data set consists\nof sender-side\nand receiver-side\npacket traces of 18,490 TCP connections\namong\n31 geographically-diverse\nlnternet hosts. The hosts were interconnected with paths ranging from 64 kbps up to Ethernet speeds, and\neach connection transferred\n100 KB of data, recorded using tcpdump.\nWe modified tcpanaly [Pax97a] to perform our analysis.\nThe rest of the paper is organized as follows. In \xc2\xa7 2 we look at the\nproblem of estimating\nRTO, beginning\nwith discussions\nof the basic\nalgorithm and our evaluation methodology.\nWe analyze the impact of\nvarying a number of estimator parameters,\nfinding that the one with\nthe greatest effect is the lower bound placed on RTO, followed by the\nclock granularity,\nwhile other parameters\nhave little effect. We then\npresent evidence that argues for the intrinsic difficulty of finding optimal parameters,\nand finish with a discussion\nof the cost of retransmitting unnecessarily\nand ways to detect when it has occurred.\nIn\n\xc2\xa7 3 we look at the problem of estimating the bandwidth\navailable to\na connection\nas it starts up. We discuss our evaluation methodology,\nwhich partitions estimates\ninto different regions reflecting\ntheir expected impact, ranging from no impact, to preventing loss, attaining\nsteady state, optimally\nutilizing the path, or reducing\nperformance.\nWe then assess a number of estimators,\nfinding that sender-side\nestimation such as previously\nproposed in the literature is fraught with\ndifficulty, while receiver-side\nestimation\ncan work considerably\nbetter. \xc2\xa7 4 summarizes\nthe analysis and possible future work.\n\n2\n\nEstimating\n\nRTO\n\nFor an acknowledgment-based\nreliable transport\nprotocol,\nsuch as\nTCP, a fundamental\nquestion is how long, in the absence of receiving\nan acknowledgment\n(ACK), should a sender wait until retransmitting? This problem is similar to that of estimating the largest possible\nround-trip time (RTI\') along an end-to-end\nnetwork path. However,\nit differs from RTr estimation\nin three ways. First, the goal is not\nto accurately\nestimate the truly maximal possible RTT, but rather a\ngood compromise\nthat balances avoiding unnecessary\nretransmission\ntimeouts due to not waiting long enough for an ACK to arrive, versus being slow to detect that a retransmission\nis necessary.\nSecond,\nthe sender really needs to estimate the feedback\ntime, which is the\nround-trip\ntime from the sender to the receiver i, lus the amount of\n\nAlgorithm\ntime\nrequired receiver\nforthe\ntogenerate fornewly\nanACK\nreceived2.1 The Basic RTO Estimation\ndata. orexample,\nF\nareceiver\nemployingdelayed\nthe\nacknowledg- Jacobson\'s algorithm, two state variables SRTT and RTTVAR esIn\nment\nalgorithm may ait pto500 before\n[Bra89] w u\nmsec transmitting\nanACK. estimating value\nThus,\nagood forthe\nretransmission timate theincurrent1 with kand a notion of the RTO. Both These values\ntimer are used Eqn RTI" = 4 to attain its variation. variables are\nnot nly\no involves\nestimating\naproperty network but lso\nofthe\npath, a\nan RTI" measurement\nRTTmeas is\naproperty remote\nofthe\nconnection Third, loss tocon- updated every timeand the corresponding ACK is timed taken. Since\npeer. if isdue\nonly one segment\nat any given\ngestion, behoovesender longer than the maximum\nitmay\nthe\ntowait\ntime, updates occur only once per R\'Iq" (also referred to as once "per\nfeedback time, in order to give congestion\nmore time to drain from\nthe network--if\nthe sender retransmits as soon as the feedback time\nelapses, the retransmission\nwould be successful.\n\nmay also be lost, whereas\n\nsending\n\nR\'ITs\nThe\n\nearly TCP specification\nincluded a notion of dynamically\nestimating\nRTO, based on maintaining\nan exponentially-weighted\nmoving average (EWMA) of the current Rqq" and a static variation term [Pos81].\nThis estimator was studied by Mills in [Mi183], which characterizes\nmeasured\nlnternet RTlrs as resembling\na Poisson distribution\noverall, but with occasional\nspikes of much higher RTf\'s, and suggests\nchanging\nthe estimator\nso that it more rapidly adapts to incleasing\nRq"I\'s and more slowly to decreasing RTI"s. (To our knowledge,\nthis\nmodified estimator\nhas not been further evaluated in the literature.)\n[Mi183] also noted that the balance between responding\nrapidly in\nthe face of true loss versus avoiding unnecessary\nretransmissions\nappears to be a fundamental\ntradeoff, with no obvious optimal solution.\nZhang [Zha86] discusses a number of deficiencies\nwith the standard TCP RTO estimator:\nambiguities in measuring RTFs associated\nwith retransmitted\npackets; the conservative\nRTO policy of retransmitring only one lost packet per round-trip;\nthe difficulty of choosing\nan initial estimate;\nand the failure to track rapidly increasing R"Iq"s\nduring times of congestion.\nKarn and Partridge [KP87] addressed\nthe first of these, eliminating\nambiguities\nin measuring\nRTTs. The\nintroduction\nof "selective\nacknowledgments"\n(SACKs) [MMFR96]\naddressed the second issue of retransmitting\nlost packets too slowly.\nJacobson [Jac88] further refined TCP RTO estimation by introducing\nan EWMA estimate of R\'VI" variation, too, and then defining:\nRTO = SRTT + k . RTTVAR\n\nSRTT\n\nuse today, to our knowledge\n\nthe only systematic evaluation\nof it against measured TCP connections is our previous study [Pax97b],\nwhich found that, other than\nfor over-aggressive\nmisimplementations,\nthe estimator appears sufficiently conservative\nunnecessary\ntimeout.\n\nin the sense that it only\n\n+-- (1 - o,_l)SRTT + o_l RTTmeas\n\n(2)\n\n1\nand Jacobson [Jac88] recommends\nc_1 = g, which leads to efficient\nimplementation\nusing fixed-point\narithmetic\nand bit shifting.\nSimilarly, RTTVAR is updated based on the deviation tSRTTRTTmeasl\nusing o2 = \xc2\xbc.\nAny time a packet retransmitted\ndue to the RTO expiring is itself\nlost, the TCP sender doubles the current value of the RTO. Doing so\nboth diminishes\nthe sending rate in the presence of sustained congestion, and ameliorates\nthe possible adverse effects of underestimating\nthe RTO and retransmitting\nneedlessly and repeatedly.\nSRTI" and RTTVAR are initialized by the first RTl\'meas\nmeasurement using SR77" +-- R"ITmeas and RTTVAR\nto the first measurement,\nRTO = 3 sec.\n\n_-- -_R\'Iq"meas.\n\nPrior\n\nTwo important\nadditional\nconsiderations\nare that all measurement is done using a clock granularity\nof G seconds,\ni.e., the\nclock advances\nin increments\nof G, I and the RTO is bounded\nby\nRTOmi n and RTOmax.\nIn the common BSD implementation\nof\nTCP, G = 0.5 sec, RTOmi n = 2G = 1 sec, and RTOmax = 64 sec.\nAs will be shown, the value of RTOmi n is quite significant.\nAlso,\nsince the granularity\nis coarse, the code for updating RTTVAR sets a\nminimum bound on RTTVAR of G, rather than the value of 0 sec that\ncan often naturally\n\narise.\n\nThree oft-proposed\ntor are to time every\n\nvariations for implementing\nthe RTO estimasegment\'s\nRTT. rather than only one per flight;\n\nuse smaller values of G; and lower RTOmi n in order to spend less\ntime waiting for timeouts. RFC 1323 [JBB92] explicitly supports the\nfirst two of these, and our original motivation behind this part of our\nstudy was to evaluate whether these changes are worth pursuing.\n\n2.2\n\nis in widespread\n\nwith a gain of c_l:\n\n(1)\n\nwhere SRTT is a smoothed estimate of RqT (as before) and R77VAR\nis a smoothed estimate of the variation of R\'IT. In [Jac88], k = 2, but\nthis was emended in a revised version of the paper to k = 4 [JK92].\nWhile this estimator\n\nSRTT is updated using an EWMA\n\nit later\n\nIt has long been recognized that the setting of the retransmission\ntimer cannot be fixed but needs to reflect the network path in use, and\ngenerally requires dynamic adaptation because of how greatly\ncan vary over the course of a connection\n[Nag84, DDK+90].\n\nflight").\n\nrarely\n\nresults\n\nin an\n\nThe widely-used\nBSD RTO implementation\n[WS95] has several\npossible limitations:\n( 1) the adaptive R\'VI" and RTF variation estimators are updated with new measurements\nonly once per round-trip,\nso they adapt fairly slowly to changes in network conditions;\n(2) the\nmeasurements\nare made using a clock with a 500 msec granularity, which necessarily\nyields coarse estimates (though [Jac88] introduces some subtle tricks for squeezing\nmore precision out of these\nestimates);\nand (3) the resulting RTO estimate has a large minimum\nvalue of I second, which may make it inherently conservative.\nWith the advent of higher precision clocks and the TCP "timestamp" option [JBB92],\nall three of these limitations\nmight be removed. It remains an open question, however, how to best rcengineer\nthe RTO estimator given these new capabilities:\nwe know the current\nestimator\nis sufficiently\nconservative,\nbut is it too conservative?\nIf\nso, then how might we improve it, given a relaxation of the above\nlimitations?\nThese are the questions we attempt to answer.\n\nAssessing\n\nDifferent\n\nRTO\n\nEstimators\n\nThere are two fundamental\nproperties of an RTO estimator\ninvestigate:\n(1) how long does it wait before retransmitting\n\nthat we\na lost\n\npacket? and (2) how often does it expire mistakenly and unnecessarily trigger a retransmit?\nA very conservative\nRTO estimator\nmight\nsimply hardwire RTO = 60 sec and never make a mistake, satisfying the second property, but doing extremely poorly with regards to\nthe first, leading to unacceptable\ndelays; while a very aggressive estimator could hardwire RTO = 1 msec and reverse this relationship,\nflooding the network with unnecessary\nretransmissions.\nOur basic approach to assess these two properties is to use tracedriven simulation\nto evaluate different estimators, using the following methodology,\nin IWS95]:\n\nwhich mirrors\n\nthe RTO estimator\n\nimplementation\n\n1. For each data packet sent, if the RTO timer is not currently\nactive, it is started.\nThe timer is also restarted when the data\npacket\n\nis the beginning\n\nof a retransmission\n\nsequence.\n\n2. For each data packet retransmitted\nin the TCP trace due to a\ntimeout, we assess whether the timeout was unavoidable,\nmeaning that either\n\nthe segment\n\nbeing retransmitted\n\nwas lost, or all\n\n_The BSD timer implementation also uses a "heartbeat" timer that expires\nevery G seconds with a phase independent of when the timer is actually set.\nWe included this behavior in our sirnulatitms.\n\nMi\xc2\xb0i\xc2\xb0umRTO\nW\n\nACKs sent after the segment\'s\narrival at the receiver (up until the arrival of the retransmission)\nwere lost. This check is\n\n1,000\n750\n500\n250\n0\n\nnecessary because some of the TCPs in the .A/\'2dataset used aggressive RTO estimators\nthat often fired prematurely\nin the face\nof high R\'I\'Ts [Pax97a], so these retransmissions\nare not treated\nas normal timeout events.\n3. If the timeout\n\nwas unavoidable,\n\nthen the retransmission\n\nis clas-\n\n4. If the timeout\nwas avoidable,\nthen it reflects a problem with\nthe actual TCP in the trace, and this deficiency\nis not charged\nagainst\n5. For each\n\nthe estimator\n\nwe are evaluating.\n\narrival of an ACK for new data in the trace, the ACK\n\nTable\n\n8.4\n6.5\n4.8\n3.5\n\n0.63%\n0.76%\n1.02%\n2.27%\n\n92,077\n229,564\n136,514\n85,878\n\nRTO = 2,000 msec\nRTO = 1,000 msec\nRTO = 500 msec\n\nsified as a "first" timeout if this is the first time the segment is\nretransmitted,\nor as a "repeated"\ntimeout otherwise. The estimator is charged the current RTO setting as reflecting the amount\nof time that passed prior to retransmitting\n(consideration\n(1)\nabove), with separate bookkeeping\nfor "first" and "repeated"\ntimeouts (for reasons explained below). The RTO timer is also\nbacked off by doubling it.\n\n144,564\n121,566\n102,264\n92,866\n\nmsec\nmsec\nmsec\nmsec\nmsec\n\n3.1\n15.6\n8.2\n4.5\n\n4.71%\n2.66%\n6.14%\n12.17%\n\nI: Effect of varying\n\nRTOmi n, G = 1 msec\n\nassess that all of these repeated retransmissions\nessary). If bi -I- gi > 0, that is, trace i included\nthen define pi\n\n---\n\nbl\n\nbi q-gl\n\n\'\n\nthe normalized\n\nwere indeed unnecsome sort of timeout,\n\nnumber\n\nof bad timeouts\n\nin\n\nthe trace; otherwise\ndefine pi = 0. Note that pi may not be a particularly\ngood metric when considering\ntransfers of varying length.\nHowever, this study focuses only on transfers of 100 KB.\nFor the 3th good timeout,\n\nlet RTO_\n\nbe the RTO setting\n\nof the ex-\n\narrival time is compared\nwith the RTO, as computed\nby the\ngiven estimator.\nIf the ACK arrived after the RTO would have\nfired we consider the expiration\na "bad" timeout, reflecting that\nthe feedback time of the network path at that moment exceeded\nthe RTO.\n\npiring timer, and R\'I"T_ be the most recently observed\nRTr (even\nif it was not an R\'IT that would have been measured\nfor pur-\n\nIf the ACK covers\noff.\n\n\'g\'i = Ej[_],\nor 0 if trace i does not include any good timeouts.\nFor a collection of traces, we then define W = _-_\'_iTi as the total\n\nIf the ACK\nknowledged\nery segment\nbased on the\n\nall outstanding\n\ndata the RTO timer is turned\n\nalso yielded an RTT measurement\n(because it acthe segment currently being timed, or because evis being timed), SRTT and RTTVAR are updated\nmeasurement\nand the RTO is recomputed.\n\nFinally, the RTO timer\n\nis restarted.\n\n6. The sending or receiving\nassessed, as these packets\nand if interpreted\nas simple\nmeasurements\nof R\'l"r.\n\nof TCP SYN or FIN packets is not\nhave their own retransmission\ntimers,\nACK packets\n\ncan lead to erroneous\n\nNote this approach contains a subtle but significant difficulty. Suppose that in the trace packet P is lost and 3 seconds later the TCP\'s\nreal-life RTO expires and P is retransmitted.\nWe treat this as a "first\ntimeout," and charge the estimator with the RTO, R, it computed for\nP. Suppose R = 100 msec.\nFrom examining\nthe trace it is impossible to determine\nwhether retransmitting\nP after waiting only\n100 msec would have been successful.\nIt could be that waiting any\namount of time less than 3 seconds was in fact too short an interval\nfor the congestion\nleading to P\'s original loss to have drained from\nthe network. Conversely,\nsuppose P is lost after being retransmitted\n3 seconds later. It could be that the first loss and the second are in\nfact uncorrelated,\n\nin which case retransmitting\n\nafter\n\nwaiting\n\nonly R\n\nseconds would yield a successful\ntransmission.\nThe only way to assess this effect would be to conduct live experiments, rather than trace-driven\nsimulation,\nwhich we leave for future\nwork. Therefore,\nwe assess not whether a given retransmission\nwas\neffective, meaning that the retransmitted\npacket safely arrived at the\nreceiver, but only whether the decision to retransmit\nwas correct,\nmeaning that the packet was indeed lost, or all feedback from the receiver was lost. Related to this consideration,\nonly the effectiveness\nof an RTO estimator at predicting\ntimely "first" timeouts is assessed.\nFor repeated timeouts it is difficult to gauge exactly how many of the\npotential repeated retransmissions\nwould have been necessary.\nGiven these considerations,\nfor a given estimator\nand a trace i let\n7\', be the total time required\nby the estimator to wait lot unavoidable first timeouts. Let g, be the number of "good" (necessary)\nfirst\ntimeouts, and b, the total number of-bad"\ntimeouts, including multiple bad timeouts\n\ndue to backing\n\noff the timer (since\n\nwc can soundly\n\nposes\n\nof updating\n\nthe\n\nSRTT\n\nand RTTVAR\n\nstate\n\nvariables).\n\nLet\n\n= RTO i /R\'Tq" i , so _i reflects the cost of the timeout in units\nof RTTs. We can then define an average, normalized\ntimeout cost of\n\ntime spent waiting for (good) first timeouts;\nW = Ei:g_ >o[\'\xc2\xa2\'i] as\nthe mean normalized\ntimeout cost per connection\nthat experienced\nat\nleast one good timeout; and B = Ei[pi] as the mean proportion\nof\ntimeouts that are bad, per connection,\nincluding connections\nthat did\nnot include any timeouts (because we want to reward estimators\nthat,\nfor a particular trace, don\'t generate any bad timeouts).\nW can be dominated\nby a few traces with a large number of timeout retransmissions,\nfor which the total time waiting for first timeouts can become\n\nvery..,high,\n\nso it is biased\n\ntowards\n\nhighlighting\n\nhow\n\nbad things can get. W is impartial to the number of timeouts in a\ntrace, and so better reflects the overall performance\nof an estimator.\nB likewise better reflects how well an estimator avoids bad timeouts\noverall.\nFor some estimators,\nthere may be a few particular\ntraces\non which they retransmit\nunnecessarily\na large number of times, as\nnoted below.\nFinally, of the 18,490 pairs of traces in A/\'2, 4,057 pairs were eliminated from our analysis due to packet filter errors in recording the\ntraces, the inability to pair packets across the two traces (this can\noccur due to packet filter drops or IP ID fields changed in flight by\nheader compression\nglitches [Pax97c]), or tcpanaly\'s\ninability to determine which retransmissions\nwere due to timeouts.\nThis leaves us\nwith 14,433 traces to analyze, with a total of 67,073 timeout retransmissions.\nOf those, 53,110 are "first" timeouts, and 34% of the traces\nhave no timeout retransmissions.\n\n2.3\n\nVarying\n\nthe\n\nMinimum\n\nRTO\n\nIt turns out that the setting of RTOmi n, the lower bound on RTO.\ncan have a major effect on how well the RTO estimator\nperforms,\nso we begin by analyzing\nthis effect.\nWe first note that the usual\nsetting for RTOmi n is two clock "ticks" (i.e., RTOmi n = 2(7), because, given a "heartbeat"\ntimer, a single tick translates into a time\nanywhere\nbetween 0 and G sec. Accordingly,\nfor the usual coarsegrained estimator of (7 --- 0.5 sec, RTOmi n is 1 sec, which we will\nsee is conservative\n(since a real BSD implementation\nwould use a\ntimeout between 0.5 sec and I sec). But for G = 1 msec, the twotick minimum\nis only 2 msec, and so setting\ncan have a major effect.\n\nRTOmi n to larger values\n\nParameters\n\nGranularity\n500 msec\n[WS95]\n\n(500 msec)\n250 msec\n100 msec\n50 msec\n20 msec\n10 msec\n1 msec\n\nTable 2: Effect of varying\n\ngranularity\n\nG, RTOmi n = 1 sec\n\nTable 1 shows W, W and B for different values of RTOmi n, for\nG = 1 msec. We see that W runs from 144,564 seconds for a minimum of 1 sec to about 64% as much when using no minimum.\nThe\ncolumn\n\nfor W shows\n\nthat the 1 sec minimum\n\nmeans\n\nadjusting the minimum RTO provides a "knob" for directly trading\noff ti reel y response with premature\nti meouts, with no obvious "sweet\nspot" yielding an optimal balance between the two.\nAs noted above, "delayed"\nacknowledgments\nin TCP can result\nin elevating RTTs by up to 500 msec, and in a number\nof common implementations,\nfrequently\nelevate RTTs by up to 200 msec.\nAccordingly,\nit is not clear that a minimum RTO of two ticks for\nG = 1 msec is sound. However, for the bulk of our subsequent\nanalysis, we consider estimators with no minimum bound, both to highlight the contribution\nto estimator efficiency of factors other than the\nquite-dominant\nminimum\nRTO, and to keep in mind that transport\nprotocols different from TCP might not introduce such a minimum.\nFor comparison,\nwe include three static timers that use a constant\nsetting for RTO (except they double the RTO on repeated timeouts).\nThe table highlights the heavy cost of not using an adaptive timer.\nThe constant estimators\ngenerate about 10 times as many bad timeouts as the adaptive estimators with similar relative performance\nfigures (W). The values of B don\'t tell the whole story for the static\ntimers, however, because their bad timeouts are clustered among relatively few traces. For example, RTO = 2,000 msec results in a bad\ntimeout in 538 traces, while for RTOmi n = 250 msec, which has a\nsimilar value of B, spreads its bad timeouts over more than twice as\nmany traces.\n\nVarying\n\nMeasurement\n\nWith the above caution\n\nGranularity\n\nregarding\n\nthe considerable\n\nimportance\n\nof\n\nRTOmi n in mind, we now look at the effect of varying G. In Table 2,\n(7 ranges from 500 msec down to 1 msec. In order to compare the\ndifferent granularities\non an even footing, we hold RTOmi n : 1 sec\nconstant, rather than having the relative differences\nbetween\nthe\ngranularities overwhelmed\nby using RTOmi n = 2G. We include one\nadditional row, "[WS95]," which is the estimator as implemented\nin\n[WS951.\nThis implementation\nincludes fixed-point\narithmetic and\nbit-shifting\nin order to estimate SRTT at an effective granularity of\n62.5 msec and RTTVAR at a granularity\nof 125 msec, though RTO\nitself is computed with a granularity of 500 msec.\nWe first note that for (7 _< 10l) msec, the performance\nfor good\ntimeouts,\n\nboth absolute\n\n(W) and relative\n\n(W) is essentially\n\nidentical,\n\nregardless of how fine the granularity becomes.\nBut we steadily gain\nin avoiding bad timeouts (minimizing\nB) as the granularity\nbecomes\nfiner. The reason for the gain is that the more coarse granularities\nwill often take no action in the face of a minor change in RTT, while\nthe finer granularity\n\nestimator\n\nwill adapt\n\nto rellect\n\nTable\n\n3: Effect of varying\n\n245,668\n241,100\n158,199\n131,180\n113,903\n102,544\n96,740\n92,077\n94,081\n90,212\n93,490\n97,098\n145,571\n\nEWMA\n\nparameters\n\n15.4\n14.7\n8.5\n4.4\n3.9\n3.4\n3.4\n3.1\n3. I\n3.0\n3.3\n3.5\n8.5\n\n0.23%\n0.25%\n0.74%\n2.93%\n3.97%\n4.28%\n3.84%\n4.71%\n5.09%\n7.27%\n19.57%\n20.20%\n1.30%\n\na_, a2\n\nthat a typical\n\nRTO costs a bit more than 8 RTI\'s, but much of this expense disappears as we decrease the minimum.\nB, on the other hand, shows\nthat for a 1 sec minimum, on average only about 1 in 150 timeouts is\nbad, while for no minimum,\nnearly 1 in 20 is (these bad timeouts are\nnot clustered among a particular small subset of the traces). Clearly,\n\n2.4\n\n[ws95]\n[ws95]-every\ntake-first (al, a2 = 0, RTOmin = 1 s)\ntake.first (al ,a2 = 0)\nvery-slow (_1 = sd6o,a2: 4-1"6o\n)\nslow-ever), (al = aA_,a2 = t-\'_)\nstow (al = _, a2 = 18)\nstd(eq = _,a2 = \xc2\xbc)\nstd-every (at = _,a2 = \xc2\xbc)\nfast(a_ = \xc2\xbd,a2 = \xc2\xbc)\ntake-last (al, a,, = 1)\ntake-last-ever)\' (al, a2 = 1)\ntake-last (al, a2 = 1, RTOmin = 1 s)\n\nthe change,\n\nand\n\nthis gives it a slight edge.\nAbove G = 100 msec, however, we start trading off reduced performance for avoiding bad timeouts.\nWe can cut the average rate of\nbad timeouts by nearly a factor of two by using G = 500 msec, but at\na cost of more than a lactor of two in pertbrmance.\nWe also note that\nthe [WS95] estimator clearly performs better than G = 500 msec,\nwith both W and B lower. It gains by performing\nbetter on some\nvery-large-RTl"\ntraces, because it is able to better reflect relatively\nsmall RTT changes due to its finer effective granularities\nfor SRTT\nand RTTVAR.\n\n2.5\n\nVarying the EWMA\n\nTable 3 shows\n\nthe estimator\'s\n\nParameters\nperformance\n\nwhen\n\nvarying\n\na_ (per\n\nEqn 2) and c_2, holding G = 1 msec and RTOmi n = 0 msec fixed,\nexcept where noted. The first two rows are the [WS95] implementation, which uses G = 500 msec, with the second row reflecting a\nvariant that derives an RT[" measurement\nfrom every ACK arriving\nat the sender. We see that the more frequent SRTT and RTTVAR updates have little effect on the estimator\'s\nperformance,\nonly making\nit slightly more aggressive.\nThe remaining\nestimators all use G = 1 msec. The take-first extreme of a_ = a2 = 0 simply uses the first R\'VI" measurement\nto initialize both SRTr +-- RTT and RTTVAR _-- \xc2\xbdRTT, yielding\nRTO <--- 3RTT. It never changes SRTT, R77"VAR, or RTO again\n(other than to back off RTO in the face of repeated retransmissions, and undo the backing off when the retransmission\nepoch ends).\nThe first variant of it reflects using RTOmi n = 1 sec, the second,\nRTOmi n = 0 sec. At the other extreme, we have take-last, which\nalways sets SRTT _-- RTI" and RTTVAR <-- ISRTTprev - Rqq" I.\nThe take-last-ever)\'\nvariant is the same except every packet is timed\nrather than just one packet per round trip, and the final variant raises\nthe minimum RT[" to 1 sec.\nIn between\nthese extremes\nwe run the gamut from ver),-slow,\nwhich uses one-tenth the usual parameters\n(which are given for the\nstd estimator),\nto fast, which uses twice the parameters,\nwith some\ntime-every-packet\nvariants.\nFrom the table we see that the settings of the EWMA parameters\nmake little difference\nin how well the estimator performs.\nIndeed,\nif our goal is to minimize the rate of bad timeouts and still remain\naggressive,\nwe might pick the exceedingly\nsimple take-first estimator, which only barely adapts to the network path conditions; -_or we\nmight pick slow, which on average incurs 25% less normalized\ndelay per timeout, and occupies\na sweet spot that locally minimizes\n2Even though rake-first and take-last show overall decent performance\ncompared to the other RTO estimators, these RTO estimators could perform\nextremely poorly over network paths that exhibit large, sudden changes in\nRTT.\n\nW\n\n[r\xc2\xa2l\n\n168,002\n144,053\n118,858\n105,681\n94,220\n92,077\n\nRTTVAR\n\n7.0\n5.7\n4.4\n3.8\n3.2\n\n0.59%\n0.81%\n1.52%\n2.43%\n4.44%\n\n3.1\n2.8\n2.5\n\n4.71%\n7.68%\n13.64%\n\n6.7\n6.5\n6.4\n5.1\n4.8\n4.0\n\nk=16\nk=12\nk=8\nk=6\nadapt\nk = 4\nk=3\nk=2\n\n3.5\n\n2.27%\n\n85,264\n78,565\n\nRTOmi n = 750 msec, k = 6\n\n128,266\n\nRTOmi n = 750 msec\ntake-first25ormec, k = 6\n\n121,566\n163,799\n\nRTOmi\nRTOmi\nRTOmi\nRTOmi\n\n112,514\n102,264\n106,139\n92,866\n\nn\nn\nn\nn\n\n=\n=\n=\n=\n\n500\n500\n250\n250\n\nmsec, k = 6\nmsec\nmsec, k = 6\nmsec\n\nTable 4: Effect of varying\n\nRTFVAR\n\nB\n\nfactor, k\n\nB. As we found for [WS95], timing every packet makes little difference over timing only one packet per RTT, even though by timing\nevery packet we run many more measurements\nthrough the EWMAs\nper unit time. This in turn causes the EWMAs to adapt SRTT and\nRTTVAR more quickly to current network conditions,\nand to more\nrapidly lose memory of conditions\nfurther in the past, similar in effect to using larger values for al and a2.\nWe note that as the timer more quickly adapts, B steadily increases, with take-last-ever),\ngenerating on average one bad timeout\nin every five, indicating correlations\nin RTF variations that span multiple round-trips.\nWe can greatly diminish this problem by raising\nRq\'_Fmin to 1 sec, but only by losing a great deal of the estimator\'s\ntimely response, and we are better off instead using the corresponding take-first variant.\nWe also evaluated\n\nvarying\n\no\n\n0.50%\n0.76%\n0.70%\n0.69%\n1.02%\n1.29%\n\nfactor\n\nthe EWMA\n\nparameters\n\nfor RTOmi n =\n\n500 msec. We find that W increases by roughly 50%, with the variation among the estimators\nfurther diminishing,\nwhile /3 falls by a\nfactor of 4-8, further illustrating the dominant effect of the RTO minimum.\nFinally, a number of the paths in 2V\'_ contain slow, well-buffered\nlinks, which lead to steady, large increases in the RTT (up to many\nseconds).\nWe might expect take-first to do quite poorly for these\nconnections,\nsince the first measured RTT has little to do with subsequent RTTs, but in fact take-first does quite well. The key is the last\npart of step 5 in \xc2\xa7 2.2 above: the RTO timer is restarted with each\narriving ACK for new data.\nConsequently,\nwhen data is flowing,\nthe RTO has an implicit extra RTF term [Lud99], and for take-first\nthis suffices to avoid bad timeouts even for RTTs that grow by two\norders of magnitude.\nIndeed, take-first does better for such connections than estimators that track the changing RTT! It does so because\nmore adaptive estimators wind up waiting much longer after the last\narriving ACK before RTO expires, while take-first retransmits\nwith\nappropriate\nbriskness in this case. But this advantage\nis particular to\nthe highly-regularized\nfeedback of such connections.\nIt does, however, suggest the notion of a "feedback\ntimeout," discussed briefly in\n\n\xc2\xa74.\n\n--\n\nVarying the RTTVAR Factor\n\nThe last RTO estimation parameter we consider is k, the multiplier of\nRTTVAR when computing\nRTO, per Eqn 1. For the standard implementation,\nk = 4. Table 4 shows the effects of varying k from 2-16,\nfor G = 1 msec and RTOmi n = 0 sec. The adapt estimator starts\nwith k = 4 but doubles it every time it incurs a bad timeout.\nk clearly provides a knob for trading off waiting time for unnecessary timeouts, with no obvious sweet spot. This balance changes a\n\ni\n\nMAX\n\nI\n\nRTT\n\n,\' _\n/\n\n"\n/\n\n8_\n//\n\n5_. *or\n\n//\'/\n\xc2\xa2._\n\n/\n\n/\n\n\xc2\xa2,a\n,::5\n\n10A-"\n\n10_-2\nRatio\n\nFigure\n\n1E_\'O\nof\n\n1: Extra waiting\n\nExtra\n\nWait\n\n1(_1\n\nNecessary\n\ntime necessary\n\n1(_2\n\n1(_3\n\n: X\n\nto avoid bad RTO\n\nbit, however, when we increase RTOmi n, as shown in the second half\nof the table. For example, we find that RTOmi n = 250 msec, k = 4\nperforms\nstrictly better than the no-minimum\nk = 6 variant, and\nRTOmi n = 250 msec, k = 6 performs better than the k = 8 variant. Even the extremely\nsimple take-first e_timator, if using k = G\nand RTOmi n = 250 msec, performs\nRTOmi n = 750 msec estimator.\n\n2.7\n\nCan We Estimate\n\na bit better\n\nthan\n\nthe regular\n\nRTO Better?\n\nHaving evaluated\nthe effects of different estimator\nparameters\nand,\nfor the most part, only found tradeoffs and little in the way of compelling "sweet spots," we now turn to the question of whether there\nare indeed opportunities\nto devise still better estimators.\nA key consideration for answering\nthis question is: when we underestimate,\nby\nhow much is it? If, for example, underestimates\ntend to be off by less\nthan RTI\', then that would suggest a modification\nto Eqn 1 in which\nSRTT has a factor of 2 applied to it.\nLet A denote the amount of additional\nwaiting time needed to\navoid a bad RTO. Figure 1 plots the cumulative\ndistribution\nof the\nratio of A to RTTVAR (solid), the maximum\nRTT seen so far (dotted), and RTO (dashed),\nfor the usual G = 1 msec estimator.\nThe\nratio of A to RTTVAR ranges across several orders of magnitude,\nindicating that finding a particular value of k in Eqn 1 that efficiently\ntakes care of most of the remaining bad timeouts is unlikely.\nAlso shown is that .4 is generally less than the current RTO and\nalso the maximum\nR\'Iq\' seen so far; this suggests adding one of those\nvalues to RTO to make it sufficiently conservative\nto avoid bad timeouts. However, doing so has much the same effect as other estimator\nvariants that wait longer based on other factors (e.g., the value of k).\nFor example,\nchanging the standard k = 4 estimator\nshown in Table 4 to use twice the computed RTO (i.e., add in an additional\nRTO\nterm)\n\nlowers/3\n\n5.7--a\n\nbit better\n\nfrom 4.71% to 0.57%,\n\nbut increases\n\nli," from 3.1 to\n\nthan just using k = 12, but not compellingly\n\nbetter.\n\nFor RTOmi n = 0.5 sec, the plot is very similar, with slightly\nmore separation\nbetween the RTO and MAX RTT lines. Thus, Figure 1 suggests a fundamental\ntradeoff between aggressiveness\nand\nsuffering\n\n2.6\n\nR\'I\'-I-VAR\n\n....\n\nbad timeouts.\n\nA related question is: if a packet is unnecessarily\nretransmitted,\ndoes it reflect a momentary\nincrease in R\'IT, or a sustained increase?\nWe find that about 62% of the bad timeouts were followed by RITs\nless than the current RTO, so the bad timeout reflected\na transient\nR\'IT increase. Another 24% were followed by exactly one more elevated RTT, though a bit more than 2% were followed by 10 or more\nelevated R\'ITs. Thus, most of the time a significant R\'IT increase is\nquite transient--but\nthere is non-negligible\ntail-weight\nfor sustained\nRTT increases.\n\n2.8\n\nImpact\n\nof Bad\n\nIn the context\n\nTimeouts\n\nWe finish our study of RTO estimators\ning the impact of bad timeouts.\n\nwith brief comments\n\nconcern-\n\nAny time a TCP times out unnecessarily,\nit suffers not only a loss\nof useful throughput,\nbut, often more seriously, unnecessarily\ncuts\nssthresh to half the current, sustainable\nwindow, and begins a new\nslow start. In addition, because the TCP is now sending retransmitted packets, unless it uses the TCP timestamp option, it cannot safely\nmeasure RTFs for those packets (per Kam\'s algorithm [KP87]), and\nthus it will take a long time before the TCP can adapt its Rift estimate in order to improve its broken RTO estimate. (See [Pax97a] for\nan illustration of this effect.)\nBad timeouts can therefore have a major negative impact on a TCP\nconnection\'s\nperformance.\nHowever, they do not have much of an\nadverse impact on the network\'s performance,\nbecause by definition\nthey occur at a time when the network is not congested to the point\nof dropping the connection\'s\npackets. This in turn leads to the observation that if we could undo the deleterious effects upon the TCP\nconnection of cutting ssthresh and entering slow start, then a more\naggressive RTO estimator would be more attractive, as TCP would be\nable to sustain bad timeouts without unduly impairing\nperformance\nor endangering\nnetwork stability.\nWhen TCP uses the timestamp\noption, it can unambiguously\ndetermine that it retransmitted\nunnecessarily\nby observing a later ACK\nthat echoes a timestamp\nfrom a packet sent prior to the retransmission. (A TCP could in principle also do so using the SACK option.)\nSuch a TCP could remember the value of ssthresh and cwnd prior to\nthe last retransmission\ntimeout, and restore them if it discovers\nthe\ntimeout\n\nwas unnecessary.\n\nEven without timestamps\nor SACK, the following heuristic might\nbe considered:\nwhenever a TCP retransmits\ndue to RTO, it measures\nAT, the time from the retransmission\nuntil the next ACK arrives. If\nAT is less than the minimum RTr measured so far, then arguably the\nACK was already in transit when the retransmission\noccurred,\nand\nthe timeout was bad. If the ACK only comes later than the minimum\nRTr, then likely the timeout was necessary.\nWe can assess the performance\nof this heuristic fairly simply. For\nour usual G = 1 msec estimator,\na total of 8,799 good and bad\ntimeouts were followed by an ACK arriving with AT less than the\nminimum\nmeasured Rq_I ". Of these, fully 75% correspond\nto good\ntimeouts,\nindicating\nthat, surprisingly,\nthe heuristic generally\nfails.\nThe failure indicates that sometimes\nthe smallest R\'l"r seen so far\noccurs right after a timeout, which we find is in fact the case, perhaps because the lull of the timeout interval gives the network path a\nchance to drain its load and empty its queues.\nHowever, if the threshold is instead f = _ of the minimum\nthen only 20% of the corresponding\ntimeouts are good (these\n\nRTT,\ncom-\n\nprise only 1% of all the good timeouts).\nFor f = _, the proportion\nfalls to only 2.5%. With these reduced thresholds the chance of detecting a bad timeout falls from 74% to 68% or 59%, respectively.\nWe evaluated the modified heuristic and found it works well: for\nf = \xc2\xbd, B drops from 4.71% to 2.39%, a reduction of nearly a factor\nof two, and enough to qualify the estimator as a "\'sweet spot."\n\n3\n\nEstimating\n\nBandwidth\n\nWe now turn to the second estimation\nproblem,\ndetermining\nthe\namount of bandwidth\navailable to a new connection.\nClearly, if a\ntransport\nprotocol sender knows the available bandwidth,\nit would\nlike to immediately\nbegin sending data at that rate. But in the absence of knowing the bandwidth,\nit must form an estimate.\nFor TCR\nthis estimate is currently made by exponentially\nincreasing the sending rate until experiencing\npacket loss. The loss is taken as an implicit signal that the rate had grown too large, so the rate is effectively\nhalved and the connection\ncontinues in a more conservative\nfashion.\n\nof TCP, the goal in this section\n\nis to determine\n\nthe ef-\n\nficacy of different algorithms a TCP connection might use during its\nstart-up to determine\nthe appropriate\nsending rate without pushing\non the network as hard as does the current mechanism.\nIn a more\ngeneral context, the goal is to explore the degree to which the timing\nstructure of flights of packets can be exploited in order to estimate\nhow fast a connection\ncan safely transmit.\nWe assume familiarity with the standard TCP congestion\ncontrol\nalgorithms\n[Jac88, Ste97, APS99]:\nthe state variable cwnd bounds\nthe amount of unacknowledged\ndata the sender can currently\ninject\ninto the network, and the state variable ssthresh marks the cwnd size\nat which a connection\ntransitions\nfrom the exponential\nincrease of\n"slow start" to the linear increase of "congestion\navoidance."\nIdeally,\nssthresh gives an accurate estimate of the bandwidth available to the\nconnection,\nand congestion\navoidance is used to probe for additional\nbandwidth that might appear in a conservative,\nlinear fashion.\nA new connection begins slow start by setting cwnd to 1 segment, 3\nand then increasing cwnd by 1 segment for each ACK received. If the\nreceiver acknowledges\nevery k segments, and if none of the ACKs\n1\nare lost, then cwnd will increase by about a factor of "7 = 1 +\nevery RTI\'. Most TCP receivers currently\nuse a "delayed acknowledgment"\npolicy for generating ACKs [Bra89] in which k = 2 and\nhence "7 = a which is the value we assume subsequently.\n7,\nNote that if during one round-trip\na connection\nhas N segments\nin flight, then during slow start it is possible, during the next RTI", to\noverflow a drop-tail queue along the path such that (7 - 1)N = N/k\nsegments are lost in a group, if the queue was completely\nfull carrying the N segments during the first round-trip.\nSuch loss will in general significantly\nimpede performance,\nbecause when multiple segments are dropped from a window of data, most current TCP implementations\nwill require at least one retransmission\ntimeout to resend\nall dropped segments [FF96, Hoe96].\nHowever, during congestion\navoidance,\nwhich can be thought of as a connection\'s\nsteady-state,\nTCP increases cwnd by at most one segment per R\'I\'T, which ensures\nthat cwnd will overflow a queue by at most one segment. TCP\'s fast\nretransmit\nand fast recovery algorithms\n[Jac90, Ste97, APS99] provide an efficient method for recovering\nfrom a single dropped segment without relying on the retransmission\ntimer [FF96].\nHoe [Hoe96] describes\na method for estimating ssthresh by multiplying the measured RTr with an estimate of the bottleneck bandwidth (based on the packet-pair\nalgorithm outlined in [Kes91 ]) at the\nbeginning\nof a transfer.\n[Hoe96]\nshowed that correctly estimating\nssthresh would eliminate\nthe large loss event that often ends slow\nstart (as discussed above).\nGiven that Hoe\'s results were based on\nsimulation,\nan important follow-on question is to explore the degree\nto which these results are applicable\nto actual, measured TCP connections.\nThere are several other mechanisms\nwhich mitigate the problems\ncaused by TCP\'s slow start phase, and therefore lessen the need to\nestimate ssthresh. First, touters implementing\nRandom Early Detection (RED) [FJ93, BCC+98]\nbegin randomly dropping segments at\na low rate as their average queue size increases.\nThese drops implicitly signal the connection\nto reduce its sending rate before the queue\noverflows.\nCurrently, RED is not widely deployed.\nRED also does\nnot guarantee avoiding multiple losses within a window of data, especially in the presence of heavy congestion.\nHowever, RED also\nhas the highly appealing property of not requiring the deployment\nof\nany changes to current TCP implementations.\nAlternate\n\nloss recovery\n\ntechniques\n\nthat do not rely on TCP\'s\n\nre-\n\n3Strictly speaking, cwnd is usually managed in terms of bytes and not segments (full-sized data packets), but conventionally it is discussed in terms of\nsegments for convenience. The distinction is rarely important. Also, [APS99]\nallows an initial slow start to begin with cwnd set to 2 segments, and an experimental extension to the TCP standard allows an initial slow start to begin\nwith cwnd set to 3 or possibly 4 segments IAFPgg]. We comment briefly on\nthe implications of this change below.\n\ntransmission have eeneveloped\ntimer b d\ntodiminish impact path\'s bottleneck bandwidth, which we assume that PBM provides.\nthe\nof\nmultiple in a flightof data.SACK-based [MM96, Thus, we use PBM to calibrate the efficacy of the other ssthresh eslosses\nTCPs\nMMFR96, provide sender\nFF96]\nthe\nwithmoreomplete\nc\ninforma- timators we evaluate.\ntionabout segments beenropped network\nwhich\nhave d\nbythe\nthan Of the 18,490 traces available in .A/\'2, we removed 7,447 (40%)\nnon-SACK implementations This\nTCP\nprovide. allows\nalgorithms from our analysis for the following reasons:\nto\nquickly\nrecover multiple\nfrom\ndropped\nsegments\n(generally\nwithin\none following detection). shortcoming\nR\'lq"\nloss\nOne\nofSACK-based Traces marred by packet filter errors [Pax97a] or major clock\napproaches, isthathey\nhowever, t require\nimplementation\nchanges problems [Pax98]: 15%. Since these problems most likely do\natboth sender the\nthe\nand receiver.\nAnother ofalgorithms, not reflect network conditions along the path between the two\nclass\nhosts in the trace, removing\nthese traces arguably does not inreferred\ntoas"NewReno" FF96,\n[Hoe96, FH99], not equire\ndoes r\ntroduce any bias in our subsequent\nanalysis.\nSACKs, can\nbut beused\ntoeffectively from\nrecover multiple\nlosses\nwithout\nrequiring\na timeout\n(though quickly swhensing\nnotas\na\nu\nTraces in which the first retransmission\nin the trace was "avoidSACK-based\nalgorithms).\nInaddition,\nNewReno requires\nonly\nimable," meaning had the TCP sender merely waited longer, an\nplementation atthe\nchanges sender. estimation\nThe\nalgorithms\nstudiedinthis\npaper\nallrequire\nchanges sender\'s implemen- ACK for the retransmitted segment would have arrived: 20%.\ntothe\nTCP\nSuch retransmissions\nare usually due to TCPs with an initial\ntation. we\nSo, assume the\nthat sender implementation\nTCP\nwillhave\nRTO that is too short [Pax97a, PAD+99].\nWe eliminate these\nsome ofthe\nform\nNewReno recovery\nloss\nmechanism.\ntraces because the retransmission\nresults in ssthresh being set\nto a value that has little to do with actual network conditions,\n3.1\n\nMethodology\n\nso we are unable to soundly assess how well a larger ssthresh\nwould have worked.\nRemoving\nthese traces introduces\na bias\nagainst connections\nwith particularly\nhigh RTTs, as these are\nthe connections\nmost likely to engender avoidable retransmissions.\n\nIn this section we discuss\na number of algorithms\nfor estimating\nssthresh and our methodology\nfor assessing their effectiveness.\nWe\nbegin by noting a distinction\nbetween available bandwidth and bottleneck bandwidth.\nIn [Pax97b] we define the first as the maximum\nrate at which a TCP connection exercising correct congestion\ncontrol\ncan transmit along a given network path, and the second as the upper\nbound on how fast any connection\ncan transmit along the path due to\nthe data rate of the slowest forwarding element along the path.\nOur ideal goal is to estimate available bandwidth\nin terms of the\ncorrect setting of ssthresh such that we fully utilize the bandwidth\navailable to a given connection,\nbut do not exceed it (more precisely:\nonly exceed it using the linear increase of congestion\navoidance).\nMuch of our analysis,\nthough, is in terms of bottleneck\nbandwidth,\nas this is both an upper bound on a good ssthresh estimate,\nand a\nquantity that is more easily identifiable\nfrom the timing structure of\na flight of packets, since for any two data packets sent back-to-back\nalong an uncongested\npath, their interarrival time at the receiver directly reflects the bottleneck\nbandwidth along the path. 4\nNote that in most TCP implementations\nssthresh is initialized to\nan essentially unbounded\nvalue, while here we concentrate\non lowering this value in an attempt to improve performance\nby avoiding loss\nor excessive queueing.\nThus, all of the algorithms\nconsidered\nin this\nsection are conservative,\nyet they also (ideally) do not impair a TCP\'s\nperformance\nrelative to TCPs not implementing\nthe algorithm.\nHowever, if an estimator yields too small a value ofssthresh,\nthen the TCP\nwill indeed perform poorly compared to other, unmodified\nTCPs.\nAs noted above, one bottleneck\nbandwidth\nestimator\nis "packet\npair" [Kes91].\nIn [Pax97b]\nwe showed that a packet pair algorithm implemented\nusing strictly sender-side\nmeasurements\nperforms poorly at estimating\nthe bottleneck\nbandwidth using real traffic. We then developed\na more robust method, Packet Bunch Mode\n(PBM), which is based on looking for modalities in the timing structure of groups of back-to-back\npackets [Pax97b, Pax97c].\nPBM\'s\neffectiveness\nwas assessed by running it over the NPD datasets (including the.A/\'2 dataset referred to earlier), arguing that the algorithm\nwas accurate because on those datasets it often produced estimates\nthat correspond\nwith known link rates such as 64 kbps, TI, El, or\nEthernet.\nPBM analyzes an entire connection\ntrace betbre generating\nbottleneck\nbandwidth\nestimates.\nIt was developed for assessing\n\nany\nnet-\n\nwork path properties and is not practical for current TCP implementations to perform on the fly, as it requires information\nfrom both the\nsender and receiver (and is also quite complicated).\nHowever, for our\npurposes\n\nwhat we need is an accurate\n\nassessment\n\n4Providing the path isn\'t "multi-charmer\'\n[Pax97b].\n\nTraces for which the PBM algorithm failed to produce a single,\nunambiguous\nestimate:\n4%. We need to remove these traces\nbecause our analysis uses the PBM estimate to calibrate the different estimation\nalgorithms we assess, as noted above. Removing these traces introduces\na bias against network conditions\nthat make PBM itself fail to produce a single estimate:\nmultichannel paths, changes in bottleneck\nbandwidth over the course\nof a connection,\nor severe timing noise.\nAfter removing the above traces, we are left with 11,043 connections for further analysis.\nWe use trace-driven\nsimulation\nto assess\nhow well each of the bandwidth\nestimation\nalgorithms perform.\nWe\nbase our evaluation\non classifying\nthe algorithm\'s\nestimate for each\ntrace into one of several regions, representing\ndifferent levels of impact on performance.\nFor each trace, we define three variables, B, L and E. B is the\nbottleneck\nbandwidth estimate made using the PBM algorithm.\nL is\nthe loss point, meaning the transmission\nrate in effect when the first\nlost packet was sent (so, if the first lost segment was sent with cwnd\ncorresponding\nto W bytes, then L = W/RTT\nbytes/second).\nIf the\nconnection\ndoes not experience\nloss, L\' is the bandwidth\nattained\nbased on the largest cwnd observed during the connection, s When\nL > B or L\' > B, the network path is essentially\nfree of competing\ntraffic, and the loss is presumed caused by the connection itself overflowing a queue in the network path. Conversely,\nif L or L\' is less\nthan B, the path is presumed\ncongested.\nFinally, E is the bandwidth\nestimate made by the ssthresh estimation\nalgorithm being assessed.\nIn addition, define seg(:r) = (.r. R\'VF)/segment\nsize representing\nthe size of the congestion\nwindow, in segments, needed to achieve a\nbandwidth of.r bytes/second,\nfor a given TCP segment size and R\'Iq\'.\n(Note that as defined, seg(x) is continuous\nand not discrete.)\n3.1.1\n\nConnections\n\nWith Loss\n\nGiven\n\nthe above definitions,\n\nand a connection\n\nwhich\n\ncontains\n\nloss,\n\nwe assess an estimator\'s\nperformance\nby determining which of the\nfollowing six regions it falls into. Note that we analyze the regions in\nthe order given, so an estimate will not be considered for any regions\nsubsequent\nto the first one it matches.\n\nof a given network\n\nor subject to routing changes\n\n5Strictly speaking, it\'s the largest flight observed during the connection,\nwhich might be smaller than twnd due to the connection running out of data\nto send, or exhausting the (32-64KB) receiver window.\n\nNo Estimate\nestimate\n\nMade. The estimator\nbefore the first segment\n\nNo Impact. The estimate satisfies\na sufficiently large overestimate\nno differently\nwere made.\n\nfailed to produce an ssthresh\nloss occurred in the trace.\nE > 3\'L. This means that E is\nthat the connection\nwill behave\n\nusing that estimate\n\nthan it would\n\nif no estimate\n\nSome Loss Prevention.\nWhen L < E < "yL holds, the given\nssthresh estimate prevents some, but not all, loss of data packets. While the estimate is greater than the loss point, it reduces\nthe size of the last slow start flight by N, = seg(q,L - E) segments. Therefore,\nup to N, segment drops may be prevented.\nSteady-State.\nWhen _ < E < L holds, we classify the ssthresh\nestimate as "steady-state."\nDuring congestion avoidance, which\ndefines TCP\'s steady-state\nbehavior [Jac88, MSMO97],\ncwnd\ndecreases\nby half upon loss detection and then increases linearly until another loss occurs. So, given the loss point of L,\nL\ncwnd can be expected\nto oscillate between\n_- and L after the\nconnection\'s\nsecond loss event. _ By making an estimate\nbeL\ntween 5- and L, the estimator has found the range about which\nthe connection\nwill naturally oscillate, assuming the loss point\nis stationary.\nOptimal.\nWhen the analysis reaches this point, we know that\nE < _ since none of the above conditions\nhold. If seg(E)\n>\nseg(B_ - 1 also holds, then the ssthresh estimate reduces the\nqueueing requirement,\nas follows.\nSince E is very close to or\nlarger than the bottleneck bandwidth,\nyet less than _, we know\nL\nthat the loss point is greater than the bottleneck bandwidth,\nyet\nthe ssthresh estimate is no less than the bottleneck bandwidth or\none segment less than the bottleneck\nbandwidth.\n(We consider\none segment less than the bottleneck\nbandwidth\nto be within\nthe range because both slow start and congestion\navoidance will\ntake a single RTT to increase cwnd to correspond\nwith B--and\nwe prefer to reach that point via congestion\navoidance\nrather\nthan slow start, so we don\'t overshoot it.)\nThus, assuming\nthe connection\nlasts long enough, the queue\nwill still be filled to L. However, we will fill the queue more\nslowly and smoothly\nthan with slow start. Furthermore,\nwhen\nwe exceed the queue during congestion\navoidance, it is only by\none segment, whereas during slow start we will exceed the capacity of the queue by as much as 3, times the capacity. 7 When a\nconnection falls into this region, the queue length is initially reduced by No = (L - E). RTT bytes. Since this region reduces\nqueueing,\nprevents loss, yet fully utilizes the network path, we\ndeem it "optimal."\nReduce\n\nPerformance.\n\nFinally,\n\nif none of the above conditions\n\nhold\n\nthen E < _ and E < B (these bounds are not tight). We therefore set ssthresh too low and force cwnd growth to continue\nlinearly, rather than exponentially.\nWhen an estimator\nunderestimates\nmin(_,B)\nby more than half in 50+% of the connections in which performance\nwould be reduced, we consider\nthis to be an especially\nbad estimate.\nIn this case, the reported\npercentage of connections\nexperiencing\nreduced performance\nis\nmarked with a "*"\n6The size of cwnd when detecting the first loss event is roughly ")L.\nTherefore. the first halving of cwnd causes it to be approximately 22L. Each\nsubsequent loss event should only overflow the queue slightly and therefore\nc_L\'ndwill be reduced to L.\n7Some implementations\nof congestion avoidance add a constant of\nI times the segment stze to cwnd for every ACK received during congestion avoidance. This non-standard behavior has been shown to lead to somenines overflowing the queue by more than a single segment every time cwnd\napproaches L [PAD+99].\n\nAlgorithm\nPBM _\nTSSF\n\n23%\n42%\n\n46%\n1%\n\n9%\n1%\n\n10%\n3%\n\n11%\n0%\n\n31%\n4%\n\n0%\n52%*\n\nC_/t\n\n_"n-u=o.l\nC_A,,--\xc2\xa2].O5\n_"nCSA "--_1\n\n62%\n53%\n45%\n\n20%\n37%\n32%\n\n6%\n5%\n8%\n\n9%\n4%\n10%\n\n2%\n0%\n2%\n\n17%\n9%\n19%\n\n2%\n1%"\n4%*\n\n_,_=2\nTCSA\nTCSA t\n\n38%\n62%\n70%\n\n24%\n14%\n10%\n\n9%\n6%\n6%\n\n13%\nI1%\n9%\n\n3%\n1%\n2%\n\n25%\n19%\n17%\n\n13%\n5%\n2%\n\nRecvmi n\nRecvavg\nRecvme d\nRecvmax\n\nI 1%\nI 1%\nI 1%\nI !%\n\n32%\n52%\n48%\n65%\n\n6%\n10%\n10%\n7%\n\n13%\n14%\n14%\n8%\n\n4%\n9%\n10%\n8%\n\n23%\n34%\n34%\n23%\n\n34% \xc2\xb0\n3%\n7%*\n0%*\n\nTable\n3.1.2\n\n5: Connections\n\nConnections\n\nWithout\n\nwith Loss (8,257 traces)\nLoss\n\nThe following regions use L\' to assess the impact ofssthresh\nestimation on connections\nin the dataset that do not experience\nloss. Each\ntrace is placed into one of the following four regions.\n(Again, note\nthat we analyze the regions in the order given, so an estimate will not\nbe considered\nfor any regions subsequent\nto the first one it matches.)\nNo Estimate\nestimate.\n\nMade.\n\nThe estimator\n\nfailed\n\nto produce\n\nan ssthresh\n\nUnknown\nEffect. When E > L\' holds, the estimate does not limit\nTCP\'s ability to open cwnd, as it is above the maximum\ncwnd\nused by the connection.\nSince we do not have a good measure\nof the limit of the network path, nothing more can be assessed\nabout the performance\nof the estimator.\nOptimal.\nWhen seg(E)\n_> seg(B) - 1 holds, the estimate is greater\nthan the bottleneck\nbandwidth and therefore does not limit performance.\nHowever,\nwe also know that E < L\' due to the\nabove region. Therefore,\nthe estimate reduces the initial queueing requirement\nsimilar to the "optimal"\nregion in \xc2\xa7 3.1.1.\nReduce Performance.\nAt this point, E < min(L\',\nB - seg -_ (1))\nholds, indicating\nthat the estimate failed to provide exponential window growth to L\', which is a known safe sending rate.\nFurthermore,\nour failure to reach L\' is not excused by providing exponential\ncwnd growth long enough to fill the pipe (B\nbytes/second).\nWe again mark with a "*" those connections\nfor\nwhich the reduction is often particularly\nlarge.\n\n3.2\n\nBenchmark\n\nAs noted above,\n\nAlgorithm\nwe use PBM as our benchmark\n\nin terms\n\nof accu-\n\nrately estimating\nthe bottleneck bandwidth.\nFor ssthresh estimation,\nwe use a revised version of the algorithm, PBM\', to provide some\nsort of upper bound on how well we might expect any algorithm to\nperform.\n(It is not a strong upper bound, since it may be that other algorithms estimate the available bandwidth considerably\nbetter than\ndoes PBM\', but it is the best we currently have available.) The difference between PBM\' and PBM is that PBM\' analyzes the trace only\nup to the point of the first loss, while PBM analyzes the trace in its\nentirety.\nThus, PBM\' represents\napplying a detailed, heavyweight,\nbut accurate algorithm on as much of the trace as we are allowed to\ninspect before perforce having to make an ssthresh decision.\nAs shown in Tables 5 and 6, the PBM\' estimate yields ssthresh\nvalues that rarely hurt performance,\nregardless\nof whether the connection experiences\nloss. Each column lists the percentage of traces\nwhich, for the given estimator, fell into each of the regions discussed\nin _ 3.1.1. The Tot. column gives the percentage\nof traces for which\nthe estimator\nimproved matters by attaining either the prevent loss,\n\nAlgorithm\nPBM j\nTSSF\n\n0%\n13%\n\n56%\n2%\n\n44%\n2%\n\n0%\n82%*\n\nCSAn= 3"\n24%\nCSA v=O\'\xc2\xb05.\n_\n19%\nCSA _;==0"1,_ 14%\nt"_ A;"=_\'2\n13%\n\n42%\n59%\n48%\n34%\n\n13%\n1I%\n11%\n11%\n\n22%\n10%\n27%\n43%*\n\nTCSA\nTCSA t\n\n24%\n27%\n\n25%\n33%\n\n8%\n11%\n\n44%\n28%\n\n1%\n1%\n1%\n1%\n\n15%\n46%\n45%\n71%\n\n2%\n23%\n28%\n27%\n\n83%"\n31%*\n26%\n1%\n\nu:O\n\n1\n\nRecvmi n\nRecvavg\nRecvme d\nRecvmax\n\n|\n\n!\n\n_=\n\nwithout\n\nLoss (2,786\n\n,\nG\n\no\n\n\xe2\x80\xa2\n\n!\n115\n\naccurately\n\nSlow Start Flights\n\nThe first technique\nwe investigate\nis a TCP-specific\nalgorithm that\ntracks each slow start "flight." The ACKs for a given flight are used\nto obtain an estimate of ssthresh. While this algorithm is TCP specific, the general idea of measuring\nthe spacing\nintroduced\nby the\nnetwork in all segments transmitted\nin one RTT should be applicable to other transport protocols. We parameterize\nthe algorithm by\n_, the number of ACKs used to estimate the bottleneck\nbandwidth.\nFor our analysis, we used T_= 3. Let F be the current flight size, in\nsegments.\n\nThe Tracking\n\nSlow Start Flights\n\n21s\n\n2: Delayed\n\n31o\n\nACK leading\n\nFor the current\n\n,5\' and F, check\n\n31s\n\nto timing\n\n(TSSF) algorithm\n\nis then:\n\n\xe2\x80\xa2 Initialize the current segment S to the first data segment\nand F to the initial value of c_t\'/ld in segments.\n\nsent,\n\nwhether\n\nthe algorithm ignores any ACKs\nwere presumably\ndelayed.\n\n.io\n"lull"\n\nS\'s ACK and the n - 1\n\nfor a single\n\nsegment,\n\n\xe2\x80\xa2 To find the next flight, advance S by F segments.\nnumber of ACKs for new data that arrive between\n\nas they\n\nIf N= is the\nthe old value\n\nof 5\' and its new value, then the size of the next flight is F + N,_\n(the slow start increase).\nWhen we find a suitable flight, we estimate the bandwidth\nas\nthe amount of data ACKed between the first and the nth ACK,\ndivided by the time between\n\nAlgorithms\n\nof these algorithms\nis their reliance on the ACK stream\nreflecting the arrival spacing of the data stream.\n\nTracking\n\n2\'.o\n\nE3\n\nsubsequent\narriving ACKs are all within the sequence range of\nthe flight. If so, then we use this flight to make an estimate.\nOtherwise,\nwe continue\nto the next flight. However,\nif any of\nthe ACKs arrive reordered or are duplicates,\nthe algorithm\nterminates. When looking forward for the n- 1 subsequent ACKs,\n\n\xe2\x80\xa2\n\nThe following is a description of the sender-side\nbandwidth estimation algorithms,\nand the corresponding\nssthresh estimates,\ninvestigated in this paper. TCP\'s congestion control algorithms\nwork on the\nprinciple\nof "self-clocking"\n[Jac88]. That is, data segments are injected into the network and arrive at the receiver at the rate of the bottleneck link, and consequently\nACKs are generated\nby the receiver\nwith spacing that reflects the rate of the bottleneck\nlink. Therefore,\nsender-side\nestimation\ntechniques\nmeasure the rate of the returning\nACKs to make a bandwidth estimate. These algorithms\nassume that\nthe spacing injected into the data stream by the network will arrive\nintact at the receiver and will be preserved in the returning ACK flow,\nwhich may not be true due to fluctuations\non the return channel altering the ACK spacing (e.g., ACK compression\n[ZSC91, Mog92]).\nThese algorithms have the advantage of being able to directly adjust\nthe sending rate. In the case of TCP, they can directly set the ssthresh\nvariable as soon as the estimate is made. However,\na disadvantage\n\n3.3.1\n\nrn\n\ntraces)\n\n[Pax97b].\n\nEstimation\n\no\no\n\nTime\n\nWe see that PBM\' provides snme benefit (_teady state, prevention\nof loss, or optimal) to 31% of the connections\nthat experience\nloss,\nand, when no loss occurs, the estimate falls in the optimal region\nfor 44% of the connections.\nThe remaining\nestimates\nare overestimates, in the case when the connection\nexperiences\nloss, or have an\nunknown impact (but, do not harm performance)\nin the connections\nthat do not have dropped segments. This indicates that much of the\ntime the available\nbandwidth\nis less than the raw bottleneck\nbandwidth that PBM measures, which accords with the finding given in\n\nSender-Side\n\n|\n[]\n\nsteady-state,\nor optimal\nregions. This column can be directly compared with the last column (reduce\nperformance)\nto assess how a\ngiven estimator\ntrades off improvement\nin some cases with damage\nin others.\n\n3.3\n\no\no\n\no\n\nFigure\nTable 6: Connections\n\n!\n\n/\n\nAs the second\n\nrows of Tables\n\nthe arrivals\n\nof these ACKs.\n\n5 and 6 show,\n\nthe performance\n\nof\n\nthe TSSF algorithm is quite poor. The overwhelming\nproblem with\nthis estimator is underestimating\nthe bandwidth,\nwhich would cause\na reduction in performance.\nThe underestimation\nis caused in part by TCP\'s delayed acknowledgment algorithm.\nRFC 1122 [Bra89] encourages\nTCP receivers\nto refrain from ACKing every incoming\nsegment, and to instead acknowledge\nevery second incoming\nsegment, though it also requires\nthat the receiver wait no longer than 500 msec for a second segment\nto arrive before sending an ACK. Many TCP implementations\nuse\na 200 msec "heartbeat"\ntimer for generating\ndelayed ACKs. When\nthe timer goes off, which could be any time between 0 and 200 msec\nafter the last segment arrived, if the receiver is still waiting for a second segment it will generate an ACK for the single segment that has\narrived. Using this mechanism\ncan fail to preserve in the returning\nACK stream the spacing imposed on the data stream by the bottleneck link. The time the receiver spends waiting on a second segment\nto arrive increases the time between ACKs, which is assumed by the\nsender to indicate the segments were further spaced out by the network, which leads to an underestimate\nof the bandwidth.\nFurthermore,\nonce a delayed ACK timer effect is injected into the\nACK stream, the flight is effectively\npartitioned\ninto two mini-flights\nfor the duration\nof slow start, since data segments\nare sent in response to incoming\nACKs. The sequence-time\nplot in Figure 2 illustrates this effect, in the plot, which is recorded\nfrom the sender\'s\nperspective,\noutgoing data segments are indicated with solid squares\ndrawn at the upper sequence number of the segment, while incoming\nACKs are drawn with hollow squares at the sequence\nnumber they\nacknowledge.\nThe first flight shown, which consists of two segments,\nelicits a\nsingle ACK that arrives at time T = 2.0.\nBut the flight of three\nsegments that this ACK triggers elicits two ACKs, one for two segments arriving at T = 2.6, but another for just one segment at time\n\nT = 2.8. The latter reflects a delayed ACK. The next flight of five\npackets then has a lull of about 200 msec in the middle of it. This\nlull is duly reflected in the ACKs for that flight, plus an additional delayed ACK occurs from the first sub-flight of three segments (times\nT = 3.3 through T = 3.5). The resulting next flight of 8 segments is further fractured, reflecting not only the lull introduced\nby\nthe new delayed ACK, but also that from the original delayed ACK,\n\nto have converged).\nWe show the effectiveness\nof using the "tracking\nclosely-spaced\nACKs" (TCSA) algorithm in Tables 5 and 6. As with\nthe CSA method described above, the TCSA algorithm does not have\na performance\nimpact on the connection\nin over 75% of the connections with loss. Furthermore,\nthe number of connections\nfor which\nthe performance\nwould be reduced is increased by roughly a factor\nof 2 for both connections\nthat experienced\nloss and those that did not\nwhen comparing\nTCSA with CSA.\nSince TCSA shows an increase\nin the number of connections\n\nand the general pattern repeats again with the next flight of 12 segments. None of the ACK flights give a good bandwidth estimate, nor\nis there much hope that a later flight might.\n\nwhose\n\nThis mundane-but-very-real\neffect significantly\ncomplicates\nany\nTCP sender-side\nbandwidth\nestimation.\nWhile for other transport\nprotocols the effect might be avoidable (if ACKs are not delayed),\nthe more general observation\nis that sender-side estimation\nwill significantly benefit from information\nregarding just when the packets\nit sent arrived at the receiver, rather than trying to infer this timing\nby assuming that the receiver sends its feedback promptly enough to\ngenerate an "echo" of the arrivals.\n\n3.3.2\n\nClosely-Spaced\n\nThe ssthresl,\n\nestimation\n\nalgorithms\n\nin [Hoe96]\n\nand [AD98]\n\nare based\n\n3.4\n\nloss (due to an inability\n\nClosely-Spaced\n\nThe ssthresh estimation\nrivals of closely-spaced\n\nor overesti-\n\nThe basic insight to how the algorithm works is that the receiver\nknows exactly which new segments the arrival of one of its ACKs at\nthe sender will allow. These segments are presumably\nsent back to\nback, so the receiver can then form a bandwidth estimate based on\ntheir timing when they arrive at the receiver.\n\nACKs\n\nalgorithm\nin [AD98] assumes that the arACKs are used to form tentative ssthresh\n\nestimates,\nwith a final estimate being picked when these settle down\ninto a form of consistency.\nWe used a CSA estimator with n = 3 and\nv = 0.1 (the sweet spot above) to assess the effectiveness\nof their\n\nSThe TCP receiver could attempt to do so by adjusting the advertised window to limit the sender to the estimated ssthJesh value, even also increasing it\nlinearly to reflect congestion avoidance But when doing so, it diminishes the\nefficacy of the "fast recovery" algorithm [Ste97, APS99], because it will need\nto increase the artificially limited window, and, according to the algorithm, an\nACK that does so will be ignored from the perspective of sending new data\nin response to receiving it.\n\nproposed approach.\nFor their scheme, we take multiple samples and\nuse the minimum observed sample to set ssthresh. We continue estimating until the point of loss, or we observe a sample within 10% of\nobserved\n\nAlgorithm\n\nFor convenience,\nwe describe\nthe algorithm\nassuming that sequence numbers are in terms of segments rather than bytes. Let Ai\ndenote the segment acknowledged\nby the ith ACK sent by the receiver. Let DI denote the highest sequence number the sender can\ntransmit after receiving the ith ACK. If we number the ACK of the\ninitial SYN packet as 0, then Ao = 0. Assuming that the initial congestion window after the arrival of ACK 0 is one segment, we have\nDo = 1. To accommodate\ninitial congestion\nwindows larger than\none segment [AFP98], we increase Do accordingly.\n\nAll of the parameter\nreduce performance\nexperience\nloss and\nconnections\nthat did\n\nto form an estimate\n\nEstimation\n\nThe receiver-side\nalgorithm\noutlined below is TCP-specific.\nIts\nkey requirement\nis that the receiver can predict which new segments\nwill be transmitted\nback-to-back\nin response to the ACKs it sends,\nand thus it can know to use the arrivals of those segments as good\ncandidates\nfor reflecting the bottleneck\nbandwidth.\nAny transport\nprotocol whose receiver can make such a prediction can use a related\nestimation\ntechnique,\nin particular, by using a timestamp\ninserted by\nthe sender, the receiver could determine\nwhich segments were sent\nclosely-spaced\nwithout knowledge\nof the specific algorithm used by\nthe sender. This is an area for near-term future work.\n\nWe chose n = 3, v = 0.1 as the sv,\'eet spot in the parameter space.\nHowever, the choice was not clear cut, as both n = 2, v = 0.05 and\nn = 2, v = 0.1 provide similar effectiveness.\nvalues shown, including the chosen sweet spot,\nfor a large number of connections\nthat do not\nyield no performance\nbenefit in over 60% of the\n\nReceiver-Side\n\ncould inform the sender of the bandwidth estimate using a TCP option (or some other mechanism,\nfor a transport protocol other than\nTCP). For our purposes, we assume that this problem is solved, and\nnote that alternate uses for the estimate by the receiver is an area for\nfuture work.\n\nOur goal was to find a "sweet spot" in the parameter space that\nworks well over a diverse set of network paths. Rows 3-6 of Tables 5\nand 6 show the effectiveness\nof several of the points in the parameter\nspace. Values of v and n outside this range performed\nappreciably\nworse than those shown.\n\nsample\n\ntoo\n\nA disadvantage\nof this algorithm\nis that the receiver cannot properly control the sender\'s transmission\nrate. s However, the receiver\n\nACKs of the closely-spaced\ngroup must arrive in order to be considered "\'close." We examined\nv values of 0.0125, 0.025, 0.05, 0.1 and\n0.2. The second parameter,\nn, is the number of ACKs that must be\nclose in order to make an estimate. We examined n = 2, 3, 4, 5. The\nbandwidth\nestimate is made the first time rt ACKs arrive (save the\nfirst) within v \xe2\x80\xa2 Rq"l" sec of their predecessors.\nThis algorithm\nhas\nthe advantage of being easy to implement.\nAlso, it does not depend\non any of the details of TCP\'s congestion\ncontrol algorithms,\nwhich\nmakes the algorithm easy to use for other transport protocols. A disadvantage\nof the algorithm is that it is potentially highly dependent\non the above two constants.\n\nthe minimum\n\nit clearly often estimates\n\nThe problems with sender-side estimation\noutlined above led to the\nevaluation of the following receiver-side\nalgorithm for estimating\nthe\nbandwidth.\nEstimating\nthe bandwidth\nat the receiver removes the\nproblems that can be introduced in the ACK spacing by delay fluctuations along the return path or due to the delayed ACK timer.\n\nWe explore a range of CSA definitions by varying two parameters.\nThe first, v, is the fraction of the RTI" within which the consecutive\n\nTracking\n\nbe reduced,\n\nACKs\n\nwhose interarrival\ntiming at the receiver then presumably\nreflects\nthe rate at which they passed through the bottleneck\nlink. However,\nneither paper defines exactly what constitutes\na set of closely-space\nACKs.\n\n3.3.3\n\nwould\n\nshow that TCSA\' is comparable\nto TCSA in most ways. The exception is that the number of underestimates\nthat would reduce performance is decreased when using TCSA\', so it would be the preferred\nalgorithm.\n\non the notion of measuring the time between "closely spaced ACKs"\n(CSAs).\nBy measuring\nCSAs, these algorithms\nattempt to consider\nACKs that are sent in response\nto closely spaced data segments,\n\nexperience\nmating).\n\nperformance\n\nlow, so we devised a variant, TCSA\',\nthat does not depend on the\nminimum observation\n(which is likely to be an underestimate).\nWe\ncompare each CSA estimate, Ei, with estimate Ei-1 (for i > 1). If\nthese two samples are within 10% of each other, then we use the average of the two bandwidth\nestimates to set ssthresh. Tables 5 and 6\n\nso far (in which case we are presumed\n\nl0\n\nAny time the receiver sends the 3\' + 1st ACK, it knows\nreceipt of the ACK by the sender, the flow control window\n\nis benefit in timing every packet. Given that such benefit is elusive,\nthe other goals of [JBB92] currently\naccomplished\nusing timestamp\noptions should be revisited, to consider using a larger sequence num-\n\nthat upon\nwill slide\n\nAj+_ - Aj segments,\nand the congestion\nwindow will increase by\n1 segment,\nso the total number of packets that the sender can now\ntransmit will be Aj+l - Aj + 1. Furthermore,\ntheir sequence numbers will be Dj + 1 through Dj+_, so it can precisely identify their\n\nber space instead. We finished our RTO assessment\nby noting that\ntimestamps,\nSACKs, or even a simple timing heuristic can be used\nto reverse the effects of bad timeouts, making aggressive RTO algorithms more viable.\nOur assessment\nof various bandwidth\nestimation\nschemes found\n\nparticular future arrivals in order to form a sound measurement.\nFinally, we take the first K such measurements\n(or continue until a data\nsegment was lost), and from them form our bandwidth estimate.\nFor\nour assessment\nbelow, we used K = 50.\n\nthat using a sender-side\nestimation\nalgorithm\nis problematic,\ndue to\nthe failure of the ACK stream to preserve the spacing imposed on\ndata segments by the network path, and we developed a receiver-side\nalgorithm\nthat performs considerably\nbetter. A lingering question is\nwhether the complexity of estimating\nthe bandwidth is worth the performance improvement,\ngiven that only about a quarter of the connections studied would benefit. However, in the context of other uses\n\n(We note that the algorithm may form poor estimates in the face of\nACK loss, because it will then lose track of which data packets are\nsent back-to-back.\nWe tested an oracular version of the algorithm\nthat accounts for lost ACKs, to serve as an upper bound on the effectiveness of the algorithm.\nWe found that the extra knowledge\nonly\nslightly increases the effectiveness\nof the algorithm.)\nThis algorithm\nprovides estimates\nfor more connections\nthan any\nof the other algorithms\nstudied in this paper, because every ACK\nyields an estimate.\nTables 5 and 6 show the receiver-based\nalgorithm using four different methods for combining\nthe K bandwidth\nestimates.\nThe first "Recv" row of each table shows the effective-\n\nor other transports,\nestimating\nthe bandwidth\nusing the receiver-side\nalgorithm may prove compelling.\nOur study was based on data from 1995, and would benefit considerably from verification using new data and live experiments.\nFor\nRTO estimation,\na natural next step is to more fully explore whether\ncombinations\nof the different algorithm parameters\nmight yield a significantly better "sweet spot." Another avenue for future work is to\nconsider a bimodal timer, with one mode based on estimating\nR\'lq"\nfor when we lack feedback from the network, and the other based\n\nhess of u_ing the utinimum of the [( measurements\nas the estimate.\nThis yields an underestimate\nin a large number of the connections,\ndecreasing\nperformance\n(34% of the time when the connection experiences loss and 83% of the time when no loss is present). The next\nrow shows that averaging\nthe samples improves\nthe effectiveness\nover using the minimum:\nthe number of connections\nwith reduced\nperformance\nis drastically\nreduced when the connection experiences\nloss, and halved in the case when no loss occurs. However, the flip\nside is the number of cases when we overestimate\nthe bandwidth in-\n\non estimating\nthe variation in the feedback\ninterarrival\nprocess, so\nwe can more quickly detect that the receiver feedback\nstream has\nstalled.\nFor bandwidth\nestimation,\nan interesting\nnext step would\nbe to assess algorithms\nfor using the estimates\nto ramp up new connections to the available bandwidth\nmore quickly than TCP\'s slow\nstart. Finally, both these estimation\nproblems merit further study in\nscenarios where touters use RED queueing\nrather than drop-tail,\nas\n\ncreases when loss is present in the connection.\nTaking the median\nof the K samples provides similar benefits to using the average, except the number of connections\nexperiencing\nreduced performance\nincreases by a factor of 2 over averaging when loss occurs. Finally,\n\nRED deployment\nshould lead to smaller R"fT variations\nof implicit feedback for bandwidth\nestimation.\n\nusing the maximum\nof the K estimates further increases the number\nof overestimates\nfor connections\nexperiencing\nloss. However, using\nthe maximum\nalso reduces the number of underestimates\nto nearly\nnone, regardless\nof whether the connection experiences\nloss. Of the\nmethods investigated\nhere, using the maximum\nappears to provide\nthe most effective ssthresh estimate.\nHowever, we note that alternate\nalgorithms\nfor combining\nfuture work.\n\nthe K estimates\n\n5\n\nFinally, we varied the number of bandwidth\n_amples, K, used to\nobtain the average and maximum estimates reported above to determine how quickly the algorithms\nconverge. We find that when averaging the estimates, the effectiveness\nincreases slowly but steadily as\nwe increase _K"to 50 samples.\nHowever, when taking the maximum\nsample as the estimate, little benefit is derived from observing more\nthan the first 5-10 samples.\n\n4\n\nConclusions\n\nand Future\n\nAcknowledgments\n\nThis paper\nand Reiner\nreviewers,\ncomments\ndetermine\nto Venkat\n\nis an area for near-term\n\nand a source\n\nsignificantly\nbenefited from discussions\nwith Sally Floyd\nLudwig.\nWe would also like to thank the SIGCOMM\nSally Floyd, Paul Mallasch and Craig Partridge for helpful\non the paper. Finally, the key insight that the receiver can\nwhich sender packets are sent back to back (\xc2\xa7 3.4) is due\nRangan.\n\nReferences\n[AD98]\n\n[AFP981\n\nOur assessment\nof different\nRTO estimators\nyielded several basic\nfindings.\nThe minimum\nvalue for the timer has a major impact on\nhow well the timer performs,\nin terms of trading off timely response\nto genuine lost packets against minimizing\nincorrect retransmissions.\nFor a minimum\nRTO of I sec, we also realize a considerable\ngain\n\nMark Allman,\ncreasing TCP\'s\n2414.\n\nSally Floyd, and Craig Partridge.\nInitial Window, September\n1998.\n\nlAPS99]\n\nWork\n\nMohit Aron and Peter Druschel.\nTCP: Improving\nStartup Dynamics\nby Adaptive Timers and Congestion\nControl.\nTechnical Report TR98-318,\nRice University\nComputer Science, 1998.\n\nMark\n\nVern Paxson,\n\nAllman,\n\nTCP Congestion\n[BCC+98]\n\nin performance\nwhen using a timer granularity\nof 100 msec or less,\nwhile still keeping bad timeouts below I%. On the other hand, varying the EWMA constants has little effect on estimator performance.\nAlso, an estimator that simply takes the first RTI" measurement\nand\n\nControl,\n\nRobert\n\nDavid\n\nBraden,\n\nand W. Richard\n\nApril\nClark,\n\nInRFC\n\nStevens.\n\n1999. RFC 2581.\nJon\n\nCrowcroft,\n\nBruce\n\nDavie, Steve Deering, Deborah Estrin, Sally Floyd, Van\nJacobson,\nGreg Minshall, Craig Partridge, Larry Peterson, K. Ramakrishnan,\nS. Shenker, J. Wroclawski,\nand\nLixia Zhang.\nRecommendations\nment and Congestion\nAvoidance\n1998. RFC 2309.\n\ncomputes\na fixed RTO from it often does nearly as well as more\nadaptive estimators.\nRelated to this finding, it makes little difference\nwhether the estimator measures only one RTI" per flight or measures\nan R\'lq" for every packet. This last finding calls into question some\nof the assumptions\nin RFC 1323 [JBB921, which presumes that thcre\n\n[Bra89]\n\nII\n\nRobert Braden. Requirements\nmunication\nLayers, October\n\non Queue Managein the lnternet, April\n\nfor lnternet Hosts - Com1989. RFC 1122.\n\n[DDK+90]\nWillibald\n\n[Nag84]\n\n[FF961\n\nKevin\n\nFall and Sally Floyd.\n\nSimulation-based\n\nparisons of Tahoe, Reno, and SACK TCP.\nCommunications\nReview, 26(3), July 1996.\n[FH991\n\nJohn Nagle.\nCongestion\nControl\nworks, January 1984. RFC 896.\n\n[PAD+99]\n\nDoeringer,\nDoug Dykeman, Matthias Kaiserswerth, Bernd Werner Meister, Harry Rudin, and Robin\nWilliamson.\nA Survey of Light-Weight\nTransport Protocots for High-Speed\nNetworks. IEEE Transactions on\nCommunications,\n38(11):2025-2039,\nNovember 1990.\n\nVern\n\nPaxson,\n\nMark\n\nComComputer\n\n[Pax97a]\n\nVeto\n\nPaxson.\n\nAutomated\n\nTCP Implementations.\n1997.\n\nSally Floyd and Tom Henderson.\nThe NewReno Modification to TCP\'s Fast Recovery Algorithm, April 1999.\nRFC 2582.\nSally Floyd and Van Jacobson.\nRandom\ntion Gateways for Congestion\nAvoidance.\nTransactions\n1993.\n\non\n\nNetworking,\n\n[Pax97b]\n\nEarly DetecIEEE./ACM\n\n1(4):397--413,\n\nScott\n\nInternet-\n\nDawson,\n\nWilliam\n\nFenner, Jim Griner, lan Heavens,\nKevin Lahey, Jeff\nSemke, and Bernie Volz. Known TCP Implementation\nProblems, March t999. RFC 2525.\n\nVern Paxson.\n\nEnd-to-End\n\nACM SIGCOMM,\n[FJ931\n\nAllman,\n\nin IP/TCP\n\n[Pax97c]\n\nVern Paxson.\n\nlnternet\n\nSeptember\n\nMeasurements\n\nEnd lnternet Dynamics.\nifornia Berkeley, 1997.\n\nAugust\n\nPacket\n\nTrace\n\nAnalysis\n\nIn ACM SIGCOMM,\n\nPh.D.\n\nPacket\n\nof\n\nSeptember\n\nDynamics.\n\nIn\n\n1997.\nand Analysis\n\nof End-to-\n\nthesis, University\n\nof Cal-\n\n[Pax98]\n[Hoe96]\n\n[Jac88]\n\n[Jac90]\n\n[JBB92]\n\n[JK92]\n\nJaney Hoe. Improving the Start-up Behavior of a Congestion Control Scheme for TCP. In ACM SIGCOMM,\nAugust 1996.\n\nVern Paxson.\nOn Calibrating\nMeasurements\nof Packet\nTransit Times. In ACM SIGMETRICS,\nJune 1998.\n\n[Pos81]\n\nJon Postel. Transmission\n1981. RFC 793.\n\nVan Jacobson.\nCongestion\nACM SIGCOMM,\n1988.\n\n[Ste971\n\nW. Richard\nStevens.\nTCP Slow Start, Congestion\nAvoidance,\nFast Retransmit,\nand Fast Recovery Algorithms, January 1997. RFC 2001.\n\n[TMW97]\n\nKevin Thompson,\nGregory\nMiller, and Rick Wilder.\nWide-Area\nlnternet\nTraffic Patterns and Characteristics. IEEE Network,\n11 (6): 10-23, November/December\n1997.\n\n[WS95]\n\nGary R. Wright and W. Richard Stevens.\nlustrated\nVolume Ih The Implementation.\nWesley, 1995.\n\n[Zha86]\n\nLixia Zhang.\nWhy TCP Timers Don\'t Work Well.\nACM SIGCOMM,\npages 397--405, August 1986.\n\n[ZSC91]\n\nLixia Zhang, Scott Shenker, and David Clark.\nObservations on the Dynamics\nof a Congestion\nControl Algorithm:\nThe Effects of Two- Way Traffic.\nIn ACM\nSIGCOMM,\nSeptember\n1991.\ng\n\nAvoidance\n\nVan Jacobson, Robert\nExtensions\nfor High\n1323.\nVan\n\nJacobson\n\nBraden, and David Borman.\nPerformance,\nMay 1992.\n\nand\n\nMichael\n\nSrinivasan\nKeshav.\nto Flow Control.\nSeptember\n\nTCP\nRFC\n\nKarels.\n\nCon-\n\nControl,\nps Z.\n\nA Control Theoretic\nApproach\nIn ACM SIGCOMM,\npages 3-15,\n\n[KP871\n\nPhil Kam and Craig Partridge.\nImproving Round-Trip\nTime Estimates\nin Reliable Transport\nProtocols.\nIn\nACM SIGCOMM,\npages 2-7, August 1987.\n\n[Lud99]\n\nReiner\nLinks.\n1999.\n\n[Mi183]\n\nDavid Mills.\nInternet\n1983. RFC 889.\n\nIMM96]\n\nMatt Mathis\n\nA Case for Flow-Adaptive\nreport, Ericsson Research,\n\nDelay\n\nand Jamshid\n\nExperiments,\n\nMahdavi.\n\nedgment:\nRefining TCP Congestion\nSIGCOMM,\nAugust 1996.\n[MMFR96]\n\nMatt Mathis,\n\nJamshid\n\nMahdavi,\n\nRomanow.\nTCP Selective\nOctober 1996. RFC 2018.\n[ Mog92]\n\n[MSMO971\n\nJeffrey C. Mogul.\nNetworks.\nIn ACM\n\nForward\nControl.\n\nWireless\nFebruary\n\nDecember\n\nAcknowlIn ACM\n\nSally Floyd, and Allyn\n\nAcknowledgement\n\nOptions,\n\nObserving\nTCP Dynamics\nin Real\nSIGCOMM,\npages 305-317,\n1992.\n\nMatt Mathis, Jeff Semke,\n\nJamshid\n\nSeptember\n\nTCP/IP llAddison-\n\n1992.\n\n1991.\n\nLudwig.\nTechnical\n\nProtocol,\n\nIn\n\nVan Jacobson.\nModified TCP Congestion\nAvoidance\nAlgorithm,\nApril 1990. Email to the end2end-interest\nmailing list. URL: ftp://ftp.ee.lbl.gov/email/\nvanj.90apr30.txt.\n\ngestion\nAvoidance\nand\nftp://ftp.ee.lbl.gov/papers/congavoid,\n[Kes91 ]\n\nand Control.\n\nControl\n\nMahdavi,\n\nand Teunis\n\nOtt. The Macroscopic\nBehavior of the TCP Congestion\nAvoidance\nAlgorithm.\nComp,ter\nCommunication\nReview. 27(3), July 1997_\n\n12\n\nIn\n\n'