b'USABILITY\nHuman Research Program - Space Human Factors & Habitability\nSpace Human Factors Engineering Project\n\nAnik\xc3\xb3 S\xc3\xa1ndor, Ph.D., Kritina L. Holden, Ph.D.\n\nSHFE RISKS ASSOCIATED WITH DRP\n\nABSTRACT\nThe Usability project addresses the need for research in the area of metrics and methodologies used in hardware and software\nusability testing in order to define quantifiable and verifiable usability requirements. A usability test is a human-in-the-loop\nevaluation where a participant works through a realistic set of representative tasks using the hardware/software under investigation.\nThe purpose of this research is to define metrics and methodologies for measuring and verifying usability in the aerospace domain\nin accordance with FY09 focus on errors, consistency, and mobility/maneuverability.\nUsability metrics must be predictive of success with the interfaces, must be easy to obtain and/or calculate, and must meet the\nintent of current Human Systems Integration Requirements (HSIR). Methodologies must work within the constraints of the\naerospace domain, be cost and time efficient, and be able to be applied without extensive specialized training.\n\nRisk of Error Due to Inadequate Information. When conducting usability testing, if usability metrics are incompletely or incorrectly defined, the\ninformation acquired from user testing may be inadequate and may lead to erroneous design decisions. In turn, those erroneous design\ndecisions can result in a user interface that does not provide the information needed by the crew.\nRisk of Reduced Safety and Efficiency Due to Poor Human Factors Design. Good human factors design depends greatly on proper humanin-the-loop testing and appropriate usability metrics. If any of these are inadequate, design flaws will not be identified, and the design will\nhave reduced safety and efficiency.\nGap addressed: Currently, Constellation usability requirements in the Human Systems Integration Requirements (Rev. C) document are\ndefined in terms of errors: minimal impact errors and significant impact errors. While the requirements specify maximum error rates, the\ndetails of how to define an error, and how to calculate error rates are not provided. Similarly, there is a gap in knowledge in defining metrics\nfor consistency and mobility./maneuverability, and this DRP will help define them.\n\nINTRODUCTION\n\nUSABILITY TESTING METHODOLOGY\n\nUSABILITY METRICS\n\n\xe2\x80\xa2 This DRP aims to develop usability metrics that will help formulate verifiable\nusability requirements.\n\xe2\x80\xa2 Currently, Constellation usability requirements in the Human Systems Integration\nRequirements (Rev. C) document are defined in terms of errors: minimal impact\nerrors and significant impact errors.\n\xe2\x80\xa2 While the requirements specify maximum error rates, the details of how to\ndefine an error, and how to calculate error rates are not provided.\n\nHuman Centered Design\n\xe2\x80\xa2 Human Centered Design (HCD) is an approach (See Figure 2) that focuses on\nmaking a system usable by incorporating human factors and ergonomics in system\ndesign (ISO 13407).\n\xe2\x80\xa2 HCD is characterized by early and frequent user involvement and an iterative\ndesign-test-redesign process. Usability testing is one of the key methods within the\nHCD approach.\n\nMetrics of Interest for FY09\n\nDefinition of Usability\nThe International Standards Organization ISO 9241-11 defines usability as \xe2\x80\x9cThe\nextent to which a product can be used by specified users to achieve specified\ngoals\xe2\x80\x9d, and recommends evaluating usability in terms of measures of\neffectiveness, efficiency, and satisfaction.\nMeasures of effectiveness (i.e. Can you accomplish the task?) relate the goals or\nsub-goals of the user to the accuracy and completeness with which these goals\ncan be achieved.\nMeasures of efficiency (i.e. Can you accomplish the task in an ideal timeframe and\nuse of resources?) relate the level of effectiveness achieved to the expenditure\nof resources.\nSatisfaction (i.e. Do you like the system?) measures the extent to which users are\nfree from discomfort, and their attitudes towards the use of the product. ISO\n9241-11 also mentions the additional metrics of cognitive and physical workload.\nISO 9126 document on Software engineering - Product Quality Metrics describes\na Software Quality Model (See Figure 1) that includes usability. Within this\nmodel, usability is defined as a quality metric along with functionality, reliability,\nefficiency, maintainability, and portability.\n\nFigure 1. ISO 9126 Software Quality Model\nJacob Nielsen\xe2\x80\x99s Definition\nNielsen (1993) describes usability in terms of five factors: learnability, efficiency,\nmemorability, errors, and satisfaction.\nLearnability refers to the ease of accomplishing basic tasks when users encounter\nthe design for the first time. Learnability expresses how well a novice user can\nuse the system, while the efficient use of the system by an expert is expressed\nby efficiency. If the system is used only occasionally, the term memorability is\nused.\nEfficiency can be defined as time needed to accomplish the task after users are\nalready familiar with the design.\nErrors can be counted during performance observation and rated based on\nseverity.\nUser satisfaction indicates how pleasant is to use the design.\n\nFigure 2. Human Centered Design process model\nTasks\n\xe2\x80\xa2 Relevant tasks have to be selected for the hardware or software to be tested. These\ntasks may be defined based on task analysis or based on the focus of the usability\ntest. Standard practice is to select several types of tasks for testing:\n1) tasks that are frequent and nominal,\n2) tasks that are difficult or expected to cause problems, and\n3) tasks that are off-nominal or rarely performed.\n\xe2\x80\xa2 Based on the tasks, the test conductor constructs realistic scenarios that are\npresented to the participant. For example, one such scenario in the context of an\nonline word processor application may be the following:\nStep 1. Log in to the website using your username and password.\nStep 2. Create a new document with the title \xe2\x80\x9cMy document\xe2\x80\x9d.\nStep 3. Save the document and close it.\nStep 4. Change the name of the document to \xe2\x80\x9cMy first document\xe2\x80\x9d.\nSelection of Participants and Sample Size\n\xe2\x80\xa2 It is recommended to select participants who are representative of the user group of\nthe software or hardware in question. Sample size is usually decided based on\navailability of participants and cost; however, it is recommended to have at least 10,\nif possible, 20 or more participants to make sure that even usability problems with\nlower probability are found during testing.\n\xe2\x80\xa2 Usability testing can be used to compare designs or products and it can be used\nalso for verification purposes. However, for the latter case, one has to define the\nsuccess criteria for the software or hardware in terms of the metrics that have been\nused during the testing phase, or that have been mandated in requirements.\nDefining the context of usability testing\nSystems should be tested in a context as similar as possible to that of the actual\nsystem, and results should be interpreted in the light of the context. For example, if\na system is used under high stress, results from a laboratory evaluation that is low\nstress must be interpreted with caution. Results can sometimes be extrapolated by\nassuming that error rates will be higher under stress, and also that task times will\nchange.\n\nErrors\n\xe2\x80\xa2\nBefore conducting usability testing, the researcher must decide on the definitions of\nerrors and on definitions of severity levels (Tullis and Albert, 2008). A very strict\ndefinition of errors could include number of comments or statements about confusing\ninterface elements (for example \xe2\x80\x9cI am not sure which button to click\xe2\x80\x9d) or longer\nresponse times. A more lenient definition might consider only erroneous clicks, or an\ninability to complete the task as errors. Currently, Constellation usability requirements\nin the Human Systems Integration Requirements (Rev. C) document are defined in\nterms of errors: minimal impact errors and significant impact errors. Although the\nrequirements specify maximum error rates, the details of how to define an error, and\nhow to calculate error rates are not provided.\n\xe2\x80\xa2\nErrors are one of the standard accepted metrics employed in usability testing - far\nmore complex than may appear on the surface.\n\xe2\x80\xa2\nSome specific questions to be addressed in this DRP with respect to errors are:\n\xe2\x80\xa2 How can errors be defined and classified?\n\xe2\x80\xa2 What is a usability error versus a human error?\n\xe2\x80\xa2 How are errors measured? What about recoverable errors?\n\xe2\x80\xa2 How is error severity taken into consideration?\n\xe2\x80\xa2 How are usability errors related to risk assessment?\nReadability and Legibility\n\xe2\x80\xa2\nReadability and legibility are important aspects of interface usability\n\xe2\x80\xa2\nThis DRP will provide a standard methodology for readability/legibility measurements\nto help verify requirements for readability/legibility\nConsistency\n\xe2\x80\xa2\nConsistency is the unification of the general operating sequence, terminology,\ncomponents, layout, color, and style in an application (Shneiderman, 1998).\n\xe2\x80\xa2\nIn a consistent interface, if one part of the software behaves in a certain way, the other\nparts will also provide the same type of interaction.\n\xe2\x80\xa2\nOzok and Salvendy (2004) developed a scale using several factors of consistency:\ntext structure, general text features, information presentation, lexical categories,\nmeaning, user knowledge, text content, communication attributes, and physical\nattributes. However, their guidelines refer to interfaces heavy in text and do not give\nenough guidance on general consistency.\n\xe2\x80\xa2\nThe work in this DRP includes development of a consistency scale that is applicable\nto more graphical user interfaces, and user interfaces in general. The new scale will\ninclude categories for:\n\xe2\x80\xa2 presentation of information to the user;\n\xe2\x80\xa2 input of information to the system, and\n\xe2\x80\xa2 method of interaction between the user and the system.\nMobility/Maneuverability\n\xe2\x80\xa2\nEven though the Cooper-Harper provides a subjective measure of a person\xe2\x80\x99s ability to\ncontrol and stabilize the hardware, it focuses on stability rather than\nmobility/maneuverability.\n\xe2\x80\xa2\nObjective data (e.g., range of motion or torque) have been used to quantify mobility of\nspace suits; however, the need for a user to subjectively rate the\nmobility/maneuverability of hardware as a whole, while completing a specific task is\ncritical and not addressed by currently available scales.\nA standardized hardware maneuverability/mobility usability measurement and\n\xe2\x80\xa2\nmethodology needs to be developed in order to help practitioners to measure the\nusability of various types of hardware.\n\n'