b'Source of Acquisition\nNASA Ames Research Center\n\nCOPING WITH TRIAL-TO-TRIAL VARIABILITY OF EVENT RELATED SIGNALS:\nA BAYESIAN INFERENCE APPROACH\nMingzhou D i n f , Kevin H. Knuthb, Yonghong Chena, Steven L. Bresslef, and Charles E. SchroedeP\na Department of Biomedical Engineering, University of Florida, Gainesville, FL 3261 1, USA;\nb NASA Ames Research Center, Moffett Field, CA 94035, USA;\nc Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton, FL 33431, USA;\nd Nathan Kline Institute. Orangeburg, NY 10962, USA\n\nABSTRACT\nIn electroneurophysiology, single-trial brain responses to a\nsensory stimulus or a motor act are commonly assumed to\nresult from the linear superposition of a stereotypic eventrelated signal (e.g. the event-related potential or ERP) that\nis invariant across trials and some ongoing brain activity\noften referred to as noise. To extract the signal, one performs an ensemble average of the brain responses over many\nidentical trials to attenuate the noise. To date, h s simple\nsignal-plus-noise (SPN) model has been the dominant approach in cogmtive neuroscience. Mounting empirical evidence has shown that the assumptions underlying this model\nmay be overly simplistic. More realistic models have been\nproposed that account for the trial-to-trial variability of the\nevent-related signal as well as the possibility of multiple differentially varying components within a given ERP waveform. The variable-signal-plus-noise (VSPN) model, whch\nhas been demonstrated to provide the foundation for separation and characterization of multiple differentially varying\ncomponents, has the potential to provide a rich source of information for questions related to neural functions that complement the SPN model. Thus, being able to estimate the\namplitude and latency of each ERP component on a trial-bytrial basis provides a critical link between the perceived benefits of the VSPN model and its many concrete applications.\nIn this paper we describe a Bayesian approach to deal with\nthis issue and the resulting strategy is referred to as the differentially Variable Component Analysis (dVCA). We compare\nthe performance of dVCA on simulated data with Independent Component Analysis (ICA) and analyze neurobiological recordings from monkeys performing cognitive tasks.\n1. INTRODUCTION\nRelations between brain and behavior are often studied by\nrecording event-related potentials (ERPs) over repeated presentations of a sensory stimulus or task performance. Each\ntask performance is referred to as a trial. On a single-trial\nbasis, the traditional model holds that the data from a given\nelectrode (channel) is the linear superposition of an eventrelated potential (ERP), which is considered the signal having a characteristic ,waveform whose amplitude and latency\nstay the same each time the event is repeated on multiple trials, and ongoing background activity (noise). We refer to this\nmodel as the signal plus noise (SPN) model and represent it\nmathematically as [l].\n\n-4)~ ( t+)t7r(t)\n=\n\n(1)\n\nThis work supported by NIMH grants MH070498, MH71620 and\nMH64204, and the NASA SISM Intelligent Systems PrOgraa\n\nwhere x r ( t ) is the EEG recording from the rth trial, s ( t ) is\nthe ERE and qr(t)is the ongoing noise process. The empirical evidence suggests that the SPN model is overly simplistic and a more realistic model should capture the trial-to-trial\nvariability in amplitude and latency of the event-related signals and the existence of multiple components with differential variability in their single-trial amplitudes and latencies.\nA model that incorporates all these features, which we call\nthe variable signal plus noise (VSPN) model, can be written\nas P I\nN\n\nxr(r)=\n\na n r S n ( t - Tnr)\n\n+~ r ( t )\n\n(2)\n\nn= 1\n\nwhere sn(t) is the nth event-related component waveform\n,\nwith trial-to-trial variable amplitude and latency given by &\nand ,,z respectively, and N is the total number of compo,\nnents. In many experimental paradigms, investigators use\nmultiple detectors positioned at different locations to measure the event-related activities of neural ensembles. In this\ncase, there is a need to consider the specific neural ensemble whose activity is responsible for a gven component that\nappears across all detectors. We call such an ensemble the\nsource or the generator for the component. The degree to\nwhich a detector records a signal from a particular source\ndepends on many factors including the relative position and\norientation of the source relative to the detector. To describe\nthis source-detectorinteraction, we introduce a coupling matrix C , where the matrix element C describes the degree\n,\nto which the mth detector (m= 1,2,.. . ,M) detects the nth\nsource. This coupling matrix is known as the mixing matrix\nin the source separation literature and as the lead-field matrix in electroneurophysiology. For the r* recorded trial we\nmodel the signal x ( t ) recorded by the mth detector in component form as\n\nwhere the notation has the same interpretation as before (see\nEquation (2)). For the purpose of this work we will refer to\nboth the single sensor model in (2) and multi-sensor model\nin (3) as the VSPN model. The goal of this work is to present\na Bayesian inference framework for the estimation of all the\nparameters in the VSPN models and to then use the trial-totrial variability information of the ERP components to gain\nunderstanding of neural functions.\n\n2. METHODS\nFor simplicity we consider in more detail the single channel\nVSPN model in (2). Bayesian inference for the parameters\nin the VSPN model in (3) can be similarly formulated. The\nposterior probability density function (PPDF) is formulated\nas\nP ( S > Q : , ~ , ~ , , / X , P(s,a,z,@q(t)lOx\nOC ~ )\n\nP(XlS,\n\na ,z,Q, @),I)\nP(Xl0\n\nQ=\n\n5E\n\nN\n\n[ x r ( t )- C, G r s n ( t- znr)\nn= 1\n\nr-1 r=l\n\n(4)\n\nwhere the boldface parameters denote the set of parameters\nfor all the components over the whole ensemble of trials.\nThe parameter e,,(f) denotes the parameters for the ongoing process and p ( s , a ,z,O, ( t )I ) is the prior probability for\nZ\nthe model parameters. For this additive model, the likelihood p(xIs,a , z, 0, ( t ) , Z ) is given by the probability model\nof the ongoing activity, i.e., p(v(t)lZ). In the absence of\nprecise knowledge about the temporal structure of the ongoing activity, we assign q ( t ) to be independent identically\ndistributed with a (unknown) time-independent variance 0\n;\nand zero mean. In this way, Equation (4) rewritten as\nis\n\nUnder the constraint of a given mean and d, following\nand\nthe principle of maximum entropy [3], a Zaussian density\nis assigned to the likelihood function. After dropping the\nnormalization term p(xlZ)-\', the PPDF can be rewritten as\n\nwhere R is the total number of trials, T is the total number\nof sampled data points in a given trial and other parameters\nhave the same interpretation as before. For simplicity we\ntake the prior distributions of a,,, znr, and s,(t) to be uniform, with appropriate cutoffs reflecting physiologically reasonable ranges of values. Treating the variance of the ongoing activity as a nuisance parameter and assigning the Jef;\nfreys prior p(o,, IZ) = 0\' [3],we marginalize the posterior\n, and\nover o , obtam\n\nA thorough evaluation of the PPDF and computation of its\nmoments can be obtained via Markov Chain Monte Carlo\n(MCMC) methods [3]. Here we summarize the PPDF by\nseeking the Maximum a Posteriori (MAP) solution, i.e., a\nset of parameters that maximize the PPDF. Specifically, the\nMAP solution for the collection of model parameters M is\n\nA = argMmax[ln(p(MII))+Mp(DIM,I))I\n\nwhere D stands for data.\nIntuition about the characteristics of the MAP solution\ncan be gained by examining the partial derivatives of the logarithm of the PPDF with respect to each of the model parameters. This leads to a practical and simple estimation algorithm. If we let\n\n(7)\n\nl2\n\n(8)\n\nlogarithm of the posterior can be simply written as\n\nRT\n\n+\n\nIn P = -- 1nQ const,\n2\n\n(9)\n\nwhere P stands for PPDF. Setting partial derivatives against\nthe unknown single-trial parameters to zero we can solve the\nresulting equations in an algorithmic fashion [2].\nThe MAP solution above leads to a simple iterative fixed\npoint algorithm. After proper initialization, at each iteration\nl\nstep the parameters for al components are updated in sequence: 6rst the latencies, then the w a v e f o e , and finally\nthe amplitudes. Specifically, let $ ( I ) , ajr, zfr denote the estimated values of the parameters of the j" source during the\nf h in the ifh iteration. To avoid degeneracy in the model,\ntrial\nthe averages of the amplitude and latency values in each it=\neration are constrained to (afr). 1 and (zfr).= 0. In this\nway, if there is no trial-to-trial variability both in amplitude\nand latency, the superposition of the estimated component\nwaveforms should equal to the simple average of single trial\ntime series referred to here as AEW. For a single channel\ndata set x,the algorithm consists of the following steps:\n0 At rn = 0, the initial guess for the amplitudes and time\noffsets are set to a;r = 1, z;r = 0. For simplicity, the decision on the number of components N is based on the inspection of the AEW. Similarly,N non-overlapping segments of the A E W are taken as the initial guesses for\nthe N components\' waveforms . After this initialization,\neach iteration consists of four steps:\nFor a given trial, estimate the latency\none component at\na\ntime,\naccording\n?+I\n- argmaxp\'(z),\nwith\np(t) =\nto\n- C:=l,n+J anrsn(t,-\n\n7)\n\nstarting from the first and proceeding up to the K h\ncomponent.\nEstimate\nthe\nwaveforms\naccording\nto:\n\nplitudes according\' to:\n\na?\'\n\n= ___ with\nXL1"\n\nx r ( r ) - Cr=l,n+J nr si+l (t -)\'7\nai\n:\n;\n\nq\').\n\nu=\n\nEL1 v2 \'\n\nand V = si+\'(t J\n\n+\n\nIncrement the iteration index: i = i 1; Repeat 2\nthrough 4 for K iterations until convergence or for\na pre-decided number of iterations.\n\nNo Variability\n\nI\n\n1\n\n0\n\n1\n\n0\n\nVariability\n\n100\n\n.\n\nms\n\n1\n\n.\n\nI\n\n.\n\n200\n\n300\n\n0\n\n100\n\n200\n\n300\n\n200\n\n300\n\n0\n\n100\n\n200\n\n300\n\n.\n\n100\n\nFigure 1: (Top) The source waveshape results for extended\nICA, and dVCA in the single-trial analysis cases. Only the\ndVCA analysis of the variable responses results in accurate\nidentification of the underlying source waveshapes (box).\n(Bottom) The associated Amari errors for the cases above.\n\n3. COMPARISON TO ICA\nThe dVCA algorithm shares many similarities to Infomax\nICA [ ] since in both cases, the sources are assumed to\n4,\nbe linearly mixed. The greatest difference is that the signal model used in dVCA is more specialized than the linear\nmixing model used in ICA; it allows for the sources to vary\nin specific ways from trial to trial. Second, dVCA solves for\nall the parameters, whereas ICA margnalizes over the source\nwaveshapes [6,7,2]. Third, ICA through the use of the logistic function makes assumptions about the amplitude density\nof the source waveshapes [5, 61, whereas dVCA makes no\nsuch specific assumptions.\nThe greatest strength of dVCA arises due to the fact that\nit explicitly models trial-to-trial variability. Here we briefly\ncompare dVCA to ICA, and show that dVCA outperfom\nICA when trial-to-trial variability is a significant factor in\nthe data. We simulated electric field potentials recorded\nfrom a linear-array multielectrode with 15 channels spanning\nthe cortical laminae in V1 by designing three synthetic ERF\'\ncomponents sampled at 2 kHz to approximate the neural ensemble response to diffuse red-light stimulation in macaque\nVI (see next section). Two synthetic data sets were used to\nperform the comparison; both of which were contaminated\nby low amplitude additive Gaussian noise (component SNRs\nof 7.2dB, -6.7dB, and 4.5dB). The first data set exhibited\n\nneither amplitude nor latency variability. The second data\nset exhibited log-normally distributed amplitude variability\nwith sample mean p = 1.0 and sample standard deviation\nI = 1.O, normally-distributedlatency variability with sample\nS\nmean p = 0.0m and sample standard deviation D = 10.0ms.\nICA was performed by applying the Infomax ICA algorithm in the EEGLAB toolbox [8]. The runica.m algorithm\nwas used with the \'extend\' option (extended ICA), which allows ICA to model sub-Gaussian as well as super-Gaussian\nsources. Since each data set has 15 channels, ICA estimates 15 source waveshapes. The three estimated source\nwaveshapes with highest correlation to the three original\nsources were chosen for analysis while the remaining estimated sources were ignored.\nThe data sets were analyzed by ICA in two different\nways: first by averaging the epochs and analyzing the average response (average analysis), and second by treating the\ndata as a long string of concatenated single-trial responses\n(single-trial analysis). Figure 1 (top) shows the source waveshape results for extended ICA, and dVCA in the single-trial\nanalysis cases. Only the dVCA analysis of the variable responses results in accurate identification of the underlying\nsource waveshapes. The Amari errors [9] in Figure 1 (bottom) quantify the degree to which the components were correctly identified in the four cases. Since dVCA can only\nanalyze single-trial responses, the dVCA analysis consisted\nof analyzing the single-trial cases only resulting in only two\nbars in the bar graph. Note that dVCA performs poorly when\nthere is no trial-to-trial variability. However, the presence\n,of trial-to-trial variability dramatically improves the performance of dVCA enabling it to surpass extended ICA in separation quality.\n4. APPLICATION TO EXPERIMENTAL DATA\nTwo male macaque monkeys were trained to perform an\nintermodal selective attention task [lo]. Streams of interdigitated auditory and visual stimuli were presented with\nan irregular interstimulus interval (ISI) (minimum of 350\nms). The attended modality was alternated across trial blocks\nwhile the physical stimuli remained the same. The eye positions were carefully monitored and controlled within a predefined window for both attending conditions. In both auditory\nand visual modalities, two types of stimuli were presented:\na \'standard\' which occurred 86% of the time and a \'deviant\'\n(created by changing the intensity of the standard stimuli),\nwhxh was presented on 14% of the trials. The visual stimuli\nwere diffusive light flashes subtending a 12-degree visual angle centered at the point of gaze. Attention to a given sensory\nmodality was maintained by requiring the subject to make a\nlever-release response to the deviant stimuli while ignoring\na l stimuli in the other modality. The level of performance\nl\naccuracy was around 92% and difficulty of discrimination\nwas equated across the two modalities. A linear array elecrrode with i 4 contacts (150 micron spacing) spaming all six\ncortical laminae was inserted in different parts of the visual\nsystems. Laminar LFPs were sampled at 2 kHz. Current\nsource density proaes were computed by taking the second\nspatial derivative of the laminar LFPs. Here only neural activity from V1 in response to the standard visual stimulus\nunder two different attending conditions will be considered\nto examine the effect of attention.\nA key question in the research on the neural correlates of\n\nVI\n\nREFERENCES\n\nAERP\n\n[ 1] Truccolo, W.A., Ding, M., Knuth, K.H., Nakamura, R.,\n\n..\n\n. . ,. . . .. .._. .\n.\n.. .\n.\n.\n\nnme (ms)\nV 1 mtency MMbut!m Cl (au-A)\n\nV l Latency Dimbuhon C1 (au--v)\n\nFigure 2: (Top) 14 averaged ERPs along the depth electrode for the attending-visual (att-V) condition (red) and the\nattending-auditory (att-A) condition (blue). (Bottom) The latency distributions of the responses in the two conditions.\n\nattention is whether early neural responses in primary sensory areas are modulated by attention. Reports on thls issue in the primate visual attention literature are conflicted\nEll, 121, with some reporting attention modulation starting\nfrom the response onset while others contending that attentional effect lagging response onset by a substantial time difference [101. While the main technique previously used is the\ngrand averaging method, we believe that single-bial parameters may provide a greater sensitivity to attentional effects in\ndifferent experimental paradigms.\nIn Figure 2 (top) we display the superposition of all 14\naveraged ERPs along the depth electrode for the attendingvisual condition (red) and the attending-auditory (ignoringvisual) condition (blue). No statistically significant differences are observed for the first component from 50 ms to 100\nms (i.e. early visual processing). To examine whether the\nsingle trial parameters are modulated by attention we measured single trial latencies by the dVCA method for the first\nevoked component. The distributions of the trial-by-trial latencies are shown in Figure 2 (bottom). The distributions for\nthe two conditions are significantly different ( p < 0.01). In\nparticular, when the monkey attended the visual modality, a\nnarrower and more focused latency distribution is seen (Figure 2 bottom right) when compared to the distribution from\nattending-auditory trials, suggesting that on a trial-by-trial\nbasis, the timing of the evoked component becomes more\nreliable. The narrower latency distribution of the first component is often accompanied by smaller trial-by-trial amplitudes (results not shown). On simple averaging this narrow\nlatency distribution is offset by the smaller amplitude distribution and the result is an AERP that is not different fromthe\nignoring-visual condition. The reason why the evoked component has a smaller amplitude for attending-visual trials is\nnot known.\n\nand Bressler, S.L., \xe2\x80\x9cTrial-to-trial variability of cortical\nevoked responses: implications for the analysis of functional connectivity,\xe2\x80\x9d Clin. Neurophysiol. 113(2), 206226,2002\n[2] Truccolo, W.A., Knuth, K.H., Shah, A.S., Bressler,\nS.L., Schroeder, C.E., and Ding, M., \xe2\x80\x9cEstimation of\nsingle-trial multi-component ERPs: Differentially variable component analysis (dVCA),\xe2\x80\x9d Biol Cybern. 89,\n426-438,2003\n[3] Gelman, A., Carlin, J.B., Stem, H.S., and Rubin, D.B.,\nBayesian Data Analysis, Chapman and Hall, New York,\n1995\n[4] A. J. Bell and T. J. Sejnowski, \xe2\x80\x9cAn informationmaximization approach to blind separation and blind\ndeconvolution,\xe2\x80\x9d Neural Comp., vol. 7, pp. 1129-1 159,\n1995.\n[5] K. H. Knuth \xe2\x80\x9cDifficulties applying recent blind source\nseparation techques to EEG and MEG,\xe2\x80\x9d in G. J. Erickson, J. T. Rychert and C. R. Smith (eds.), Maximum\nEntropy and Bayesian Methods, Boise 1997, Kluwer,\nDordrecht, pp. 209-222, 1997.\n[6] K. H. Knuth, \xe2\x80\x9cA Bayesian approach to source separation,\xe2\x80\x9d in J.-F. Cardoso, C. Jutten and P. Loubaton (eds.),\nProceedings o the First International Workshop on Inf\ndependent Component Analysis and Signal Separation:\nICA\xe2\x80\x9999, Aussois, France, Jan. 1999, pp. 283-288, 1999.\n[7] K. H. Knuth, W. A. Truccolo, S. L. Bressler and M.\nDing \xe2\x80\x9cSeparation of multiple evoked responses using\ndifferential amplitude and latency variability,\xe2\x80\x9d in T.-W.\nLee, T.-P. Jung, S. Makeig, T.J. Sejnowslu (eds.), Proceedings o the Third International Workshop on Indef\npendent Component Analysis and Blind Signal Separation (ICA 2001), Dec. 9-12, 2001, San Diego CA, pp.\n463-468,2001.\n[8] A. Delorme, S. Makeig \xe2\x80\x9cEEGLAB: an open source\ntoolbox for analysis of single-trial EEG dynamics including independent component analysis,\xe2\x80\x9d J. Neurosci.\nMeth. 134,9-21,2004.\n[9] S.-I. Amari, A. Cichocki, H. H. Yang \xe2\x80\x9cA new learning\nalgorithm for blind signal separation,\xe2\x80\x9d in D. Touretzky,\nM. Mozer, M Hasselmo (eds.), Advances in Neural Information Processing Systems 8, Cambridge, MA: MIT\nPress, pp. 752-763, 1996.\n[lo] Mehta, A.D., Ulbert, I., and Schroeder, C.E., \xe2\x80\x9cIntermodal selective attention in monkeys I: distribution and\ntiming of effects across visual areas,\xe2\x80\x9d Cereb. Cortex.\n10(4), 343-58, 2000\n[ l l ] Luck, S.J., Chelazzi, L., Hillyard, S.A., and Desimone,R., \xe2\x80\x9cNeural mechanisms of spatial selective atrention in areas V i , V2 and V4 of macaque v i s u i cortex,\xe2\x80\x9d J. Neurophysiol. 77, 24-42, 1997\n[12] Motter, B.C., \xe2\x80\x9cFocal attention produces spatially selective processing in visual cortical areas V1, V2 and V4\nin the presence of competing stimuli,\xe2\x80\x9d J. Neurophysiol.\n70,909-919,1993\n\n'