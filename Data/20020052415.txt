b'Input\n\nDecimated\n\nEnsembles\n\nKagan Turner and Nikunj C. Oza\nNASA\nAmes Research Center\nMoffett\n\nField,\n\nCA 94035\n\n{kagan, oza}@ptolemy, arc. nasa. gov\nNovember\n\n27, 2001\n\nAbstract\nUsing an ensemble of classifiers\niastead of a single classifier\nhas been\nshown\n\nto improve\n\nproblems.\nthe\n\ngeneralization\n\nHowever,\n\namount\n\nthe\n\nof correlation\n\nfore, reducing\n\nthose\n\nlevels\n\nhigh\n\ninput\n\nperformance\n\nextent\n\nof such\n\namong\n\ncorrelations\n\nis an important\n\ndecimation\n\nthe errors\nwhile\n\narea\n\n(ID),\n\nin many\n\nkeeping\n\nwhich\n\nreduction,\n\nsonar\nand\n\ndata\ntwo\n\nsets,\n\naiong\nthree\n\nsynthetic\n\nwith\n\nresults\n\nbenchmarks\n\nIn\n\nselects\n\ndata\n\nsets.\n\nof our\nfrom\n\nThe\n\nusing\n\nUsing\nedly\n\nan\n\nanMysis,\n\nof classifiers\n\nensemble\n\nshown\n\nproblems\nment,\n\ninstead\n\nthe\n\nto improve\n[9,\n\none\n\n17,\n\ntheir\n\nperformance\n\narticle,\n\nfeature\n\nmethod\n\nwe explore\n\nsubsets\n\non\n\nindicate\n\ntwo\n\nunderwater\nrepositories,\n\nthat\n\nwhose base\nof features;\n\nfor their\n\nto decouple\nthe\nbenefits\nof cor-\n\nProbenl/UCI\ninput\n\ndecimated\n\nclassifiers\nuse all the\nand features\ncreated\n\non a wide range\n\ndiversity\n\nportion\n\nperformance,\n\nOne\navailable\n\nparticularly\n\nthat,\n\nare\n\nthe\n\nmany\n\nsurface,\n\n58,\n\n64].\n\nof domains.\n\nlower\n\nbase\n\nmodels\n\nwhen\n\nby\n\nis\n\nused\nthe\n\ndata\n\nsets\n\nand\n\nin this\n\nrepeat-\n\nrecognition\nsuch\n\nimprove-\n\nof performance\n\nreduce\n\ntheir\n\nin the\n\ncorrela-\n\nactively\noutputs)\n\nhowever,\nThese\n\nwith\n\nis that\n\nsmall\n\nbeen\n\nthat\n\n[22]).\n\nlearning.\nare\n\nobtain\n\nfield,\n\nthem\n\nmethods\n\nhas\npattern\n\nlevel\n\ncorrelations\n\ntraining\n\nduring\n\nto\n\nmethods\n\n[9], Boosting\n\nof such\n\norder\n\nensemble\n\nwork\n\nBagging\n\ndrawback\n\nclassifier\nin many\n\na reasonable\n\nensemble\n\nMost\n\n(e.g.,\n\ndata\n\nin\n\nmaintain\n\nconstitute\n\nerror\n[49,\n\nin the\nset.\n\nof the\n\nthat\nThere\n\nselection\n\ntraining\n\nis well-known\n\n49, 64].\n\nclassifiers\n\npattern-level\nabout\n\nIt\n\nof a single\nperformance\n\nsimultaneously\n\nmodify\n\n(e.g.,\n\nbase\n\nto\n\nclassifiers\n\n[1, 29, 44,\n\ndiversity\n\ngeneralization\n\n69].\n\nneeds\n\nbase\n\ntions\n\nthe\n\ncomponents\n\nthis\n\non\n\nThere-\n\nIntroduction\n\n1\n\nin\n\nprincipal\n\nclassifiers.\n\nuses them\ntheoretical\n\nthe\n\nresults\n\nensembles\n(IDEs)\noutperform\nensembles\ninput features;\nrandomly\nselected\nsubsets\n\nbase\n\ns greatly\n\nthe classifiers\'\n\nability\nto discriminate\namong\nthe classes and\nbase classifiers.\nWe provide\na summary\nof the\nrelation\n\nrecognition\n\ndepend\n\nof the\n\nof research.\n\na method\n\npattern\n\nimprovement\n\nmethods\n\nbegin\n\ncan\n\non\nbring\n\nsubsets\n\ndefinition,\n\nThis\nto\n\namong\nfocuses\n\ndifferent\nby\n\npromote\n\nlead\nwith.\n\nof\n\nonly\nto\n\npoor\n\nTraining\n\na\n\nthe base classifiers\nusing different subsets of features\navoids this issue as all the\npatterns\ncan be used in the training while still yielding base model diversity.\nTwo possible feature selection/extraction\nmethods are Principal\nComponent\nAnalysis\n(PCA)[31,\n521 and random subspace selection [t0, 25]. PCA constructs\nnew features\nsuch that the data has maximum\nvariability\nover those features.\nHowever, PCA, when used in combining,\nnot only generates\nthe same features\nfor all potential\nclassifiers in the pool, but also fails to take class information\ninto account.\nRandom\nsubspace\nselection\novercomes\nthe first shortcoming\nof\nPCA, but it too does not consider the class labels when generating\nthe feature\nsubsets.\nThese two methods\ndo not attempt\nto choose features in a manner\nthat\nis helpful in the classification\ntask.\nIn this paper, we present input decimation--a\nmethod\nsubsets\nof the original\nfeatures\nbased on the correlations\nfeatures and class labels, and training classifiers on those\nbining. This method\nnot only reduces the dimensionality\n\nof choosing\ndifferent\nbetween\nindividual\nsubsets prior to comof the data, but uses\n\nthis dimensionality\nreduction\nto reduce the correlations\namong the classifiers\nin an ensemble,\nthereby improving\nthe classification\nperformance\nof the ensemble [51,63,6z].\nOur results indicate that input decimation\nreduces the error up to 90% over\nsingle classifiers\nand ensembles\ntrained on all features,\nrandomly-selected\nsubsets of features,\nand principal\ncomponents.\nWhile we expected\nstrong ensemble\nperformance,\ninput decimation\nals0 provided\nimprovements\nin the base classitiers in many cases by pruning extraneous\nor irrelevant\nfeatures,\nthus simplifying\nthe learning\nproblem\nfaced by each base classifier.\nIn this study we focus on\nthe "averaging"\ncombiner for two reasons:\n(i) despite its simplicity\n(or perhaps\nbecause of it) this combiner\nhas been shown to perform\nwell and hold its own\nagainst a wide array of more sophisticated\nmethods [16, 17]; and (it) by choosing\na simple combiner we isolate the effects of input decimation\nfrom those of the\ncombining\nmethod.\nFurthermore,\npattern-level\nensemble methods\nsuch as bagging, boosting,\nand stacking\ncan be used in conjunction\nwith input decimation\nwhich is a feature-level\nensemble\nmethod\n(i.e., input decimation\nis orthogonai\nto those methods).\nTherefore,\none can make meaningful\ncomparisons\nbetween\naveraging\ncombiners\nwith and without input decimation,\nor say, between\nstacking or bagging with and without input decimation\n(not reported\nin this article),\nnot between input decimated\nensembles and bagging or boosting.\nIn Section\n2, we summarize\na theory of classifier ensembles\nthat highlights\nthe connection\nbetween correlation\namong base classifiers\nand ensemble performance,\nalong with a brief overview of different dimensionality\nreduction\nmethods. In Section 3 we present the details of the input decimated\nensemble,\nand\nin Section 4 we provide experimental\nresults on two real underwater\nsonar data\nsets, three data sets from the PROBEN1/UCI\nbenchmarks\n[6, 54], and two\nsynthetic\ndata sets which allow a systematic\nstudy of input decimation.\nWe\nconclude with a discussion the effectiveness input decimation under varion\nof\nous circumstances along with future researchdirections Section 5.\nin\n\n2\nModel\n\nBackground\nselection\n\nis a ubiquitous\n\nNeither the selection\nbor algorithm),\nnor\n\nproblem\n\nin many\n\npattern\n\nrecognition\n\nproblems.\n\nof the method\n(e.g., multi-layer\nperceptron,\nnearest\nneighthe tuning of that algorithm\ncan yet be fully automated\n\nfor all problems\n[15, 20, 23]. The use of ensembles\nprovides\npartial relief since\nby pooling the classifiers\nbefore a decision is made, potential\nsensitivity\nto any\nsingle model is greatly reduced.\nOf course, the more similar the classifiers\nare,\nthe less likely it is that new information\nwill be present in the ensemble,\nresulting in little more than a "rubber stamping"\ncommittee.\nIn this section we first\nformalize\nthis connection\nbetween the correlation\namong the classifiers\'\nerrors\nand ensemble performance\nand then discuss various methods\nthat aim to reduce\nthat correlation.\n2.1\nIn this\n\nCorrelation\narticle\n\nand\nwe focus\n\nEnsemble\n\non\n\nPerformance\n\nclassifiers\n\nthat\n\nmodel\n\nthe\n\na pasteriari\n\nprobabili-\n\nties of the output\n.classes. Such algorithms\ninctudeBayesian\nmethods\n[4], and\nproperly\ntrained feed forward neuraJ networks\nsuch as Multi-Layer\nPerceptrons\n(MLPs) [56]. We can model the ith output\nof such a classifier as follows (details\nof this derivation\nare in [63, 64]):\n\n\xc2\xa3(x) = P(C{Iz)\n+\nwhere P(Cilx)\nis\n_i(x) is the error\none classifier,\nwe\nInstead,\nif we\nthe outputs\nof N\n\nthe posterior\nprobability\nof the ith class given pattern\nx, and\nassociated\nwith the ith output.\nGiven an input x, if we have\nclassify x as being in the class i whose value fi(x) is largest.\nuse an ensemble\nthat calculates\nthe arithmetic\naverage\nover\nclassifiers\nf_(x),\nm E {1,...\n,N}, then P(C{lx ) is given by:\nN\n\n1\n\nrn:l\n\nwhere:\n1\n\nN\n\nand U/n(x) is the error associated\nwith the ith output\nNow, the variance\nof _i(x) is given by [64]:\nN\n\n2\n\n_\n\nN\n\n]\nl:1\n\n1\nN2\n\nm:l\n\nN\n\n_\nm:l\n\n1\n\nN\n\n2\nrn=l\n\nl-_m\n\nof the\n\nruth\n\nclassifier.\n\nIf we express the covariances\nin terms of the correlations\n(coy (x, y) = corr (x, y)G= a_),\n2\nand use the average correlation\nassume the same variance G_, across classifiers,\nfactor among classifiers,\n5i, given by\n\nN\n\nI\n\n(2)\n\n6,= :v(N- 1)\nrn=l\n\nthen\n\nthe variance\n\nbecomes:\n\nN\n\nand,\n\nI\xc2\xa2rn\n\nN - 1_\n\n2\n\n1 + 5{(N\n\n(3)\n\n- 1)o._,(.)"\n\nBased on this variance, we can compute the variance of the decision boundary\ngeneralizing\nthis result to the classifier error, we obtain\nthe relationship\n\nbetween the model error (beyond the Bayes error)\nthat of an individual\nclassifier\n(E,_od_) [63, 64]:\n\nEm\xc2\xb0del\n_e\n\n=\n\n(l\n\nof the ensemble\n\n(Eam_oedel)and\n\n(4)\n\n+ 5(N-1)}E._odel\nN\n\nwhere\nL\n\n=\n\n(5)\ni=1\n\nand P_ is the prior probability\nof class i.\nEquation\n4 quantifies\nthe connection\nbetween\n\nerror\n\nreduction\n\nrelation\namong the errors of the base classifiers.\nThis result\nreduce the correlation\namong classifiers prior to using them\n2.2\n\nCorrelation\n\nAs shown\n\nabove,\n\nReduction\nif the\n\nclassifiers\n\nand\n\nthe cor-\n\nleads us to seek to\nin an ensemble.\n\nMethods\nto be combined\n\nrepeatedly\n\nprovide\n\nthe same\n\n(either erroneous\nor correct)\nclassification\ndecisions,\nthere is little to be gained\nfrom combining,\nregardless\nof the chosen scheme. As equation\n4 shows, reducing\n5 and increasing\nN are two ways to improve the performance\nof a classifier\nensemble.\nHowever,\nthese two ways are not independent.\nThis phenomenon\nis best illustrated\nby Figure\n1, where the error reduction\ndepending\non the\ncorrelation\namong the classifiers\nis displayed\nas a function\nof the number\nof\nclassifiers\n(based on Equation\n4). For example,\neven though\nincreasing\nthe\nnumber\nof classifiers\nfrom 4 to 8 does not provide any sizeable gains when the\ncorrelation\nis .9, it provides significant\ngains if the correlation\nis .1. That is,\nkeeping the correlations\nlow not only provides better error reduction\nfor a given\nnumber\nof classifiers,\nbut provides greater gains when adding classifiers.\nTo improve ensemble performance\none must either actively promote\ndiversity\nduring training or achieve diversity through the selection of the data presented\nto\n\n1\n0.9\n\n1,0\n\n:.\'.:.\' "-...,,. .......,\n,_\n.\n.\n\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\n\n0.8\n\xc2\xa2,.\n0\n\n:,=0.7\n0\n\n,\n\ncr 0.5\n_0.4\n\n\'.\n\n-,..\n\n-.-o..\n\n0.3\n0.2\n\n--\n\n...........\n............\n..........\n_,.-=.-...........\n..... _.....\n............\n............\n..............\n..........\n\n0.1\n0\n\nr\n\n2\n\n0\n\n,\n\n4\n\n_\n\nL\n\n6\n\n8\n\nL\n\n10\n\n,\n\n12\n\nN\nFigure I: Effect of correlation error reduction.\non\n\nthe base classifier training algorithms.\nExamples\nof the former include distorting\nthe output\nspace through\nerror-correcting\noutput\ncodes [18], using principal\ncomponent\nanalysis\non the output space [45],\nusing genetic algorithms\nto train\nthe classifier\n[50, 61] or modifying\nthe error function\nused for training\n[58].\nExamples\nof the latter include bagging\n[9], cross-validation\npartitioning\n[40, 64]\nand even boosting\n[22] (though\nthe goal there is not to reduce correlation,\nthe\nnet effect is the same). The most common\ndata selection\nmethods\nfocus on the\n"pattern"\nspace,\nthough dimensionality\nreduction\nmethods\nwhich manipulate\nthe feature space can also be used. Feature\nspace methods\nhave the advantage\nthat they do not reduce the number\nof patterns\navailable\nfor training\neach\nclassifier.\nThey generally fall into one of two different classes of methods:\nfeature\nselection or feature extraction.\nFeature\n\nextraction\n\nalgorithms\n\nsuch as Principal\n\nComponents\n\nAnalysis\n\n(PCA)\n\n[5, 31, 52] or Independent\nComponent\nAnalysis\n(ICA) [28] reduce the dJmensionality\nof the data by creating\nnew features.\nLinear PCA, perhaps\nthe most\ncommonly\nused feature extraction\nmethod,\ncreates new features\nthat are linear\ncombinations\nof the original features.\nThe aim of PCA, however,\nis to devise\nfeatures\non which the data shows the highest variability,\nwhether\nthose features\nare useful for classification\nor not [5]. Furthermore,\nbecause all the information\npresent in the initial features\nis "crammed"\ninto fewer principal\ncomponents,\nthere is a danger that classifiers\ntrained\non the principal\ncomponents\nwill have\nhigher, not lower correlations\namong them.\nFigure 2 demonstrates\nthe perils of\nnot using class information.\nThe left half of the figure shows a case in which PCA\nworks effectively.\nIn this case the first principal\ncomponent\n(Yl) corresponds\nto\nthe variable with the highest discriminating\npower.\nThe right half shows a similar data set (similar data distribution\nand linearly separable).\nHowever,\nbecause\nthe first principal\ncomponent\nis not "aligned"\nwith the class labels, selecting\nthis\n\nx2\n\nx2\n\n\xe2\x80\xa2\n_\'-\n\nxl\xc2\xa2\n\n.\n\n\xc3\x97\n\n._"..x\n\nx\nXX\n\nX\nX x\n\n\xc3\x97\n\nX_X\'AvJ\n\nX\n\nX\n\n"_ _\n\n_\n\nl_rl\n\nX\n\nV\n\nv\n\nXl\n\nXl\n\nFigure\n2: PCA and classification:\nThe first principal\n(Yl) can provide a good\ndiscriminating\nfeature\n(left) or a poor one (right), since the class membership\ninformation\nis not used.\n\ncomponent\nis a poor choice for this problem.\nIndeed,\nan input set consisting\nof only the first component\nwouldprovide\npractically\nrandom\ndecisions\non this\ndata set}\nYet, PCA remains\none of the most frequently\nused dimensionality\nreduction\nmethods\nin many classification\ndomains, including\nmedicaI and space\napplications\n[55, 60].\nFeature\nselection\nalgorithms\nfocus on selecting\na subset of the features\nto\npresent\nto the classifiers.\nOne example is the random\nsubspace\nmethod\n[10, 27]\nwhere random\nsubsets of the original features\nare presented\nto the classifiers.\nHowever, looking at Yz and y2 (assuming\nthose two are the original features)\nin\nFigure 2 shows a pitfall of random feature selection.\nRandomly\nselecting feature\nyz in the class configuration\nshown in (a) will lead to satisfactory\nclassification,\nwhereas randomly\nselecting\nfeature yl in (b) will lead to all discriminating\ninformation\nbeing lost. Many other feature selection methods\nuse various criteria\nfor deciding the relevance\nof each feature to the task at hand and choose some\nsubset\nof the features\naccording\nto those criteria\n[3, 7, 8, 19, 30, 43].\nThe\nsubset\nselection\ncan be distinct\nfrom the learning,\nwhich is the case with filter methods.\nHowever,\nmost of these feature\nselection\nmethods\nattempt\nto\nchoose features\nthat are useful in discriminating\nacross all classes.\nUsing such\na method\nwithin an ensemble learning scheme would have limited effectiveness\nsince it would choose the same features for every base classifier,\nleading to relaZThere\n\nare\n\nvariations\n\nmensionality\nreduction\nfor some\nclass information\nsification\n\nproblems,\n\nthey\n\non PCA\n\nthat\n\nuse\n\nlocal\n\nand/or\n\nI13, 33, 34, 47, 48, 59]. Although\nand therefore\nare better\nsuited\ndo not\n\ndirectly\n\nuse\n\nclass\n\nnonlinear\n\nprocessing\n\nthese\nmethods\nthan\nglobal\nPCA\n\ninformation.\n\nto\nimplicitly\nmethods\n\nimprove\n\ndi-\n\naccount\nfor clas-\n\ntively small correlation\nreduction.\nOne exception\nis to break an L-class problem\ninto (2 two-class problems\nz)\nand perform feature selection within each of those\nproblems\n[41]. In many real-world problems,\nthere are features\nthat are useful at distinguishing\nwhether\na pattern\nis of one particular\nclass but are not\nuseful at distinguishing\namong the remaining\nclasses.\nIn the next section\nwe\npresent input decimation,\nwhich takes advantage\nof this fact to reduce both the\ndimensionality\nand correlation\nin classifier ensembles.\n\n3\n\nInput\n\nDecimated\n\nEnsembles\n\nInput Decimation\ndecouples\nthe classifiers by exposing them to different aspects\nof the same data, ID trains L classifiers,\none corresponding\nto each class in an\nL-class problem. 2 For each classifier,\nthe method\nselects a user-determined\nnumber\nof the input features\nhaving the highest\nabsolute\ncorrelation\nto the\npresence\nor absence\nof the corresponding\nclass. 3 The objective\nis to "weed"\nout input features\nthat do not carry strong\ndiscriminating\ninformation\nfor a\nparticular\nclass, and thereby\nreduce the dimensionality\nof the feature space to\nfacilitate\nthe learning\nprocess.\nAdditionally,\nthe classifiers\'\nfeatures\nare selected\nusing different relevance\ncriteria, which leads to different feature subsets for each\nbase classifier and a reduction\nin their correlations.\nLet the training\nset take the following form:\n{(Xl, Yl),\n\n(x2, Y2),...,\n\n(Xm, Ym)},\n\nwhere m is the number\nof training\nexamples.\nEach xi has I]FI] elements\n(where\nF is the set of input features)\nrepresenting\nthe values of the input features\nin\nexample i. Each Yi represents\nthe class using a distributed\nencoding,\ni.e., it has\nL elements,\nwhere L is the number of classes, Ya - 1 if example i belongs\nto\nclass l and Yiz = 0 otherwise.\nIn this study our base classifiers\nconsist of MLPs\ntrained with the backpropagation\nalgorithm#\nGiven such a data set, and a base classifier\nlearning\nalgorithm,\ninput decimated ensembles\noperate as follows:\n\xe2\x80\xa2 For each\n\nclass l E {1,2,...,L},\n\n1. Compute\nthe absolute\nvalue of the correlation\nbetween\neach feature\nj (xlj for all patterns\ni) and the output for class I (yu for all patterns\n\ni).\n2. Select the nz features\nhaving the highest absolute correlation,\nresulting in new feature set Fl. One can either predetermine\nn_ based on\n2More generally, one trains nL classifiers where n is a positive integer.\nSNore that this method requires the problem to have at least three classes. In a two-class\nproblem, features strongly correlated with one class will be strongly anti-correlated with the\nother class, so the same features would be chosen for both classifiers.\n4In principle, any learning algorithm that estimates the a posteriori class probabilities can\nbe used.\n\npriorinformation\naboutthedataset,or learnthe value optimize\nto\nperformance.\n3. Construct\nxl\'s\n\na new training\nset by retaining\ncorresponding\nto the features\nF_ and\n\n4. Run the base classifier learning\nCall the resulting\nclassifier fz.5\nGiven\n\na new example\n\n\xe2\x80\xa2 For each\npresenting\nft.6\n\xe2\x80\xa2 Return\n\nx, we classify\n\nclass k 6 {1, 2,...,\nthe proper features\n\nthe class\n\nalgorithm\n\nonly those elements\nall the outputs.\non this\n\nof the\n\nnew training\n\nset.\n\nit as follows:\n\nL}, calculate\nf_Ve(x)\n= -_ _=1/lk(X),\nby\nFi of example x to each of the L classifiers\n\nK = argmaxkff_e(x).\n\nFundamentally,\ninput decimation\nseeks to reduce the correlations\namong\nindividual\nclassifiers\nby using different subsets of input features,\nwhile patternlevel methods\nsuch as bagging and boosting\nattempt\nto do so by choosing\ndifferent subsets of training\npatterns.\n\n4\n\nExperimental\n\nResults\n\nIn this section,\nwe present the results of input decimation\non two underwater\nsonar data sets, three Probenl/UCI\nbenchmark\ndata sets and two synthetic\ndata sets. In all results reported\nbelow, the base classifiers Consist of Multi-Layer\nPerceptrons\n(MLPs) with a single hidden layer trained with the backpropagation\nalgorithm.\nThe learning rate, momentum\nterm, and number of hidden units were\nexperimentally\ndetermined.\nIn all cases, we report test set error rates averaged\nover 20 runs, along with the differences\nin the means. 7\n\n4.1\n\nPassive\n\nSonar\n\nA real\nis that\n\nworld problem\nof classifying\n\nSignals\n\nwith all the characteristics\nshort duration\nunderwater\n\nrequired for a complete\nstudy\nsignals obtained\nfrom passive\n\nsonar signals [14]. Both biological\nand non-biological\nphenomena\nproduce such\nshort duration\nsounds, and experts can determine\nthe cause by studying\ntheir\npulse signatures\nor spectrograms.\nAutomating\nthis classification\nprocess\nis a\ndifficult\nprocess because these signals are highly non-stationary,\nhave different\nspectral\ncharacteristics\ndepending\non sources\nor propagation\npaths\nand may\n5If one is training nL classifiers for n > 1, then the algorithm calls the base classifier\nlearning algorithm n times to create n classifiers fm f_\n.., f_,_ with feature set _.\n6If we are instead training nL classifiers for n > 1, then we caJculate f_ (x) -a_e\n\nZThat is, for an error with mean # and vaziance _2, we report the # 4- _/v/K where K\nis the number of repetitions (K=20 for experiments reported here). Confidence inter_\'als of\ndesired sensitivity can be obtained directly from the differences in the meatus.\n\nhave\nsignificant verlap. moredetailed\no\nA\ndescription f thesonar ignals\no\ns\nand\nthedifficultyassociated\nwiththeirclassification be found in [24, 63].\ncan\nThe two data sets used for this experiment\nare both extracted\nfrom shortduration\npassive sonar signals due to four naturally\noccurring\noceanic sources\n(sound of ice cracking, porpoise and two different whale sounds).\nAlthough\nthere\nis some complementarity\namong the data sets, for the purposes\nof this study\nwe will treat them as different\ndata sets. s The first set, SONAR1,\nconsists of\n25 features,\nincluding\n16 Gabor wavelet coefficients, 9 signal duration\nand other\ntemporal\ndescriptors\nand spectral\nmeasurements.\nThere were 496 training\nand\n823 test patterns.\nThe second set, SONAR2,\nconsists of 24 features,\nincluding\nreflection coefficients\ncorresponding\nto the maximum\nbroadband\nenergy segment\nusing both short and long time windows, signalduration and other temporal\ndescriptors.\nThere were 564 trainingand 823 testpatters. For both data sets,\nwe used an MLP with 50 hidden units.\nTables I shows the error rates, differences\nin the mean,\nand correlation\namong the base classifiers for both the full feature set and the input decimated\nset. In this case, each base classifier had an input decimated\nset of 22 features\nfor\nboth SONAR1\nand SONAR2\nafter features\nwith little correlation\nto each output were deleted.\nRetaining\nmore features\ndid not result in a significant\ndrop\nin correlations,\nwhereas removing\nmore features resulted\nin drops in individual\nclassifier performance\nthat were too large to be compensated\nby combining.\nfact, this data set is not particularly\nwell-suited\nfor input decimation\nbecause\nhas a stool] number\nof carefully-extracted,\nrelevant\nfeatures.\n\nTable\n\nSONAR1\n\nSONAR2\n\nI: Ensemble\n\nI1\n4\n8\n1\n4\n8\n\nPerformance\n\non both\n\nsonar\n\nIn\nit\n\ndata.\n\nFull Feature\nSet\nError Rate I d\n7.47 4- .10\n7.05 + .07\n.89\n\ninput\nDecimation\nError Rate\n8138 4- .15\n7.10 4- .07\n.68\n\n7.17 + .05\n9.95 4- .16\n\n6.99 4- .06\n9.73 4- 1\'16\n\n9.26 4- .15\n8.94 4- .11\n\n.76\n\n8.80 4- .06\n8.62 4- .06\n\n.72\n\nFor SONAR1,\nthe deletion of even lowly-correlated\ninputs affects the performance of the base classifier significantly.\nHowever, due to the correspondingly\nlarge reduction\nin the error correlation,\ninput decimated\nensembles\nperform\nat\nthe level of the full feature set, with IDE for N = 8 providing\na statistically\nsignificant\ngain over the full feature\nset ensemble\nand IDE for N = 4 at the\n= .05 level. For SONAR2,\nthe gains are more significant\nin that even the\ninput decimated\nbase classifier\nimproves\nslightly\nupon the full featured\nbase\nclassifier,\nallowing for sizable gains by the input decimated\nensemble.\nThis is\nSSee [63] for a study where the two data sets were used in conjuc_ion.\n_Gabor wavelet coeffi, .;ents provide a muItiscale representation that does not assume signal\nc\nstationarity [12].\n\nachieved spiteof the relatively odest ropin the errorcorrelation mong\nin\nm\nd\na\nthebase\nclassifiers. note that for SONAR1, because the correlation is high\nAlso,\nfor the base classifiers\ntrained on the full feature set, increasing\nthe number of\nclassifiers from 4 to 8 does not provide any gains (instead it provides statistically\nequivalent\nerrors).\n\n4.2\n\nProbenl/UCI\n\nBenchmarks\n\nIn the SONAR data presented\nabove each feature carried a significant\namount\nof discriminating\ninformation.\nIn fact, because\neach feature was carefully\nextracted\nfrom the raw data, one should not have expected\nmuch improvement\nthrough\ninput decimation.\nIn this section we perform\na more detailed\nanalysis\non three benchmark\ndata sets where we gradually\ndecrease\nthe dimensionality\nuntil we end up with 5-10% of the original features.\nOn these benchmark\nsets,\nwe expect this more extreme\ncase of input decimation\nto expose the strengths\nand weaknesses\nof this method.\nThe three data sets from the UCI/PROBEN1\nbenchmarks\n[6, 54 l selected for\nthis study were: The Gene dataset from the PROBEN1\n(i.e., using train/test\nsplit from PROBEN1),\nand the Splice junction\ngene sequences\nand Satellite\nImage datasets\n(Statlog\nversion) from the UCI Machine\nLearning\nRepository.\nThe Gene data set has 120 input features\nand three classes [46, 54]. The MLP\nhas a single hidden\nlayer of 20 units, a learning\nrate of 0.2 and a momentum\nterm of 0.8. The Splice data consists of 60 input features\nand three classes [6].\nHere we selected an MLP with a single hidden layer composed\nof 120 units, a\nlearning rate of 0.05, and a momentum\nterm of 0.1. The Satellite\nImage data\nset has 36 input features and 6 classes [6]. We selected\nan MLP with a single\nhidden layer of 50 units, and a learning rate and momentum\nterm of 0.5. The\nensembles\nconsisted\nof three classifiers for Gene and Splice and six classifiers\nfor\nSatellite Image--the\nsame as the number of classes.\nFigures 3-5 show the classification\nperformance\nand classifier correlations\nfor\nall three data sets, averaged\nover 20 runs. For clarity we omit the error bars,\nsince they ranged from 0.05 to 0.25% and as such were smaller than the symbols\nrepresenting\nthe data points.\nThe rightmost\npoint in each graph (e.g., the point\ncorresponding\nto 120 features for the Gene data set) shows the full feature\nset\nperformance.\nFor the Gene data, the full feature ensemble\nis significantly\nmore\naccurate\nthan the single classifier, while for the Satellite\nImage and Splice data\nsets, the ensemble is only marginally\nmore accurate.\nIn case of the Gene data, the average ensembles\nwith 20, 30, and 40 inputs are significantly\nmore accurate\nthan both the original\nnetwork ensembles\ndescribed\nin the previous\nsection and their PCA counterparts.\nWith IDE, the\nperformance\nof the ensemble goes up as the number of features\nincreases\nuntil\naI1 the relevant features are included and then starts declining\nwith the addition\nof irrelevant\nfeatures.\nThe average correlation\nbehaves the same way. For 10 or\nfewer features, we expect the average correlation\nto be low because different sets\nof 10 features have the highest relevance to each class. As the number of features\nincreases\nup to 30, the base classifiers have an increasing\nnumber\nof common\n\n10\n\nClassification\n\nPerformance\n\nfor Gene\n\nData\n\n95\n90\n85\nCD\nO\n\n80\n\no\n\n75\n\nr-\n\n7O\n,,"\n\n65\n\n_a_-_"\n_\n\nQ_\n\n,,"\n,,\'" j\n,,_" /_\n\n60\n55\n\n" _\n\n50\n\n/\n\nID Ensemble\nPCA Ensemble\nRandom\nEnsemble\nID Single\nPCA Single\n\n,\n\n,\n\nRandom\n\n20\n\n0\n\n,\n\n40\n\n60\n\n80\n\nNumber\nClassifier\n\n..... \xe2\x80\xa2 .....\n.... o ....\n..... A .....\n--E----e---\n\nSingle\n\n---A----\n\n100\n\n120\n\nof Features\n\nCorrelations\n\nfor Gene\n\nData\n\n0.8\n\xc2\xa2-\n\n.o\n\nI\n\n,,I,-,\n\nO\n\no\n....... A-...... A .....\n\n0.2\n\nID _\nPCA .... _ ....\nRandom\n......_ ......\n\n/\n\n...\n\xe2\x80\xa2\'\n0\n\n20\n\n40\n\n60\n\nNumber\nFigure\n\n3: Performance\n\n(a)\n\nand\n\nCorrelations\n\nii\n\n80\n\n100\n\n120\n\nof Features\n(b)\n\nfor\n\nthe\n\nGene\n\ndata\n\nset.\n\nClassification\n95\n\nI\n\nPerformance\n(\n\n,m ............\n\nfor Splice\n\nu\n\nData\n\n_\n\n(\n\nI\n\ntN ......\n\n90\n85\n\xc2\xa23\n\n8O\no\n\nO\n\n75\n\n\xc2\xa2(3\n\n70\n\n13.\n\n_A\n,,,"\n\n65\n60\n\n,\n\n,\n\n,\n\n10\n\n0\n\n20\n\n30\n\nNumber\nClassifier\n\n..... \xe2\x80\xa2 .....\n..... \xe2\x80\xa2 .....\n\nPCA Single\nID\nRandom\nSingle\n\n,_\n\n55\n\nEnsemble\nEnsemble\n\nRandom\n\n40\n\n---e--I\n\n50\n\n60\n\nof Features\n\nCorrelations\n\nfor Splice\n\nData\n\no,\n0.8\n\nG\n\n",,\n\n....\n\n_F-I\n\n............\n\nt-q\n\n........\n\nc-\n\n._o\n._,...\nO\n\nO\n\n0.2\n\nID\nPCA\nRandom\ni\n\n0\n\n10\n\ni\n\ni\n\n20\n\n30\n\nNumber\nFigure\n\n4: Performance\n\n40\n\nr\n\ni\n\n50\n\n60\n\nof Features\n\n(a) and Correlations\n\n12\n\nr\n\n+\n.... -e- ....\n......z_......\n\n(b) for the Splice\n\ndata\n\nset.\n\nClassification\n\nPerformance\n\nfor Satellite\n\nData\n\n9O\n85\nO\n\n8O\n\n},...\n\nO\n\no\n\xc2\xa3\n\n75\nr-/"\n/_\n\n70\n\nO_\n\nID Ensemble\nPCA Ensemble\nRandom\nEnsemble\nID Single\nPCA Single\n\n/\n/\n\n65\n,\n\n,\n\n.,\n\n5\n\n10\n\n15\n\n60\n0\n\n, Random\n\nNumber\nClassifier\n\n20\n\n25\n\nSingle\n30\n\n4(\n\n35\n\nof Features\n\nCorrelations\n\n.:::.:::: -::--.......... --::\n\n.... \xe2\x80\xa2 .....\n..... \xe2\x80\xa2 .....\n..... \xe2\x80\xa2 .....\n---EF----e--\n\nfor Satellite\n\nData\n\n_-i-.-_-i-._\n.............................\n\n0.8\nc"\n\n.o\n\n0.6\n\no\nO\n\n0.4\n\n0.2\n\nID\nPCA\nRandom\n,\n\n0\n\nI\n\nl\n\nI\n\n5\n\n10\n\n1\n\n15\nNumber\n\nFigure\n\n5:\n\nPerformance\n\n(a)\n\nand\n\nCorrelations\n\n13\n\nI\n\n20\n\n.... -e- ....\n......_,.....\n\nI\n\n25\n\n30\n\nI\n\n35\n\n4O\n\nof Features\n(b)\n\nfor\n\nthe\n\nSatellite\n\ndata\n\nset.\n\nfeatures.\nAt 30, the base classifiers have virtually\nall common\nfeatures--the\nthat are relevant\nto all three classes; therefore,\nwe would expect maximum\n\n30\nav-\n\nerage correlation.\nBeyond 30, each base classifier is getting (probably\ndifferent)\nirrelevant\nfeatures,\nleading to a reduction\nin correlation.\nWith PCA, the performance of the ensemble is relatively stable and inferior to 1-DE. This is consistent\nwith the fact that principal\ncomponents\nare not necessarily\ngood discriminative\nfeatures,\nand adding principal\ncomponents\nbeyond\nthe first few would likely\nhave little effect on the classification\nperformance.\nThe performance\nof the ensemble with random feature subsets increases\nin random\nincrements\nwith the\naddition\nof features depending\non how relevant they are. On this dataset,\nthe\nperformance\nof random\nfeature ensembles\nwas uncompetitive\nbecause\nrandom\nselection\nnever yielded good feature subsets.\nIn the Splice data experiments,\nall the decimated\nfeature-based\nensembles\nsignificantly\noutperformed\nboth the original ensemble\nand the PCA-based\nensembles.\nRandom\nfeature-based\nensembles\nperformed\nsomewhat\nbetter\nhere\nthan in the Gene data set.\nWith 40 and more features,\nit was competitive\nto input decimation.\nHowever, the best performing\npredictor\noverall is clearly\nthe input-decimated\nensemble\nwith 10 inputs per classifier.\nWhat is particularly notable\nin this case is that a reduction\nof dimensionality\nbased on PCA\nhas a strong negative\nimpact on the classification\nperformance.\nWith 20 principal components\nfor example,\nthe performance\nof the single classifiers\ndrops\nby 7% relative\nto the single classifier with all the input features,\nwhereas\nthe\nperformance\nof the ID single classifier increases\nby 3%. The improvement\nof\nthe performance\nof the single classifiers\ndue to decimation\nis an initially\nsurprising aspect of these experiments\nsince one may not expect to find too many\n"irrelevant"\nfeatures\nin these real data sets. However,\nan analysis\nshows that\nthe inputs that were decimated\nwere in fact providing\n"noise" to the classifier.\nAlthough\nit is theoretically\ntrue that the classifier with more information\nwill\ndo at least as well as the classifier with less information,\nin practice\nwith only a\nlimited amount\nof data, extracting\nthe correct information\ncan cause a problem\nfor such classifiers\ncausing them to perform\nworse than their counterparts\nwith\nless information.\nOn the Satellite\nImage data however,\nthe input decimated\nensemble\nwith\n27 features\nwas the only one that did not perform significantly\nworse than the\nsingle classifier\nand the original\nensemble.\nBoth the PCA and random\nfeature ensembles\noutperformed\nIDE. Because the single IDE classifiers\nperformed\nmuch worse than the PCA and random feature single classifiers,\nwe examined\nthe features that were chosen in each ensemble.\nFigure 6 shows the average correlations\namong the features chosen for the base classifiers in the three types of\nensembles.\nThe features\nthat IDE chose have a much higher correlation\namong\nthemselves\nrelative to random\nand PCA ensembles,\nespecially\nfor smaller numbers of inputs.\nThis means that IDE often chooses several features\nwith high\ncorrelations\nto the class without\nrealizing that they may be redundant.\nRandom feature\nselection\ndoes not fall into this trap since it does not consider\ncorrelations\nat all. PCA\'s correlations\nare the lowest because it creates features\nspecifically\n\ndesigned\n\nto have low correlations\n\n14\n\namong\n\neach\n\nother.\n\nAmong\n\nthe\n\nAverage\n\nFeature\n\nCorrelations\n\nfor Satellite\n\nData\n\n0.8\n\n.o\n\nO\nO\n\n0.2\n\nIDE\no ....\n\nPCA\n"G--........... ,...... _._. ............. Random\n\n0\n0\n\n5\n\n10\n\n15\n\n20\n\nNumber\nFigure\n\nthree\n\n6: Average\n\nProbenl/UCI\n\nshows\n\ntwo\n\ncan\n\nthey\n\nneed\n\nhas\n\nwas\n\nand\n\nto be included\n\nif there\n\nwith\n\nare\nare\n\nfeature\n\nobserved\n\none\n\nadvantage\n\nas there\n\n(ii)\n\nin the\n\nWe\n\nis the\n\n25\n\nlikely\n\nfeatures\nset\n\nin Satellite\n\nthe\n\nof the sum\n\nlowest\n\nof input\nto\n\nhave\n\nregardless\n\ninput\nthe\n\ndecimation\nclasses\n\n4.3\n\nand\n\nis to\n\ncorrelation\n\nthat\n\nin Figure\n\n"wild\n\nthem\n\nin each\n\neigenvector was\n\ncard"\n\nfeatures\n\ndecimated\n\ndiscriminative features\n\nA potential\nbased\n\non\n\nimprovement\ncorrelation\n\n\xe2\x80\xa2 Set\n\nfollowing\n\nwith\n\nto\nall\n\nsubset.\n\nData\n\nerties of input decimated\nthe\n\nto the\n\nof four features in the\n\nIn this section we construct synthetic data sets to enable us to study\n\nuse\n\nfeatures\nmeaning,\n\nof the four spectra] values across all the pixels. In\n\nselect\n\nincJude\n\nSynthetic\n\nirrelevant\n\ninitial\n\nthat the highest eigenva]ue\n\n2(a)).\n\nlike\n\nand\n\nthe\n\nto spectra] values for a given pixel. In ex-\n\nprovide good\n\n"looks"\n\nData.\n\nsignificant\n\nof their\n\n(i.e.,\n\ndata\n\n40\n\ndimensionality,\n\nbe more\n\nthis case, the higher principal components\nthe\n\nImage\n\nof the eigenva]ues, and the corresponding\n\na simple linear combination\n\n35\n\ndecimation,\n\nthat\n\nthe eigen%zlues and eigenvectors, we found\n\n91.6%\n\n30\n\nthat consecutive groups\n\ndata set correspond\n\n......_......\n\nof Features\n\nof Features\n\nto take\n\nto be high,\n\nbe removed;\n\nsatelliteimage\n\nthis\n\n(i) in order\n\nparticular output.\n\namining\n\nCorrelation\n\ndatasets,\n\nthings:\n\ndimensionality\nthat\n\n.... _ ....\n\ntwo\n\nensembles\n\nsynthetic\n\ndata\n\nin a systematic\nsets:\n\nA:\n\n15\n\nmanner.\n\nthe prop-\n\nTo that end\n\nwe\n\n- Three\n\nclasses-one\n\nunimodal\n\n-\n\n300 training\npatterns\npatterns\nper class.\n\n-\n\n100 features\n\nand\n\nGaussian\n\nper class.\n\nper pattern\n\n150 test patterns-100\nwhere\n\nthere\n\ntraining\n\nand 50 test\n\nare:\n\n* 10 relevant\nfeatures per class.\nPatterns\nthat belong\nare generated\nfrom a multivariate\nnormal distribution\n\nto a class\nin 10 in-\n\ndependent\ndimensions\ndistributed\nas N(40, 5_). There are no dimensions\nin common\namong the three classes.\nTherefore,\nthere\nare 30 relevant\nfeatures.\nFor patterns\nin each class, the 20 features that are relevant to the other two classes are distributed\nas\nU[-100,100].\n1\xc2\xb0\n* 70 irrelevant\n\nfeatures-distributed\n\nas U[-100,\n\n100].\n\nSet B: Same as Set A, except that there is overlap among the relevant\nfeatures\nfor each class. That is, each class has three relevant\nfeatures\nin\ncommon with every other class, but there are no features\nthat are relevant\nto all three classes.\nIn data\n\nset A there\n\nis an abundance\n\nof features\n\nthat\n\nare irrelevant\n\nfor the\n\nclassification\ntask.\nThis data set was chosen to represent\nlarge data mining\nproblems\nwhere the algorithms\nmay get swamped\nby irrelevant\ndata.\nIn data\nset B the overlap\namong features relevant\nto each class provides\na more difficult problem\nwhere the base classifiers\nare now forced to select some common\nfeatures,\nreducing\nthe potential\nfor correlation\nreduction.\n4.3.1\nFigure\n\nSynthetic\n7 presents\n\nSet\n\nA\n\nthe classification\n\naccuracies\n\nand base classifier\n\ncorrelations\n\non\n\nSynthetic\ndataset\nA as a function\nof the number\nof inputs\n(which are either\nthe number of selected principal\ncomponents\nor the number\nof features selected\nfor each base classifier through\ninput decimation).\nThe original single classifier\nand original ensemble use all the input features. 11 The points for the maximum\nnumber\nof features\n(e.g., 100 features\nin this data set), always represent\nthe\nperformance\nof the original classifier/ensemble.\nAn important\nobservation\nthat is apparent\nfrom these results is that neither\nPCA ensembles nor PCA base classifiers are particularly\nsensitive to the number\nof inputs.\nThe correlations\namong the base classifiers\nreinforce\nthis conclusion.\nFewer input features in PCA means the base classifiers\nare more correlated\nsince\nthey all share the same principal\nfeatures.\nNote however,\nthat input\nbase classifiers\nhave low correlation\nfor small numbers\nof features,\ncorrelation\nup to 30 features,\nand decreasing\ncorrelation\nafter that.\n\ndecimated\nincreasing\nThe base\n\nl\xc2\xb0Cleazly, because of this, all 30 features have some relevance to all three classes; however,\nthe 10 features used to generate patterns belonging to each class ave clearly substantially more\nrelevant than the other 20 features.\n11The base classifier used was an MLP with a single hidden layer consisting of 95 units,\ntrained using a learning rate of 0.2 and a momentum term of 0.5.\n\n16\n\nClassifier\n\nPerformance\n\nfor Synthetic\n\nData A\n\n100\n95\n\nm--m...... _ m .....\nmm\n...... m\n_--o ...... _\n\n85\n\no\n\nm\n\nn...\n\n_:-:-ll_-:.-_\n\n9O\n\n-_m"\n...... l...\nA\n_\nt_2:-=_1\n\n--\n\n8O\nO\n\nO\n\n75\n\nc-\n\n7O\n_D\n\n65\nQ.\n\nEnsemble\n\n....... "\n\n6O\n\n._\n\n,,"\n,_\' _\n/\'_\n\n55\n5O\n\nPCA\nRandom\n\n_\n\n,\n\n,\n\n2O\n\n40\n\n45\n0\n\n,\n\nl_nsemble\nEnsemble\nID Single\nPCA Single\n\n..... \xe2\x80\xa2 .....\n..... \xe2\x80\xa2 .....\n----EF----e---\n\nRandom\n\nSingle\n\n6O\n\nNumber\nClassifier\n\n..... \xe2\x80\xa2 .....\n\n_,\n\n8O\n\n,,\n100\n\nof Features\n\nCorrelations\n\nfor Synthetic\n\nData A\n\n0.8\n(-\n\n.o\n\n/\n\nc_\n\nO\n\nO\n\n.z5 ........ _x\n.\'"\n\n0.2\n\nID\nPCA\nRandom\n\n/k\'\n\n0\n\nI\n\n0\n\nq\n\nL\n\n7: Data\n\n40\n\n60\n\nNumber\nFigure\n\n20\n\nof Features\n\nset A Performances\n\n17\n\nand\n\n-----D--.... -e- ....\n......_ ......\n\n80\n\nCorrelations\n\n100\n\nclassifiers\'\naverageerformance\np\nfollows similarpattern. Interestingly,\na\ninput\ndecimated\nensembles\narenot adversely\naffected y thepoorperformance\nb\nofthe\nbase\nclassifiers input decimated\n(e.g.,\nensembles 5 features utperformed\nwith\no\ninputdecimated\nensembles\nwith50features\nwhilebase\nclassifiers\nwith5 features\ngave\nsignificantly orse\nw\nresults\nthanbase\nclassifiers 50features).\nwith\nIn cases here\nw\nmorethan30features ereused,the performance the\nw\nof\nensemble\ndeclined iththeaddition\nw\nofadditional\nfeatures,\ni.e.,asmore\nandmore\nirrelevant\nfeatures ereincluded.\nw\nHowever,\nallthe input decimation\nensembles\nprovidedtatistically ignificant\ns\ns\nimprovements theoriginal nsembles,\nover\ne\nPCA\nensembles,\nandrandom-feature\nensembles.\nThesingledecimated\nclassifiers 20 and more features outperformed\nwith\nthe original\nsingle classifier.\nThis perhaps\nsurprising\nresult (as one might have\nexpected\nonly the ensemble\nperformance\nto improve when using subsets of the\nfeatures)\nis mainly due to the simplification\nof the learning\ntasks, which allows\nthe classifiers\nto learn the mapping\nmore efficiently.\nInterestingly,\nthe average correlation\namong classifiers\ndoes not decrease until a very small number\nof features remain.\nWe attribute\nthis to the removal\nof noise---removing\nthe base classifiers.\nremoved\nrelevant\nbecause\nfeatures.\n\nnoise increases the amount\nof information\nIndeed,\nthe correlation\nincreases\nsteadily\n\nshared\nbetween\nas features\nare\n\nuntil we reach 30 features (which corresponds\nto the actual number\nof\nfeatures).\nAfter that point, removing\nfeatures\nreduces\nthe correlation\nthe base classifiers\'\nfeature sets have a decreasing\nnumber\nof common\nThe base classifiers\'\nperformances\nalso decline; however,\nthe ensemble\n\nperformance\nstill remains high. This experiment\nclearly shows a typical trade-off\nin ensemble\nlearning:\none can either increase individual\nclassifier performance\n(as for input decimation\nwith more than 30 features)\nor reduce the correlation\namong classifiers\n(as for input decimation\nwith less than 20 features)\nto improve\nensemble\n4.3.2\n\nperformance.\nSynthetic\n\nSet\n\nB\n\nFigure 8 presents\nthe results for the second synthetic\ndata set, which is similar\nto the first data set except that there is overlap among the relevant features\nfor\nthe classes. 12 Because of this overlap, this feature set has fewer total relevant\nfeatures\n\nand\n\nthus\n\nit constitutes\n\na more\n\ndifficult\n\nproblem\n\n(as indicated\n\nby com-\n\nparing the results on the full feature base classifiers\nand ensembles\non this data\nset to the previous\none).\nNote that the correlations\nin this data set remained\nfairly constant\nacross\nthe board for IDE and PCA-based\nensembles.\nInput decimation\ndid not reduce\nthe correlations\ndramatically\nfor small feature sets in dataset\nB the way it did\nin case of dataset\nA. This is mainly caused by the "coupling"\namong the base\nclassifiers due to their common\nIn spite of these difficulties,\n\ninput\ninput\n\nwell.\n\noutperform\n\nIndeed,\n\nthey\n\nsignificantly\n\nfeatures.\ndecimation\n\nensembles\n\nthe original\n\nperform\n\nensemble,\n\nextremely\nPCA\n\nensem-\n\n12The single classifier used was an MLP with a single hidden layer consisting of 95 units,\ntraSned using a learning rate of 0.2 and a momentum term of 0.5.\n\n18\n\nClassifier\n\nPerformance\n\nfor Synthetic\n\nData\n\nB\n\n100\n!1..\n"I ...... i ...... I\n\n90\no\n\n.......\n\nI\n\n--\n\n80\n\nO\n\nG)\n\n70\n\nt-c.)\nco\n\n60\n\n..._\'"\n,_-"\n\n50\n\n/\'\n_y_\n\n_\n\nPCA\n\n_\n\nEnsemble\n\nEnsemble\nID Single\nPCA Single\nRandom\nSingle\n\n_\n,\n\n,\n\n20\n\n40\n\n60\n\nNumber\n\n..... \xe2\x80\xa2 .....\n----e--\n\nof Features\n\n4O\n0\n\n..... \xe2\x80\xa2 .....\n\nRandom\n\nClassifier\n\nCorrelations\n\n80\n\nfor Synthetic\n\n1 O0\n\nData\n\nB\n\no,\n\n0.8\n\n"\'9 ---_\nc-\n\n._o\n.\n\n-\'_X "\'\xc2\xb0\'"\n\n/x,......... zh.....\n\nO\n\n(3\n./x ........ ,2_\'\n\n0.2\n\nID\nPCA\nRandom\n\nZ_\n\n0\n\nt\n\n0\n\nt\n\nf\n\nt\n\n8: Data\n\n40\n\nset\n\n60\n\nNumber\nFigure\n\n20\n\nof Features\n\nB Performances\n\n19\n\nand\n\n.... -e- ....\n......_ .....\n\n8O\n\nCorrela,tions\n\n100\n\nbles,\n\nand\n\nrandom-feature\n\nensembles\n\non all but\n\na few subsets\n\nwhere\n\nthey\n\nonly\n\nprovide marginal\nimprovements.\nFurthermore\nthe input-decimated\nsingle classitiers also outperform\ntheir original and PCA counterparts\nfor all but the 60 and\n70 feature subsets.\nThis is particularly\nheartening\nsince this feature set is a more\nrepresentative\nabstraction\nof real data sets (data sets with "clean" separation\namong classes are quite rare). This experiment\ndemonstrates\nthat when there\nis overlap among classes, class information\nbecomes particularly\nrelevant.\nPCA\nand random feature selection\noperate without\nthis vital information,\ntherefore\nthey are unlikely to provide competitive\nperformance.\n\n5\n\nDiscussion\n\nThis paper discusses input decimation,\na dimensionality\nreduction-based\nensemble method\nthat provides good generalization\nperformance\nby reducing\nthe correlations among the classifiers in the ensemble.\nThrough\ncontrolled experiments,\nwe show that the input decimated\nsingle classifiers\noften outperform\nthe single\noriginal classifiers\n(trained\non the full feature set), demonstrating\nthat simply\neliminating\nirrelevant\nfeatures\ncan improve\nperformance.\nIn addition,\neliminating irrelevant\nfeatures\nin each of many classifiers\nusing different\nrelevance\ncriteria\n(in this case, relevance\nwith respect to different\nclasses) yields significant improvement\nin ensemble performance\nthrough\ncorrelation\nreduction,\nas\nseen by comparing\nour decimated\nensembles to the original ensembles.\nSelecting\nthe features\nusing class label information\nalso provides\nsignificant\nperformance\ngains over PCA-based\nensembles and ran_om feature subset selection.\nThrough\nour tests on synthetic\nand real data sets, we examined\nthe characteristics\nthat data sets need to have to fully benefit\nWe observed\nthat input decimation\nyields the greatest\n\nfrom input decimation.\nimprovements\nover the\n\noriginal ensemble when (i) there are a large number of features\n(i.e., where it is\nlikely that there will be irrelevant features);\nand (ii) when the number of training\nexamples is small relative\nto the input dimensionality\n(i.e., where it is difficult\nto properly\nlearn all the parameters\nin a classifier based on the full feature set).\nIn both cases, by removing\nthe extraneous\nfeatures,\ninput decimation\nreduces\nnoise and thereby reduces the number of training\nexamples needed to produce\na\nmeaningful\nmodel (i.e., alleviating\nthe curse of dimensionality).\nOur synthetic\ndata sets were generated\nusing multivariate\ndistributions\nwhere the feature values were generated\nindependently.\nWe plan to generate\nsynthetic\ndata sets with\ndependencies\namong the features\nto see how they affect our method.\nOur experiments\nwith real datasets--especially\nthe Satellite\nImage dataset--showed\nthat input decimation\nmay benefit by keeping out redundant\nfeatures\nand including those features\nthat have a high correlation\nwith all classes on average\neven though they do not have high correlation\nwith any one class. We plan to\ninvestigate\nvarious possible methods of doing this.\nNote that input decimation\nshares the central aim of generating\na diverse\npool of classifiers\nfor the ensemble with many methods\nsuch as bagging.\nHowever, by focusing\non the input features\nrather than the input patterns,\ninput\n\n2O\n\ndecimation focuseson a different\n"axis"of correlation\nreductionthan does bagging. Consequently,\ninput decimation\nis orthogonal\nto bagging,\nand one can use\ninput decimation\nin conjunction\nwith bagging.\nWe plan to experiment\nwith this\nin the future.\nA final observation\nis that input decimation\nworks well in spite of our rather\ncrude method\nof feature selection\n(i.e., using statistical\ncorrelation\nof each feature individually\nwith each class). One reason why this simple method\nsucceeds\nis that we have greatly simplified\nthe relevance\ncriterion:\nunlike other feature\nselection\nmethods\nthat consider the discriminatory\nability across all classes, we\nonly consider the relevance of the features to a single class. This typically\ncauses\neach classifier in the ensemble\nto get a different\nsubset of features,\nleading\nto\nthe superior performance\nwe have demonstrated.\nNevertheless,\nwe are currently\nextending\nt.his work in four directions:\nconsidering\ncross-correlations\namong the\nfeatures;\ninvestigating\nmutual information-based\nrelevance\ncriteria;\nincorporating global relevance into the selection\nprocess; and selecting\na different\nnumber\nof features for each classifier.\n\nReferences\n[1] K. M. AIi and M. J. Pazzani.\nOn the link between\nerror correlation\nand\nerror reduction\nin decision\ntree ensembles.\nTechnical\nReport\n95-38, Department\nof Information\nand Computer\nScience,\nUniversity\nof California,\nIrvine, 1995.\n[2] F.M. Alkoot and J. Kittler.\nImproving\nsifters. In J. Kittler and F. Roll, editors,\ntional Workshop\non Multiple\nClassifier\nBerlin, 2001.\n[3] 1%. Battiti.\nUsing mutual\nneural net learning.\nIEEE\nJuly 1994.\n[4] J. O. Berger.\nEd.), Springer,\n\ninformation\nTransactions\n\nStatistical\nDecision\nNew York, 1985.\n\n[5] C. M. Bishop. Neural Networks\nPress, New York, 1995.\n\nproduct\nby moderating\nk-NN cIasProceedings\nof the Second InternaSystems,\npages 429-439.\nSpringer,\n\nfor selecting\no_ Neural\n\nTheory\n\nfor Pattern\n\nand\n\nBayesian\n\nRecognition.\n\n[6] C.\nBlake,\nE.\nKeogh,\nand\nC.J.\nMerz.\nitory\nof\nmachine\nlearning\ndatabases,\nhttp://www.ics.uci.edu/_mlearn/MLRepository.html).\nI7] A. Blum and P. Langley.\nmachine learning.\nArtificial\n\nfeatures\nNetworks,\n\nAnalysis.\n\nOxford\n\n(2nd\n\nUniversity\n\nUCI\n1998.\n\nSelection\nof relevant\nfeatures\nand\nIntelligence,\n97:245-272,\n1997.\n\n21\n\nin supervised\n5:4:537-550,\n\nrepos(URL:\n\nexamples\n\nin\n\n18]K.\n\nD. Bollacker\n\ninformation.\nRecognition,\n[9] L. Breiman.\n[I0]\n\nand J. Ghosh.\n\nLinear\n\nfeature\n\nextractors\n\nbased\n\non mutuM\n\nIn Proceedings\nof the lYth International\npages pp. IV:720-724,\n1996.\n\nConference\n\nBagging\n\n24(2):123-140,\n\npredictors.\n\nMachine\n\nLearning,\n\non Pattern\n\n1996.\n\nK. J. Cherkauer.\nHuman expert-level\nperformance\non a scientific\nimage\nanalysis\ntask by a system using combined\nartificial\nneural networks.\nIn\nWorking\nNotes of the AAAI\nWorkshop\non Integrating\nMultiple\nLearned\nModels, pages 15-21, 1996.\n\n[11] S. Cohen and N. Intrator.\nAutomatic\nmodel selection\nin a hybrid\nperceptron/radial\nnetwork.\nIn J. Ki\xc2\xa2ller and F. Roll, editors,\nProceedings\nof\nthe Second International\nWorkshop\non Multiple\nClassifier\nSystems,\npages\n440-454.\nSpringer,\nBerlin, 2001.\n[12] J.M. Combes,\nTime-Frequency\n\nA. Grossman,\nand Ph. Tchamitchian\n(Eds.).\nWavelets:\nMethods and Phase Space. Springer-Verlag,\n1989.\n\n[13] D. de Ridder and R. P. W. Duin. Sammon\'s\nmapping\nusing neural networks:\nA comparison.\nPattern\nRecognition\nLetters, 18:1307-1316,\n1997.\n[14] L. Deuser\nsignals:\nAmerica,\n\nand D. Middleton.\n\nOn the\n\nAn environmentally\n65:438-443,\n1979.\n\nadaptive\n\n[15] P.A. Devijver\nPrentice-Hall,\n\nand J. Kittler.\n1982.\n\nPattern\n\nclassification\napproach.\n\nRecognition:\n\n[16 t T.G. Dietterich.\nMachine\nlearning\nMagazine,\n18(4):97-136,\n1998.\n\nresearch:\n\nof underwater\nThe\n\nFour\n\nAcoustic\n\nA Statistical\n\ncurrent\n\nacoustic\nSociety\n\nof\n\nApproach.\n\ndirections.\n\nAI\n\n[17] T.G. Dietterich.\nEnsemble\nmethods\nin machine\nlearning.\nIn J. Kittler\nand F. Roll, editors,\nProceedings\nof the First International\nWorkshop\non\nMultiple\nClassifier Systems,\npages 1-15. Springer,\nBerlin, 2000.\n/18] T.G. Dietterich\nand G. Bakiri.\nSolving multiclass\nerror-correcting\noutput\ncodes. Journal\nof Artificial\n2:263-286,\n1995.\n[19] P. Domingos.\nContext-sensitive\nIntelligence\nReview, 11:227-253,\n\nfeature\n1997.\n\nselection\n\n[20] R. O. Duda, P. E. Hart, and D. G. Stork.\nNew York, NY, second edition, 2001.\n\nlearning\nproblems\nvia\nIntelligence\nResearch,\n\nfor lazy learners.\n\nPattern\n\nClassification.\n\nArtificial\n\nWiley,\n\n[21J R.P.W. Duin and D.M.J. Tax. Experiments\nwith classifier combining\nrules.\nIn J. Kittler\nand F. Roli, editors,\nProceedings\nof the First International\nWorkshop\non Multiple\nClassifier Systems, pages 16-29. Springer,\nBerlin,\n2000.\n\n22\n\n[22]Y. Freund and R..Schapire.Experiments with a new boosting Mgorithm. In\nProceedings\nof the Thirteenth\nInternational\nConference\non Machine\nLearning, pages 148-156. Morgan Kaufmann,\n1996.\n[23] K. Fukunaga.\nPress, second\n\nIntroduction\nedition, 1990.\n\nto Statistical\n\n[24] J. Ghosh, L. Deuser, and S. Beck.\nfor detection,\ncharacterization\nand\nsignals.\n\nIEEE\n\nJournal\n\nPattern\n\nRecognition.\n\nAcademic\n\nA neural\nnetwork based hybrid\ncIassif_cation\nof short-duration\n\nof Ocean Engineerin9,\n\n17(4):351-363,\n\nsystem\noceanic\n\nOctober\n\n1992.\n\n[25] T. K. Ho. The random space method\nfor constructing\ndecision forests. IEEE\nTransactions\non Pattern\nAnalysis and Machine\nIntelligence,\n20(8):832-844,\n1998.\n[26] T. K. Ho, J. J. Hull, and S. N. Srihari.\nclassifier systems.\nIEEE\nTransactions\nIntelligence,\n16(1):66-76,\n1994.\n[27] T.K.\n\nHo. Data\n\ncomplexity\n\nanalysis\n\nDecision\ncombination\non Pattern\nAnalysis\n\nfor classifier\n\ncombination,\n\nand F. Roll, editors, Proceedings\nof the Second International\nMultiple Classifier Systems,\npages 53-67. Springer,\nBerlin,\n[28] A. Hyvarinen.\nputing Surveys,\nf29] Robert\nNeural\n\nSurvey on independent\n2:94-128,\n1999.\n\ncomponent\n\nJacobs.\nMethod\nfor combining\nexperts\'\nComputation,\n7(5):867-888,\n1995.\n\nin multiple\nand Machine\n\nanalysis.\n\ntn J. Kittler\nWorkshop\n2001.\nNeural\n\nprobability\n\non\n\nCom-\n\nassessments.\n\n[301 G. H. John, R. Kohavi,\nand K. Pfleger.\nIrrelevant\nfeatures\nand the subset selection\nproblem.\nIn Proceedings\nof the International\nConference\non\nMachine\nLearning\n(ICML-9$),\npages 121-=129, July 1994.\n[3I]\n\nLT.\n\nJolIiffe.\n\nPrincipal\n\nComponent\n\nAnalysis.\n\nSpringer-Verlag,\n\nI986.\n\n[32] T.M. Jorgensen\nand C. Linneberg.\nFeature\nweighted ensemble classifiers - a\nmodified decision scheme. In J. Kittler\nand F. ltoli, editors,\nProceedings\nof\nthe Second International\nWorkshop\non Multiple\nClassifier\nSystems,\npages\n218-227. Springer,\nBerlin, 2001.\n[33] N. Kambhatla\nJ. D. Cowan,\nformation\n\nand T. K. Leen.\nFast non-linear\ndimension\nG. Tesauro,\nand J. Aispector,\neditors,\nAdvances\n\nProcessing\n\n[34] N. Kambhatla\nand\ncomponent\nanalysis.\n\nSystems-6,\n\npages\n\n152-153.\n\nMorgan\n\nreduction.\nin Neural\n\nKaufmann,\n\nT. K. Leen.\nDimension\nreduction\nby local\nNeural Computation,\n9:1493, 1997.\n\n[35] J. Kittler.\nCombining\nclassifiers:\nA theoretical\nand Applications,\n1:18-27,\n1998.\n\n23\n\nframework.\n\nPattern\n\nIn\nIn-\n\n1994.\nprincipal\n\nAnalysis\n\n[36] J. Kittler and F.M. Alkoot. Relationship\nof sum and vote fusion strategies.\nIn J. Kittler\nand F. Roll, editors,\nProceedings\nof the Second International\nWorkshop\n2001.\n\non Multiple\n\nClassifier\n\nSystems,\n\npages\n\n339-348.\n\n[37] J. Kittler,\nM. Hatef, R.P.W.\nDuin, and J. Matas.\nfiers. IEEE\nTransactions\non Pattern\nAnalysis\nand\n20(3):226-239,\n1998.\n[38] R. Kohavi\nIntelligence\n\nand G. H. John. Wrappers\nfor feature\nJournal,\n1-2:273-324,\n1997.\n\n[39] D. Koller and M. Sahami.\nof the 13th International\n1996.\n\nin Neural\n1995.\n\nInformation\n\nsubset\n\nnetwork\nensembles,\nD. S. Touretzky,\nProcessing\n\nBerlin,\n\nOn combining\nclassiMachine\nIntelligence,\n\nselection.\n\nToward optimal feature selection.\nConference\non Machine\nLearning,\n\n[40] A. Krogh and J. Vedelsby.\nNeural\nand active learning.\nIn G. Tesauro,\neditors,\nAdvances\n238. M.I.T. Press,\n\nSpringer,\n\nArtificial\n\nIn Proceedings\npages 284-292,\n\ncross validation\nand T. K. Leen,\n\nSystems-7,\n\npages\n\n231-\n\n[41] S. Kumar, M. Crawford,\nand J. Ghosh. A versatile framework\nfor labelling\nimagery with a large number of classes. In Proceedings\nof the International\nJoint Conference\non Neural Networks\n(IJCNN-gg},\n1999.\n[42] L.I. Kuncheva\nand C.J. Whitaker.\nFeature\nsubsets\nfor classifier\ncombination:\nAn enumerative\nexperiment.\nIn J. Kittler\nand F. Roll, editors,\nProceedings\nof the Second International\nWorkshop\non Multiple\nClassifier\nSystems,\npages 228-237. Springer,\nBerlin, 2001.\n[43] P. Langley.\nSelection\nof relevant\nings of the AAAI Fall Symposium\n\nfeatures in machine learning.\non Relevance,\n1994.\n\n[44] R. Melt.\nBias, variance,\nand the combination\nof estimators;\nleast linear squares.\nIn G. Tesauro,\nD. S. Touretzky,\nand\neditors,\nAdvances\nin Neural Information\nProcessing\nSystems-7,\n302. M.I.T. Press, 1995.\n[451 C. J. Merz. A principM\ncomponent\nmates.\nMachine\nLearning,\n36:9-32,\n\napproach\n1999.\n\nto combining\n\nIn Proceed-\n\nthe case of\nT. K. Leen,\npages 295-\n\nregression\n\nesti-\n\nf46] M. O. Noordewier,\nG. G. Towell, and J. W. Shavlik.\nTraining\nknowledgebased neural networks\nto recognize genes in DNA sequences.\nIn R.P. Lippmann, I.E. Moody, and D.S. Touretzky,\neditors,\nAdvances\nin Neural Information Processing\nSystems-3,\npages 530-536. Morgan Kaufmann,\n1991.\n[47] E. Oja. Subspace Methods\nof Pattern\nLetchworth,\nEngland,\n1983.\n\n24\n\nRecognition.\n\nResearch\n\nStudies\n\nPress,\n\n[48]E. Oja. Pricipalcomponents,\nminorcomponents,\nandlinearneuralnetworks.Neural Networks, 5:927-936, 1992.\n[49] D. W. Opitz and J. W. Shavlik.\nActively\nsearching\nral network\nensemble.\nConnection\nScience,\nSpecial\nArtificial\n\n[5o]\n\nNeural\n\nEnsemble\n\nApproaches,\n\npages\nN.\n\neditors,\n\n535-541.\n\nC. Oza\n\nAdvances\n\nM.I.T.\n\nand\n\nin Neural\n\nPress,\n\nM. Partridge\npca.\n\n[53]\n\n1996.\n\nInformation\n\nK. Turner.\n\nand\n\nIntelligent\n\nInput\n\nR. A. Calvo.\n\nData\n\nProcessing\n\nSystems-&\n\n1996.\ndecimated\n\nensembles:\n\nthrough\ndimensionality\nreduction.\nIn J. Kittler\nceedings of the Second International\nWorkshop\ntems, pages 238-249.\nSpringer,\nBerlin, 2001.\n\n[52]\n\n8(3 & 4):337-354,\n\nD. W. Opitz and J. W. Shavlik.\nGenerating\naccurate\nand diverse members\nof a neural-network\nensemble.\nIn D. S. Touretzky,\nM. C. Mozer, and M. E.\nHasselmo,\n\n[51]\n\nNetworks:\n\nfor an effective\nneuIssue on Combining\n\nAnalysis,\n\nFast\n\ndimensionality\n\n2:203-214,\n\nDecorrelation\n\nand F. Roll, editors,\non Multiple\nClassifier\n\nreduction\n\nand\n\nProSys-\n\nsimple\n\n1998.\n\nE. Pekalska and R.P.W. Duin. On combining\ndissimilarity\nrepresentation.\nIn J. Kittler\nand F. Roll, editors,\nProceedings\nof the Second International\nWorkshop\non Multiple Classifier Systems,\npages 248-257.\nSpringer,\nBerlin,\n2001.\n\n[54]Lutz\n\nPrechelt.\n\nPROBEN1\n\n--\n\nA\n\nset\n\nof benchmarks\n\nand\n\nbenchmark-\n\ning rules for neural\nnetwork\ntraining\nalgorithms.\nTechnical\nReport\n21/94,\nFakultgt\nflit Informatik,\nUniversit//t\nKarlsruhe,\nD-76128\nKarlsruhe, Germany,\nSeptember\n1994.\nAnonymous\nFTP: /pub/papers/techreports/1994/1994-21.ps.Z\n\n[55]\n\nN. Ramanujam,\nM. F. Mitchell,\nA. Mahadevan,\nT. Wright, and N. Atkinson\nR. Richards-Kortum.\nvatirate\nstatistical\nalgorithm\nto analyze human\nspectra\n\n[56]\n\nM.D.\n\nacquired\nRichard\n\nBayesian\n1991.\n\n[57]\n\nin vivo.\nand\n\na posteriori\n\nR.P.\n\nLasers\n\nin Surgery\n\nLippmann.\n\nS. Thomsen,\nDevelopment\ncervical tisue\n\nand Medicine,\n\nNeural\n\nnetwork\n\nNeural\n\nprobabilities.\n\nComputation,\n\nA. Malpica,\nof a multifluorescence\n\n19:46-62,\n\nclassifiers\n\n1996.\nestimate\n\n3(4):461-483,\n\nF. Roli, G. Giacinto,\nand G. Vernazza.\nMethods\nfor desig-ning\nmultiple\nclassifier\nsystems.\nIn J. Kittler\nand F. Roli, editors,\nProceedings\nof the\nSecond International\nWorkshop\non Multiple\nClassifier\nSystems,\npages 7887. Springer,\n\n[ss]\n\non ffp.ira.uka.de.\n\nBerlin,\n\n2001.\n\nB. Rosen.\nEnsemble\nlearning\nnection\nScience,\nSpecial Issue\nEnsemble\n\nApproaches,\n\nusing decorrelated\nneural\nnetworks.\nConon Combining\nArtificial\nNeural Networks:\n\n8(3 & 4):373-384,\n\n25\n\n1996.\n\n[59] J.W. Sammon Jr. A nonlinear\nmapping\ntransactions\non Computers,\n18:401-409,\n[60] N.\n\nShort.\n\nRemote\n\nfor data\n1969.\n\nsensing\n\nstructure\n\ntutorial,\n\nanalysis.\n\n2000.\n\nIEEE\n\nURL:\n\nhttp://rst.gsfc.nasa.gov/starthere.html.\n[61] K. Sirlantzis,\nM.C. fairhurst,\nand M.S. Hoque. Genetic algorithms\nfor rnulticlassifier\nsystem configuration:\nA case study in character\nrecognition.\nIn\nJ. Kittler\nand F. Roli, editors,\nProceedings\nof the Second International\nWorkshop\non Multiple\nClassifier Systems,\npages 99-108. Springer,\nBerlin,\n2001.\n[62] D. Tax, M. van Breukelen,\nR. Duin, and\nclassifiers\nby averaging\nor by multiplying.\n348, February\n1996.\n[631 K. Turner\nbined\n\nand\n\nneural\n\n/64] K. Turner\n\nJ. Ghosh.\n\nclassifiers.\n\nAnalysis\nPattern\n\nand J. Ghosh.\n\n[65] K. Tamer\n\nand J. Ghosh.\n\nand\n\nJ. Ghosh.\n\nboundaries\n\nand error\n\ncom-\n\nFebruary\n\n1996.\n\nreduction\n\nin ensem-\n\nScience, Special Issue on Combining\nArtificial\nApproaches,\n8(3 _z 4):385-404,\n1996.\n\nLinear\n\nand order\n\nRobust\n\nstatistics\n\ncombiners\n\nfor pattern\n\neditor, Combining\nArtificial\nNeural Nets:\nSystems, pages 127-162. Springer-Verlag,\n\norder\n\nstatistics\n\nbased\n\ntributed\ndata mining.\nIn H. Kargupta\nand P. Chan,\nDistributed\nand Parallel Knowledge\nDiscovery,\npages\nPress, 2000.\n[67] K. Turner\nalization.\nNetworks,\n\nin linearly\n\n29(2):341-348,\n\ncorrelation\n\nclassification.\nIn A. J. C. Sharkey,\nEnsemble\nand Modular Multi-Net\nLondon,\n1999.\n[66] K. Tumer\n\nof decision\n\nRecognition,\n\nError\n\nble classifiers.\nConnection\nNeural Networks:\nEnsemble\n\nJ. Kittler.\nCombining\nmultiple\nPattern\nRecognition,\n29(2):341-\n\nensembles\n\neditors,\n185-210.\n\nfor dis-\n\nAdvances\nin\nAAAI/MIT\n\nand N. C. Oza. Decimated\ninput ensembles\nfor improved\ngenerIn Proceedings\nof the International\nJoint Conference\non Neural\n1999.\n\n[68] D. Windridge\nand J. Kittler.\nClassifier\ncess. In J. Kittler\nand F. Roli, editors,\ntional Workshop on Multiple\nClassifier\nBerlin, 2001.\n\ncombination\nas a tomographic\nproProceedings\nof the Second InternaSystems,\npages 248-257.\nSpringer,\n\n[69] D. H. Wolpert.\n\nNeural\n\nStacked\n\ngeneralization.\n\n26\n\nNetworks,\n\n5:241-259,\n\n1992.\n\n'