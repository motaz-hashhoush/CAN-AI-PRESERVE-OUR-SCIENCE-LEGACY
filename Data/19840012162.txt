b'VECTORIZED MULTIGRID POISSONSOLVER\nFOR THE CDC CYBER 205\n\nDAVID BARKAI\nAND\nMAYNARD A. BRANDT\nCONTROL DATA CORPORATION\nINSTITUTE FOR COMPUTATIONAL STUDIES\nAT COLORADO STATE UNIVERSITY\nFORT COLLINS, COLORADO\n\nVECTORIZED MULTIGRID POISSON SOLVER\nFOR THE CDC CYBER 205*\n\nD. BarkaiM,\nA. Bra&t***\nInstitute\nfor Computational\nStudies at Colorado\n*Control\nData Corporation,\nState University,\nPO Box 1852, Fort Collins,\nColorado 80522; l **Weiztaann\nInstitute,\nDepartment of Applied Mathematics,\nRehovot, Israel 76100.\nABSTRACT\nThe full multigrid\n(PMG) method is applied to the two\ndimensional\nPoisson equation with Dirichlet\nboundary\nconditions.\nThis has been chosen as a relatively\nsimple\ntest case for examining the efficiency\nof fully\nvectorizing\nData structure\nand programing\nof the multigrid\nmethod.\nconsiderations\nand techniques are discussed,\naccompanied by\nperformance details.\nApril\n1.\n\n1983\n\nINTRODUCTION\n\nsolver\nThe multigrid\n(NG) method has been shown to be a very efficient\nfor discretlted\nPDE boundary-valve\nproblems on serial\n(scalar)\ncomputers.\nHowever, it was not clear how well can the MG approach be adapted to\nexecute effectively\nand efficiently\non a vector processor,\nsuch as the CDC\nCYBER 205, where considerations\nother than operations-count\nmay play an\nimportant\nrole.\nThe purpose of this paper is to. report our experience\nin\nimplementing-an\nMG code on the CDC CYBEB 205. More specifically,\nthe\ntest-case\nconsidered\nis the two-dimensional\nPoisson equation with Dirlchlet\nIt will be assumed here that the reader has some\nboundary conditions.\nfamiliarity\nwith the philosophy,\nthe motivation\nand the basic computational\nThese processes are described in detail\nprocesses of MC as a fast solver.\nin a number of papers in these proceedings\nand [l] and 121 and references\ntherein.\nThe algorithm\ndescribed in this paper is basically\nthe same as\nthe one given in the appendix of [3], whose description\nis detailed\nin\nno full description\nof the MC\nsections 8.1 and 6.4 of [3].\nTherefore,\nalgorithm\nis given here, but the relevant\ndetails\nare included in the\nThe main emphasis of this paper is the vectoritation\nappropriate\ncontext.\nof these processes.\nThus, we will\nnot assume an in-depth knowledge or\nexperience in applying HG solvers on a vector-processor\ntype of a computer\nsystem.\n\n+ Presented\nColorado,\n\nat the International\n6-8, 1983.\n\nMultigrid\n\nApril\n\n299\n\nConference,\n\nCopper Mountain,\n\nConsequently,\nSection 2 contains a brief summary of architectural\nand\nconceptual features of a vector processor (specific\nto the CDC CYBER 205),\nwhich are relevant to this application,\nas veil as software tools available\nfor a tight correlation\nbetween the hardvare and the computational\nprocess.\nSections 3, 4 aad 5 are devoted to the description\nof the techniques used\nfor vectorizing\nthe procedures for the relaxation,\nthe residual transfer\ncalculatioa\nand the iaterpolatioa,\nrespectively.\nThe total full multigrid\n(EXG) process and various parameters and constraints\nare described in\nSection 6 interleaved\nwith convergence and timings (performance) details.\nFinally,\nSection 7 contains some concluding remarks and comments regarding\nfuture plans.\n2,\n\nVECTORPROCESSING\n\nThe most significant\ndifference\nbetween a traditional,\nserial computer\nand a vector processor is the ability\nof the latter\nto produce a whole\narray ("vector")\nof results upon issuing a single hardware instruction.\nThe input to such a vector-instruction\nmay be one or two vectors,\none or\nor a combination of the above. The instructions\ntwo elements ("scalarsn),\nfall into two main categoriesthose that perform floating-point\narithmetic\n(including\nsquare root, sum, dot-product,\netc., as well as the basic\noperations),\nand those which may be collectively\ncalled "data-motion"\ninstructions.\nThese may be used,\nfor example, to "gather" elements from\none array into another using an arbitrary\n"index-list";\nto "compress" or\n"expand" an array; to "merge" two arrays into one (with arbitrary\nWiaterleaving"\npatterns),\netc.\n\none\n\nThe need for vector data-motion instructions\nbecomes apparent when\nconsiders the definition\nof a vector on a CDC CYBER 205. A vector is a set\n(array) of elements occupying consecutive\nlocations\nin memory. It means,\nby the way, that a vector may be represented in FORT&W by a multidimensional array; i.e;,\na two- or three-dimensional\narray may be used in\ncomputations as a single vector.\nThe reason for this vector definition\nis\nthat vhen performing vector operations on the CDC CYRER 205 the input\nelements are streamed directly\nfrom memory to the vector pipes and the\noutput is streamed directly\nback into memory without any intermediate\nregisters.\nThe timing formula for completing a vector\ninstruction\ncontains two\ncomponents. Oae is fixed, i.e.,\nindependent of the number of elements to\nbe computed, and is called "start-up"\ntime.\nIn fact, it amounts to\nstart-up and shut-down; it involves fetching\nthe pointers\nto the input and\noutput streams, aligning\nthe arrays so as to eliminate\nbask conflicts\nand\ngetting the first\npair of operands to the functional\nunit (the pipe-line)\nTypical time for the "start-up"\ncomponent\nand the last one back to memory.\nis 1 microsecond, or about 50 cycles (clock periods).\nThe other component\nof the timing formula is the l\'stream-time"\nwhich is proporatioaal\nto the\nnumber of elements in the vector.\nThe result rate for a Z-pipe CDC CYRER\n205 for an add or multiply\nis 2 results per cycle.\nIt is apparent now that\nin order to offset\nthe "wasted" cycles of starttimes it is beneficial\nto work with longer vectors.\nThe system is better utilized\nif a single\noperation is performed on a long vector,\nrather thaa several operations\nto\ncompute the same number\nof results.\nGiven a vector length, N, one can\nevaluate the efficiency\nof the computation\nas the ratio between the number\nof cycles used to compute results and the total number of cycles the\ninstruction\nhas taken; i.e.,\n(N/2)/(N/Z\n+ 50).\nThe maximum vector length\n300\n\nthe CDC CYBER 205 hardware allows is 65,535 elements.\nbecomes quite negligible\nlong before that.\n\nThe start-up\n\ntime\n\nare inserted through a\nThe vector "argumeats" for vector instructions\nIt is a quantity occupying 64 bits which\ncoastruct\ncalled Descriptor.\nfully describes a vector\nthrough two integer values:\none is the virtual\nthe other is the number of\naddress of the starting\nlocation of the vector,\nor the length,\nof the vector.\nAn element may be a bit, a byte, a\nelements,\nhslfrord\n(32-bits)\nor a word (64-bits)\ndepending on the intructioa\nand the\nThe CDC CYBER 205 FORTRAN provides the\nargument within the instruction.\nability\nto declare\nvariables\nof "type" Descriptor\nand Bit, as well as,\nextensions for assigning Descriptors\nto arrays and syntax for coding vector\ninstructions\nwithout such an explicit\nassociatioa.\nBit arrays\noccupy\nexactly one bit per element, since the CDC OIBER 205 is bit-addressable.\nBit vectors\nare used for creating a "mapping" between an array containing\nA Bit vector may be used to control\na\nnumerical values and a subset of it.\nvector\nfloating-point\noperation (hence the term "control-vector"\nvhich is\ncommonly used for a Bit vector) as follows:\nTake, for example, an add\nAll the elements of the two input arrays are added up, but only\noperation.\nthose result elements\nvkre\nthe corresponding element of the control-vector\nis 1 v-ill be stored into the results vector.\nThe other elements will not\none may specify storing on zeros in the\nAlternatively,\nbe modified.\nand discarding\nresults corresponding to a 1.\ncontrol-vector,\nAnother cormnon use of bit vectors\nIS associated\nwith some of the dataT\'wo examples will be given here:\nmotioa instructions.\nThe "compress"\ninstruction\nis tied to create a vector which is a subset of another vector.\none points to a numeric vector,\nThis operation has two input descriptorsWhenever a 1 is encountered in the bit-vector\nthe other to a bit vector.\nthe corresponding numeric\nelement\nis moved to the next location\nof the\noutput vector, i.e.,\nthe input array is "compressed" (the reverse process\nmay\nmay be accomplished with an "expand" instruction).\nA single bit-vector\ntvo numeric vectors into one. The bit-vector\nis\nalso be used to "merge"\nscanned and vhen a 1 is encountered the next element of the first\ninput\nvector is put into the next location of the output vector, vhen a iero is\nfound in the bit-vector\nthe aext element of the second input vector is\nThe timing for both\nmoved into the next location\nof the output vector.\nthese instructions\nis dictated\nby the total length of the bit-vector.\nThe\nresult-rate\nis the same as that of vector arithmetic,\ni.e.,\non a two-pipe\nCYBER 205 it is two elemets per cycle (whether they are moved or not).\nIt\nwill be noted here that there are vector instructions\nfor creating repeated\nbit patterns at a rate of 16 bits per cycle.\nof\nBefore concluding this section let us briefly\nmention the existence\nwhich computes an average of two vectors,\nor\naa "average" inStNCtion,\nadjacent means of a single vector, at the rate of a single floating-point\nOae can also "link",\nfor example, an add and a multiply\noperaoperation.\ntion, provided at least one of the three inputs is a "scalar,,,\nand perform\nthe two operations\nas if it were only\none. AU the instructions\nmentioned\nabove are directly\navailable\nthrough Fortran in-line\nfunction calls.\n3r\n\nBXLUXTION\n\nNov ve are ready to examine the vays in which to utilize\nthe tools and\nthe vector processing concepts discussed in the previous section for\nThe success of such an exercise\nvectorlzing\nthe Hultigrid\napplication.\n301\n\nhinges, to a large exteat, upon the efficieacy\nprocess may k accomplished.\n\nvith\n\nwhich the relaxatioa\n\nDiscrctitatioa\nof the two-dimensational\nPoisson equation is achieved\nvia the S-points differeaciag\nscheme. Thus\' assuming geometric iattrpreta&ion of the indices for the momeat, the set of the simultaneous equatioas\nto be solved may be written\nas\nui,j-1\n\n+ \xe2\x80\x9c1-l,j\n\n+ ui+i,j\n\n+ ui,j+~\n\n- 4 * ui,j\n\n- h2Fi,j\n\nvhcre u is the uaknova function,\nh Fs the InterPal\nbetveen two grid points\n(in either directioa)\nand F is the right-hand\nside function.\n1 varies from\n2 to HI-1 and j from 2 to N2-1, where Nl and N2 are the number of\ngrid points along the two directions.\nOne may vant to coasider the usual (lexicographic)\nGauss-Seidel relaxaThis, however, vi11 be fn conflict\nvith vectorizatioa,\nas\ntion procedure.\nis characterized\nby the\nmay be easily deduced. The Gauss-Seidel relaxation\nuse of updated values as sooa as they become available.\nVectorization\nmeans\nnot waiting\nfor the previous\nprocessing many such values in parallel,\ni.e.,\nThe obvious alternative\nis the red-black or\nelement to be updated.\nvhere\nall the four neighbors of each point belong\nchecker-board ordering,\nThe convention used here is that the \xe2\x80\x9ccolor\xe2\x80\x9d\nof the\nto the other "color".\nThe grid may accordgrid points at the corners of the rectangle is red.\ningly be divided into tvo vectors and the relaxation\nperformed in two\nthe values at red points are updated using \xe2\x80\x9cold\xe2\x80\x9d values,\nstages : first,\nthen the values at black points are updated using the \xe2\x80\x9cnew\xe2\x80\x9d red values.\nThroughout the code the tvo vectors of the unknown functioa\n(and of the RHS\nfunctioa)\nare stored consecutively\nfollowing\neach other, vhere inside each\nvector the values are stored column-vise an shown in Figure\n1. This\nstorage applies, of course, to all the grids used.\n\nFigure\n\ndotted\n\n1. Mapping of the Lexicographic\ninto the \xe2\x80\x9cRed-Black,\xe2\x80\x99\nune indicates\nthe separationa of the grid polnts.intO\n302\n\nOrdering.\nThe\ntwo vectors.\n\nThe reader vi11 uotict\nthat cht vectors thus created\nart not confined\nto one column, but txttnd\nover cht entire grid.\nIt waa done in order to\nachieve longer\nvectors\nin lint with the desire expressed\nin Section\n2.\ncht hazard of ovtrvriting\nvalues residing on the\nThis, however, introduces\nboundary of the grid.\nTo avoid this a bit control-vector\nwas created for\neach grid, in a set-up\nroutine,\nwhich concains zeros where boundary points\nWe uat this "boundary control vector"\nexist and ones for interior\npoints.\nto assure storing new values only inro the interior\nof ehe grid.\nThe computation requires the sum of cha 4 neighbors for tach grid\npoint.\nOne can easily verify\nchat, using vtceor add operations\nthis can be\nOne to add a vector into itself,\nwith some\ndone with tvo opcracions only.\nStarr with tltmtnm\n2 and 5 in Figure\n1) and the second\nto\noffset (e.g..\nadd the rtsultane\nvector\ninto itself\n(vith some other appropriate\noffset).\nThe remaining calculation\ninvolves subtracting\nthe result from the RHS\nvalues and multiply\nby a constant (being -0.25), vhich is accomplished aa a\nlinked-triad\noperation;\nthe result is ehta stored into place under the\nThus, each of the two stages\n(two\ncontrol of the boundary bit-vtccor.\n"colors")\nrtquiru\nthrtt\nfloating-point\noptraciona using vector length of,\nIn fact, some more savings\napproximarely,\n(N1 l N2)/2 elements long.\nin the compucaeions occur in the first\nrelaxation\nsweep afttr\nmoving to a\nneed not bt computed for the\ncoarser grid, since the sum of the "neighbors"\nbeing known to bc zero.\n\xe2\x80\x9ccolor,\xe2\x80\x9d\nfirst\nThis is because we art btginning to\ncompute a correction-function\nvhost first\napproximation\nis zero.\nThe\nvector-operations\ncount for this relaxation\ns~ttp Is thus reduced from 6 to\na solution-function\n(noC "correction")\nto a\n4. Also, vhtn transferring\ncan be used which\nfiner grid, as part of the FMC process, an interpolation\nviJ.I. save cht relaxation\non cht first\n"color"\n(see Sec. 5).\nfast\n\nprocess can obviously be done txtrtmtly\nIn concLusion, the rtlaxacion\nwill be given in Section 6.\non the CYEER 205. Timing details\n\n4.\n\nFIMZ TU COAESEBESIDUALTBBNSFEB\n\nRtsiduals have to be computed\nat those\nfine-grid\npoints which also\nThese residuals\nart directly\ntransferred\nto\nbelong to the coarser\ngrid.\nthe corresponding coarse-grid\npoines weighted by l/2 ("half\ninjtcclon";\nthe\nfactor of l/2 is motivated by the fact that eht fine-grid\nresidual is zero\nat black fine-grid\npoints, htnct the ocher residuals should be multiplied\nby l/2 to rcprtstne\nthe correct\naverage).\nSet\nFigure\n2.\nThe computation\n\ninvolves four floating-point\noperations\n(tvo of thtm\nlinktd triads)\nfor evaluating\nthe residuals of the red points\non the\nfine-rid\nand multiplying\nthem by l/2.\nThis, however, does not conclude\nthe procedure.\nAt this stage ve need to apply the "comprtsa"\noperation\nusing a prt-dtfintd\nbit-vector\nve extract\nthe\nthree times aa follow:\nrtaidual\nvalues corresponding\nto coarse-grid\npoints, i.t,\nbelonging to\n(Note that ve\noddaumbtrtd\ncolumns of the red scctioa of-the finer grid.\nhave throwa away b&f the calculattd\nrtsidnala.\nThis procedure is both\nsimpler and a little\nfaattr\nthan having to perform all the comprasa\noperationa needed for computing only the required rtsidurls.)\nNow, as is\nevident from Figure 2, we have all the dtrirtd\nvalues for the coarser grid\nrtortd\nia ltxlcographic\norder.\nTo separate them into \xe2\x80\x9cred\xe2\x80\x9d\nand "black"\nsections the I\'compreas" instruction\nis applied Mce (once for each color)\nusing a prc-dtfintd\n\xe2\x80\x9cpicket fence"\nbit-tcfor.\nThe procedure as described\nhere products opeimum performance even though somt redundant operation8 are\nart\n\n303\n\nThe alternatives\nare to perform different\n(more\n"costly")\ndata\nperformed.\nmotions or to operate on much shorter\nvectors.\nFinally,\nanother vtccor\noperation is txtcuttd\nto zero out the unknown functioa\nof the coarser grid\nIn total\nthe procain preparation\nfor evaluating\nthe correction\nfunction.\nassociated\nuith 5 operations\nof approxldure rtquirts\n8 vector "start-ups"\nmate length of (Nl * N2)/2, and 3 operations of length (Nl l N2)/4, vhtre\nNl and N2 are the dimcnsioas of the finer grid.\n\n.\n\n.\n\nI\n\nTransfer tu a Coarser Grid:\nThe residual\ncalculation.\n2.\n\'90xB\' contains the fine grid points involved in the computation\ncorresponding coarse grid point.\nFigure\n\n5.\n\nEach\n.for the\n\nINEBPOLATION\n\nInterpolation,\nin the context of this paper, is the process by which we\nare\ntransfer from a given grid to a finer one. Two types of inttrpolacions\nType I interpolation\nis used vhtn a correcrioa\nis \'interpoemploytd here:\nThe Type II interlaced from the coarser grfd and added to the finer grid.\npolation is used to compute a first\napproximatiou\non the finer grid, based\nThe use of the red-black ordering,\non existing\nvalues on the coarser grid.\ncombined with the fact that a relaxation\nalways follows an interpolation,\nimpliu\nthat only one color of the finer-grid\npoints need to be interpolated\n(the other color vlll\nbe computed by a rtlaxacioo\npass on that color).\nType I interpolation\nis bilintar\nemploying points at shown in Figure 3.\nOnly interior\nblack points oa the finer grid need to bt evaluated.\nDue to\nthe rtquirtd\naveraging of the coarse grid values it is coavtnitnt\nto first\nmerge ehe red and black points of this grid wing the "picker-fence"\nbit\nNext, tvo averages art\nvector\nto produce the ltxicographic\nordering.\n304\n\ncomputed. The average over the coarse grid, vhere the two input vectors\nare offset\nby a column, viU produce the quantities\nto be added into black\npoints OP evenumbered\ncolumos on the fine grid.\nA second average, vhere\nthe offset between the tvo vectors is one elemnt,\nis executed for fine\ngrid black points corresponding\nto odd numbered columns.\nThis last operation produces redundant values (at the end of each coarse grid column)\nvhich are throwa away using the "compreso" operation\nwith an appropriate\nwedefined\nbit vector.\nThe two resultant\ncoarse\ngrid "average-vectors"\nunder the control\nof the\nare then interleaved,\nusing a \xe2\x80\x9cmerge\xe2\x80\x9d instruction,\nbit vector vhere the "l\xe2\x80\x98s"\nand "0\'s" correspond\nto odd and even columns,\nPixklly,\nthe urged\nvalues are added to \xe2\x80\x9cblack" points of\nrespectively.\nthe finer\ngrid under the control of the \xe2\x80\x9cboundarf\nbit-vector\nwhich inhibits\nThe whole procedure amounts\nstoring values into the boundary of the grid.\nto 3 floating-point\noperationa,\n2 "merges" and 1 "compresu."\nThe 6 vector\noperations\nmay also be divided into 4 operations of length (N1 l N2)/4\n(Nl and N2\nand 2 operations of length (Nl l N2)/2, approatelp.\nare the dimensions of the finer\ngrid.)\n\nIt\nFigure 3. Type I Interpolation.\nvalues are added into "Black" points\n\nshovs vhere averages\ncm the fine grid.\n\nof coarse\n\ngrid\n\nType II interpolatioa\nla a 4th order one, described,\nfor example, in\nsecrion 6.4 of [31.\nIt produces nev red unknowa-function\nvalues on a finer\nThe values at the black points\ngrid using rotated difference\noperators.\npaas over the\na relaxation\nare produced by half a relaxation\nsweep, i.e.,\n(This pass may be regarded aa part of the interpofine-grid\nblack points.\nIn the timing tables below, however, the time spent in\nlation process.\nThe process is described pictothis pess is counted as relaxation\ntime.)\nrially\nin Figure 4. All the interior\ncoarse grid values are moved to occupy\n305\n\nThe relaxation\noperator is applied to\ncorresponding fine-grid\npoints.\nthese vslues in order to compute interior\nred points of the even-n-bared\nThe only difference\nbetween\nthe relaxation\nhere\ncolomns on the fine grid.\nspd the one described in Section 3 is that the operator is the \xe2\x80\x9crotated\xe2\x80\x9d\nand the interval\nktween\neach point and its neighbors is\nS-point Lap&clan\nThe EES function values rsquircd for this relaxachanged\nfrom h to @h.\ntion are avsilable\nfrom the fine grid RJlS array (a "compress" operation is\nperformad to retrieve\neven-nwsbersd calm\nvalues).\nThe whole procedure,\nthus, requires 2 "merges" (one for merging red-black valuss of the coarse\nand \xe2\x80\x9crelaxed\xe2\x80\x9d\nvalues\nof the\ngrid, the other for merging the "trsnsferred"\nred fine grid points);\n3 floating-point\noperations\nfor the relaxation;\n2\n\xe2\x80\x9cC~l3!88\xe2\x80\x9d\noperations\n(one for throwing away redundant, incorrect\naverages\nand one for collecting\nREfSvalues);\nand, finally,\none vector-move operation\nunder the control of the boundary bit-vector\nfor storing\nthe aefy red fine\ngrid values into place.\nFive out of the 8 vector operations have length of\nabout (N1 * N2)/4,\nthe other 3 are associated with a length of (Nl l N2)/2;\nNl and N2 being the dimensions of the finer grid.\nthe\n\nI\n0\nx\n0\n*\n0\nx\n0\n*\n\n0\n\nx\n\n0\n\nX.\n\nx\n\n6L*\nI\xe2\x80\x99\n\nCoarse grid values are transferred\nto\nFigure 4. Type II Iatcrpolation.\nThese values are wed to compute,\nodd numbered columns on the fine grid.\nthe even aumbered column values.\nvia ths relaxation\noperator,\n6.\n\nPEEFOBllANcE AND COHYEBCENCE\n\nThe bssic computational\nprocedures, studied in the previous three\nFigure 5 is\nset eons, can now bs linked together to form the FXG process.\na scksmatic description\nof the sequence of events vhich leads to an\nThe finest grid (where a\napproximate solution of the difference\nequations.\nsolution\nis sought) is assigned the highest level number. The example\n306\n\nin Figure 5 descrfbes ap Fnc tith\n5 levels vhere the process\nstats\nat level mmber 2. This may not be aecessary,\nas vill\nbs argued\nMow,\nand onm may visurLite\nthe FMG starting\nat a higher\nlevel sfmply\nby\ndeleting\nthe left-hand-side\nof the figure.\nThis starting\nlevel is a\nThe F?fG shown la Figure\n5 la composed\nof\nparameter\ncontrolled\nby the user.\nvhrt\nis knowll as \xe2\x80\x987\xe2\x80\x9d\ncycles.\nIn each "P\' cycle one performs relaxation. ..uutil\nreaching the coarsest\ngrid, then a\nrsslduel\ncalculation-relaxation\naquasme\nof interpolation7e3.axatiou\nis uscuted.\nThe transfer\nfrom one\n\xe2\x80\x98YP cycle to tha next la achieved da Typm II interpolation.\nMore\nsFecifi&y,\nthr FMC m Fnplemated my be chamctrrfzed\nas\nF?fG (H,L,Xl,X2,&3,B4),\nvhsre n is the nuder\nof\xe2\x80\x99levels\nand L is the\nsturting\nlevel;\nEl and B2 indiute\nthe number of rslpr,tions\nbefore svinz\nto a coarser\ngrid and before moping to a finer grid, respectively.\nR4 have the ssme maning and apply to the last V\' cycle only.\nAll these\nparmeters\nare provided by the user*\nThe use rsay also specify\nthe rlza of\nthr coarsest\ngrid\nto be rued.\nIt mast heve an even number of internals\nin\n(In our experiments\nthe coarsest\ngrid had 3 by 3 points;\nssch directioa.\n1.8. * 2 by 2 InterPals.)\nThe user also specifies\nthe mesh size h (assumed\nto be the same in both directions)\non the finest\ngrfd.\ndepicted\n\n4\n3\n2\n1\n\nl\n5.\nThe Full MuLtigrid\n(FMG) Process:,\nFMG (5, 2, U, B2, 83, B4).\ncircles\nindicate\nthe number of relaxations\nperformed\nat a given level.\nDouuwards orTow signifies\nresidu\ncalculation\nbemean rsJ.alations,\nupwards\n(Uhen a level is encountered for the first\narrow fmplies fnterpolatlon.\ntime the interpolation\nis of Type II,\nindicated\nby a double line above,\notherwise it is of Type I.)\nWhen level 1 contafns\nonly one lntarior\npoint\nonly one relsxation\nsweep is perfowd\nthereon , regardlass\nof the values\ngiven to El and 83.\nFigure\n\nThe\n\nThe process described above is deterministic,\nia the sense that the\nuser defines the steps to be tShSI, bawd on prior knowledge of the\ncharacteristics\naad smoothness\nof the functioa\nto k solved.\nIt in also\nkaoue that if L-2 the FUG guarantees a solutioa\nerror\nsmaller than the\ntmmation\nemor (introduced\nby the differencing\nscheme), for L2 sons.\n!& have &lowed,\nhowever, as a usemption,\nthe evsluation\nof\nfor etnmple.\n307\n\nthe Ll, L2 and L,noms\ndone for problems vhich\n\nof the residual at var%cms points.\nhave solution\nof the Saxa:\nc * cos (k\n\nTesting\n\nwas\n\n(x + 23))\n\nwith and without the addition of a 6th degree polyaomial which vanishes on\nIn all these uses the FHG process\nvith L=2 indeed produced\nthe boundary.\na solution with an algebraic\nerror (error in solving\nthe difference\nermr, in the L1, L2 and L,\nequations) much smsller than the truncation\naorms .\n\nOoly "V(2,l)"\ncycles were used for the results\nand tinrings to be quoted\nhere. This turns out to bs the optima\ncmshimtion\nfor the Poisson\nMore relaxstions\nat esch stage do wt improve the final result\nequation.\nl oough to justify\nthe additional\nvork, less tiurtions\nmay cause deterioration in the accuracy.\n(If full weighting were used instead of half\ninjection,\nthe optimal cycle would be "V(l,l)".\nThis would, however, be\nless efficient\nthan the present procedure since full weighting is substantially\nmore costly\nthan a relaxation\nsweep.)\nIn the performance details\nvhich follow,\nve vill\ngive results for various Prlues of L since, in many\ncases, in particular\nvhen a reasonable initial\nguess is available,\nhigh\nvalues of L, even L-H, may provide sufficient\naccuracy.\nThis is, in\nthe situatioo\nwhen the PO~SSOII solver is used within some\nparticular,\nexternal iterative\nprocess, or at each time step of an evolution\nproblem.\n&fore dig-sing\nthe timings ve should briefly\nmention SOW set-up\nA routine is provided for re-orderiag\nthe initial\narray (from\nprocedures.\nl&cographic\nto red-black)\nif it is aot so structured\nyet.\nThis is done\nthrough two "picket-fence\ncompress" operations and amounts to 0.185 msecs.\nback into lexicofor a 65 by 65 grid, for example. Putting ths solution\ngraphic order is done with a single "merge" instruction\nand takes half as\nNext, there ig a routine vhich defines vsrious pointers and lengths\nlong.\nFor\ndiscussed earlier.\nfor all the grids used, as well as the bit-vectors\nmany applications,\nvhere the solver is used mny times vith the S+W? grid\nbe\ndefinition,\nthis v-ill be done only once. It will aot, therefore,\nincluded in the total times quoted below (it takes 0.29 msecs. for a 65 by\nThe last set-up routine is included in the timings\n65 grid with 6 levels).\nthis routine\ndefines the boundary velues and the EHS for all\nfnfonnation.\nguess on the level\nthe levels bet-en L and M-1. It also sets the initial\nL grid.\nThe code ves mn with grid sizes of 33 by 33, 65 by 65 and 129 by 129\n(H = 5, 6 and 7, respectively)\nwith L-2,...,&\nTotal execution times are\ngiven in Table 1. It shows, for example, that a 65 by 65 grid may be\nsolved in as little\nas 1 msec., and, at most, in 2 msecs. By examining the\nprocssalng time per grid-point\none can see the effect of vector-instructions\nstart-up\ntimes or the dependence of the perfonnrnce upon vector lengths.\nOn a serial processor the time per element wuld\nhave been, approximately,\na constant across each line in Table 1. We obseme, however, that the\nprocessing of the 129 by 129 grid is roughly twice as efficient\nas that of\nThis\nis due to the fact that even though the aumber of\nthe 33 by 33 grid.\nremains nearly\nthe same (across\na given\nline),\nthe\nvector \xe2\x80\x9cstart\xe2\x80\x99~ps\xe2\x80\x9d\nnumber of elements solved for has increased by a factor of 16. Hence, more\ntime is spent doing useful arithmetic\nin the vector\npipelines.\n\n308\n\nTABLE 1. Execution times for various paramcccrs of the FMG. The entries\non the left are total times in milliseconds.\nThe entries enclosed in\nparenthesis\nare the execution times in microseconds per grid-point\n(only\ninterior\npoints are taken into account).\nH-L+1\nI\n1 (.No. of "V"\'s)\nI\n1\nI\n2\nI\n3\nI\n4\nI\n5\n6\nI\n\nI\n\n65 by 65\n(X ; 6)\nI 1.006 (0.25)\nI 1.552 (0.39)\n\' 1.810 (0.46)\n1 1.947 (0.49)\nI 2.009 (0.51)\n\nI\n\nI\n\nI\n1\n1\n1\n1\n1\n\n33by33\n(HG 5)\n0.360 (0.37)\n0.604 (0.63)\n0.729 (0.76)\n0.801 (0.83)\n\nI\n\n1\nI\nI\n1\nI\nI\n1\n1\n\n129 by 129\n(H ; 7)\n3.293 (0.20)\n4.910 (0.30)\n5.440 (0.34)\n5.687 (0.35)\n5.807 (0.36)\n5.875 (0.36)\n\n1\nI\n\nI\ni\nI\nI\n1\n1\n\nTables 2 and 3 present a more detailed\nanalysis of timings for a single\nat\nexample, namely for solving a 129 by 129 grid with 7 levels and starting\nlevel 2. The entries in Table 2 show timings in msecs. by level and by\nprocedure.\nOne notices that the total time spent performing relaxations\nis\nless than 50X of the total time.\nThis is to be compared against the go-902\nof total time used for relaxations\non a serial processor.\nThis is, of\ncourse, due to the fact chat the vectorized\nrelaxation\nis extremely\nefficient\nand does aot fnvolve any data-motion operations.\nThe interpolation and the residual calculations,\nthough fully vectorired.\ninvolve\nsome\ndata-notion\noperatioas,\nand, therefore,\nconsume a relatively\nhigher proportion of the execution time than they would on a "scalar"\ncomputer.\nAnother\nobsemation\nvorth mentioning is that the contributions\nto all the procedures\narising from levels 2 to 4 is roughly the same, even though the amount of\nvork\ndiffers\nby a factor of 4 bctvetn levels.\nThis is a consequence of the\nrelatively\nshort vectors\nvhich characterize\nthe coarser grids.\nXt also\ncode\nexplains the larger weight the coarse grids have in the vectorized\ncompared to that of the serial process.\n\nTABLE 2. kecution\ntimes in milliseconds\nfor solving a 129 by 129 grid\nwith starting\nlevel 2. Breakdown by procedure and by level.\nFor the\nresidual calculation\nand the interpolations\nthe entry in the table\ncorresporxis to the f her grid involved.\nI\nI\n\nI\n\nLevel\nI 1 (3x3)\nI 2 (5x5)\nI 3 (9x9)\n\nI\nI\nI\n\nI\n1\n1\nI\n\n(17x17)\n(33x33)\n(65x65)\n(129x129)\n\n1\nI\nI\nI\n\nTOTAL\n\nI\nI\n\nI\nI\n\n4\n5\n6\n7\n\nGrid\n\nI\n\nI Residual\n\nl\n\n0.015\n\n1\n\n0.034\n0.106\n0.388\n\nI 0.189\nI 0.320\n1 0.690\n\nI CalculaI\ntion\nI\nI 0.014\nl\n0.060\nI 0.068\n1 0.117\n1 0.261\n\nI Interpolation\nI Type 1 1 Type II\n\n0.011\n\nI RelaxaI\ntion\n1 0.010\nI 0.179\n\nI\nI\n\n1.257\n\nl\nI\n\nI\n\n2.805\n\n1 1.017\n\nI InitialiI ration\n\n0.554\n\n0.160\n\n309\n\n0.497\n\n0.011\n0.049\n0.053\n0.095\n0.194\n0.357\n\nI\n1\n1\n1\nI\n1\n\nI\n\n0.024\n0.028\n0.053\n0.141\n0.494\n0.740\n\nI 5.875 1\n\nI\n\nI 0.759\n\nI\n\nI\n\nI\n1\n1\nI\n1\nI\n1\nI\nI\n\nI\n\n1\n1\nI\n1\nI\n1\n\nI\n\nI\n\nTotal\n0.010\n0.215\n0.308\n0.372\n0.691\n1.674\n2.605\n\nI\nI\nI\nI\n1\n1\nI\nI\n1\nI\n\nIn Table 3 we have measured the time in microseconds for each time a\nprocedure is executed for a given level,\naccompanied\nby the number of times\nthe procedure is performed.\nIt should be noted here that when level 1 is\ninvolved in any of the procedures a scalar code was used, since it has only\nAgain, the effect of vector lengths is such that the\none interior\npoint.\nlevel 3 relaxation\nis comparable to that of level 2, for example.\nOnly\nwhen we get to the finest grids do we observe timing ratios which\ncorrespond to the ratios of the number of elements processed.\nThe reader\nshould be reminded that the average time of the relaxation\nprocedure is aot\nsince some relaxations\nare not quite "complete" as was\nfully accurate,\nafter Type II interpolation\nand after\nexplained ia Section 3 (i.e.,\nThe residual calculatim\ntakes longer than the\nresidual calculation).\nrelaxation\n(in contrast\nto the scalar case), which is understandable\nfrom\nthe discussion\nin Sections 3 and 4.\ncount sad average times ia microseconds per\nTABLE 3. Procedure-calls\nBreakdowa by levels for the 129 by 129 problem with starting\nlevel\ncall.\nSome of the relaxations\n\nNote:\nI\n\nI\nLeVd\nI\nI 1 (3x3)\nI 2 (5x5)\nI 3 (9x9)\n\n1\n1\n1\n1\n\n4\n5\n6\n7\n\n(17x17)\n(33x33)\n(65x65)\n(129x129)\n\nare not "complete."\n\n(See Section\n\n2.\n\n3)\n\nInterpolation\nI\nI\nI\nl.Belaxation\nI\nResidual\n1 Type I\nType II:\n1 No.\nI Time I No. I Time I No. I Time\nNO-!- Time\nI\n6 I\n1.7 I\nI\nI\n1\n;\n1\nI 18\nI\n9.9\nI\n6\n2.3\n6\n1.8 1\nI\n1 I 24.0\nI 15\nI\n10.7\nl\n5\nl\n12.0\nI\n5\nl\n9.8\n1\nI 12 I 15.8 I 4 I 17.0 l 4 l 13.3 l 1 I 28.0\nI9\nI\n35.6 \'I 3 I 39.0 I 3 I 31.7 l 1 l 53.0\nI 6 1 115.0 1 2 1 130.5 1 2 I 97.0 1 1 I 141.0\nl\n1\n1 494.0\n1 3 1 419.0 1 1 1 497.0 I 1 I 357.0\n\nI\nI\nI\nI\nI\n\n1\n1\nl\n\n1\n1\n\nTo conclude the performance discussion we vill\nmeation that the vectorired code executes about 15 times faster than the scalar version on the CDC\nCXBEB 205, and roughly 500 times faster than the CDC CYBEB 720.\nThe lesson from what was said above is that relaxations\nare relatively\n"cheap" in terms of execution times , and computations on the coarser grids\n(compared with the ratios found on scalar\nare realtively\n"costly"\nprocessors).\n7.\n\nCONCLUXNG9EMARKS\n\nOne important le\'ssoa, knowa very well to those involved ia vector\nis that it demands careful data structuring\nand analysis of the\nprocessing,\n"mapping" between the data and the operations\nto be performed, if the\nvector capabilities\nof the processor are to be efficiently\nutilized.\nWe\noperations-count\nas a measure\nhave also demonstrated that the traditional\nof processing time is not sufficient.\nOn a vector processor one has to\ntake into account\nthe\nnumber\nof vector operations\n(or\nthe lengths of the\nvectors) and the data-motion operations\n(which occur on a serial processor,\ntoo, but are often ignored when algorithms\nare evaluated).\nThe result of\nthe above is that one may have to re-examine the various parameters of the\nalgori4hm vhen migrating\nthe Multigrid\napplication\nfrom a serial to a\nvector processor.\nThis aspect requires further\ninvestigation.\n310\n\nVe feel\nthat\nthe experiment with the model-case studied in this paper\nwas successful and the performance achieved very pleasing.\nIt\ncertainly\nvarrants continuation\nuork.\nSome obvious areas we intend\nto engage in are\nthe following:\nExtending the application\nto three-dimensional\nPoisson\ncqua tioas\n; code a similar\napplication\nto cater for the, more general,\nDiffusion\nequation; and implement "full-weighting"\nresidual calculation\nand\ncubic interpolation.\nIn addition one may, of course, generalize\nthis uopk\nLn many directions.\nt\xc2\xb6ore general boundary conditions\n(Nemarm,\netc.) can\nbe implemented.\nThe solucioa of non-linear\nproblem8 (using PA8 multigrid\nversion) and systems of equations can also be vectorlted\nin a similar\nfashion.\nYore difficult,\nbut potentially\nimportant,\nis the exteneioo to\ngeneral drnnains, which vfll\nrequire a lot of thought about data structures\nand data motion.\nAs a last comment, it vlll\nk noted that all the timings\nquoted here were achieved using 64-bit arithmetic.\nOn the CDC CYBER 205\none can we 32-bit arithmetic\nas veil,\nand, thus, double the result rate\nfor vector\noperations Mile halving the memory requirements.\nFor the\nsmeller than truncation\nerrors in\npurpose of obtaining albebraic errors\nsolving second order equations,\nthe 32-bit arithmetic\nis indeed enough. We\nintend to examdne this option.\n\n[I.1\n\nA. Baadt,\nproblems",\n\n12.1\n\nV. Heckbusch and U. Trottcnberg,\ncd.,\n\xe2\x80\x9cMultigrid\nMethoda",\nProceedings of a Conference (Koln-Porz,\nNov. 1981), SpringerVerlag,\n1982.\n\n13.1\n\nK. Stuben, K. Trottenberg,\nalgoritbme,\nmodel problem\nI-176.\n\n\'Wa.lti-level\nadaptive solutions\nto boundarp-p8lue\nMath. Camp. 31, (1977), 333-390.\n\n"Multigrid\naa~lyris\n\n311\n\nand\n\nMethods: Fundamental\napplications".\nIn [21 pp.\n\n'