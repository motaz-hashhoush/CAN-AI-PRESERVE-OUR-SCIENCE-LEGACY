b'Differentially Variable Component Analysis (dVCA):\nIdentifying Multiple Evoked Components using Trial-to-Trial\nVariability\nKevin H. Knuth\xe2\x80\x99, Ankoor S. Shah2\xe2\x80\x993,\nWilson Truccolo4,Mingzhou Ding\xe2\x80\x99,\nSteven L. Bressler\xe2\x80\x99, Charles E. S ~ h r o e d e r ~ \xe2\x80\x99 ~\n1 Computational Sciences Division, Code IC, NASA Ames Research Center, Moffett\nField CA 94035\n2 Department of Neuroscience, Albert Einstein College of Medicine, Bronx NY 10461\n3 Cognitive Neuroscience and Schizophrenia Program, The Nathan S. Kline Institute,\nOrangeburg, NY 10962\n4 Neuroscience Department, Brown University, Providence RI 029 12\n5 Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca\nRatoii FL 3343 1\nPhone: +1-650-604-4279\nFax: +1-650-604-4036\nEmail: kevin.h.knuth@nasa.gov\nAbbreviated title: Differentially Variable Component Analysis (dVCA)\nNumber of Figures: 8\nNumber of tables: 2\nKey words: Source separation, ICA, EEG, MEG, ERP, evoked responses, Bayesian\nmethods\nOctober 8,2003\n\nSLWRY\nElectric potentials and magnetic fields generated by ensembles of synchronously active\nneurons in response to external stimuli provide information essential to understanding the\nprocesses underlying cognitive and sensorimotor activity. Interpreting recordings of\nthese potentials and fields is difficult as each detector records signals simultaneously\ngenerated by various regions throughout the brain. We introduce the differentially\nVariable Component Analysis (dVCA) algorithm, which relies on trial-to-trial variability\nin response amplitude and latency to identify multiple components. Using simulations\nwe evaluate the importance of response variability to component identification, the\nrobustness of dVCA to noise, and its ability to characterize single-trial data. Finally, we\nevaluate the technique using visually evoked field potentials recorded at incremental\ndepths across the layers of cortical area VI, in an awake, behaving macaque monkey.\n\n2\n\nINTRODUCTION\nThe field of neuroelectrophysiology relies on the analysis of electric potentials or\nmagnetic fields produced by the brain in response to sensory stimulation, or in\nassociation with its cognitive and/or motor operations. These signals arise from\ntransmembrane current flow produced by multiple ensembles of synchronously firing\nneurons. Far from being independent, these neural ensembles, also referred to as\ngenerators or sources, are often dynamically coupled in unknown ways that are of interest\nto the experimenter. Unfortunately, recording channels such as electrodes in\nelectroencephalography (EEG) and superconducting quantum interference devices\n(SQUIDS) in magnetoencephalography (MEG) record linear mixtures of the activity from\nthese sources in addition to ongoing background activity and sensor noise. Thus, the\nindividual responses of each source are mixed within the recorded signal making it\ndifficult to identify them and study their dynamical interactions. Furthermore, it is\nstandard practice to enhance the signal-to-noise ratio by averaging event-related\npotentials (ERPs) over a number of experimental trials. However, implicit in this\nconstruction is the assumption that the evoked waveform is constant over trials and that\nany variability represents noise. In this practice, the possibility of assessing trialdependent effects in the data is sacrificed.\nThe last decade has seen great developments in linear blind source separation\n(BSS) and independent component analysis (ICA) techniques, such as Infomax ICA (Bell\n& Sejnowski, 1995), FastICA (Hyvarinen & Oja, 1997), and second-order blind\nidentification (SOBI) (Belouchrani et al., 1993). These algorithms have been useful in\nidentifying sources in EEG and MEG signals using both ensemble-averaged data\n(Makeig et al., 1997; Sarela et al., 1998; Vighrio et al., 1999) and single trials (Jung et al.,\n1999; Cao et al., 2000; Makeig et al., 2002; Tang et al., 2002). However, with the\nexception of SOBI, the general assumption that the ERP sources are independent is\nphysiologically implausible. It is hard to argue that activity produced by two areas, such\nas V1 and V4, are independent when the areas are clearly structurally and functionally\ninterconnected. Most important, by assuming independence of the sources, the\nexperimenter assumes away one of the most interesting and important aspects of the\nbehavior of active neural ensembles in the brain: the nature of their dynamical\ninteractions.\nIn this paper we describe a more realistic model of the evoked response, which\nexplicitly acknowledges trial-to-trial amplitude and latency variability in the evoked\nresponses generated by each active source. Using this more realistic model of the ERP\nwe will derive the differentially Variable Component Analysis (dVCA) algorithm, and\ndemonstrate how different variability patterns in neural ensemble activity can be used to\nseparate and identi@ their component signals. Using simulations, we will evaluate the\nability of dVCA to characterize single-trial data, as well as its robustness to noise. Last,\nwe will demonstrate how to use dVCA by analyzing visual-evoked field potentials\nrecorded intracortically in macaque V 1 using a linear multi-electrode array. Throughout\nthis paper we will demonstrate that this approach allows us to: (1) more accurately\naccount for observed trial-to-trial variability, (2) utilize the differential variation in the\namplitudes and latencies to separate and identify sources, (3) avoid enforcing statistical\nindependence of the sources, and (4) more accurately estimate the ongoing background\nactivity.\n\n3\n\nMODELING EVOKED RESPONSES\nTrial-to-trial variability of evoked responses can conceivably take many forms: latency\nshifts, amplitude variation, and even waveshape changes. In our experience, much of the\nobserved variability can be described by amplitude and latency variations of a stereotypic\nresponse waveshape (Truccolo et al., 2002). For this reason, we model the response\nevoked from a single neural ensemble by assuming that the signal possesses a stereotypic\nwaveshape that can vary in amplitude and onset latency from trial to trial. We write the\n)\nresponse evoked in a given trial mathematically as a s(t - z , where the function s(-)\nrepresents the waveshape of the response as a function of elapsed time t , a represents\nthe trial-specific amplitude scaling factor of the response, and z represents the trialspecific onset latency shift. Furthermore, when multiple neural ensembles are engaged in\nthe response to a stimulus, the activity of each ensemble is represented in the model as a\nseparate waveform with a distinct trial-specific amplitude and latency shift. It is\nimportant to note that by modeling both the waveshape and its amplitude and latency,\nthere is degeneracy in the model as an overall change in amplitude scale or latency shift\ncan be described either by the amplitude and latency parameters or by the overall scale\nand shift of the waveshape. To eliminate this degeneracy, we take as a convention that\nthe ensemble average amplitude scaling factor of the response over the recorded trials is\nunity, < a > = 1, and the ensemble average latency shift is zero, < z > = 0.\nIn many experimental paradigms, investigators use several detectors positioned at\ndifferent locations to measure the evoked responses of neural ensembles. The degree to\nwhich a detector records a signal evoked by a particular source depends on many factors\nincluding the relative position and orientation of the source relative to the detector. To\ndescribe this source-detector coupling, we introduce a coupling matrix C ,where the\n,\nmatrix element C describes the degree to which the mth detector detects the nth source.\nThis coupling matrix is known as the mixing matrix in the source separation literature\nand as the lead-field matrix in electrophysiology.\nDuring the course of an experiment the investigator records responses to multiple\npresentations of a stimulus. Each presentation is typically called a trial. For the r\n"\nrecorded trial, we model the data x ( t ) recorded by the mth detector in component form as\n\n+\nn=l\n\nwhere \',,(t> describes our model of the recorded data x,,(t) , n indexes the N neural\nsources activated by stimulus presentation, C,, is the coupling between the mth detector\nand the nth source, anr is the amplitude scale of the n source during the rthtrial, z, is\n"\n,\nthe latency shift of the n source during the rthtrial, s,(-) is the waveshape of the n\n"\n"\nsource, and qm,(t) is the un-modeled part of the data recorded in the mth detector during\nthe Y* trial. The un-modeled part of the dzta record is typically a combination of the\nrecorded background activity along with any noise in the detector. For simplicity, we\nassume that qmr(t) has zero mean. Thus (1) describes the data recorded in a given\ndetector during a given trial as a sum of the signals generated by each of the N neural\n\n4\n\nsources appropriately scaled in amplitude and shifted in latency for that trial and also\nscaled according to the coupling between each source and that detector plus the signals\nthat we do not yet understand or care to understand. We call (1) the multiple component\nevent-relatedpotential (mcERP) model of evoked activity.\nBy adopting the mcEW model of the evoked responses, we implicitly adopt a\nwell-defined set of characteristics capable of describing a neural source. The term\ncomponent refers to the stereotypic waveshape describing the temporal activation pattern\nof a particular neural source. As no information regarding the spatial locations or\ndistributions of the neural sources has gone into the model, this model does not\ndistinguish between two spatially distinct groups of neurons that produce the same\nstereotypic waveshape varying identically with latency and proportionally in amplitude.\nHowever, in such a situation, examination of the estimated coupling matrix would reveal\ntwo spatially distinct sources if there are detectors positioned within the proximity of\neach source. The major advantage obtained by estimating the coupling matrix is that\npractical experience in conjunction with previously obtained physiological or anatomical\ndata suggesting source distributions can be utilized to independently evaluate the\nsolutions obtained with this technique. The disadvantage, aside from withholding\ninformation from the algorithm, is that there remain two degeneracies in the model. First,\nthere is no specified order to the sources. Second, there is no specified scale for the\ncoupling matrix; one could halve the coupling matrix elements while doubling the\nmagnitude of the source waveshapes and obtain an equally valid solution. These\ndegeneracies, which are present in every other linear source separation algorithm, pose no\ndifficulties to the interpretation of the results. In our implementation we have chosen to\nnormalize the columns of the coupling matrix so that the maximum value is equal to one.\nHowever, it should be noted that this scaling degeneracy could be remedied by adopting a\nphysical model of the source currents (Knuth & Vaughan, 1998).\nOne last notable benefit of the mcERP model is that no component waveform is\nrequired to be present in every trial. In other words, a single-trial amplitude of zero is\nallowable for any component. By not assuming a priori that identical neural processes\nare active during every trial of the experiment, one is able to investigate the possibility\nthat different processing strategies are used during the course of the recordings.\n\nALGORITHM DEVELOPMENT\nBayes\' Theorem is the natural starting point for explaining the dVCA algorithm because\nit allows one to describe the probability of the model in terms of the likelihood of the data\nand the prior probability of the model parameters\np(model I data, I ) =\n\np(data I model, I ) p(mode1 I I )\n9\n\nP(dat4 I )\n\nwhere I represents any prior information one may have about the physical situation. The\nprobability on the left-hand side of (2) is referred to as the posterior probability. It\nrepresmts the probability that a given set of hjpthesized values of the model parameters\naccurately describes the physical situation in which the data were collected. The first\nterm in the numerator on the right-hand side is the likelihood of the data given the model.\nIt describes the degree of accuracy with which we believe the model can predict the data.\n\n5\n\nThe second term in the numerator is the prior probability of the model, or the prior. This\nprior describes the degree to which we believe the hypothesized values of the model\nparameters to be correct based only on our prior information about the problem. The\nterm in the denominator is called the evidence and as we will see in this problem, it acts\nonly as a normalization factor. It is through the assignment of the likelihood and the\npriors that we express all of our knowledge about the particular source separation\nproblem. Bayes\' Theorem can be viewed as describing how one\'s prior probability,\np(modeZ I I ) , is modified by the acquisition of some new information in the form of data.\nTo apply this theorem to our problem, we consider the change in our knowledge\nabout the model with the acquisition of new data, which consists of the set of recorded\ndata x(t) from A detectors over the course of R trials. In this case, Bayes\' Theorem can\n4\nbe written as\n\nwhere boldface symbols represent the entire set of parameters of each type in the mcERP\nmodel, eg. a = { a,,a2,..., aR}. The most probable set of model parameters maximizes\nthe probability in (3), and thus in practice the equation can be expressed as a\nproportionality with the inverse of the evidence p(x(t)I I ) as the implicit proportionality\nconstant. Equation (3) then becomes\n\np(c,w, a, 7 I x(t), I )\n\np(W\n\nI c,s(t), a, 7, I ) p ( c , s(t), a, 7 I I ) .\n\n(4)\n\nFor simplicity, the joint prior can be factored into four terms each describing what\nwe know about the source-detector coupling, the source waveshapes, the single-trial\namplitudes and the single-trial latency offsets,\nP(C, s(t>,a, \' I x(G7 1)\nI\n\np(x(t>I\n\nc,s(t>,4 T I )P(C I I ) p(s(t>I I ) P(" I 1)P(\' I 1).\n\n(5)\n\nFor the amplitude and latency priors, p(a I I ) and p ( t I I ) respectively, we assign\nuniform densities with appropriate cutoffs denoting a range of physiologically realizable\nvalues. Note that a joint uniform density p(a, z 1 I ) can always be factored in this way.\nAs the amplitude and latency priors are each represented by a uniform density, we can\nabsorb those two factors into the implicit proportionality constant.\nOur derivation continues by utilizing the principle of maximum entropy to assign\na Gaussian likelihood (eg. Sivia, 1996; Jaynes, 2003) by introducing a parameter o\nreflecting the expected square-deviation between our predictions and the mean\n\nwhere p ( o I I ) is the prior probability for u. Q represents the sum of the square of the\nresiduals between the data and our model in (l), written\n\n6\n\n.\n\nwith M representing the number of detectors, R the number of experimental trials, and T\nthe number of recorded time points per trial. Some consider this a fancy way to state that\nthe noise is Gaussian distributed. However, as Jaynes demonstrates (2003), this is only a\nThus higher-order\nstatement that the variance of the noise is equal to some value 0 2 .\nstatistical structure in the noise is still acceptable and the noise does not have to be\nGaussian distributed.\nThe fact that we do not actually know the value of 0 is not a problem as we can\nobtain a conservative result by considering all possible noise levels by integrating the\njoint posterior over all possible values of 0 . Symmetry considerations require that we\nassign what is called a Jeffreys prior for o p ( o I I ) = 0 - l (Sivia, 1996). Performing the\n,\n,\nintegral of the joint posterior over all possible values of 0 we obtain a marginal posterior\nprobability for our original set of model parameters\n-MRT\n\np(CY ( 0 ,7 I W Y\n~ a,\n1)\n\nQ2 p(C I I ) p(s I I>,\n\n(8)\n\nwhich is related to the Student t-distribution (Student, 1908).\nIf we were more knowledgeable, prior information regarding the source\nwaveforms could be used to improve our inferences. In addition, knowledge of the\nsource-detector coupling, which is found by solving the electromagnetic forward\nproblem, could be utilized to create an algorithm that simultaneously performs source\nseparation and localization (Knuth, 1998; Knuth & Vaughan, 1998). For simplicity, in\nthis development we choose to assign uniform priors to p(C I I ) and p ( s I I ) , and absorb\nthe terms into the implicit proportionality constant. It is more convenient to work with\nthe logarithm of the posterior probability ( S ) , which can be compactly written as\n\nMRT\n1nQ\n1nP = -2\n\n+\n\nconst ,\n\n(9)\n\nwhere P is the posterior probability p(C, s(t),a,z I x(t), I ) .\nAn iterative algorithm to identify mcERPs, which we call dzferentially variable\ncomponent analysis (dVCA), is implemented by solving (9) for the most probable set of\nmodel parameters. Such a solution is called the Maximum A Posteriori (MAP) estimate.\nThis most probable set of model parameters is represented as a peak in the space of\nposterior probabilities. At this maximum, the partial derivative of the posterior\nprobability with respect to any one of the model parameter values is zero. For this\nreason, our first step in deriving an optimal estimate of the component waveshape by\nequating each of the partial derivatives of (9) with zero. The partial derivative with\ncomponent waveshape at time q gives\nrespect to thej\xe2\x80\x99\xe2\x80\x9d?\n\nwith\n\nwhere\n\nThe term W is important as it deals with the data, which has been time-shifted according\nto the latency shift of the component being estimated, xmr(q+ z j r ) . From this, one\nsubtracts all other components after each has been appropriately scaled and time-shifted,\nC,, anr ( q -rnr + z,\') . The derivative of the log probability is zero when the scaled,\ns,\nestimated wavesha e equals W. Thus one can obtain an expression for the optimal\nwaveshape ofthejtRsowce zt time q in terns of the other sources\n\nm=l r=l\n\nSimilarly for the source amplitudes, one obtains an optimal estimate by\nconsidering the derivative of the log probability with respect to the amplitude of thejth\nsource during the p* trial, Setting this derivative equal to zero results in\n\nhjP\n\n-\n\nm=l t=l\n\n(14)\n\nm=l t = l\n\nwhere\n\nf\n\nn# j\n\nand\n\nV = Cmjsj(t-rjp),\nsuch that the solution is given by the projection of the detector-scaled component\nC , s (t - rJp onto the data after removing the other scaled, time-shifted components.\n)\nThis can be viewed in terms of a dot product, which is related to a matching filter\nsolution.\nThe optimal source-detector coupling coefficients are found similarly with\n\n8\n\nr=l t=l\n\nwhere\n\nx=\nand\n\nY = aJr sJ. ( t - z j , )\n.\nEstimating the latency shift of thej* source during the pfh\ntrial using the approach\ntaken for the other parameters leads to a complex solution as the latency appears\nimpiicitiy as the argument of tine waveshape function. instead we examine tine necessary\nconditions for maximizing the quadratic form Q. Expanding the square in (7), one can\nsee that as the latency shift z , ~ is varied, only the cross-terms corresponding to thejth\nsource change as long as the source waveshapes are zero outside of a closed time interval.\nThe optimal estimate of the latency shift tIp be found by maximizing the crosscan\ncorrelation between the estimated source and the data after the contributions from the\nother sources have been subtracted such that\n\ntJp=\n\nargmaxZ(zJp)\n\nwhere\n\nIn practice, as a discrete model is being used for the component waveshapes s ( t ) , we\nutilize a discrete set of latency shifts with resolution equal to the sampling rate.\nPerhaps the greatest challenge is determining the number of sources warranted by\nthe data. In our investigations to date we have been focused on understanding the major\nsources responsible for the recorded data. This typically entails first modeling a single\ncomponent and examining the un-modeled residual signal in detail. We then attempt to\nmodel the data with two components and so on to greater numbers of sources. As we will\ndemonstrate, the responses we are examining are sufficiently complex that this approach\nrapidly reveals previously unknown characteristics of the neurophysiology. It is\nhowever, straightforward to apply more analytical methods of determining the\nappropriate numbers of sources. One such method is the Akaike Information Criterion\n(AIC) (Akaike, 1974; Victor & Canel, 1992). Increasing the complexity of the model by\nadding sources will always describe the data better and will increase the likelihood of the\nsolution. However, by adding sources we increase the model complexity as we now have\nmore model parameters. The idea is that a balancing point is reached when the increase\n9\n\nin likelihood is no longer worth the increase in model complexity. This point represents\nthe optimal number of sources. For N sources in dVCA, the AIC is\nAIC(N) = MRTlog(Q)\n\n+\n\nC*\n\n(NT + 2NR\n\n+N2).\n\n(22)\n\nThe first term is minus one times the logarithm of the likelihood. It is a data-dependent\nterm, which decreases with an increasing number of sources due to the fact that more\nsources can better describe the data resulting in lower residual variance Q. The second\nterm is a model-dependent term, which is a constant times the number of parameters in\nthe model. This term increases with the number of sources. The constant c2 is the tstatistic, with c 2 = 4 corresponding to a conservative p = 0.95 . The solution for N\nsources is statistically significant ( p I ) when AIC(N) < AIC(N - 1) ,which is\n0.05\nequivalent to\nlog(Q,-,)\n\n- log@,)\n\n> 4\n\n(T + 2R + 2N -1)\nMRT\n\ne,,-,\n\nrefer to the residuals of the model in (7) with N sources and\nNote that QN and\nN - 1 sources respectively. While this is an attractive technique, we have found that the\nnumber of sources warranted by this criterion is far greater than those we have been able\nto fully understand by analyzing the data manually source by source.\nThere are many ways that one can use the equations above to implement the\ndVCA algorithm. A useful iterative method (Figure 1) begins by modeling a single\nneural source with the single-trial amplitudes set to unity and the latency shifts set to\nzero.\n1.\nEvent-related potentials (ERPs) are computed for each detector by averaging the\ndata over all trials. The ERF\' for each detector is full-wave rectified, and its\nintegral (area under the curve) is approximated. The ERP having the largest area,\nor total signal content, is chosen as the initial approximation of the first\ncomponent waveshape.\nThe single-trial amplitude scales are all initialized to one and the single-trial\n2.\nlatency shifts are initialized to zero. This is consistent with the implicit\nassumptions of the ERP.\nThe coupling matrix is estimated using (1 7).\n3.\n4.\nSingle-trial latency shifts are estimated using (20).\nSingle-trial zimplitudes are estimated using (14).\n5.\n6.\nEquations (13), (17), (20), and (14) are then iterated until the average change in\nthe waveshape from the previous iteration is less than 1% or until a maximum\nnumber of iterations has been performed.\n7.\nAt this point the residual signal for each detector is computed by subtracting the\nmodel Gom the data, as in the argument of (7). The residual signals are then\naveraged across trials to obtain a residual ERP for each channel.\n8.\nIf N 2 2 , the AIC criterion is applied. If satisfied (or if N = l), another source is\nadded and modeled.\n\n10\n\nThe initial approximation of the next component waveshape is chosen to be the\nresidual ERP of the detector having the largest total signal.\nThe single-trial amplitudes and latency shifts of the new component are set to one\n10.\nand zero, respectively.\nEquations (13), (17), (20), and (14) are used to obtain the parameters\n11.\naccompanying the new component in addition to refining the estimate of the first\ncomponent. The set of equations is further iterated to refine the solutions until the\naverage changes in each component waveshape is again less than 1% or until the\nmaximum number of iterations has been reached.\nAdditional components are added until the AIC criterion fails or the investigator\n12.\nchooses to stop.\nThis implementation represents only one possible approach to utilizing these equations to\nobtain useful solutions.\n\n9.\n\nSIMULATIONS\nThe dVCA algorithm was evaluated using synthetic data to demonstrate the utility of\namplitude and latency variability in the identification of multiple evoked components and\nalso to assess the robustness of the algorithm in the presence of noise. We simulated\nelectric field potentials recorded from a linear-array multielectrode with 15 channels\nspanning the cortical laminae in V1. Specifically, we designed three synthetic ERP\ncomponents (Figure 2A) sampled at 2 kHz to approximate the neural ensemble response\nto diffuse red-light stimulation in macaque V1 (Givre et al., 1995; Mehta et al., 2000).\nFigure 2B shows the field potentials derived from the noise-free synthetic data as the\ndetectors in the multielectrode would record them. Superimposing the field potentials\nfrom the three sources and approximating the second spatial derivative of this summed\nactivity yields the current source density (CSD) profile shown in Figure 2C. The value of\nthe CSD technique is evident as it both localizes the neural .activity at the current sources\nand sinks, and eliminates volume-conducted activity (Le. see c3 below).\nThe first component, cl, represents the initial biphasic activation in lamina 4,\nfollowed by the second component, c2, representing activation in the supragranular\nlayers. The third component, c3, represents a far-field source that volume conducts to the\nelectrode channels and is observable in the field potential waveforms (Figure 2B), but it\nis absent from the CSD plot (Figure 2C) as the coupling between this component and the\nchannels is nearly constant (linear with a small slope). The spatial distribution of\ncomponent amplitudes across the electrode array is defined by the coupling matrix, which\nwas chosen to simulate the expected spatial distributions of the neural ensembles (current\nsource-sink pairs) located in lamina 4, in the supragranular layers, and at a distant site.\nWhile this is a simplistic model, it captures features we expect to see in real recordings.\nIn the majority of simulations, uncorrelated Gaussian-distributed, additive noise\nwas introduced, as in (1). The signal-to-noise ratio ( S N R ) is different for each\ncomponent from trial to trial because the three simulated components have different\nsingle-trial amplitudes. For this reason, we specify the trial-average S N R for each\ncomponent. As the mean amplitude scale of the comFonents is set to one, this is easily\ncomputed from the standard deviation of the component waveform divided by the\nstandard deviation of the additive noise,\n\n11\n\nwhere ocomponent = 0.357, 0.072, and 0.260 for c l , c2, and c3 respectively. These o\nvalues were calculated by taking the product of the standard deviation of the original\ncomponent waveshapes and the absolute value of the maximum coupling coefficient for\nthat source. Thus the SNRs reported in this paper represent the maximal SNR for each\nparticular component.\nThe performance of the dVCA algorithm was evaluated in several ways. First,\nthe ability of dVCA to separate the three mixed signals was measured using the Amari\nerror (Amari et al., 1996), which derives from the fact that the estimated coupling matrix\ncan be determined to within a scaled permutation of the true coupling matrix\n\ne\n\nc\n\n=\n\ncxn,\n\n(25)\n\nwhere Z is a diagonal scaling matrix and Il is a permutation matrix. Here the quantities\ndecorated with a carat denote estimated quantities and those without a carat denote the\nunknown true values. Ideally, the source estimates are therefore related to the original\nsources by a simple matrix transformation\n\nwhere s is the NxT matrix of the original source waveshapes, is the NxT matrix of the\nestimated source waveshapes, and M is a transformation matrix found by\n\nThe deviation of M from the ideal form suggested in (27) provides a measure of the\nquality of separation. We estimate M from (26) by\n\nand compute the Amari error by summing the rescaled cross-terms, which describes the\ndegree of component mixing\n\nNote that we have normalized the Amari error to have a maximum value of one rather\nthan a number dependent on the number of sources.\nSecond, the ability of dVCA to estimate each component waveshape was\nevaluated by calculating the fractional RMS error of the estimated waveshape as\ncompared to the correct waveshape. Note that before this comparison could be made the\n12\n\nestimated components G(t) needed to be rescaled and permuted because of the\nindeterminacy described by (25). For thej\' component\n\nLast, the accuracies of the amplitude and onset latency estimates for the j"\ncomponent were evaluated by computing the deviation from the correct values for the\nentire set of single-trial estimates using\n\nand\n\nand then computing the range of values contained within both the 68" and 95fh\npercentiles. This not only provided a measure of accuracy, but also allowed us to\ncompare the advantage of employing dVCA over the standard technique of averaging.\nThis comparison is made by noting that with a trial-to-trial variability summarized by a\nstandard deviation o, the error one obtains by working with the average ERP is greater\nthan or equal to the level of the variability 0. Thus errors in the dVCA estimates below\nthe standard deviations of the variability represent a significant improvement over the\nstandard assumption that amplitudes and latencies do not vary.\n\nEFFECT OF AMPLITUDE VARIABILITY\nWe first demonstrated that amplitude variability aids in the separation process. Eleven\nexperiments using synthetic data, each consisting of 50 simulated trials from the source\ncomponent configuration described above, were performed where the degree of\nvariability of the single-trial amplitudes was controlled. To generate the synthetic data,\n50 single-trial amplitudes for each component were randomly sampled from a log-normal\n=\ndistribution with a sample mean pomp1.0 and sample standard deviations of\noamP(0, 0.0625, 0.125, 0.1875, 0.219, 0.25, 0.375, 0.5, 0.625, 0.75, l.O} for each of\n=\nthe 11 sets of synthetic data. As latency variability was not simulated in this set of\nexperiments, the single-trial latency parameters were set to zero (z,, = 0). Given the\ncomponent waveshapes, the coupling matrix, and the single-trial amplitudes and\nlatencies, equation (1) was used to generate the synthetic data. A standard deviation of\ncnorse was used for the noise resulting in SNRs of 4.3 dB, - 9.6 dE3,1.5 dl3 (1.64,\n= 0.217\n0.33, 1.18 in terms of standard deviation ratios) for c l , c2, and c3 respectively. The\ndVCA algorithm was used to estimate the coupling matrix, component waveshapes, and\nsingle-trial amplitudes and latencies from the synthetic data. As the true parameters were\nknown, the performance of the algorithm was evaluated as previously described.\nWithout amplitude variability, dVCA was unable to separate the components as\ndemonstrated by the high Amari error of 0.219 (see Figure 3A). A small degree of\n13\n\namplitude variability, oUmp\n= 0.25, renders the problem solvable as demonstrated in\nFigure 3A, where the Amari error drops below 0.05 corresponding to about a 10%\nfractional RMS waveshape error (about a 23 dB S N R ) for three equal variance\ncomponents, and remains relatively constant with increasing variability hovering about\nan average of 0.028. Figure 3B shows the dramatic improvement in the waveshape\nestimation as quantified by the RMS errors, which rapidly drop to levels commensurate\nwith the SNRs of the individual components. The effect of amplitude variability on the\nsingle-trial amplitude estimates (not shown) remained relatively constant for oump\n1 0.25\nwith mean standard deviations of the errors in the single-trial amplitude estimates of\n0.014, 0.076 and 0.010 and for cl, c2, and c3 respectively. This is well below the\nstandard deviation of the amplitude variability. Last, even though the true onset latencies\nwere set to zero in these simulations, dVCA still estimates these quantities. The errors of\nthe onset latency estimates (not shown) also remained relatively constant with respective\nmean standard deviations of 0.417, 2.059, and 1.000 ms.\n\nEFFECT OF LATENCY VARIABILITY\nNext we demonstrated that latency variability also aids in the separation process. Eight\nexperiments using synthetic data, each consisting of 50 synthetic trials from the source\ncomponent configuration described above, were performed where the variability of the\nsingle-trial latencies was controlled. In these experiments there was no amplitude\nvariability (a,, = 1). To generate the simulated data, 50 single-trial latencies were\n=\nrandomly drawn from a Gaussian distribution with sample mean plUl 0 and sample\n=\nstandard deviations of olul (0, 1.25, 2.5, 3.75, 5.0, 6.25, 7.5, lO.O} ms for each of the\neight simulations. The same noise variance was used as in the amplitude variability case.\nAmari error was found to decrease with increasing latency variability, Figure 3C,\n1\ndropping to below 0.05 with olUt7 . 5 ms. Component waveshape estimation was also\nfound to improve with increasing onset latency variability, but the effect is not nearly as\ndramatic as in the amplitude variability case. Moreover, with the onset latency variability\nranges considered, the accuracy of the algorithm\xe2\x80\x99s estimates never attained the levels\nfound with amplitude variability. The most likely reason for this effect is that in terms of\nsignal amplitude, amplitude variability is a first order effect whereas latency variability,\nwhen written as a Taylor expansion, is a second order effect dependent on the first\nderivative of the waveshape. While the amplitudes were all set to one in the simulated\ndata, dVCA still estimates these quantities. Again, these amplitude estimate errors were\n27.5 ms with the mean standard deviation of the errors in the estimates\nlow for olor\nequal to 0.017, 0.077, 0.01 1 for c l , c2, and c3 respectively, which is on the order of the\nerrors seen in the amplitude variability trials. Onset latency estimate errors also remained\nrelatively constant with standard deviations of 0.250,2.250, and 1.142 ms respectively.\n\nSENSITIVITY TO ADDITIVE WHITE GAUSSIAN NOISE\nIn this first noise study we examined the robushess cf dVCA in the fixe of additive\nwhite Gaussian noise by simulating twelve different noise levels. Each simulation relied\non 50 trials of synthetic data where variability in both the amplitudes and latencies of the\ncomponents was modeled. Variability ranges were in accordance with those observed in\n\n14\n\nour previous investigations (Truccolo et al. 200 1). Single-trial amplitudes were drawn\n=\nfrom a log-normal distribution with sample mean pump 1.0 and sample standard\ndeviation camp\n= 1.0. Single-trial latencies were drawn from a normal distribution with\n=\n=\nsample mean ,uta, 0 and sample standard deviation otut 10.0 ms. The synthetic signal\nfrom each electrode (detector) was contaminated with a unique white Gaussian noise\nwaveform as specified in (1).\nThe component specific SNRs and resulting Amari errors, listed in Table 1 and\nshown in Figure 4A, indicate a relatively smooth increase in Amari error with decrease in\nc l S N R . However a significant jump in error occurs as the SNR of c2 passes below -25\ndB (cl SNR = -1 1 dB) suggesting that an SNR level of about -25 dB may denote a\ntransition from a useful data set to a prohibitively noisy one. These results can be\ncompared with the waveshape fractional RMS errors, which grow exponentially with\ndecreasing SNR (Figure 4B). The errors reach 1.0, signifling that the deviations in the\nestimates are on the scale of the waveshape itself, at about -23 dB to -25 dB for the\nlocalized components and about -31 dB for the far-field component (cl SNR = -28 dB).\nFigure 4E shows waveshape results for four different noise levels providing a better idea\nof the quality of the separation under these conditions. Most noteworthy is the fact that\nthe majority of the waveshape error is due to the high-frequency contamination of the\nGaussian noise rather than mixing of the components. Much of this could easily be\nimproved by incorporating a prior probability describing the expectation that components\nshould be slowly varying with respect to typical sampling rates, which would effectively\nfilter the results. As we will demonstrate, explicitly filtering a real data set can remove\nimportant signals, and alter others (Mocks et a]., 1986; Bogacz et al., 2002). Regardless,\nsome distortion in the positive phase of cl can be seen at a S N R , = - 10.8 dB , which is\nmuch more easily noticeable at S N R , =-22.8 dB where only c l and c3 remain\ndetectable. The improved accuracy in the estimation of the far-field component c3\n(illustrated in Fig. 4B) is most likely due to the fact that each electrode in the array\nprovides information about the far-field source, whereas for local sources there are\ndetectors with small signals.\nNext we examine the quality of the single-trial amplitude estimates. Figure 4C\nshows the relationship between the errors of the estimates and the range of variability of\nthe amplitudes. The performance of dVCA exceeds that of signal averaging when the\nestimation errors are less than the degree of variability in the original single-trial\namplitudes. This is because by simply averaging the responses, one implicitly assumes\nthat the single-trial amplitude is consistently equal to the mean resulting in errors equal to\nthe degree of variability. The horizontal black lines in Figure 4C represent the degree of\nvariability of the component amplitude as one standard deviation of the single-trial\namplitudes about their mean. The solid blue and dashed red curves represent one and two\nstandard deviations, respectively, of the single-trial amplitude estimate errors. Again, the\nfar-fi.eld estimates (c3) were more accurate than those of the local sources (cl and c2).\nNinety-five percent of the amplitude estimates were within the degree of variability down\nto SNRs between -1 8 to -23 dB with 68% of the estimates within the range well down to\n-23 to -25 dB, which is consistent with the performance indicated by both the Amari\nerror and the waveshape RMS error. To demonstrate the quality of the amplitude\nestimates, Figure 4F shows scatter plots of the true amplitude scales versus the estimates\n\n15\n\nfor c l at the same four SNR levels as in Figure 4E. Note that different values for the\nsingle-trial amplitudes were used in each simulation. Correct estimates will lie on the\ndiagonal, whereas incorrect estimates are off diagonal. While the errors in the values of\nthe amplitude estimates become unacceptable around -22.8 dB the pattern still exhibits a\nstrong correlation ( r = 0.948).\nFigure 4D shows the behavior of the onset latency estimates, which are less well\nestimated than the amplitudes. In addition, the degradation of the quality of the far-field\nestimates was not noticeably different than those of the local sources with 95% of the\nestimates being within the range of variability down to an SNR of -8 to -12 dB, and 68%\nof the estimates within the range of variability down to -20 to -25 dB. Figure 4G shows\nscatter plots of the true onset latencies versus the estimates for c l . The diagonal pattern,\nwhich indicates a high level of predictability, is almost lost by -22.8 dB ( Y = 0.392 ).\nNote that due to the difference in variability levels of the amplitudes and onset latencies,\nthe distribution of points in Figure 4F should not be directly compared to those in Figure\n4G as they are effectively at different magnifications.\n\nSENSITIVITY TO CORRELATED FAR-FIELD NOISE\nIn an effort to more accurately simulate the conditions that may be experienced during a\nreal experiment, we designed a second noise study to determine the effect of ongoing\ncorrelated far-field activity on dVCA performance. The ongoing far-field activity was\nmodeled as a time-series with a l/f spectrum so that correlations would exist at all time\nscales. To simulate its far-field nature the ongoing activity was coupled identically to\neach detector.\nAgain twelve levels of the ongoing noise amplitude were tested; however, in this\nstudy Gaussian noise was not added to the individual electrodes. The specific noise\nlevels, SNRs of the three components and resulting Amari errors are listed in Table 2.\nFigure 5A shows the Amari error smoothly increasing with decreasing S N R . The Amari\nerror reaches the same level of error as in the Gaussian noise case with 20 dB less noise\namounting to large errors around a c l SNR of 0 to -5 dB. It is apparent that this type of\nnoise more severely limits the ability of dVCA to separate signals. Figure 5B shows the\ncomponent waveshape errors blowing up around -10 dE3, with the effect on the far-field\ncomponent c3 (red) being understandably catastrophic as both c3 and the noise are\ncorrelated across detectors. The amplitude errors reach unacceptable levels around -5 to\n-8 dB (Figure 5C). However, most interesting are errors of the onset latencies (Figure\n5D). The errors for the local sources reach high levels (68% of estimates being within\nthe range of variability) between -7 and -8 dB, whereas the errors for the far-field\ncomponent c3 are large across the entire SNR range considered with 95% of the estimates\nnever being within the range of variability. This is because the far-field noise is severely\ninterfering with the ability to identify the far-field component in any given trial.\nAPPLICATION TO REAL DATA\nTo further demonstrate the utility of dVCA, we applied the algorithm to signals recorded\nfrom primary visual cortex of a male macaque monkey during the cuing period of an\nearlier attentional study (Mehta et al., 2000). During data collection, the subject was\nrequired to make a discrimination between standard and target visual stimuli for a juice\nreward. The standard visual stimulus was a 10-ps red-light flash presented through a\n\n16\n\ndiffusing plate subtending a 20" visual angle, centered on the point of visual fixation,\nwhile the target was presented similarly but slightly differed in intensity. Stimuli were\npresented at irregular interstimulus (ISI) intervals (minimum of 350 ms, average of 2\nstimuli per second). Intracortical field potentials were recorded using a linear-array\nmultielectrode with 14 contacts, equally spaced at 150 pm, inserted into V1 and\npositioned so that the channels spanned all six laminae (see Figure 2A, middle). The\ncontinuous field potential record from all channels, incorporating the stimulus tags, was\nsampled at 2 kHz and recorded on a PC-based data acquisition system (Neuroscan, El\nPaso, Texas).\nThe signals examined with dVCA were recorded during 171 trial presentations of\nthe standard visual stimulus and were epoched from 0 to 300 ms (0 ms indicates stimulus\npresentation). No other pre-processing of the data was performed. The average field\npotential signals in each electrode contact were calculated and utilized to determine the\ncurrent source density (CSD) profile of these data (Figure 6A). The CSD profile was\napproximated using a 3-point, second-order difference of the field potentials (Nicholson\n& Freeman, 1975; Schroeder et al,, 1995). This profile is a useful tool because it indexes\nthe local regions of transmembrane current flow and eliminates volume-conducted\nactivity generated at a distant site, which often contaminates local field potential\nrecordings. Consistent with earlier studies reviewed by Schroeder et al. (1995; 2001), the\nvisually-evoked CSD profile sampled fiom V1 during this experimental session shows\nthat the earliest activation occurs in the granular subdivisions of Layer 4, which are the\nmajor target of thalamic afferents. A second focus of activity localizes to the\nsupragranular (laminae 2 and 3) layers and is thought to index a feedforward activation of\npyramidal cells by interneurons in the granular layer (Schroeder et al. 1990; 1991). Note\nthat as we are using a 3-point, second-order difference to approximate the CSD, the top\nwaveform in Figure 6A is associated with electrode channel 2 rather than channel 1.\nAlthough it is an attractive idea to apply dVCA as an automated computer\nprogram that analyzes the data and produces an answer (as we did with simulated data),\nwe demonstrate that it must be used manually as a tool to understand the data being\nanalyzed. To stress the importance of this, instead of following the flow chart exactly as\nin Figure 1, we first applied dVCA to the single-trial local field potential signals to\nextract a single component. As shown in Figure 6B, the CSD profile of this component\nillustrates a biphasic response localized in the granular layer suggesting that it represents\nthe initial response to the thalamic inputs. Not surprisingly, this waveshape and its\ndistribution are very similar to the CSD profile of the ensemble averaged ERP of Figure\n6A, as the initial approximation to the first component derives from the ensemble\naverage. However, note that it excludes much of the minor variations not associated with\nthis main activation pattern. In addition to providing information about the component\nwaveshape and its spatial distribution, dVCA provides information about the amplitude\nand latency shift of the component in each trial. Figure 6C shows the distribution of\nsingle-trial amplitude scales for this component. In accordance with the dVCA\nnormalization condition, the mean of the sample equals one ( p1 = 1). More importantly,\nthe distribution illustrates that there is very littie single-trial amplitude variability\n( CT, = 0.1044), which is on the order of 10% variation. In contrast, the distribution of\nsingle-trial latency shifts (Figure 6D) shows something quite unexpected - it is bimodal\nwith an early latency peak at -4.625 ms and a late latency peak at 2.125 ms with a\n\n17\n\nI\n\ndifference of 6.75 ms. Moreover, the ratio of late to early responses is 2.29:l (119:52),\nas defined by a - 1.25 ms cutoff between the two modes.\nWe performed several different analyses to confirm the existence of the bimodal\nlatency distribution displayed in Figure 6D. First, Figure 6E displays a colorized plot of\nthe actual, single-trial field potentials (FPs) recorded from electrode channel 10, which is\nlocated in the granular layer. Time runs along the horizontal axis, chronological trial\nnumber runs along the vertical axis, and color indicates the amplitude of the FP. The red\ndashes on the left side of the plot indicate trials for which dVCA determined the latency\nto be early. It is easily seen that these highlighted trials have a response onset before the\nlate trials, and it confirms that the bimodal latency distribution of the dVCA estimation\nconcurs with observations from the actual data. Second, we performed a crosscorrelation analysis between each single-trial waveshape and the trial-averaged, FP\nwaveform between 25 and 95 milliseconds (this time period captures the initial negative\ndeflection of the averaged FP waveform). This analysis also yielded a bimodal\ndistribution (not shown). Finally, we selectively averaged the early and late FP\nrecordings in channel ! I showed them to be signif;.cant!y different ir? both latency\n!\nand\nand waveshape (Figure 6F). The trial-averaged FP waveshape (black) lies in-between the\nearly (red) and late (blue) waveshapes. As expected, the minimum of the early averaged\nresponses occurs before the minimum of the late averaged responses (5.5 ms difference).\nWhile this time difference reflects the fact that these early responses precede the late\nresponses, it is however not as accurate as the dVCA estimate as it is derived from a\nsingle-time point rather than from the entire waveshape. It is also not surprising that the\nlate responses more closely resemble the ensemble averaged FP as they outnumber the\nearly responses by more than 2 to 1.\nFigure 6E illustrates one more interesting phenomenon: the early and late\nresponses tend to be grouped in chronologically in time, indicating that the effect is not\nrandom. This suggests that the two response modes may be related to the attentional or\narousal state of the subject.\nAs these two types of granular responses suggest different physiological\nmechanisms, we split the data set into two subsets: an early subset and a late subset. .We\nthen used dVCA to re-estimate the first component in each subset. Figures 7A and B\nshow the CSD of the first component from the early and late subsets, respectively. The\nonset latency of the prominent granular sink over source configuration was determined by\ndescending down the left side of the peak and identifying the point in time where five\nconsecutive previous data points were monotonically increasing. With this measure the\nresponse onset latency of the early subset (Figure 7A) was 28 ms, which is clearly earlier\nthan its counterpart in the late subset (Figure 7B), which was 42.5 ms, as indicated by the\ndrop lines. The waveshapes of the two types of Layer 4 responses differ significantly\nindicating a difference in activation patterns between subsets. This is also confirmed by\nthe different degrees to which this activation appears in the supragranular layers, as the\nlater responses seem to be more strongly coupled to the supragranular activation than the\nearly responses.\nAfter any analysis the resihal signals must be examined since they represent\nactivity that was not modeled by the algorithm. In Figure 7C we show the trial-averaged\nFP residuals for the early (red) and late (blue) subsets for odd-numbered electrode\nchannels. First, the black arrows indicate activation that is almost identical in both\n\n18\n\nsubsets (mean peak time at 37.75 ms for the early subset and 37.68 ms for the late\nsubset). Due to the timing and the laminar distribution of this field potential, we believe\nthat this negativity reflects the initial signal from the thalamocortical afferents. The early\nresponses onset just after onset of this thalamic signal, which occurs at about 25 ms,\nwhereas the late responses are delayed. After 50 ms the residual signals from the two\nsubsets diverge significantly. This difference is striking in channel 1 where the\noscillations are 180 degrees out of phase. Of significant interest are the ultra-high\nfrequency (160-220 Hz) oscillations (UHF) in the granular and the supragranular layers\nof the early subset (red arrow in channel 11). These UHF oscillations, which onset with\nthe initial granular response, are absent in the averaged residuals of late responses.\nTo verify that these UHF oscillations were not present in the late responses, we\napplied a Morlet-based wavelet transformation to the two residual subsets of channel 11.\nTo preserve information about time-frequency behavior of oscillations not precisely timelocked to the stimulus, we applied the wavelet transform to each single-trial separately\nand the wavelet results were then averaged. These results show that there is a burst of\nUHF activity ranging from about 160-220 Hz and occurring between 42-57 ms in the\nearly subset, which is represented as an \xe2\x80\x9cisland of activity\xe2\x80\x9d in the time-frequency plot\n(Figure 7D). Such an island of activity is completely absent in the late subset (Figure\n7E). This finding demonstrates the ability of dVCA to identify distinct physiological\nmodes of activation using the single-trial characteristics of the responses, even in\nsituations where the data are unfiltered.\nTo demonstrate how dVCA can be used to extract multiple components, we will\nnow focus on the late subset. A complete analysis of the entire data set will be published\nelsewhere. In this example we model three components (Figures SA, B, C). Components\n2 and 3, (c2 and c3) have been magnified by a factor of 2 to make them easily visible,\nand are thus not on the same scale as component 1 (cl), which is the largest response. To\nassure that the SNR is within the acceptable range for dVCA, we used equation (24) to\ncompute the SNR for each component in each channel. For a given component this is\naccomplished by estimating the standard deviation of the component in that channel and\nthe standard deviation of the residual signal in that channel. The average S N R for each\ncomponent was then computed by averaging its SNRs across channels. We found that all\nthree components are well within the range of acceptable performance with average\nSNRs of 1.59 dB, 1.68 dB,0.98 dB for c l , c2, and c3 respectively.\nFirst, c l is in all practical respects identical to the single component we\npreviously estimated from the late subset. The prominent granular sink over source\nconfiguration is activated at about 35.5 ms in this more detailed model. While the onset\ntime is easy to quantify in the large biphasic response, the more complex oscillatory\ncomponents required a more sophisticated onset measure. By fitting the pre-response\ninterval with a line, we computed the standard deviation of the pre-response signal\ndeviations from that line. Component onset was then defined as the first of 5 consecutive\ntime points where the component amplitude was greater than two standard deviations.\nThis was useful as we did not filter the data and low-frequency oscillations could produce\nconfounding baseline deviations. With this measure, we found small, but significant,\nactivations in c l as early as 26 ms (see drop line in Fig SA) occurring after the thalamic\nsignal at 25 ms and before the massive biphasic response at 35.5 ms.\n\n19\n\nA low frequency biphasic response is described by c2 (Fig 8B). Significant onset\nof this component occurs at 37 ms (see drop line), which is after c l begins the biphasic\nactivation in the granular layers. Note that the source and sink distribution of c2 is in\nsuperficial layers as compared to the small supragranular response of cl . At first glance\nit appears that the spatial distribution of c3 is the same as that of c2. However, while\nthese components may involve the same populations of cells, c3 also involves cells in the\ngranular layers. The initial activation of c3 is at 30 ms, but it is not until 45 ms when the\ncomponent begins to display a biphasic activation followed later by a slow wave.\nFigure 8D shows the single-trial amplitude histograms along the diagonal, along\nwith scatter plots showing the relationships between the single-trial amplitudes of the\nthree components. The initial granular response, c l , has the lowest amplitude variability\n( oamP, ), whereas the later supragranular responses display greater variability\n= 0.102\n( cramp* 0.248, crump3 0.362). Figure 8E shows the single-trial latency results along\n=\n=\n\nwith scatter plots depicting the trial-by-trial relationships between component latencies.\nComponent cl again displays the least variability ( cr\'la,l= 1.08ms), but in this case, c2\n=\n= 1.58\ndisplays greater variability than c3 ( oh129.85 ms, o,~,~ ms).\nCorrelations in the scatter plots between component parameters indicate dynamic\ninteractions among these components. Of the amplitude-amplitude interactions (Figure\nSD), we see that the amplitudes of c2 are correlated with c3 ( r = 0.495, p < 10-7), so that\nwhen c2 is larger than average, c3 is also larger than average. Such correlation might\nsuggest that these components have not been separated. However, note that the latency\nvariability of each component is very different (Figure 8E), and that there is little\ncorrelation between the latency of c2 and the latency of c3 ( r = 0.171 , p = 0.075).\nOf the latency-latency interactions, the largest correlation is between the c l and\nc2 latencies ( r = 0.327, p < 0.001 ), so that when c l is earlier than average, c2 is also\nearlier than average. More interesting are the amplitude-latency interactions. Figure 8F\nshows the two most probable relationships. First the amplitude and latency of cl are\ncorrelated ( r = 0.297 ,p < 0.002 ), so that when c l is early its response is smaller, and\nwhen it is late its response is larger. Such\' a self-interaction contains much information\nabout the underlying dynamics of the neural response represented by cl. More apparent\nis the relationship between the amplitude of c l and the latency of c2\n( r = 0.483 , p < lo-\'). In this case, when.the c l granular response is larger than average,\nc2 onsets later than average. This result is somewhat counterintuitive, as generally we\nexpect that if the c l is driving the c2, a larger c l might produce an earlier onset c2. The\ndifference between c l and c2 is further highlighted by noting that the amplitude of c i is\nanti-correlated to the latency of c3 ( r = -0.190, p = 0.048) (not shown).\n\nDISCUSSION\nIn this paper, our goal was not to understand the details of these responses in visual\ncortex and their dynamic interactions, but merely to describe how tec\'miques i k e d V C k\nnow enable neuroscientists to tease apart and study these dynamic interactions among\nneural ensembles during cognitive or sensorimotor processing. Detailed studies of the\nsingle-trial interactions among neural components in a variety of experimental situations\n20\n\nwill provide much insight into the information processing strategies employed by the\nvarious cortical areas and by the brain as a whole.\nMaximum likelihood techniques have been used previously to approach the\nproblem of trial-to-trial variability of evoked responses. These past works have relied on\nsingle-component signal models that incorporate latency variability (Woody, 1967; Pham\net al., 1987), and more recently both amplitude and latency variability (Jaskowski &\nVerleger, 1999) to characterize evoked responses. Multiple-component models were\nintroduced by Lange et al. (1997) by adopting a template model of the ERP waveshapes,\nand were used to characterize both the amplitude and latency of multiple components in\nsingle-channel EEG recordings. Early versions of the dVCA algorithm also dealt with\nmultiple-component, single-channel estimation (Knuth et al., 2001; Truccolo et al., 200 1;\n2002; 2003), but instead employed a completely general waveshape model that did not\nrely on a given functional form. The dVCA algorithm presented here allows one to use\nmultiple channel recordings to identify and characterize multiple ERP components in the\nsingle trial by taking advantage of the trial-to-trial variability.\nThe dVCA algorithm was derived by approaching the problem as an exercise in\nBayesian parameter estimation. There are several advantages to a Bayesian approach.\nFirst, the strategy is model-based in the sense that given a quantitative model of the\nphenomena of interest, probability theory can be used to estimate the values of the model\nparameters from the data. Any failure in the algorithm can be traced back to either\ninadequacies of the model to represent the physical situation, assumptions or\napproximations made in its implementation, or a situation produced by an insufficiently\ninformative data set. Second, any inadequacies in the model, once identified, are readily\nremedied leading to improvements in the algorithm. This is the crux of the scientific\nmethod. Third, once the model component set has been adequately modeled, the residual\ndata can be examined to identify previously hidden phenomena. For example, in this\npaper we have discovered UHF oscillations associated with early initial granular layer\nresponses and absent in the more frequent late responses. By accurately identifying the\nevoked components in the single-trial epochs, one can more accurately study the ongoing\nactivity, which has been purported to contain signals important for communication\namong brain regions (Bressler et al., 1993; Singer, 1993; Truccolo et al., 2003),\nperception, working memory, and sensorimotor integration (Engel et al., 2001; Lee et al.,\n2003). Fourth, the Bayesian methodology allows one to incorporate additional prior\ninformation into the problem to improve one\xe2\x80\x99s inferences, which is a major advantage\nthat we plan to capitalize on in future work.\nThe dVCA algorithm itself has a number of technical strengths. The first is that\nneither the components, nor their underlying neural sources are assumed to be\nindependent of one another. This avoids the adoption of physiologically implausible\nassumptions and enables one to study the dynamical interactions among neural sources.\nThe second strength is that by taking into account the trial-to-trial variability in amplitude\nand latency, one is able to quanti6 the interactions among sources as well as study their\ntime-dependent properties over the duration of the entire experiment. Single-trial\nanalysis also makes it possible for dVCA to detect Components, which are not present in\nevery experimental trial thus allowing an experimenter to study cognitive or sensorimotor\nprocesses, which may employ different processing strategies at different times.\nAdditionally, one can use dVCA to study how attention, arousal, and varying disease\n\n21\n\nstates may modify neural responses on a single-trial basis. Finally, while no explicit prior\ninformation about the values of the model parameters was included in the development of\nthe algorithm, much prior information went into the choice of the model. This is in\ncontrast to other approaches such as ICA and other general BSS techniques, which strive\nto make very general assumptions about the model and the distributions of the parameter\nvalues (Knuth, 1997; 1999).\nWe have found that dVCA is robust in the presence of noise allowing accurate\nestimates of all parameters down to SNRs on the order of -20 dB for white Gaussian\nnoise and -7 dB for correlated far-field noise. The estimation of the far-field signals in\nthe presence of correlated far-field noise was difficult. This is undoubtedly due to the\nfact that the noise in this case possesses the same spatial distribution as the source\nmaking the two difficult to distinguish. In reality however, the behavior of the\nbackground noise will fall somewhere between these two extremes. When using dVCA,\nwe recommend that one calculate the S N R of the estimated components to assure that the\nalgorithm is operating in a regime where the quality of the parameter estimates is assured.\nIn addition, the spatial distribution of the sources across the array should be examined to\nassure that they are physiologically reasonable. We have been able to construct cases,\nwhich remain inseparable by the algorithm. This can typically be detected by examining\nthe CSD map of the estimated components across the array. In cases where two sources\nremain mixed, each are characterized by multiple sets of neural sources in identical\nlocations. However, we have found this to be more difficult to deal with in intracortical\nlaminar recordings as the cortical architecture only allows for a small number of current\nsources and sinks, and tight couplings between sources is extremely likely (Shah et al,\n2002). One possible solution to this problem is to restart the algorithm using starting\nconditions consistent with the source locations indicated by the CSD. The value of the\nposterior probability of these solutions obtained from different starting points can also be\ncomputed and compared (when using the same model order) to assure that one has the\nmost probable solution and is not merely stuck at a local maximum. Searching such\nenormous solution spaces is a difficult problem faced by all source separation algorithms.\nWe are currently working to improve the dVCA algorithm along several lines.\nFirst, there are often situations where the experimenter has knowledge about the forward\nproblem, which describes the propagation of the signals to the detectors. Such\nknowledge can be incorporated by adopting a more specific source model (eg. current\ndipole model) and expressing the coupling coefficients in terms of the new ERP source\nparameters, the detector coordinates, and head geometry (Knuth & Vaughan, 1998), or by\nusing information about the propagation law to derive appropriate prior probabilities for\nthe coupling matrix elements (Knuth, 1998; 1999). Similarly, mere detailed information\nabout the correlation structure of the background noise can be used to derive more\naccurate likelihood functions (Sivia, 2003). Second, by employing a discrete model of\nthe component waveshapes, we are restricted to estimating only discrete values of onset\nlatency shift. By adopting a waveshape model that relies on a set of continuous basis\nfunctions, continuous values of latency shift could be investigated. An example of a\ncontinuous time model is the frequency domain model of the ERP employed by Pham et\nal. (1987) and Jaskowski & Verleger (1999). In that model the waveshape is described as\na sum of a discrete set of sinusoids, which is continuous in the time domain, but discrete\nin the frequency domain. In principle, such a model allows one to describe latency shifts\n\n22\n\nI\n\nwith arbitrary precision. Third, this algorithm represents a MAP estimate based on an\niterative, or fixed-point solution. While each step in this algorithm has intuitive appeal, it\nis perhaps not the most effective means to obtaining a solution. Markov chain Monte\nCarlo with simulated annealing is well suited to performing simultaneous model selection\ncalculations and parameter estimations. We have briefly investigated such an approach\nonly to find it to be extremely time consuming given the large number of parameters in\nthe model. It is expected that a clever implementation based on the mcERP model and\nposterior probability derived in this paper would outperform the algorithm presented here\nby automatically identifying the number of components warranted by the data, avoiding\nlocal optimal solutions, and readily providing error bars for the results. Last, in principle\nthis algorithm is equally applicable to human scalp EEG and MEG data. In practice,\nthere are several challenges. The number of detectors utilized in these paradigms is\ntypically an order of magnitude greater than those we have demonstrated here. It is\ncertain that the algorithm in its present implementation will run more slowly. Also,\nwhole-head studies expose the experimenter to an order of magnitude more sources than\nwe work with in our intracortical recordings. This increases the possibility that the\ndVCA algorithm can become trapped in non-optimal local solutions. This of course is a\npotential problem for all source separation and localization techniques, and dVCA is no\nexception. We are beginning to examine the application of dVCA in human scalp\nstudies, and expect that if such pathological solutions are encountered, they can be\navoided by utilizing more sophisticated search algorithms.\n\nACKNOWLEDGEMENTS\nKHK supported in part by a NARSAD Young Investigator Award, the NASA\nIDUASKICT Program and the NASA Aerospace Technology Enterprise. AS supported\nby the Medical Scientist Training Program (T32M07288). CES supported by NIMH\nMH060358. SLB and MD supported by grants from NSF (IBN0090717) and NIMH\n(MH64204 and MH42900). We thank Drs. Ashesh Mehta and Istvan Ulbert for data\ncollection, Dr. Peter Lakatos for helpful comments and discussion, and Dr. Len Trejo and\nSam Clanton of NASA Ames Research Center for their assistance in optimizing the\ncoding of portions of the algorithm.\n\n23\n\nREFERENCES\nAkaike H. (1974). A new look at the statistical model identification. IEEE Trans.\nAutomat. Cont. 19(6), 716-723.\nAmari, S,, Cichocki, A. and Yang, H.H. (1996). A new learning algorithm for blind\nsignal separation. In Advances in Neural Information Processing Systems 8. D.\nTouretzky, M. Mozer, and M. Hasselmo, eds. (Cambridge, MA: MIT Press), pp.\n752-763.\nBell, A.J., and Sejnowski, T.J., (1995). An information-maximization approach to blind\nsource separation and deconvolution. Neural Comp 7, 1129-1 159.\nBelouchrani, A., Abed-Meraim, K., Cardoso J.-F., and Moulines, E. (1993). Second-order\nblind separation of correlated sources. In Proc. Int. Conf. on Digital Sig. Proc.,\nNicosia, Cyprus, pp. 346-35 1.\nBogacz, R., Yeung, N., Holroyd, C.B. (2002) Detection of phase resetting in the\nelectroencephalogram: an evaluation of methods, SOC.\nNeurosci. Abstr., Vol. 28,\nProg. No. 506.9\nBressler, S.L., Coppola, R., Nakamura, R. (1993). Episodic multiregional cortical\ncoherence at multiple frequencies during visual task performance. Nature 366,\n153-156.\nCao, J., Murata, N., Amari, S., Cichocki, A., Takeda, T., Endo, H., and Harada, N.\n(2000). Single-trial magnetoencephalographic data decomposition and\nlocalization based on independent component analysis approach. IEICE T Fund\nElectr 9,1757- 1766.\nEngel AK, Fries P, Singer W (2001). Dynamic predictions: oscillations and synchrony in\ntop-down processing. Nat Rev Neurosci 2,704-716.\nGivre, S.J., Arezzo, J.C., Schroeder, C.E. (1995). Effects ofwavelength on the timing and\nlaminar distribution of illuminance-evoked activity in macaque V 1. Vis Neurosci\n12(2),229-39.\nHyvarinen, A., and Oja, E. (1997). A fast fixed-point algorithm for illdependent\ncomponent analysis. Neural Comp 9, 1483-1492.\nJaskowski, P., Verleger, R., (1 999). Amplitude and latencies of single-trial ERP\'s\nestimated by a maximum-likelihood method. IEEE Trans. Biomed. Eng. 46, 98793.\nJaynes, E.T. (2003). Probability Theory - The Logic of Science. (Cambridge: Cambridge\nUniversity Press).\nJung, T.-P, Makeig S., Westerfeld M., Townsend J., Courchesne E., Sejnowski T.J.\n(1999). Independent component analysis of single-trial event-related potentials. In\nProceedings of the First International Workshop on Independent Component\nAnalysis and Signal Separation: ICA\'99. J. F. Cardoso, Ch. Jutten, Ph. Loubaton,\neds. (Aussois France: ICA\'99), pp. 173-8.\nKnuth, K.H. (1997). Difficulties applying recent blind source separation techniques to\nEEG and MEG. In Maximum Entropy and Bayesian Methods, Boise 1997, G.J.\nErickson, J.T. Rychert and C.R. Smith, eds. (Dordrecht: Kluwer Academic\nPublishers), pp. 209-222.\nKnuth, K.H. (1998). Bayesian source separation and localization. In Proceedings of SPIE:\nBayesian Inference for Inverse Problems, vol. 3459, A. Mohammad-Djafari, ed.\n(Bellingham WA: SPIE), pp. 147-58.\n\nKnuth, K.H. (1999). A Bayesian ap roach to source separation. In Proceedings of the\nFirst International Workshop on Independent Component Analysis and Signal\nSeparation: ICA\'99. J. F. Cardoso, Ch. Jutten, Ph. Loubaton, eds. (Aussois,\nFrance: ICA\'99), pp. 283-288.\nKnuth, K.H., Vaughan, Jr., H.G. (1998). Convergent Bayesian formulations of blind\nsource separation and electromagnetic source estimation. In Maximum Entropy\nand Bayesian Methods, Garching, Germany 1998, W. von der Linden, V. Dose,\nR. Fischer, R. Preuss, eds. (Dordrecht: Kluwer Academic Publishers), pp. 2 17-26.\nKnuth, K.H., Truccolo, W.A., Bressler, S.L., Ding, M. (2001). Separation of multiple\nevoked responses using differential amplitude and latency variability. In\nProceedings of the Third International Workshop on Independent Component\nAnalysis and Blind Signal Separation: ICA 2001. T.-W. Lee, T.-P. Jung, S.\nMakeig, T.J. Sejnowski, eds. (San Diego, CA: ICA 2001).\nLange, D.H., Pratt, H., Ingbar, G.F. (1997). Modeling and estimation of single evoked\nbrain potential components. IEEE Trans. Biomed. Eng. 44, 791-9.\nLee: K.H., Williams, L.M., Breakspear, M., Gordon, E. (2003). Synchronous gamma\nactivity: a review and contribution to an integrative neuroscience model of\nschizophrenia. Brain Res Brain Res Rev 41, 57-78.\nMakeig, S., Jung, T.-P., Bell, A., Ghahremani, D., Sejnowski, T.J. (1997). Blind\nseparation of auditory event-related brain responses in independent components.\nProc. Natl. Acad. Sci. USA 94, 10979-84.\nMakeig, S., Westerfield, M., Jung, T.-P., Enghoff, S., Townsend, J., Courchesne, E.,\nSejnowski, T.J. (2002). Dynamic brain sources of visual evoked responses.\nScience 295,690-694.\nMehta, A.D., Ulbert, I., Schroeder, C.E. (2000). Intermodal selective attention in\nmonkeys. I: distribution and timing of effects across visual areas. Cereb Cortex\n10(4), 343-58.\nMocks, J., Gasser, T., Kohler, W., De Weerd, J.P. (1986). Does filtering and smoothing\nof average evoked potentials really pay? A statistical comparison.\nElectroencephalogr Clin Neurophysiol 64,469-480.\nNicholson C., Freeman J.A. (1975). Theory of current source-density analysis and\ndetermination of conductivity tensor for anuran cerebellum. J. Neurophysiol.\n38(2), 356-368.\nPham, D.T., Mocks, J., Kohler, W., Gasser, T. (1987). Variable latencies of noisy signals:\nestimation and testing in brain potential data. Biometrika 74, 525-33.\nSarela, J., Vigario, R., Jousmaki, V., Hari, R., Oja, E. (1998). ICA for the extraction of\nauditory evoked fields. In 4th International Conference on Functional Mapping of\nthe Human Brain. (Montreal Canada: HBM98).\nSchroeder, C.E. (1995). Defining the neural bases of visual selective attention:\nConceptual and empirical issues. Int J Neurosci 80, 65-78.\nSchroeder, C.E., Tenke. C.E., Givre, S.J., Arezzo, J.C., Vaughan, H.G., Jr. (1990).\nLaminar analysis of bicuculline-induced epileptiform activity in area 17 of the\nawake macaque. Brain Res. 515(1-2), 326-30.\nSchroeder, C.E., Tenke. C.E., Givre, S.J., Arezzo, J.C., Vaughan, H.G., Jr. (1991). Striate\ncortical contribution to the surface-recorded pattern-reversal VEP in the alert\nmonkey. Vision Res. 31,1143-57.\n\n25\n\nSchroeder, C.E., Steinschneider, M., Javitt, D.C., Tenke, C.E., Givre, S.J., Mehta, A.D.,\nSimpson, G.V., Arezzo, J.C., Vaughan, H.G., Jr. (1995). Localization of ERP\ngenerators and identification of underlying neural processes. Electroencephalog\nClin Neurophysiol Suppl44,55-75.\nSchroeder, C.E., Mehta, A.D., Givre, S.J. (1998). A spatiotemporal profile of visual\nsystem activation revealed by current source density analysis in the awake\nmacaque. Cereb Cortex 8(7), 575-92.\nSchroeder, C.E., Mehta, A.D., Foxe, J.J. (200 1). Determinants and mechanisms of\nattentional modulation of neural processing. Front Biosci. 6, D672-84.\nShah A.S., Knuth K.H., Truccolo W.A., Ding M., Bressler S.L., Schroeder C.E. (2002).\nA Bayesian approach to estimating coupling between neural components:\nevaluation of the multiple component event related potential (mcERP) algorithm.\nIn Bayesian Inference and Maximum Entropy Methods in Science and\nEngineering, Moscow ID 2002, AIP Conference Proceedings 659, C. Williams,\ned., (Melville, NY: American Institute of Physics), pp. 23-38.\nSinger W. (1993). Synchronization of cortical activity and its putative role in information\nprocessing and learning. Annu Rev Physiol55,349-374.\nSivia, D.S. (1996). Data Analysis. A Bayesian Tutorial (Oxford: Clarendon Press).\nSivia, D.S. (2003). Some thoughts on correlated noise. In Press: Bayesian Inference and\nMaximum Entropy Methods in Science and Engineering, Jackson Hole WY 2003,\nAIP Conference Proceedings, G.J. Erickson, K.H. Knuth and C.R. Smith, (eds.),\n(Melville, NY: American Institute of Physics).\nStudent. (1908). The probable error of a mean. Biometrika 6, 1-24.\nTang, A.C., Pearlmutter, B.A., Malaszenko, N.A., Phung, D.B. (2002). Independent\ncomponents of magnetoencephalography : single-trial response onset times.\nNeuroImage 17, 1773-1789.\nTruccolo, W.A., Knuth, K.H., Ding, M., Bressler, S.L. (2001). Bayesian estimation of\namplitude, latency and waveform of singie trial cortical evoked Components. 111\nBayesian Inference and Maximum Entropy Methods in Science and Engineering,\nBaltimore MD 2001, AIP Conference Proceedings 617, R.L. Fry and M.\nBierbaum, eds. (Melville, NY: American Institute of Physics), pp. 64-73.\nTruccolo, W.A., Ding, M., Knuth, K.H., Nakamura, R., Bressler, S.L. (2002). Trial-totrial variability of cortical evoked responses: implications for the analysis of\nfunctional connectivity. Clin Neurophysiol 113(2), 206-26.\nTruccolo, W.A., Knuth, K.H., Shah, A.S., Bressler, S.L., Schroeder, C.E., Ding, M.,\n(2003), Estimation of single-trial multi-component ERPs: Bayesian foundation\nand applications. In press: Biol Cybern.\nVictor, J.D., Cane1 A. (1992). A relation between the Akaike criterion and reliability of\nparameter estimates, with application to nonlinear autoregressive modeling of\nictal EEG. Annals of Biomedical Engineering 20, 167-180.\nVigario, R., Sarela, J., Jousmaki, V., Oja, E. (1999). Independent component analysis in\ndecomposition of auditory and somatosensory evoked fields. In Proceedings of\nthe First International Workshop on Independent Component Analysis and Signal\nSeparation: ICA\xe2\x80\x9999. J. F. Cardoso, Ch. Jutten, Ph. Loubaton, eds. (Aussois,\nFrance: ICA\xe2\x80\x9999), pp. 167-72.\n\n26\n\nWoody, C.D. (1967). Characterization of an adaptive filter for the analysis of variable\nlatency neuroelectric signals. Medical Biological Engineering 5, 539-53.\n\n27\n\nI\n\nAdditive White Gaussian Noise\n\n8\n9\n10\n11\n12\n\n1.233\n1.742\n2.460\n4.909\n9.794\n\nI\n\nI\n\nJ\n\n-10.8 -24.7\n-13.8 -27.7\n-16.8 -30.7\n-22.8 -36.7\n-28.8 -42.7\n\n-13.5\n-16.5\n-19.5\n-25.5\n-31.5\n\n0.050\n0.108\n0.100\n0.198\n0.421\n\nTable 1. This table shows the effect of additive white Gaussian noise on the separability\nof the three components. The component SNRs are calculated using the component\nstandard deviations of 0.357, 0.072, 0.260 for components c I, c2, and c3 respectively.\n\n28\n\nAdditive l/f Far-Field Noise\nCase\n\nComponent\n\nNoise\nStd Dev\n\n3\n\n1 0.063\n\n10\n11\n12\n\n0.634\n0.846\n1.I28\n\nError\n\nI\n\n15 I\n\n1.1 1\n\n-5 -18.9\n-7.5 -21.4\n-10 -23.9\n\n12.2 I 0.017\n\n-7.8\n-10.3\n-12.8\n\n0:fil\n0.359\n0.365\n\nTable 2. This table shows the effect of additive l/f correlated far-field background signal\non the separability of the three components. The component SNRs are calculated using\nthe component standard deviations of 0.357, 0.072, 0.260 for components c l , c2, and c3\nrespectively.\n\n29\n\nCOMPONENT\nWAVESHAPE\n\ns largest AERP fro\n\nWAVESHAPE\n\nDEFINE NEW\nCOMPONENT\n\nFigure 1 Flow chart describing the dVCA algorithm.\nThis flow chart describes the implementation of the dVCA algorithm, which relies on\nequations (13), (17), (20), and (14) in the text. It is based on a hierarchical model, which\nbegins with a single component model and adds components until AIC is no longer\nsatisfied.\n\n30\n\nA\n\ncomoonent 2\n\ncomponent 3\n\nc\n\ntime (m)\n\na\n\n100\n\n200\n\ntime (Ins)\n\nB\n\ncomponent 1\n\ncomponent 2\n\nC\n\nv\n\n1\nL\n\nFigure 2 Synthetic data used in the simulations.\nThe synthetic data utilized in this paper are derived from a model of expected responses\nin macaque V1 when stimulated by a red-light flash. (A) These data represent electric\nfield potentials recorded from a 15 channel linear-array multielectrode spanning the\ncortical laminae in V1. The thalamic input activates the spiny stellate cells in layer IV,\ngenerating a biphasic field potential (component 1). The feedforward connections from\nthe stellate ce!ls activate the pyramidal cel!s in the supragranular !ayes (component 2).\nComponent 3 models a signal generated by a far-field source which volume conducts to\nthe multielectrode array. (B) The noise-free synthetic field potentials generated by these\nthree components are recorded differently by each electrode in the multielectrode array.\n\n31\n\nNotice that the polarity and amplitude of the recordings depends on the physical positions\nof the current sources and sinks in the cortical laminae (see C). (C) The current source\ndensity (CSD) of the summed field potentials in B is computed using an approximation of\ntheir second spatial derivative. The CSD focuses the activity at the location of the current\nsinks (negative) and sources (positive) relative to the detector positions. This technique\nclarifies the laminae in which the field potentials are generated. Notice that far-field\nsources do not appear in the CSD.\n\n32\n\nC\n\n-\n\namplitude variability -\n\nQ\n\n-\n\n.\n\nI\n\na\n0,\n\n(scale)\n\nD\n\nB\n\n,\n\nr\n\n8\n\n_ _ ~ . r _ _ " _I\n\nT\n\n4\n\n1\n0\n\n_\n\n- Cl\n?\n\n_\n\n-.-\n\n,=----y--.-.\n\ni\n\namplitude variability - ua (scale)\n\n6\n\n2\n\nlatency Variability - p {ms)\n\nlatency variability -\n\n(ms)\n\nFigure 3 Amplitude and latency variability aids component identification.\n(A) The Amari error, which measures the degree of signal separation, decreases with\n,,\nincreasing amplitude variability with o , 2 0.25 being sufficient for signal separation.\n(B) The waveshape RMS errors also indicate the importance of amplitude variability. ( C )\nAmari error decreases with increasing latency variability, with o , ~ , 7.5 ms being\n2\nnecessary to achieve effective separation. (D) Increasing latency variability also\nimproves the estimate of the component waveshapes, but not as dramatically as\namplitude variability.\n\n33\n\nB\n\nA\nE\n\na\n\n030\n\n-20\n\n0\n\n10\n\n0\n\n-10\n\n10\n\nSNR of component 1 (dB)\n\nSNR of component 1 (d6)\n\nD\n\nC\nzs\n\n-15\n\n-5\n\n0\n\n5\n\nY\n\nSNR of individual components (dB)\n\n40\'\n\n~\n\n-2s\n\n-15\n\n-5\n\n0\n\n5\n\n.\n\nSNR of individual components (dB)\n\nF\n\nG\n\nj\n\n, e,,"\n\nSNR, 1 7.2 dB\n\n1\n\n\'0\na\n\na\nY\n-0\n\ne\n\n(D\n\n-1\n\n0\n\n100\n\ntime\n\n200\n\n(ms)\n\n300\n\ntrue amplitude (scale)\n\nFigure 4 The dVCA algorithm is robust to noise.\n(A) The Amari error increases with decreasing S N R (see text). (B) The quality of the\nwaveshape estimates degrades with decreasing SNR. Note that the graph is drawn with\nrespect to c l SNR. The waveshape RMS error reach 1.O, signifying that the deviations in\nthe estimates are on the order of the waveshape itself, at about - 23 dB to - 25 dB for\nthe localized components and about - 28 dB for the far-field component. (C) The singletrial amplitude estimates are also robustly recovered. T i e horizontai biacic iines denore\nthe level of variability in the single-trial quantities. The blue solid and red dashed curves\nindicate one and two standard deviations, respectively, of the single-trial amplitude\nestimate errors. When the blue curves are within the black lines, dVCA is performing\n\n34\n\nbetter than the standard practice of averaging (see text). High quality estimates are\nachieved down to - 18 dB to - 23 dB , and adequate estimates down to - 25 dB . (D)\nSingle-trial latency estimates are less well estimated than amplitudes. High-quality\nestimates are possible down to - 8 dB to - 12 dB , and adequate estimates down to about\n- 20 dB. (E) These plots demonstrate the quality of the waveshape estimates for four\nSNR levels. At a c l SNR of - 22.5 dB,the smallest component, c2, was unable to be\nextracted from the data. (F) Scatter plots of the true c 1 single-trial amplitudes versus the\nestimated c 1 single-trial amplitudes demonstrate the quality of amplitude estimates, and\nshow that useful information can be obtained down to an SNR of - 22.5 dB . (G) Scatter\nplots of the true c l single-trial latencies versus the estimated c l single-trial latencies\ndemonstrate that latencies can be estimated down to SNR levels below - 10.8 dB , but\nbecome inaccurate by - 22.5 dB .\n\n35\n\nB\n\nA\n\n10\n\n0\n\ni\n20\n\nSNR of component 1 (de)\n\nSNR of component 1 (dB)\n\nD\nCl\n\nCl\n\n-ul\n\n0\n\n10\n\n20\n\nSNR of individuaf components (dB)\n\n40\'\n\n-Io\n\n0\n\nio\n\n20\n\nSNR of individual components (de)\n\nFigure 5 The dVCA algorithm is also robust to correlated far-field noise, although to a\nlesser degree than independent Gaussian noise.\n(A) The Amari error increases with decreasing S N R and reaches the same level of error\nas in the Gaussian case with 20 dB less noise. (B) The component waveshapes blow up\naround - 10 dB , with the far-field component, c3, being more dramatically affected. (C)\nThe amplitude errors reach unacceptable levels around - 5 dB to - 8 dl3. (D) The\nlatency estimates become unacceptable between -7 and -8 dB for the local sources,\nwhereas the far-field component is even poorly estimated at levels above 5 dB .\n\n36\n\n2\n3\n- 4\n& 5\n\n2\n\nS 7\n6\n\n=\na\nJ\n\nc -8\nK\n\nm\n5\n\nc\n\nB\n\nA\n- -\n\n-\n\n2/ 3\n\nv\nL\n\na\nl\n. 10\nn\n\n- - :Dl\n\n9\n\n-\n\nK\n\n1\n\n0.8\n\n48\n\nv\n\nlo\n11 _TJ_IL____\n\n-\n\n1.2\n\n1.4\n\namplitude (scale)\n\nD\n\n4crf\n\nv\n\n3\n\n40\n\nY\n\nL\n\n12 0\n13\n\n100\n\n200\n\n4cp\n\n, 1 mV/m&\n\n,\n\n0\nI\n100\n200\n9 20\n\n5\n\ntime (rns)\n\ntime (ms)\n\n= o\n\nE\n\n-10\n\n-5\n\n0\n\n5\n\nlatency shift (ms)\n\nF\n\ntime (ms)\n\ntime (ms)\n\nFigure 6 Single component analysis of V1 responses.\n(A) CSD of the trial-average ERP shows granular and supragranular activation in\nmacaque V1 in response to a red light flash (number of trials = 171). (B) Estimating a\nsingle component with dVCA results in a waveshape with a CSD profile that captures the\nmost prominent responses in the data. (C) A histogram of the single-trial amplitudes of\nthis component shows that the amplitude rarely varies more than f 20%. (D) The\nhistogram of the single-trial latencies reveals that there are two response modes: an early\nresponse that happens 1/3 of the trials and a late response that happens about 2/3 of the\ntrials. The peak latency difference between these two modes of activation is 6.75 ms.\n(E) To verify the existence of these two activation modes, this figure shows all 176 trials\nof the field potential recorded in channel 10. Each trial is represented as a horizontal line\nwith its time-varying color representing the time-varying amplitude of the field potential.\nTrials designated as belonging to the \xe2\x80\x9cearly\xe2\x80\x9d subset are indicated by the red dashes on the\nleft side of the plot. Note that the \xe2\x80\x9cearly\xe2\x80\x9d trials are characterized by a negative (yellow)\nfield potenti.al onset occurring before the onset seen in the \xe2\x80\x9clate\xe2\x80\x9d trials. (F) Further\nverification of this finding is provided by comparing the average FP recorded in channel\n10 with the average obtained from the \xe2\x80\x9cearly\xe2\x80\x9d subset and the average obtained from the\n\n37\n\n\xe2\x80\x9clate\xe2\x80\x9d subset. While both the early and late responses show initial activation occurring at\nthe same time, the early response\xe2\x80\x99s activation continues to grow, while the late response\xe2\x80\x99s\nactivation decreases before growing again. The difference in latency between the minima\nof the two sub-averages is 5.5 ms.\n\n38\n\nA\n\nB\n\nC\nI\n\nv\n\n3\n\n13\n\n,\n0\n\n100\n\nI\n\n10\n.\n\nrnV/mm*\n\n200\n\ntime (ms)\n\n-\n\n0\n\n200\n\n100\n\ntime (ms)\n\nI3\n\nW\'""\n/\n\n,\n\n0\n\n,\n\n,\n\n100\n\n.\n\n>\n\n200\n\ntime (msj\n\nD\n\nE\n\ntime (ms)\n\ntime (ms)\n\nFigure 7 Examination of the early and late responses.\nThe dataset has been split into two subsets: the early subset and the late subset, and a\nsingle component has been re-estimated for each subset. (A) The CSD profile of the\nearly component with a drop line showing onset of the major response at 28 ms. (B) The\nCSD profile of the late component. The drop line shows its major response onset at 42.5\nms. The waveshapes of the two responses are noticeably different, as are their laminar\nprofiles. (C) The average residual field potentials, computed by subtracting the singletrial model from the single-trial data and averaging over all trials, further demonstrates\nthe differences between these two modes of activation (early-red, late-blue). Only the odd\nchannels are shown. These residuals represent other responses not modeled by the single\ncomponent in each subset. The black arrows indicate thalamic input, which is timelocked to the stimulus as it is visible in the average. Subsequent activation of the two\nresponse types is very different. The late oscillations in channel one after 100 ms are 180\ndegrees out of phase. The red arrow in channel 11 at about 50 ms shows time-locked\nultra-high frequency (UHF) oscillations in the early responses that are not present in the\nlate subset. (D) Wavelet analysis was performed on the residuals to characterize these\noscillations and verify that the categorization indicated by the dVCA results is justified.\n\n39\n\nThe residuals in the early subset show a burst of UHF oscillation between 42-57 ms with\nfrequency ranging from 160-220 Hz. (E) These oscillations are completely absent in the\nlate subset.\n\n40\n\nC\n\nA\n\n0\n\n100\n\n200\n\ntime (ms)\n\nF\n\n0\n\n1\n\n2\n\namplitude\n\nlatency (ms)\n\ncl amplitude\n\n(scale)\n\nFigure 8 Three components were estimated fiom the late subset.\n(A) The CSD profile of component 1 shows that it represents the granular response with\nsome activation in the supragranular layers. The drop line marks the onset of the\nresponse triggered by the thalamic input at 26 ms. (B) Component 2 represents slowwave activity in the supragranular layers. Its onset is considerably later at 37 ms. (C)\nComponent 3 also describes supragranular activation with a pulse-like activation\nfollowed by some slow wave activity. The similarity in the Iamincir profiles of c2 and c3\nsuggests that these responses are probably taking place in the same population of cells.\nNote however that these laminar profiles are not identical as c3 displays some granular\nactivation. @) The single-trial amplitude histograms as well as amplitude scatter plots\nare shown. C1 shows very little amplitude variability, whereas that of c2 and c3, while\ncomparable, are not equal. The scatter plot of c2 and c3 amplitudes are correlated with\nY = 0.495 ( p < 10-7). (E) The single-trial latency histograms and scatter plots are shown.\nNote that the slow-wave activity in c2 shows great latency variability. The scatter plot of\ncl and c2 latencies shows some correlation at r = 0.327 ( p < 0.001). (F) These scatter\n\n41\n\nplots show the relationship between cl amplitudes and c l and c2 latencies with\n( p < 0.002) and r = 0.483 ( p < lo-\xe2\x80\x99), respectively.\n\nY\n\n= 0.297\n\n42\n\n'