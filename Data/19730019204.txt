b'General Disclaimer\nOne or more of the Following Statements may affect this Document\n\nThis document has been reproduced from the best copy furnished by the\norganizational source. It is being released in the interest of making available as\nmuch information as possible.\n\nThis document may contain data, which exceeds the sheet parameters. It was\nfurnished in this condition by the organizational source and is the best copy\navailable.\n\nThis document may contain tone-on-tone or color graphs, charts and/or pictures,\nwhich have been reproduced in black and white.\n\nThis document is paginated as submitted by the original source.\n\nPortions of this document are not fully legible due to the historical nature of some\nof the material. However, it is the best reproduction available from the original\nsubmission.\n\nProduced by the NASA Center for Aerospace Information (CASI)\n\nAtllil 1973\nby\n\nJEROME B. LERMAN\nArtillc" Intelligenc. lIbontary\n\n. LOUIS L SUTRO\n\na.tts S1Irk OlllJlF LIIIofItDry\n\nMASSACHUSETTS INSTITUTE OF TeCHNOLOGY\n\n1.\\\n\nEO I\n\nv:\n\n-0 .\n\nf\'il,a\n\ntts lnst. ot Te e .\xe2\x80\xa2 )\nC l..L\n\n\' P\n\nUncIa.:;\n\n.. 3 /\n\n..\n\n1:) ... 7\n\nBased on \xe2\x80\xa2 PI. . presented to\nThe Firat NatioNI Conf. .nc::e on Remote Manned Syttems\nIn SIIptlmber 1972\n\nROBOT VISION\nLOUIS L. SUTRO\nASSISTA%\'T DIRECTOR\nThe Charles Stark Draper Laboratory\nA DIVISION OF MASSACNIISE TTS INSTITUTE OF TEGHNOT O(,V\nC\xe2\x80\x94b\'\'fq., MetYCh\xe2\x80\x94"\'\n\nJEROME B. LERMAN\nArtifiral Intelligence Laboratory\nMASSACHUSETTS INSTITUTE Of TFCHNOLOGy\nC.mbrl dg. , IV.\xe2\x80\x94h--\n\nN\n\n\xe2\x80\xa2\nS\n\nApril 1973\n\nBased on a paper presented to\nThe First National Conference on Remote Manned Systems\nin September 1972\n\n^.\t\n:\t\n\nApprover\':\t\n\nr\t\n\nApproved:\t\n\nJohn W. Hursh, Associate Director\t\nThe Charles Stark Draper Laboratory\t\n\n1j/j^^\t\n\n^^N.^ZL 3\t\n\n7 ^ J^ ^\n\nWilliam G. Denhard, Associate Director\nThe Charles Stark Draper Laboratory\n\n\xc2\xa9 Copyright by the Massachusetts Institute of Technology, 1973.\nPublished by The Charles Stark Draper Laboratory\nA Division of Massachusetts Institute of Technology\nPrinted in Cambridge, Massachusetts, U.S.A., 1973.\n\n^\n\n^\' u^"^A_\n\nI\n\nACKNOWLEDGEMENT\nThe contributions of the following are gratefully acknowledged:\nFrederick Zeise, Richard Warren, Charles Sigwart and Dr. Roberto\nMoreno-Diaz in modelling neurophysiology; Larry Baxter, Jerome\nKrasner, and David Tweed in the design of electronics; Robert Magee\nin the design of optics; Joseph Convers and Benjamin Smith in\nmechanical design; Daniel Moulton, James Bever and John Hatfield\nin programming; and John Hursh in administration. Special appreciation is due the late Dr. Warren McCulloch, Prof. Jerome Lettvin,\nDr. Bela Julesz and Prof. Whitman Richards, for describing vertebrate vision in lariguage that the authors and the above could under stand and attempt to model.\nThe work reported here was sponsored by the National\nAeronautics and Space Administration, Headquarters, Office\nof Space Sciences and Applications, through Contract NSR\n\t\nI\n\n22-009-138.\n\nii\n\nABSTRACT\nHere is described the operation of a system built both to\nmodel the vision of primate animals, including man, and serve as a\npre-prototype of a possible object recognition system. It was\nemployed in a series of experiments to determine the practicability\nof matching left and right images of a scene to determine the range\nand form of objects.\nThe experiments started with computer-generated random -dot\nstereograms as inputs and progressed through random-square stereograms to a real scene. The major problems were the elimination of\nspurious ma t ches, h?tween the left and right views, and the interpretation of ambiguou3 regions, on the left side of an object that can be\nviewed only by the left camera, and on the right side of an object that\ncan be viewed only by the right camera.\nRules were developed for eliminating spurious matches in the\nprogressively more difficult objects. An arbitrary rule was developed\nfor interpretation of ambiguous regions.\nIn the experiments reported, comparison of left and right views\nwas performed in terms of gray values, but the comparison could be\nmade in terms of edges. An economical method of detecting edges\nwas demonstrated.\nStereo camera assemblies were designed and one of them built\nto permit the cameras to converge and together pitch, roll and yaw.\nA second steroo TV camera assembly has been built which has as its\nonly moving parts two mirrors and a means of focussing. The above\nexperiments were performed, before either of these camera assemblies\nwas available, by exposing a single-visw camera in two positions.\nWe show that a scene on Mars, reported to earth in terms of its\nfeatures, can be reconstructed on earth.\nPerhaps the two most useful results were (1) development of\n\niii\n\nthe concept of a match space where the detected three-dimensional\nproperties of a scene can be plotted and then examined for their form,\nand (2) the conclusion that a stereo TV camera is needed which will\nacquire both central and peripheral stereo pairs of images.\n\niv\n\n,\n\'FABLE OF CONTENTS\nChapter\nI\n\nTitle\n\nPage\n1\n\nA.\n\nThe Main Objective\n\n1\n\nB.\n\nA Supporting Objective\n\n1\n\nC.\n\nTest Systems\n\n2\n\nD.\n\nApproach\n\n4\n\nE.\n\nOther Supporting Objectives\n\n4\n\nF.\n\nHow This Paper Differs from First Version\n\n4\n\nMEANS OF AUTOMATICALLY DETERD41NII 14G\nTHE RANGE OF SMALL AREAS OF A SCENE\n\n7\n\nA.\n\nObjective\n\n7\n\nB.\n\nOpto -Electro -Mechanical Strategies\n\n7\n\nC.\n\nAutomatic Comparison of One Scan Line of Left\nwith One Scan Line of Right TV Image\n\n9\n\nD.\n\nMethods of Mapping Binocular Space into\nMatch Space\n\n12\n\nE.\n\nHow Matches Are Made and Viewed\n\n14\n\nF.\n\nUse of Model in Match Space\n\n18\n\nG.\n\nGeometry of Binocular and Match Spaces\n\n19\n\nH.\n\nRules for Processing Random-dot Stereograms\n\n22\n\nI.\n\nHow Should Ranges in a Scene Be Presented?\n\n27\n\nJ.\n\nEliminating Areas of Spurious Matches\n\n27\n\nK.\n\nRandom-Square Stereograms\n\n32\n\nL.\n\nProcessing of a Real Scene\n\n32\n\nM.\n\nDetermining Form\n\n33\n\nN.\n\nf,\n\nINTRODUCTION\n\nRange Accuracy\n\n38\n\nCOMPUTATION TO EXTRACT OTHER \t\nFEATURES\n\n41\n\nII\n\na\n\n`.\n\n:.,\n\nIII\t\n\nAl Square-Wave Frequency Response of Camera \t\n\nv\n\n41\n\ni\n\nPage\n\nTitle\t\n\nChapter\nA2,\n\nComputation of Fdges\t\n\n41a\n\nB.\n\nFormation of Line Drawing\t\nHardware to Detect Edges\t\nComputation of Reflecting Properties\t\n\n43\n46\n\nCe\nD.\n\n48\n\nIV\n\nRECONSTRUCTING THE APPEARANCE\t\nOF A SCENE\n\n49\n\nV\t\n\nSTEREO TV CAMERAS\t\n\n51\n\nSUMMARY AND CONCLUSION\t\n\n57\n\nVI\t\n\nAPPENDIX A\nEQUATIONS FOR RANGE AND UNCERTAINTY\t\nIN RANGE\nA. 1\t\nThe Geometry of Stereo TV Optics with\t\nParallel Axes\nA. 2\t\nDerivation of Equation of Range Uncertainty\t\nfor Stereo TV Cameras with Parallel Axes\nREFERENCES\t\n\n61\n61\n62\n\n65\n\nvi\n\nLIST OF ILLUSTRATIONS\nFig. No.\n0\n\nBrief Title\nEquipment for simulating light-weight,\nlow-power hardware: Camera-computer\nchain and binocular, or stereo, TV camera,\n\nPage\n3\n\n1\n\nMars -like scene.\n\n5\n\n2\n\nTwo strategies of visual processing.\n\n6\n\n3\n\nDiagram of computation to form a model\nin match space of an object in binocular\nspace.\n\n11\n\n4\n\nBinocular space divided into quadrilaterals of uncertainty by rays drawn\nfrom pixels in left and right images.\n\n13\n\n5\n\nComputation of firrst-stage matches at\nthe right ends of three lines of the x\' -d\nplane pictured in Fig,\t 3.\n\n15\n\n6\n\n(a)\t Geometry of Fig. 3, when object viewed\nis between parallel optical axes.\n(b)\t Match space corresponding to the\nbinocular space in (a).\n\n20\n\n7\n\nGeometry of Fig. 3 when the object viewed\nis at the left of both optic axes.\n\n21\n\n8\n\nWhy the range of the parts of the background,\nat the left and right :;ides of an object.\t is\nambiguous.\n\n23\n\nA random-dot stereogram depicting a square\nfloating before a background.\n\n24\n\n10\n\nAn x\' -d section of the m -space generated from\nthe stereogram in Fig. 9.\n\n26\n\n11\n\nx\' -d sections of m -space showing, in (a),\nambiguous r__ dons on the left and right sides\nof the model of the object and, in (b), how both\nthe image of the object and the background\nbehind it are modelled.\n\n26\n\ni\n\n9\n\nvii\n\ni\n\nFig. No.\n12\n\nBrief Title\nx\' -d sections of three match spaces\nformed from the same stereogram,\n\nPage\n29\n\n13\n\n30\n\n14\n\nRandom-block stereogram of a square floating\nbefore a background,\n\n33\n\n15\n\nx\' -d section of model in m -space generated\nfrom the stereogram in Fig. 14.\n\n33\n\n16\n\nStereo\t riages of the Mars-like scene of\nFig.\t 1,\t acqui.-ed by television can;era,\ndigitized, then displayed one at a time on\nan oscilloscope,\n\n35\n\n17\n\nx\' -d section of the match space formed\nfrom the stereogram of Fig. 16.\n\n35\n\n18\n\nRange map of one match space of the stereogram in Fig.\t 16,\n\n37\n\n19\n\nTest pattern and amplitude response\t\nof TV camera.\n\n20\n\nOperations performed on each 7 x 7 pixel array\nof a digitized image to detect edges.\n\n21\n\n44\nNegative of a display of the result of performing\nthe operations of Fig. 20 on the images of Fig. \t 16.\n\n22\n\nEdges of Fig. 21 after thinning.\n\n44\n\n23\n\nDetection of edge and thinning of edge.\n\n45\n\n24\n\nInsertion of means of detecting edges between\nthe stereo TV camera assembly and match space.\n\n47\n\n25\n\nEfferA of a source of collimated light such as\nsunlight.\n\n50\n\n26\n\n!\n\nDiagram showing the functions of the three\nsimulation programs, STEREO, EYPER\nand FUSER.\n\nReconstruction of the appearance of an object\nfrom its shape,\t its reflectance and the sources\nof light.\n\n50\n\nviii\n\n_i\n\n40 and 41a\n\n411)\n\nF ig. No.\t\n\nBrief Title\t\n\nPage\n\n27\t\n\nType C3b stereo TV camera assembly. \t\n\n52\n\n28\t\n\nType D1 stereo TV camera a,,,\xe2\x80\xa2embly.\t\n\n53\n\n29\t\n\nType E1 stereo TV camera assembly.\t\n\n53\n\n30\t\n\nDiagram of images erected on the face of\t\neach vidicon in the El camera assembly.\n\n55\n\n31\t\n\nPossible configuration of a Mars rover.\t\n\n56\n\nGeometry of parallel optics for range finding.\t\n\n63\n\nA-1\t\n\nR\n\nJ\n\nLIST OF TABLES\nTable No.\t\n1\t\n2\t\n\nVALUES OF THE MATCH VARIABLES\t\nTEST CONDITIONS FOR EXAMPLE OF II L\t\n\nix\n\nPage\n17\n36\n\n1\n\nI. TN\' \'.O : ,DUCTION\nA. Th e M ain O bjective\n\nc\n\nThe main objective of the work reported here was to develop\nautomatic means of classifying three-dimensional objects. The\nproblem requiring solution at the start of this work was how to guide\nan automatic vehicle (robot) in the exploration of the surface of Mars,\nrecognize objects and report these findings to earth. Problems of a\nsimilar nature have since arisen. One is the automatic classification\nof plankton when viewed under a binocular microscope. Another is the\nautomatic classification of objects to be machined, assembled or\notherwise manipulated in the course of manufacture.\nWhile classification is tnt- goal, we have found the word "recognition" easier to use. Thus, "automatic recognition of objects" is the\ngoal most often inentioned here.\nB. A Supporting Objective\nSince the only systems able to recognize three-dimensional\nobjects are animals, an objective that was pursued, supporting the\nmain objective, was to model animal vision. The first efforts in this\ndirection were to devise models of the vision of a lower vertebrate,\nnamely, the frog. The reasons for giving attention to this animal\nwere, first, that it performs recognition within its eyeball, and,\nsecond, that the neurophysiology of the frog\'s eye is well understood.\nFor example, a moving insec t is recognized there and reported via\nthe frog\'s optic nerve to its brain. Thus, by modelling a frog\'s eye,\none devises an ope-r ating model of a recribmition system. Reports on\nthis part of our effort describe a first :rude model of the frog\'s eye\n(Ref. 1) . a fine grain model of the bug-detector cell in the frog\'s Eye\n(Ref. 2), a more rigorous model of this bug detector (Refs. 3-5),\n\n1\n\nar.d a first description of the shift register scheme (Ref. 6).\nMcCulloch described animal nervous Systems as multiple\nloops of information flow, with computation in every loop (Ref. 7).\nExcept for the frog (Ref, 8). however, it was not possible to\ndescribe these systems in sufficient detail to enable immediate\nprogress to a useful design. What was needed was an operating\nmodel which could be shaped by a series of small changes. Particularly needed was a test system which would permit modelling\nthe binocular vision of primates, inciuding man.\ni\n\nC. Test Systems\nTwo test systems were built. both to model the vision of\nanimals, inc!- ding man, and serve as pre-prototypes of possible\nobject recognition systems. The first system is described in Ref. 9.\nThe second is pictured in Fig. 0. The TV camera of this second\nsystem was usually aimed at the simulated Mars scene (Fig. 1),\nwhich consisted of rolling terrain, made of papier-mache on a 4 ft.\nby 8 ft, piece of plywood, and a painted backdrop, all created by\nDustin Thomas. Lighting was usually unidirectional from the right.\nThe Type C3 camera was occasionally aimed out th? window to test\nthe ability of the system to determine the range of objects on the\nroofs of adjoining buildings. The edge of a building on the Boston\nskyline was ima-ed to serve as an infinity point in the adjustment\nof the mirrors of the camera assembly.\nThe TV camera, the central element in stereo TV, generated\na TV image displayed 30 times a second or, the monitor. The TV\ncamera was also scanned slowly along vertical lines, under computer control, to acquire an image for processing. Any image\nstored either on magnetic tape or in core memory can be displayed\nand photographed on the , scilloscope, the intensity of which can be\nmodulated to display gray values.\n\n2\n\n-.^^^\n-\n\nAAPV\n\nr\n\nr\n\nMIRRORS\t\n\nL;\n\n\xe2\x80\x94^ LENSES\t\nELECTRONICS\n\n^^ 1\n\nLONG-PE\xe2\x80\xa2SISTENCE\n11 SPLAY\nf\n__\n\n1\t\n\nGRAY-SCALE\nDISPLAY\n\n^rf\n\nCAMERA\n\nMONITOR\n\nFig. 0. Equ.pment for simulating light weight, low-power hardware\n(above) Binocular, or stereo, TV camera Type C3a\n(below) Camera-computer chain for simulating robot vision\n\n3\n\nD. Approach\n\nH\n\nL_\n\nOnce this equipment was completed, we turned our attention\nto making it work. There were two examples before us. Line was\nthe modelling of the vert\xc2\xb0brate visual system, one cell type at a\ntime, progressing from the retina through the lateral geniculate\nnucleus to the visual cortex. Fukishima demonstrated that this was\npossible (Refs. 1", 11).\nTre other approach was that of Julesz, in which he had\nemployed a computer to compare the un p rocessed left and right\nimages of a stereogram to extract range data., thus assuring that\nthe structure of the scene would be acquired automatically. We\ndecided to follow the latter route. Having done that, we now find\nthat it is often desirable to introduce another stage of computation\nbetwt,en the acquisi^ion of left and right i .ages of a scene and\ncom parison of these images. It might appear that we will thus\narrive at the same restilt as if we had taken the first approach. In\nfact, hr..wever, by providing for the structure first we have sought\nand found economies in data handling that we might not have found\nin taking the first approach.\nE.\n\nOther Supporting Objec tives\n\nTo move toward the main objective stated in A above, other\nsupporting objectives had to be pursued; namely, detection, of edges\nwith a minimum amount of hardware, reconstruction of the appearance of a scene frorn detected features, design of stereo TV cameras\nane design of the mounting of one such camera on a rover.\nF.\n\nHow This Pap er D iffers from First Version\n\nThe first printed version of this paper is Ref. 12. It is revised\nhere to provide a more complete introduction, to clarify the\n\n4\n\nA\n\ni\n\ndescription of th(^ simulation program EXPER in II J, show in II N\nhow range accuracy can be increased over that in the example of II L,\nprcpose another method of stereo processing in II I, expand III to\ninclude examples of the detection of spatial frequency, describe in\nmore detail the developn,%:nt of stereo TV ca-neras in V, and add\ncomments on how this work models primate vision. The only new\nillustration in Sections I and II is Fig. 0, placed ahead of all of the\npreviousl y -numbered illustrations. With the addition of Fig. 19 to\nSecti:,a III, all of the previously-numbered figure numbers, from 19\nto 29, move up one. With the addition of Fig. 30, the final figure\nnumber move up taro. Because many more references have baen\nadded, the reference numbers differ here from the first printed\nversion.\n\nFig. 1. Mars -like scene.\n\n5\n\neIf.\nI\t\n\nF--^-\n\n^1\'\n\nTv CAME RAS\n\nPLATFORM\n\nFig. 2. Two strategies of visual processing: Locating,\nrepreserted by rectangles, and identifying,\nrepresented by dashed lines.\n\n6\n\nII. MEANS OF AUrONLATICALLY DETERMINING THE RANGE\nOF S1iA LL AREAS OF A SCENE\nA.\n\nObjective\nThe immediate objective of the work reported in this se%;tion\n\nwas automatic determin:.tion of the range of small areas on the\nsurfaces of objects. A further objective was to employ this range\nA\n\ninformation to automatically determine the form of those objects.\nThe method employed in pursuing the first objective consisted\nof comparing the gray values of picture element (pixels) in a :eft\nimage of the field of view with gray values of picture elements in a\nright image. Because this appeared to be the simplest possible way\nof comparing left and right images it enabled us to define concepts\nbasic for future work, such as match space, spurious matches and\nambiguities. Fort hat future work, comparison of other variables\nin the left and right images is described in III,\nTwo automatic means of determining range compete for our\nattention. One uses a laser beam th at is deflected by a mechanicallydriven mirror under computer control. The other uses a stereo TV\ncamera which is both controlled and interpreted 6y computer,\nBecause our long-term goal is automatic recognition of objects\ncharacterized by edges and lines (features) ihich a laser may not be\nable to detect, we pursued the development of stereo -TV -competing.\nLaser range-finding can supplement stereo -TV -computing to determine to greater accuracy the range of objects selected by stereo -TV computing.\n\nB. Opto -Electro -Me c hanic a l Strateges\nIn bringing a stereo pair of images onto the face of a car-sera\ntube or tubes, several opto-electro-rmechan:c,.l strategies are\npossible. Figure 2 illustrates twc\xe2\x80\xa2 thai can be emploved consecutively.\n\n7\n\nA-_\n\nThe first works on a coarse scale and is called "locating". The\nsecond works on a fine scale (high resolution) and is called "identifying" (Ref. 13). Followirb the first strategy each camera subtends\nthe wide view represented by the rectangles to discover the rock, the\nhole and the cliff. Following the second strategy the two cameras investigate with higher resolution optics details along the edge of the rock.\nAs part of the first strategy, the cameras of Fig. 2 are shown\nmounted on a table that tilts within one gimbal and turns on another.\nEach camera is also supported by a gimbal on the platform, represented by a black dot on the camera case. The range of each object\ncan be computed from the angles formed by the camera axes and the\ndistances between the cameras, when both cameras are centered on\nthe object. The first strategy has been pursued in the design of the\ngimballed mirrors and gimballed carneras described in Section V.\nThe second strategy is to identify the features and, from the\nposition of those features in three-dimensional space, the form of\nobjects. The second strategy has been pursued in the work described\nin Sections II and III of this paper, In Section II the features are gray\nlevels formed from the pattern of luminances in the scene. In Section\nIII they are edges.\nThe second strategy consists of two sub-strategies. The first\nsubstrategy, called "stereopsis", requires two views and yields in\nman "the experience of relative depth only" (Ref. 14). The method\nof comparing two views, on the other hand, that we have designed,\n;yields absolute depth. The second substrategy, called "cognitive\nprocessing", requires the storage of features and the relations\nbetween features so that these can be compared to features and their\nrelations in the image. This second substrategy is not employed in\nthe examples presented in this paper. How it could be employed is\nconsidered in II H.\nA. third strategy, while not optical or mechanical, is electronic\n\nS\n\nr\n\n3\n\nry}\n\nin the sense of being computational. It can determine the class of\nthing the stereo-TV computer looks for. This strategy was described\nfirst in "Assembly of Computers to Command and Control a Robot"\n(Ref. 15), and is being described in more detail in Refs. 16 and 17.\nIn those reports the word "robot" is used, as McCulloch used it, to\nmean an animal or a machine (Ref. 18),\nC. Automatic Comparison of One Scan Line of Left with One Scan\nLine of Right TV Image\n\nk\nIt\n\n\xe2\x80\x9e\t\nx\xe2\x80\xa2\n_\t\n\nBoth of the first two strategies require a method of comparing\nleft and right images. The method about to be described stems from\nthe work of Bela Julesz (Ref. 19), and was developed into its present\nform by the second author (Ref. 20).\nFig. 3 shows at the left two TV cameras whose parallel optical\naxes extend into a space with coordinates x, y and z (measured in\nmeters). At the lower right is a three -dimensional structure where\na model is formed of the scene at left. It has the dimensions, shown\nin the lower right corner: x\' and y\' in pixels, d in integral values of\ndisparity. We call the structure "match space". Disparity is related\t\nto range by Eq. (1) in II G.\nIn our simulations of proposed hardware, match space is only\n36 values of disparity deep, from d = 0, corresponding to infinity, to\nd = 35. Only one x 1 -d plane is shown in the match space (m -space)of\nFig. 3. Note that the black squares in this plane approximate the\nshape )f a section of the rock at left. Each black square represents\na 1 in the computer memory.\nThe information in this match plane comes from the scan lines\non the face of the left and right camera tubes. The left scan line is\nan image of a V-shaped area projecting From the left camera lens\ninto space, the right scan line the image of a similar V-shaped area.\nWhere the two V-shaped fields overlap is the binocular field of the\n\n9\n\n{\n\nP\n\ni\n\ncameras.\nAs the electron beam in the left camera tube starts to sweep\nacross the line pictured on the face of that tube, the voltage of the\ncamera\'s output is converted to one of 2 5 or 32 levels, which we call\n"gray levels". Expressed as a five bit word, this gray level enters\nthe first column of the left-image shift register. When the electron\nbeam advances to the next pixt1, the first column is shifted to the\nright and a new column takes its place. This process continues until\nthe left-image shift register is full.\nAfter 36 five bit words have been formed and shifted, as\ndescribed, the same process begins in the right camera tube and\nright-image shift register. Thus, when the left image reaches the \t\nend of its shift register, the right is only 36 positions behind. Thereafter, the right marches past the left and the two are compared after\neach shift.\nI\n\nThe number of pixels by which the image of a point in the right\nimage falls short of overlapping the image of the same point in the left\nimage is called the disparity of that point. In Fig. 3, the effect is\nshown of comparing the left and right images when the disparity\nbetween left and right images of a point is 35, 34, 33, 32, 31 and 30.\nFor example, when the disparity is 35, a spurious match is formed\ndue to the fact that images with the same gray level are not necessarily images of the same point in the scene. When the disparity is\n34, two matches are made, one for each side of the rock. The process\nof shifting and comparing continues until, at d = 30, the edges of the\nrock have been mapped.\nThat comparisons are made for only 36 values of disparity is\ndue to the size of the memory in the computer used for the simulations. Only one form of an x\' -d plane is shown in Fig. 3, namely,\none in which successive lines of matches are "justified" to the right,\nas viewed from the direction of the camera. (The word "justify" is\na printer\'s term which means to line up lines of type evenly. )\n\n10\n\n+\n\n\t\t\n\n=\t\n\n.y\t\n\n^.^\t\n\n.0\t\n\n4\t\n\nN\n\ncr\nl.r\t\n\n$\nu\t\n\nO\t\n_\t\n\n4\t\n\nO\t\n\nN\n^\n\nT\n\nL\n\nU\t\n\nQ\n\nJ\t\n\t\n\nI\n\n^\t\n\n,\n\xe2\x80\x94 \xe2\x96\xba\n\nS\n\nV1\n\n= LL\t\n\nQ\t\n^\n\n1\t\n\n2\t\n\nQ\t\n\no\t\n\n\t\n\nac\n\nr.7\n\ntt\n\nW\n\nU\t\n\nQ\n\na\nLA\n\nZ\t\njo\n\nN\n\nN N\t\nS\n\nd Q\n\nN\n\\\t\n\n/\n\n\\\n\no\n\n^\n\nLL\nzZO\n\n\\\\\n\nv\t\n\n\\\\\n\nZ K d\n\nTom\xc2\xb0\n\nr\n\n11\n\nX t/f\n\n\\\n\n^ U\ns^ a\nJ =\nd U\n\nac\n\na\n\nO\n\n3\n\ncd\nr^\n\n^, a\n\ncb\n\n\xe2\x80\xa2a U\n\nO\ncd\n\nA\n\nN\n\n4^\n\nO .S\n\nO\n\nY\n\nr\n\n0\n\nO\n\na\n\nCU\n\nCL M\n\nO O\nU\n\nU (1)\nO cd\n\n4-4\n\nE\n^ ++\nU\ncd Cd\n\nAE\nM\n\nrr^\n\xe2\x96\xba^1\n\nD. Methods of Mapping Binocular Space into Match Space\nFig. 4 shows a plane of binocular space divided into quadrilaterals of minimum uncertainty, determined by the division of each\nscan line into pixels. We say "minimum" because the number of\nquadrilaterals will be as few as illustrated only if, (1) a camera tube\nis employed, of which the maximum resolution in TV lines approximates the division into pixels shown here, and (2) the spatial frequency of the high-contrast detail in the scene is that which leads to\nthis resolution.\nThe uncertainty pictured in Fig. 4 is due to the choice of focal\nlength of the lens and to the characteristics of the camera tube. This\nuncertainty is optoelectronic. The uncertainty due to positioning of\nthe camera is electromechanical. The need to add the two uncertainties can be obviated, at least during a fixation, by rigidly attaching\nthe optoelectronics for strategy 1 to the optoelectronics for strategy\n2. The Type E1 stereo TV camera, described in V, is designed this\nway.\nFig. 4 shows the effect of projecting the pixels on the face of\nthe two camera tubes out into binocular space. Because we see them\nhere only in plan view, we call the intersection of two pixel rays from\nthe left a "quadrilateral of uncertainty". Actually it is a polyhedron\nof uncertainty (Ref. 21).\n\ni\n\nNote that the number of quadrilaterals formed by intersecting\nrays increases as z increases.\nWhen the lines of matches formed in match space are justified\nto the left, as shown in the lower right corner of Fig. 4, a rightcamera view is formed. This can be verified by following, first, the\nleft ray of the right camera which can be seen to form a continuous\nsuccession of quadrilaterals with rays from the left camera. Next\nfollow the succession of rays from the right camera which intersect\na single ray from the left camera in a manner that leads to the jagged\n\nt\ni\n12\n\ni\ni\n\nr\n\nV\n\nl\n\nF\nI\n\n,\n\nNI.,.^1 (.A! \xe2\x80\xa2 I PA VIf V.\n(1f -IA TC1 1 %PA( I\n\nI F 7 A-It NA \',It\'.\t\nit ".:1( - 14 %PA( f\t\n\n\'IITl^\t\n\nlil\n\nf\nl\t\n\nL7\n\nI NTf\n\nt\t\n1\t\n\nW\t\n\n1l :. \' f "AT, 1. \'\n\nT &- f\n\nFig. 4.. Rinoculnr space divided into quadrilaterals of\nuncertainty by rays drawn from pixels in left\nand right images. Along any ray only altermite\nquadrilaterals of uncertainty are shaded.\n\nright edge of the right-camera view.\nA left-camera view and a center view are also presented in\nFig. 4. The left- and right-camera views are needed for the method\nof eliminating spurious matches of areas presented in II J. In hardware, only one model of matches will need to be formed of binocular\nspace. The two views can be obtained by two sets of inter-wiring.\n{\t\n\nE. How Matches Are Made and Viewed\nFig. 5 shows how first-stage matches are made in the system\nof Fig. 3. Fig. 5 pictures, from above, the right ends of the two\nshift registers at three different positions of one scan line of the\nright image. At the first position, where disparity equals 32, two\nmatches are made, one of them spurious. In the second position,\nwhere d = 31, another match is made and in the third position where\nd = 30, another.\nExact (E = 0) first-stage matches such as these can usually be\nmade only between computer-generated images. Between images of\na real scene, tolerances are needed at both first and second stag\xc2\xb0s\nof match, to allow for noise in the electronics or noise in the scene.\nBy the latter we mean, fc,r example, unequal reflection of light from\nthe same spot in the scene to the different viewpoints of the two\ncameras.\nThe tolerance at the first stage of match is the allowed difference between the gray value of one pixel in the left image that is\nconsidered matched to a gray value of one pixel in the right image.\nWe call this tolerance c and assign it the values shown in the second\ncolumn of Table 1.\nThe tolerance at the second stage of match is in the form of a\nthreshold and accompanies our requirement that an N x N area of pixels\nsurrounding one pixel in the left image be compared to an N x N area of\n* This is analogous to "local stereopsis" in man (Ref. 22).\n\n14\n\nSHIFT PEGISTERS\n20\t\n\n24\t\n\n21\t\n\n23\t\n\n7\t\n\n19\t\n\n10\t\n\n12\t\n\n21\t\n\n6\t\n\n27\t\n\n20\t\n\n5\t\n\n25\t\n\n629\t\n\n19\t\n\n11\t\n\n10\t\n\n31\t\n\n12\t\n\n20\t\n\n1\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n1\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\n\nOF RIGHT IMAGE\n31\t\n\n21\t\n\n\t\n\nOF LEFT IMAGE\n\nd=32\n\nSPURIOUS MATCH\n\n2\t\n\n20\t\n\n24\t\n\n21\t\n\n23\t\n\n7\t\n\n19\t\n\n10\t\n\n12\t\n\n21\t\n\n6\t\n\n20\t\n\n5\t\n\n25\t\n\n6\t\n\n29\t\n\n19\t\n\n4\t\n\n10\t\n\n31\t\n\n12\t\n\n20\t\n\n31\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n1\t\n\n0\t\n\n0\t\n\n0\t\n\n0\n\n27 I\n21\n\n\t\n\nd=31\nLINES IN\nM-SPACE\n\n12\t\n\n2\t\n\n20\t\n\n24\t\n\n21\t\n\n23\t\n\n7\t\n\n19\t\n\n10\t\n\n12\t\n\n21\t\n\n6\t\n\n27\n\n20\t\n\n5\t\n\n25\t\n\n6\t\n\n29\t\n\n19\t\n\n4\t\n\n10\t\n\n^1\t\n\n12\t\n\n20\t\n\n31\t\n\n21\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n0\t\n\n1\t\n\n0\t\n\n0\t\n\n0\t\n\nd=30\n\nFig. 5. Computation of first -stage matches at the\nright ends of three lines of the x\' -d plane\npicture- in Fig. 3. The tolerance of first-stage\nmatch, E, is here C.\ni\n\n15\n\npixels surrounding a second pixel in the right image. The number\nof first stage matches required for a second stage match is\n? N 2 (LOWLIM/100), where LOWLIM/100 is a preselect,, I percentage\nof the first stage matches. Values of N and LOWLIM/100 used in the\nexamples of this paper are shown in Table 1. As will be .explained in\nII K, the program EXPER increases the search area beyond N 2 to\nresolve "ties", i, e. , surfaces of the same number of first-stage\nmatches, in an attempt to find the larger area.\nThe above description glosses over the step that preceded the\nsimulation of the matching of the left and right images of the Marslike scene pictured in Fig. 16. First, the left image of Fig. 16 was\ndigitized and stored on magnetic tape. The camera was then moved\n50. 8mm. to the right and the right image digitized and stored on magnetic tape. A human operator then determined the vertical disparity\n(y shift) of the left and right images by matching the digitized video\nwaveform of one scan line of the left image with the digitized video\nwaveform of a scan line of the right image with the aid of the program\nMSTUDY. The scan lines he compared were of a region of uniform\nrange, namely, the flat back drop. The operator eliminated the\nvertical disparity by entering its amount, usually only a few lines,\ninto the program STEREO.\nSTEREO forms on magnetic tape the match space required by\nthe next simulation program. (Magnetic tape is used only in the\nsimulation.) The proposed hardware, described in Section IIi, would\nhold only as many digitized lines of an image as are required by the\nfilters that examine them. It would store only N planes of m -space.\nThe need for the y shift is eliminated in the C3 camera\ndescribed in Section V, but not in the D1 or E1 cameras. For the\nlatter two, either the amount of the vertical disparity will have to be\ncomputed automatically and used to match the two images vertically\nor the next stage of computation must be designed to tolerate vertical\n\n16\n\nTABLE 1\nVALUES OF THE MATCH VARIABLES\n\nSubject\n\n\t\n\nE\n\nin STEREO prograrn\nRandom-dot stereogram\n(Fig. 9)\n\n\t\n\n0\n\nN\t\n\nLOWLIM/ 100\n\nin STROUT program\n\n3\t\n\n. 50\n\nin EXP ER pro ram\n\n}\n\nRandom -block stereogram \t\n(Fig. 14)\n\n0\n\n3\t\n\n. 50\n\nMars-like scene\t\n(Fig. 16)\n\n4\n\n13\t\n\n. 50\n\n11\t\n\n1;\n\n1\n\ndisparity. The nervous system of vertebrate animals tolerates some\nvertical disparity in computing depth (Ref. 23) and interprets this\ndisparity in the "induced effect" (Ref. 34).\nThe contents of m-space can be displayed in the form of either\nx\' -d sections, such as those of Figs. 10, 11, 12, 15 and 17, or in\nthe form of a range map. An x 1 -d section of m-space can be generated\nfor any value of y by the program MSTUDY. The range map is\ndescribed in II I.\nF. Use of Model in Match Space\nAfter a model has been formed in match space at least two\nquestions need to be asked: (1) Is each match plotted there probably\ntrue or probably spurious? 2) Are there surfaces in the original\nscene that are viewed by one camera, not the other and are therefore not modelled in match space? We call such surfaces "ambiguous".\nSubsections II H to II L present the rule: we have devised to\nanswer the above questions. The rules evolve in three stages and are\nillustrated by scenes of increasing complexity.\nThe simplest scene is oi.^ generated by computer from dots of\nrandom values of gray, briefly called "random dots". ( Our usage\nhere differs from that of Bela Julesz (Ref. 24). He uses the term\nit\n\t dot" to describe a two-value, black-and-white display. )\nSuch a scene is simplest because the probability of a spurious surface\nin match space is negligible.\nA scene of higher order complexity is again computer-generated\nbut now of random uniform areas of gray. We call such scenes "random square" or "random-block" pictures. Here the probability of a spurious\nsurface of matches is greater, because any spurious event will automatically be a surface.\nA third order of complexity is a real scene. This can be\nprocessed by the rules devised for random square pictures.\n\n18\n\nt\n\nG. Geometry of__Binocular and match Spaces\nBefore proceeding with rules for interpreting a mc^dt-1 in match\n\nspace we need to take another look at the geometry of a stereo TV\ncamera assembly. Figure 6 diagrams the same two cameras pictured in Fig. 3, the same binocular and match spaces, except that\nthe views are now from behind the cameras, The disparity between\nthe images S L and S R of the point P is the difference, dL-dR,\nwhether the object is between the optical axes, as in Fig. 6 or at\none side of them as in Fig, 7,\nThe example for which range needs to be computed is the\nMars -like scene of Fig. 16. The range, z, of any point, P,\nmeasured from the optical centers of the lenses, is\nR\'\n\n2bf\t\n\n(1)\n\nCT\nti\n\nwhere the variables have the meanings given in Fig, 6, (For a\nderivation, see Appendix A. 1. ) There is an uncertainty, ^z, in this\nmeasurement for which an equation is derived in Nppendix A. 2.\nIn the system, the operation of which we describe here, the\nnumber of pixels in each line scanned by the TV camera for both left\nand right views is 512. That is, the counter that determines the\nposition of the electron beam along the horizontal axis of each TV\nimage counts to 512. The counter that determines the vertical\nposition also counts to 512. However, only the central 256 columns\nand 256 scan lines were used in acquiring the images of Fig, 16.\nEach computer-generated image used as an example in the next\nthree subsections also measures 256 x 256 pixels. Only 128\ncolumns, approximately at the center of these images, were com pared and the matches plotted in the x\' -d sections shown in Figs,\n10, 11, 12, 15 and 17.\n\n19\n\n\t\n\nBINOCULAR SPACE\n\nP\t\n\n\t\n\n/I\n\nP;\t\nT-\t\n\ni\n\n1\n\nPLANES a\n\nINTEGRAL\t\nVALUES OF\t\n\nI\n\nDISPARITY\t\n\n1\n1\n\n1\n\nfi\n^i\n\nMATCH\t\n\nI\n\nSPACE\t\n\nk\n\nREPRESENTING\t\n\nY\n\n---------------\n\nBINOCULAR\t\nSPACE\t\n\n_I\n\n1\n\nI\t\n\n1\n\n- L\xe2\x80\x94 f \xe2\x80\x94\t\n1\t\ni\t\n\n--\n\n1\n\nI\nI\n\n1\t\nIDI\t\n\nz\xe2\x80\xa2\n\n1\n\nI\t\n\nI\t\n\nf\t\n\nto \xc2\xb1pz\n\n1\n\nf\nI\n\nI\n\n^\t\n\nI\t\nZD \xe2\x80\xa2 INTEROCULARDISTANCE\t\n\n1\n\nI\t\n\nZ\t\n\nI\n\n1\n\n1\t\n\nI \xe2\x80\xa2FOCAL LENGTH\nd\t\n\nDISPARITY BETWEEN POSITIONS OF IMAGE\t\n\nII\t\n\nd ,d\t\n\nI\'\nY\t\n\nI /\n-^\n\n1\n\nLENSES\t\n\nV\n\nI\n\n1\n\nSL\t\n\nI\t\n\n/ 111`I\n\n^\t\n\n^\n\n1\t\n\n^\n\n11\n\nIMAGE PLANE\n\n4RF^\n\n^\t\n\ndl\n\nRIGHIJIMAGE\n\nLEFT IMAGE\t\n^\t\nul\n\nFig. F. (a) Geometry of Fig. 3, when object viewed is\nbetween parallel optical 2xes.\n(b) Match space corresponding to the binocular\nspace in (a).\n\n20\n\nz\n\n\t\n\n3^\t\n\nul\n\nu^\nv\t\n\n^\t\n\n^\n\n1\n\nz\t\n\nL[ NSE 5\n^JIIVACE PEANL\nSE\tSR\n4R\n\ndE^\t\n\nFig. 7. Geometry of Fig. 3 when the object viewed is at\nthe left of both optic axes.\n\n21\n\nH. Rules for Processing Ran.iom-aot Stereograms\nRandom-dot stereugrams were generated in our experiments to\ninvestigate the detection of spurious matches and the elimination of\nambiguous regions. Figure 8 is a plan view of objects, O, which\nhang in space t f ore backgrounds, B. Figure 5 is a stereogram of\na scene \'like that in Fig. 8 formed from identical random dot patterns.\nI\n\nA 64 x 64 pixel region in the left backgrnlind was shifted 5 pixels to\nthe right so that it appears nearer than the background. The region\nuncovered by the shift was tilled with more random dots.\n\ni\n\nThe simulation program STERZ\'O compares the. images of\nFig. 9, makes first-stage snatches and ma p s them in center-view\n\nt^\n1\n\nm -spa:,e. STEREO produces one plane of first stage matches at d=C\ncorresponding to the background, a second plane of matches at a=5\ncorresponding to the S q uare floating \'n space and some spurious\ni\ng n. tches . Figure 10 s\xc2\xb1iows an x\' -d section of th i s m -space. (The\ncheckered area of m-space i.-i Fig. 3 is a similar section. )\nThe simulation program STROUT examines each "region" which\nis a volume extending from front to back of m-space, N pixels high\nand N pixels wide, seeking a surface of at least N2(LOWLIM/100)\nmatches in a plane perpendicular to the optical axes of the cameras.\nA more complex simulation program (and a more useful one) would\nsearch for arbitrarily oriented surfaces in m-space, When, in\nexamining the stereogram of Fig. 9, STROUT finds a plane that\nmeets the conditions N = 3, LOWLIM = 50, it maps the point in a "range\nmap". By this we mean a one-eyed view of the scene in which each\nA good quality viewer for the stereograms in this report is the\nModel PS-2, made by Air Photo Supply Corp. , 158 South Station,\nYonkers, New York, 10705. Use 63mm sepa: at i.on of lenses in\nthis viewer unless your eyes are closer together or further apart.\nMost copies of this report contain a viewer made of cardboard\nand plastic lenses. This viewer can be purchased only in a large\nquantity.\n\n22\n\nREGION\t\nvit WtU\nMr a(".. ^\n^ ^ AMl R41\t\n\n\t\n\nW\n\nRI IN\nV^Iw1U\n\n^ HIDDEN REGION ^ L Rv ., .\nrt: AMI PAS\nBACKGROUND B\n\nOb,EC.T 0\n\n\\\\\n\nLEFT\t\nRIGHT\nCAMERA CAMERA\nnl -;,\xe2\x80\x94, pirhn ed in the .trrcoK rum .d 1\'iKnrr \'\xe2\x80\xa2\n..nd wurl. ll y d in the m-,lmcd or I .Kure 10.\nHraiun a l 1, arnbiguou, hecau.e \\u \xe2\x80\xa2 wed only by\ntha Irtt c.unera. lteaion it is amhi Kuou- he-m-e\nviowed oniv b- the right camera.\n\nREGIONS v15wED By\nFRIAS\nB.O.T.IIH CAMI{\n\nI\n\n1t\xe2\x80\x94\t\n\nBACKGROUND ,0\n\n^^\n\n1\t\n\n\'\n\n\\\t\\\t\n\n\\ I\n\nI\n\nOBJECT. U\n\nI\t\n\nit\t\n\n^\n\n^/ \\\\1\nRIGHT\nCAMERA\n\nLEFT\t\nCAMERA\t\n\nb) Scene \xe2\x80\x941,11ed in the rn-apace of 11911. c II11.\nThe object 1. now so small and near that\nthe camera, view the hackKround behind it.\n\nFig, a. Why the range of the parts of the background,\nat the left and right sides of an object, is\nambiguous.\n\n23\n\nFig. 9. A rar_dom -dot stereogrum depicting\nsquare flo.-Aino b^fure a background.\n\n24\n\npoint represents, by a gray value, the disparity (or range) of the\nnearest surface in the scene at that pixel,\nBecause it requires that a percentage of the points in an N x N\nplane be first-stage matches, STROUT rejects isoiated first-stage\nmatches as spurious. When the image of a plane perpendicular to\nthe z axis (Figs. 8 and 9), is modeled in m -space (Figs. 10 and 11),\nall the matches lie in one x\' -y\' plane. Thus a spurious match in a\ngiven x\' -y\' plane will not have many neighboring matches in that\nplane, while a second stage match will adjoin other matches.\nWhat should be done in a region of m -space where there is no\nsecond-stage match is simple for the examples given in this report,\nbut can be complicated for other examples. Le; us consider first\na\t\n\nthe simple examples so far presented. STROUT labels as ambiguous\nregions of m-space where no match is found (Figs. 8, 10 and 11). A\nsubroutine RESOLV then follows the observation of Julesz (Ref. 25)\nabout simple stereograms such as that in Fig. 9: "Regions of ainbiguity are always perceived as being the continuation of the adjacent\narea that seems farthest away". RESOLV searches m-space one\nplane at a time for regions marked as ambiguous. When it encounters\none, it examines the first non-ambiguous region to the left and right\nand chooses the one which represents a surface farthest from the\ncamera to replace the marked ambiguity. The regions a and a\n(Figs. 8 and 10) will then have been filled in and the ambiguity\nremoved.\nThis method of treating ambiguous regions is appropriate\nwhen tie object viewed is a plane, as in Fig. 9, but suppose the\nThis process is analogous to what Julesz calls "global stereopsis".\nTo quote him: "With increased dot density the visual system canr_ot\nfind uniquely the corresponding points, and a new process has to be\ninvoked which can resolve ambiguities by global considerations.\n(Ref. 22)\n\n25\n\naL\n\n-^\n\nI ---\t\n\nW\t\n\ni ^-\n\naR\n\nd\ndob\n\nd i ---4-\n\nX\n\nFig. 10. An x\'- d section of the m-space generated\n\nfrom tho stereogram in Fig. 9.\n\naL ^\nd\n\nI\' ^ ^\xe2\x80\x94aR\n\nd\n\ndob\n\ndol\n(a)\n-\' W ^\xe2\x80\x94\t\n\nFig. 11, x\' -d sections of m -space showing, in (a),\nambiguous regions a and a g on the left and\nright sides, respectively, of Lne model of the\nobject and, in (h), how both the image of the\nobject and the background behind it are\nmodelled when w < (d\nobj - db).\n\n26\n\n(b)\n\n1s\n\nobject is solid and the left side is viewed roily by the left camera\nand the right side only by the right? For such a scene, another\ntechnique is needed; namely, one in which the memory of textures\nand patterns, viewed before, is applied to interpreting what can be\nviewed by only one camera. This is cognitive processing.\nI. How Should Ranges in a Scene Be Presented?\nIn building a sequence of coniputcltions into a process, such as\nthe detection of range and shape, means are needed of viewing the\nsteps in the process. Such a means is a projection onto an x\'-y\'\nplane of all values of disparity in in -space. Since an array of\nnumbers is difficult for a human being to interpret, we have written\n\xe2\x80\xa2 program PICT which displays these numbers its levels of gray on\n\xe2\x80\xa2 cathode ray tube where they c:in be photographed. Figure 18 is\nsuch a photograph. While the levels of gray correspond to measurements of disparity, the impression given the viewer is that of range.\nHence we call this display a "range map".\nAfter using range maps for several years we find that they\nsuffer a defect. When, as in Fig. 8b, the Object O is small and\nnear enough to the cameras that they vie ^- both the object and its\nbackground, the range map is unable to report both. In Fig. 8b the\nregion between the two ambiguous regions, a and a R , is visible to\nboth cameras, but the range map cannot show it.\nA stereogram range map could obviate this difficulty. How ever, if EXPER did not eliminate the more distant of two surfaces\nin the same region, it might not eliminate spurious area matches\neither. Therefore, for the present, we accept the limitation that a\nsurface behind a front surface cannot be shown in a range map.\nJ. Eliminatin g A reas of S purious Matches\nBecause areas of spurious dot matches are not likely to be\n\n27\n\ni\n\nI\n\nformed from a random-dot stereogram, STROUT is not equipped to\nreject such areas. Such areas are likely to be formed both from\nrandom-square stereograms (Fig. 14) and from stereograms of real\nscenes (Fig. 16). The latter tends to contain areas of the same\nvalue of gray (within tolerance\n\n0 because its dots are not random.\n\nTo eliminate areas of spurious dot matches we devised two\nsimulation programs, EXPER and FUSER. Let us consider with\nthe aid of Fig. 12 how EXPER might be used alone for this task. It\nwill be shown in Fig. 15 that a model in m-space of an area of\nuniform gray in a scene appears as a parallelogram in an x\' -d\nsection of m-space. Because parallelograms are awkward to\nillustrate in this stage of our discussion, we will continue to use a\nline to represent a plane in Figs. 12 and 13.\nAssuming again that the background of the scene pictured in a\nstereogram will be continuous, we can see how right-view and leftview m-spaces can be used to reveal whether or nct a surface is\nspurious. Consider again the scene mapped in Fig. 8b. Three\nforms of the simulation program STEREO form, from a stereogram\nof this scene, left-view, center-view and right-view models in\nm -spaces. (The program MSTUDY generates the x\' -d sections of\nthese m-spaces shown in Figs. 12a, 12b and 12c. ) Because, in\nFig. 12a, the model of the object hides the right ambiguous region,\nand, in Fig. 12c, the model of the object hides the left ambiguous\nregion, the model of the object is cons-_dered true, not spurious.\nTo make this check automatically, the data of the left-camera and\nright-camera views needs to be converted ;o center-view m-spaces\nand these m-spaces compared. That will be done by the program\nF USE R.\nEXPER performs more operations than we have so far\ndescribed, as Fig. 13 shows. Figure 13 begins at (a) with the plan\nview of a simple scene: an object O in front of a background B.\n\n28\n\n-K-\n\na\n\n\t\n\na\n\nt:\nd\n\na\n\nL\n\n(a) Left -camera\n\nvieti\xe2\x80\xa2\n\ndF\n\nd L\n(b) C:unter view\n\nd\n\n((,) Eight-camera view\na\nd\n\nFig. 12. x\' -d sections of three match spaces formed\nfrom the same stereogram.\n\n29\n\n\t\n\n^\t\n\nA\t\n\n/\n\nStFRFC CAMrPA\nA) VISUAI SPACE\nIM STUDY\n\n%S\n\nal-a\t\n\nI\t\n\n-4 I-\n\nsr\n\ng\ni\n\ndLe\'\t\n/\t\n\naR\n\n0\n\nbl m-space of the scene above\t\n\n\\\n\ncontaining spurious surfaces.\n: I and S2\nSTEREO\t\n\nSTEREO\n\nSt -----Z -_\n\nSt\t\n1\t\n\nSPURIOUS\nSURFACES,\t\n\nB\t\n\nB\t\n0 \xe2\x80\x94\t\n\ng\n\nB\t\n0\n\nt\t\n\nS/\t\n2\n\nLeft-view m-SPICe\t\n\nS\t\n\nRight-view m-space\n\ncl\t\n\nEXPER\n\nEXPER\t\nIS\n1\t1\n\nS ?\t\n\n1\n\nA\t\n\nSt\t\n\nB\t\n\n1\n1\n\nB\t\n\nB\n\n0\n\ndl m -s paces filer EXPER has eliminated smaller of each pair of overlapping\nsurfaces and RESOUI has filled in ambiguous region.\n\\FuSER\t\n\nFUSER/\nSi\n\nS\t\n\ng\t\n\ng\n\nB\t\n\n2\t\t\n\nB\n^\xe2\x80\x94\n\nel FUSER forms both m-spaces as i\' viewed from center viewpoint\n\nt\xe2\x80\x94\n\n^\xe2\x80\x94\n\nIf FUSER preserves In Final m-space only those surfaces\ncommon to both m-spaces. RESOIV fllls In ambiguous regions.\n\nFig. 13. Diagram showing the functions of the three\nsimulation programs, STEREO, EXPER\nand FUSER.\n\n30\n\nStep (b) is performed by AISTUDY to show ;in x\' -d sect ion of renterview m-space anti two spurious niAche , S ;,nci S 2 , which happen\nto hide the two Ambiguous regions\n\n,aid\nSTEREO forms leftaR\'\na\nview and right-view m-spaces of which NIS71\'DY forrns the x\'-d\nsections shown at (c).\nWhere there :ire two or more surfaces in :i region of either\nleft-view or right-view m-space, EXPER eliminates ;ill except the\nlargest in the following manner. At each set of values of x\' and ,-,\nEXPER begins by searching an N x N region, in ,his case 3 x 3, for\npossible surfaces. If two or more are encountered, EXPER enlarges\nits area of search until it finds the largest. It then retAins only the\npoint on this largest surface. For example, in the left -view of\nm -space (Fig. 13c), because S is smaller than O, EXPER eiim -\n\na\t\n\ninates S 1 ; and, because S 2 is smaller th:in O, EXPER preserves\nonly those points in S 2 which do not hide pc,ints in O. In the right\nview of m-space, because S is smaller than B, EXPER preserves\nonly those points of S 2 not hidden b y B; and, because S 2 is smaller\nthan B, EXPER eliminates S2.\nEmploying the subroutine RESOLV, described in II H, EXPER,\nin the right-view m-space, extends S leftward into the ambiguous\nregion between O and S 1 . The results of these operations \')y EXPER\nare shown in Fig. 13d. Note that S. survives inthe left-view m-space\nand S 1 has grown in the right-view m-space. Thus EXF-R alone\ncannot eliminate all areas of spurious matches.\nA simulation program that compares the EXPER-processed\nleft- and right-view m-spaces to complete the elimination of areas\nof spurious matches is FUSER. From each rn -space illustrated in\nFig. 13d, FUSER forms the two centrr-view m -spaces shown at (c).\nFUSER then compares these two m -spaces, preserving only those\nsurfaces common to them both. Finally, FUSER emplo,,-s RESOLV\nto fill in remaining ambiguous regions.\n\n31\nI\n\nThe assump^ion is that each surface in the scene is opaque,\nand therefore blocks out, behind it, one region from the left camera\nand another region from the right camera. Forming left-view and\nright-view m -spaces is an attempt to check for this condition, Comparison of what is mapped in the left-view m-space with what is\nmapped in the right eliminates surfaces that do not satisfy this\nconstraint.\n\ni\n\n.\t\n\nK. Random -Square Stereograms\nFigure 14 is a random-square stereogram of a large square\nfloating in front of a background. It is formed of random -gray -value\nsquares measuring 4 pixels on a side, figure 15 is an x\'-d plane\nof center-view m-space formed by MSTUDY from this stereogramr,.\nThe match of left and right views of each random square of\nFig. 14 is a diamond because each square in binocular space is\nperpendicular to the z\' -axis. If the square is skewed with respect\nto this axis, the match is a parallelogram. The shape, whether a\ndiamond or parallelogram, is formed as the left view marches past\nthe right in the scheme of Fig, 3. At the first overlap of left and\n\'right images of a uniform area of gray, the point of the parallelogram is formed. After a shift of the right image, the next line of\ntht parallelogram is formed, two pixels wide, at a larger value of\ndisparity. When the two sma.11 squares overlay each other, the\nwidest part of the parellelogram of matches is formed.\nIn its search for the largest number of matches along each y\' -d\nline EXPER finds the widest part of each diamond and retains only\nthat. Thus the diamonds of Fig. 15 are reduced to the lines of Fig. 13.\n\nr,\n\nA\t\n\nL. Processing of a Real Scene\nFigure 16 is a stereogram of the scene of Fig. 1 recorded when\n\n32\n\nFig. 14. Random -block stereogram of a square floating\nbefore a background. Each block is 4 x 4 pixels\nof uniform gray value.\n\nFig. 15. x\' -d section of model in m -space generated\nfrom the stereogram in Fig. 14.\n\n33\n\ny\n\nn\n\nthe TV camera was approximately 2m from the rock and the lighting\nwas from upper right. Other test conditions are given in Tables 1\nand 2.\nOne camera was used to obtain the left view, then moved\n50. 8mm (2 in. ) to the right to obtain the right view. The axes of\nthe camera in its two positions were parallel. Since the axes of the\ncameras were not changed, this was the fixation viewing of the\nsecond strategy of II B. Since a lens of 25. 4mm focal length was\nused, the resolution was that of the first strategy. Such a compromise is necessary when only one focal length is employed.\nFigure 17 is an x\' -d section through the left-justified m -space\nformed by STEREO from 128 columns of a stereogram similar to\nFig. 16. The section is at the y-value of the stereogram indicated\nby the two black lines in the margins of Fig. 16. One of the diamonds\nin Fig. 17 models the stick. Another may model the shadow of the\nstick. The right side of Fig. 17 contains at the front spurious\nmatches and further bank true matches of features in the painted\nbackdrop.\nFigure 18 is a range map formed by EXPER from the m-space\nof which Fig. 17 is a section. Lightness of gray indicates nearness,\ndarkness of gray, distance. Thus the rock stands out ;Iearly. Two\nshades of gray at the top of the rock, one shade hooking to the right,\nindicate how long the rock is. The stick and its shadow get progressively darker as it recedes, leading to the backdrop which is a\nuniform black. Because occluded regions have not yet been found\nand filled in, two of them appear as blacx areas below the rock and\nbelow the stick. A picture was made of the output of FUSER after\nit had filled in these occlusions, but it was poorly displayed so we\ndo not include it.\n\n34\n\ni\n\nt\n\nv\n\nI\n\ny\n\nFig. 16. Stereo rmages of the \\lass -like scene of Fig. 1,\nacquired Fy television camera, digitized, then\ndisplayed one at a time or. an oscilloscope.\n\nFig. 17. a\' -d section of the right camera view of march space\nformed from the siereogram of Fig. 16. The y value\nof this section is indicated by lines in the margins of\nFig. 16\n\n35\n\nTABLL 2\nTEST CONT)YVIOr?S\n\nOR EXA "/IPL,E OF !; L\n\n(In sr?C:iion tc ;hose give.i in the bottom line of fable 1)\n\nInterocular distance, 2b\nFocal length, f\nt\n\n= 50, 8mm (: i n. )\n= 25.4mm (1 in. )\n\nDistance from camera to\nnearest object\n\n= 2rn (approx. )\n\nCamera tube type\n\n= GEC TD84e4\n\nTV camera and control\n\n= Colorado Video, Inc. , Type 501\n\nComputer\n\n= Digital Equipment Corp. PUP-9\nwith 8K Kurds of core memory\nand 2 DEC tape drives\n\nDisplay oscilloscope\n\n= Tektronix 5Y\n\nDisplay-tube type phosphor\n\n= P4\n\n36\n\nI Lg. 18. Range map of one match space of the stereogram\nin Fig. 16. The black :areas below the rock are\nambiguous because they were viewed by the\ncamera in one position and not in the other\n\n7\n\nM. Determining Form\nMcCulloch observed that texture and form are not primarily\nvisual phenomena. They are "the way a surface o^ object would\nfeel if you could feel it" (Ref, 26). To determine form this way\nrange data should be fed both to a touch system and to the controls\nof an arm and hand that is capable of reaching into the space\nviewed by the stereo TV camera. Since judgement of form from\ncamera input data will be only a guess, the arm and hand can confirm or deny this guess. Design of a system to operate this way\nis considered in the final paragraph of V.\nN. Range Accuracy\n\ni\n\nHow accurate is the above system, assuming that the ambiguities just described have been removed and the spurious matches\neliminated? How can range accuracy be increased?\nThe stick in Figs. 1, 16 and 18 is 1. 27m (50 in. ) long. In the\noriginal Polaroid print of Fig. 18, there are seven levels of gray\nalong the length of the stick. Dividing seven into 1. 27 indicates that\nintervals of range have been detected of about 18cm (7. 1 in. ). This\nis approximately the uncertainty that is predicted when measured\ncharacteristics of the camera are substituted into Eq. (2) below.\nThe rock, the length of which is 20cm from front to back, is shown\nas two levels of gray.\nAppendix A derives the following formula for range uncertainty\n\nAs \xe2\x80\xa2 z2\nAz = bf -As \xe2\x80\xa2 z\n\n(2)\n\nBecause As is very small with respect to z, the second term of the\n\n38\n\ni\n\ndenominator may be ignored. Eq, (2) then becomes\nA\nAz = ^^\t\n\n(3)\n\nAssuming that the camera tube is selected for the smallest\npossible uncertainty, Ns, in the position of a point, it can be seen\nthat range accuracy can be increased either by increasing the focal\nlength, f, of lenses, by increasing the separation of the optic axes,\n2b, or by bringing the camera nearer to the objects to be examined\n(smaller z). If longer focal length lenses are used to examine\ndetails, short focal length lenses are still needed as finders of the\nde;;ails to be examined (strategy 1). Thus, a stereo TV camera is\nneeded with lenses of two focal lengths. If wider separation is used\nbetween the optic axes, the axes need to be converged on the objects\nof Fig. 16, requiring trigonometric functions in Eq. (2). Such\nfunctions can be employed but were avoided in this first pas- through\nthe problem. Obviously the camera could have been brought nearer,\nbut then it could not have viewed as many objects as in Fig. 16.\nIncreasing by a factor of ten the focal length of the camera lens\nemployed in the test described in II L will reduce range uncertainty\nto 1. 8cm (0. 7 in. ). Increasing the interocular distance by a factor\nof 4, to widths described in V, will further reduce the range uncertainty to 0. 45cm (0. 18 in. ). However, these changes can bring\nother problems. Lenses of focal length this long cannot be accommodated in the Type C3b camera. While such lenses can be accommodated in the Types D1 and E1, there is a problem of vertical\nmisregistration (vertical disparity) in these types, as explained in\nII E, which special computation is required to remove. Solutions to\nthese problems are being devised.\nti\n\n39\n\nSUUAHE-WAVE SPATIAL\nY IN TV LINES\nOF TEST PATTERN\n\na) "Line Selector" test pattern (Westingl.ouse resolution chart\nET-1332 purchased from Tele-Measurements Inc, ,\n14:1 Main Avenue, Clifton, N. J. ), reproduced 1/3 full size,\n\nSQUARE -WAVE SPAI IAL FREQUENCY\nIN TV I INES PER MOTH OF TEST PATTERN\n120\t\n\n36\t\n\n25;\t\n\n375\t\n\n530\t\n\n692\t\n\n9\xe2\x80\x9eJ\n\n30 ^ ----n P\'I\n\n25 -\n\n20\nI\nS\n\nGRAY\n\nLf\n\nVALUE\n\nUl\nOFD\n60\t\n\nI\n70\n\nI\n00\t\n\nI\t\n\nI\t\n90\t\n\n- I\t\n\nI\t\n100\t\n\nr- T\t\n110\t\n\nI\t\n\nJ^\n- 1\t\n\n120\t\n\n1\t\n\nI\n130\t\n\nI\n140\t\n\n150\t\n\n160\n\nPOSITION ALONG SCAN LINE\n\nb) Plot of digital words formed by the system of Fig. 0 as the\nelectron beam in the camera sweeps the image of (a).\n\nFig. 19. Test pattern ;tnd amplitude response of TV camera\n\n40\n\n170\t\n\n190\n\na\n\nIII. COMPUTATION TO EXTRACT OTHER FEATURES\n\nAl. Square-Wave Frequency Response of Camera\nOther features, besides range and form, that may be detected\nin a scene, are the edges and lines shown to be detected by cats and\nmonkeys (and probably also by human beings), and the reflecting\nproperties of surfaces of interest.\nThe computation of an edge by a TV camera-computer system\nneeds to be described in the language of TV cameras, computers and\npicture processi.ng . A TV camera makes a "square-wave response".\nwhich is co.verted into "digital words" for the computer, Each word\nindicates the "gray value" of a "pixel". An edge is a difference in\ngray values (Ref. 27).\nTo determine the square-wave frequency response of the camera\nwe replaced the stereo optics of Fig. 0 by a single lens and aimed the\ncamera at the transparency of Fig. 19a which we :lighted from behind.\nWe positioned the camera so that the transparency and its margin\nwere just included within the scanned area of the camera tube,\nFigure 19a provides spatial square waves of 11 different frequencies, seven of which are labelled. A square -wave spatial frequency, to a TV camera, is the number of lines, both "black" and\n"white", that can be imaged in the scanned area of the camera tube.\nWe measure this frequency in TV lines per width of test pattern. Be cause there are both a "black" and a "white" TV line in each cycle,\nthe number of cycles per width of test pattern is one half this number.\nAs the electron beam in the camera tube scans along one scan line,\nthe camera generates a voltage which an analog-to-digital converter\nchanges to digital words. The voltage from one scan line is converted\nto 512 digital words, one for each of 512 positions along the scan line.\n\n41\n\nFigure 19b plots digital words formed as the beam sweeps through 120\nof these positions. (Of the 512 positions in the camera\'s image of\nFig. 19a, the central 256 are presently acquired by the computer,\nOf these, positions 60 to 180 are plotted in Fig, 19b. )\nEach step in Fig. 19b is a pixel whose gray value is indicated\nby a digital word on a scale from 0 to 31. Figure 19c plots the\naverage change in amplitude between the black and white halves of\neach square wave. The plot is made relative to the 35 TV lines/\nframe frequency. From this plot can be read numbers that charac terize the system, namely, the square wave response, in TV lines,\nat 10 1% 50 76 and 100 /\'o modulation: 470, 230 and 36.\n0\n\n1R\n\n\t\n\n1\n\n0\n\nA2. C om putation of Edges\nFigure 20 pictures the al\xe2\x82\xac orithm we designed to detect 11 coarse"\nedges. It is composed of six arrays of 0, 1\'s and -1\'s which we call\n,\n\n100\n\nR! I ATIVF\nAMPLITUDI\nOF RI SPONSt\t\nTOSOUARt YiAVI\n\ntb\n\nINPUT PAT TI RN\n\n61\n\nI I\t\n\nI\t\n\nIql\t\n\nII\n\n11.\t\n\nI\t\n\t\n\nJW\t\n\n1.\t\n\n!fib\t\n\n1\n\nW1U\n\nTV L.NFSIN WIDTNOF TEST PATTI RN\n\nc) Amplitude response. O\'s are a plot of the relative amplitude\nof response in (b) to the image of (a). X\'s mark the square w:ive spatial frequencies detected by the top three filters of Fig. 20.\nFig. 19 (Cont\'d) Test pattern and amplitude response of TV camera\n\n41a\n\nNumber of TV lines that can be detected in image of Fig. 19a:\n308\t\n\n146\n\n205\t\n\nFi Iter\nConvolution-type process\n\n1\n\nI\t\n\nA-\t\n\nMultiplication\n\n`\t\n\n1 0 -1\t I\t\n1 0 -1 * In1AGE\t\n] 0 -1\t\n\nx\t\n\n1 0 0 0 1\t\n1 0 0 0 1* IMAGE\t\n1 0 0 0 -1\t\n\nx\t\n\n1 0 0 0 0 0 -1\n1 0 0 0 0 0 1* IMAGE\n1000001\n\n1\n1\t\n1\t\n\nB=\t\n\n1\t\n\n1^\t\n\nx\t\n\n0\t\n\n1\n\n1\n\n0\n\n1\n\n0\t\n\nI\t\n\n0\t 0\t 0\t *\t (IMAGE\nu\t\n1\t\n-1-1-)\t\n\n1\t\n\n0\n\n0\n\n0\n\n0\t 0\t 0\t\n000\n\n*\t\n\nNIAGE\tX\nJ\n\n0 o 00\n000\n0 0 0\n\n*\t\n\nIn1AGE\n\nl\t l -1 -1\nK\t (A-,-B)\t\nK\t\n\nOutput for each position in the image\nConstant\n\nIf\n\nFig. 20. Operations performed on each 7 x 7 pixel array\nof a digitized image to detect edges.\n\n41b\n\n"filters". We assign a special meaning to *, namely, that each num ber in the filter will be multiplied by the gray value in a submatrix\nof the image, that is the same size as the filter, and the products\nsummed. (Actually, all filters are made the same size by filling\nthem out with zeros to measure 7 x 7 digits. ) To each 7 x 7 submatrix\nof the image all six filters are applied.\nSince our original goal was to form a line drawing from spatial\nfrequency information and our thinking was influenced by the methods\nof Fourier analysis, we attempted to add each of the terms of Fig. 20.\nThe result: were of little val l, e. Then we were advised by Dr. Azriel\nRosenfeld to multiply the terms as he did in Ref. 28. The results are\nshown first in Fig. 21* and, after thinning by searching for local maxima,\nin Fig. 22. * Multiplication here, as he said, is "counterintuitive\nit serves to detect an edge.\nThe filter at the upper left of Fig. 20 detects two TV lines (one\ndark, one light) in three pixels. Across the full width of the pattern\nof Fig. 19a,\n2/3 x 512 = 308 TV lines\ncan be detected by this filter, as indicated at the top of Fig. 20. The\nupper center filter detects two TV lines in five pixels, the upper\nright filter two TV lines in seven pixels. Along the lower row of\nFig. 20 are detectors of the same square-wave frequencies turned\n9 r1 0 . At the top of Fig. 20 are the square -wave spatial frequencies\ncetected by the filters below them. Plotting these frequencies as X\'s\non the graph of Fig. 19c shows the relative response of the camera\n\n* To get the effect of three dimensions, look first at the rock in the\nforeground, then at the crater in the distance, then at the rock, then\nat the crater, and so on. Occasionally alter the route by following\nthe stick or exploring the hills.\n\n42\n\ntube to the frequencies detected.\nWhen all six filters are overlayed, the 0 at their center is the\naddress of the edge that they detect.\nB. Formation of A Line Drawing\ni\n\n\t\n\nPlotting all of the edres detected by the algorithm of Fig. 20,\nin the images of Fig. 16, results in images which are printed in negative form in Fig. 21. That is, sharpness of edge results in brightness\nof display which is represented as blackness in Fig. 21. While the\nspan of gray values that can be displayed is only 0 to 31, the result of\nthe operations of Fig. 20 is often greater than that. For the display,\neach result is truncated at 31 so that, for every edge in Fig. 16, there\nare usually several lines of dots in Fig. 21. We call this a "coarse\nline drawing.\nA "drawing" was thought desirable, when this work began, as a\n\nI\n\n\t\n\nmeans of transmitting to earth the appearance of a Mars scene with\nminimum power. The power saving results from the use of a binary\n\n\xe2\x80\xa2 code in a raster that is always the same size. The position of a bit\nin the raster is thus given by the time of its arrival after an initial\nsynchronizing pulse. Figure 22 requires about 1/30 as much power\nto transmit ?= F\' i o. 16.\nFigure 23 shows how a coarse line drawing is thinned.\nFig. 23a plots the average of the gray values along seven adjacent\nscan lines in a digitized TV image. There is one gradual transition\nfrom light to dark at x 1 , one abrupt t: ansition at x 2 . Both are edges.\nFigure 23b shows the result of performing the computation of Fig. 20\non the scan line of Fig. 23a. The thinning routine detects local\nmaxima in gray value (A and B in Fig. 23b), locates x 1 and x 2 and\ndisplays them on the oscilloscope as the lightest gray (Fig. 22c).\nWe prefer to present the\n\n. "J\n\n43\n\n1\t\n\n^\n\ni\n^\t\n\n1\n:r\n\nFc\n^J\n\nr\t\n\ni.\n\ni\n\nF:g. 21. Negative of a d; -splay of the result of performing\nthe operations of Fig. 20 on the images of Fig. 16.\n\n1"\n\n^\\1\n\nFig. 22. Edges of Fig. 21 after thinning.\n\n44\n\nGRAY\nVALUE\n\n31\n\nX\ns\n\n0\n1\t\n\n-\n\na) Average gray values along seven adjacent scan lines of figure 16\nI\t\n\nI\n\nI\t\n\nGRAY\nVALUE\n\nI\n\nA\n\nX\nXI -\t\n\nX2\n\nb) Result of processing above lines by method of figure 20\n\nGRAY\t\nVALUE\n\nI\t\n\n1\nI\n\n31\n\n0\nX 1\tX2\nc) Result of detecting local maxima at X 1 and X2\n\nFig. 23. Detection of edge and thinning of edge.\n\n45\n\nnegative of such a display. F?gore 22 is a negative of the result of\napplying this edge-thinning operation to the data of Fig. 21.\nC. Hardware to Detect Edges\nDetection of edges in left and right images can be performed\n\n1\n\nby the assembly shown in Fig. 24. Between the analog -to -digital\nconverters and the comparator pictured in Fig. 3, are two banks\nof shift registers and computing elements behind each bank. As the\nelectron bean, say, of the left camera tube, detects the signal at\nthe first pixel of a scan line, that signal is converted to a five bit\nword and fed into the top level of the lower bank of shift registers.\nAs the electron beam advances to the second pixel, the first digital\nword advances one position along the top level of the bank of shift\nregisters and another word takes its place.\nWhen the electron beam reaches the end of the first .:can line,\nit snaps back to start a second line and the first word that entered\nthe top level of the bank of shift registers is shifted through connections not shown to become the first word in the second level. At the\nsame time the first signal from the second scan line is digitized into\na five - bit word and fed into the top level. ( Words are shown s ix bits\nlong because we aim to digitize to this number. ) The process just\ndescribed continues until, when seven levels are full, computation\noccurs on each 7 pixel x 7 pixel array that passes before the detector\nof edge (Fig. 24 top center). From then on, after each shift, another\n7 x 7 array is processed until the entire digitized image has been\nprocessed this way.\nBoth the gradient of each edge and its polarity (light-to-dark\nor dark-to-light) were lost by multiplying convolution sums together\nand taking the absolute value of the product. Ii, instead, each convolution sum is retained, it can be coded and fed to the comparator\nin the background of Fig. 24. Comparison between left and right\nviews will then be between gradient, polarity and direction of\n\n46\n\n1\nZ\n\n0\n\n1\n\nv\nc,\n\no\t\n\n=o\n^o\no^\n\nv\n\no ^\n^ u\n\nu\n\n^WW\nN ^ ^\ns ^\n\n^W\n\n47\n\nQ^\n\nU\ncd\nc a\nv ^\n^ U\nN cd\n\n.Q c\n\n^\n\nU \'L7\nd0\nU\n\nL1D\'^\n\nU N\nNm\nGJ CA\n,D cd\n\nw d\nO\n\nU\n\ntd\n\nrn E\n\nU\n\nO\n\nE-\n\nE>\n0\n\nw\n\no^\n\nv^\n\nN\n\nrr^\nW\n\ns\n\na\n\ndetected edges. When range is computed, it will be the range of a\nfairly specific edge (or a line, a corner, or other feature). In the\nopinion of the first author, there should be fewer problems of\nspurious matches.\nColor differences can be determined by employing a three\ncolor camera in place of the monochromatic one pictured in Fig. 24\nand feeding three digitized color signals into the top-level of each\nshift register\xe2\x80\xa2 . Computation will then be to detect color differences\nas well as gray-level differences.\nThe advantages of the shift registers pictured in Fig. 24 are\nthat they are simple and fast. Shift registers of this kind were\n^i\n\nx\n\noesigned and are partly constructed (Ref. 29). Performing like cells\nin the retina and cortex of vertebrate animals, this single detector\nand thinner of edge is time-shared with the entire area of an image.\nA further advantage of separating the effect of each spatial\nfrequency is that each can then be employed in an automatic focussing\nroutine. That is, changes in the low spatial frequency response can\nbe used to indicate in which direction focus can be improved, while\nthe high spatial frequency response can be used to indicate that focus\nhas been achieved.\nD. Computation of Reflecting Properties\nThe reflecting properties of a surface can be determined from\nthe incident illuminance onto a surface and the luminance of that\nsurface. Illuminance, if sunlight, can be measured either by the\ncamera-computer with the face of the camera tube protected by a\nneutral density filter, or it can be measured by a sun sensor.\nLuminance can also be measured by the camera-computer. These\nmeasurements would be performed through other channels than those\npictured in Fig. 24.\n\n48\n\nN. RECONSTRUCTING THE APPEARANCE OF A SCENE\nBy detecting not only features on the surfaces of objects, but also\nproperties of the scene such as the amount and direction of the illuminance, the appearance of a scene on Mars may be reconstructed on\nearth (Ref. 3\'O). Figure 25 diagrams rays of light from a single source\nof illuminance, such a;, sunlight, onto a cylinder. Fig. 26 shows how\na computer, using information on the shape of Zn object, on the sources\nof illuminance (mainly from upper right, but also from upper left) and\non the reflectance can recreate the appearance of that object. The\nreflectance of the cylinder is here assumed to be diffuse and 100 per\ncent. The reflectance of the background is assumed to be diffuse and\n50 per cent.\nDetection of the reflecting properties of a surface, like the\nrecognition of objects, requires more than the passive exarnin^tion of\na scene. There needs to be an active effort to relate incident illuminante to reflected luminance. This is particularly true in the detection\nof spPCVlar reflectance where highlights have to be found and measured.\nThus, determination of reflectance needs to be directed by a higher\nauthority than the passive visual computers described in this paper,\nBecause it will enhance a sense of presence, information on the\nappearance of the scene before a Mars rover should be presented to\nits earth operators stereoscopically. A small room can be built with\nwalls that are stereo displays, refreshed by disc or drum ?nem.:rie.S.\nAfter the robot has criss-crossed an area several times, it will\nhave sent more information to earth than can be displayed at one time.\nA computer can be used in the manner shown in Fig. 26 to picture what\nis known about the scene which the robot will encounter if it makes a\nnew traverse across the area.\n\n49\n\n^p\n\nJ\n\nL^\t\n\nl\n\ni\n\nFig. 25. Effect of a source of collimated light\nsuch as sunlight.\n\nFig. 26. Reconstruction of the appearance of an object\nfrom its shape, its reflectance and the sources\nof light.\n\n50\n\nV. STEREO TV CAMERAS\nIn pursuit of the objectives stated in the Introduction, four\nv\n\nfamilies of stereo TV cameras were designed.\t\n\nAfter two studies,\n\nwhich we called "A" and "B", we designated the cameras as follows:\nC.\t\n\nSingle TV camera with t-:-o optical paths provided by\nmirrors (Figs. 0 and 27).\n\nD.\t\n\nE\nr\nn\n\nTwo TV cameras gimballed separately for vergence,\ntogether for pitch.\t\n\nE.\t\n\nLenses are of one focal length (Fig. 28).\n\nTwo TV cameras with the same gimballing as in D plus\nroll and azimuth gimbals for the whole assembly (Fig. 29).\nLight entering each camera is split into two paths, one of\nwhich passes through a long focal length lens, the other\nthrough a short focal length lens.\t\n\nEach path of light forms\n\na separate image on the face of the camera tube.\nF.\t\n\nStereo facsimile camera for a crawling vehicle (Ref. 31).\n\nThe C1 was a conventional TV camera with commercially available\nsterec attachment.\t\n\nWith its short focal length lenses and 63mm\n\n(2. 5 in. ) interocular distance it proved to be of little value for our\nwork.\t\n\nThe C2 was a study.\t\n\nThe C3 was built and is now operating.\n\nThe first configuration of the C3 was the C3a, shown in Fig. 0,\nwhich employed a three-color wheel between the lens and mirror on\n\nI\n\neach side of the system to permit detecting color differences as well\nas luminance differences in the scene. \t\nset and bolted in a fixed position. \t\n\nEach mirror of the C3a was\n\nExperiment showed that the angles\n\nof the mirrors needed to be adjustable.\t\n\nAccordingly, we mounted\n\neach mirror in a bearing and arranged that each mirror be turned\nby a micrometer acting against a spring (see Fig. 27).\t\n\nWe had to\n\nremove the color wheels to make room for these mechanisms.\t\n\nIn\n\nboth the Types C3a and C3b, the focal length of the lenses is 50mm\nand the interocular distance is 21cm (8. 25 in. ).\t\n\n51\n\nFocussing is by a\n\ni\n\nr\n\nY\n\n4.\n\n0.\n\n1\ni\n\n1\t\n\nW\n\nm\na\na\nw\n\na\n\nU\n\nO\nw\n\na\n\nU\nLL\n\ncr\n\n52\n\nr-1\n\n^n\ncd\n\nro\nc,\n\nv\n\nU\n\nO\n4!\nf,\ny\n\n.n\n\na^\n\nU\n\nH\n\nN\nCNI\n\ntw\nf^\n\nr\n\nI.\n\nFig, 28. Type D1 stereo TV camera assembly,\nScale in front of assembly is 6 inches.\n\n101,\n..\'s\n\nSSC M^^T\n\n,^ 1n5\n\n1IIYUTH &.IS\n\nFig. 29. Type El stereo TV camera assembly.\n\n53\n\n1I\nI\'.\n\nworm gear that moves the TV camera. The C3b could be used in\ntests like that in II L when adjustments of the camera assembly\nhave been completed and the program STEREO has been modified\nto compute range from images received along converged axes. The\naxes need to be converged because the angles of acceptance of the\nlenses in the Type C3b are smaller than those of the lenses used in\nj\t\nthe test of II L and the binocular base wider. The advantage of the\nC3b camera assembly over the assemblies about to be described is\nIthat it places both images on the face of one camera tube, thus\nmaking it easier to eliminate vertical disparity.\nBoth the D and E types of stereo TV camera assemblies\ninclude two separately gimballed cameras. The D1 frame has been\nbuilt and its pitch and yaw electromechanisms operated under servo\ncontrol (Fig. 27). The E1 assembly, pictured in Fig. 29, is a design\non paper of an assembly in which each camera contains optic trains\nof both 40n,m and 400mm focal lengths. The axes of the bearings\nsupporting the cameras are 2 1c (8. 25 in.) apart in the Type D1,\n17. 7 3c (7 in. ) apart in the Type E1 (Ref. 32).\nEach optic train of the E1 assembly reflects light upward onto\nthe face of a vertical camera tube, providing the two images shown\nThe optic trains are folded Lpward to keep the frontin Fig. 30.\t\nto-back measurement of the camera as small as possible. The two\npitch axes of the assembly will permit it to look both straight down\nand straignt up. The azimuth gimbal will permit it to look in any\ndirection. The roll gimbal will permit it to keep the camera pitch\naxis horizontal.\nThe assembly of Fig. 29 has been estimated to weigh 11. 4kg\n(25 lbs. ) when made of light-weight spacecraft materials, as much\nas 34. 2kg (75 lbs. ) when made of aluminum and steel. In either case\nthe assembly can be mounted most effectively on a rover directly\nabove the axle, as shown in Fig. 31. An arm and one-fingered hand\nare shown attached to a shoulder of the robot to test `he estimate,\n\n54\n\nmade by the visual system, of the size and shape of an object. A\nsecond arm with a hand for picking up small objects was also\ndesigned but is not shown here.\nThe human eyes with their narrow-angle high-resolution\ncentral fields and their wide-angle low-resolution peripheral fields\nmeet the requirements stated in II N. However, there is no camera\ntube that can provide, with a short focal-length lens, the angular\nresolution of human central .ision. The only way to achieve this\nhigh resolution today is to use a long focal length lens with a\ncurrently available camera tube. As in the human eye each long\nfocal-length lens should be rigidly attached to its finder lens so that\nwhen a feature, found in the finder, is centered, it will \'.,e in the\nfield of the long focal-length lens. Such a co -axial - i.Aput \'.-sign was\nachieved in the Type El assembly, but no provision was made for\nfocussing. A redesign that provides for focussing is being proposed.\n\n20\nLOW\t\n10 0\tRESOLUTION\nIMAGE\n\nCAMERA TUBE\n\nHIGH\nRESOLUTION\nIMAGE\n2\xc2\xb0\n\nFig. 30. Diagram of images on the face of each\ncamera tube in the type E1 camera assembly.\n\n55\n\nrj\n\nt\n\n3\n\n-- -\t\n\n---ZT\n\n-\t\n\n^--J1 ,`C\n\nFig. 31. Possible configuration of a Afars rover.\nStereo TV camera views scene along; ILnes\nof sight (1), while hand and arm (2) feel -Lid\naccelerometers (3) detect inclination.\n\n56\n\nVI. SUMMARY AND CONCLUSION\nProgress toward automatic recognition of three-dimensional\nobjects, reported here, can be judged from ;it least two points of\nview. One is in terms of hardware built .Lnd p rograms operated.\n4 second point of view is of the modelling of the vision of animals\nthat recognize three -dimensional objects.\nin this section we summarize our work from the first poii.t\nof view and draw conciag ions from it. The same work seen from\nthe second point of view is s ,immarized in Ref. 16. here it is\nsuggested that the advantage of binocular or s`:reo vision in a robot,\nas in an animal, is economy in the computation of form. When the\nform of a three-dimensional object has been determined, recognition\nappears achievable by a serial snatching of stored features with those\ndetected in the environment. Such serial matching has been demonstrated to be characteristic of human object recognition.\nin this report we considered two approaches to the computation of range from a binocular, or stereo, input. In the first\napproach, described in Section II, gray values in the left image of\na scene were matched with gray values in the right image to determine the range of the objects imaged. In the second approach\n(Section III), edges were extracted from each irr:?ge.\nWhen using the first approach, we say that an N x N area of\npixels in the left image is if\n\t to an N x N area cf p i xels in\nthe right image when a preselected percentage of tho first se: of\npixels has gray values that are the same, within a tolerance E, as\nthe gray values of the second set. If the axes of the cameras providing the two imagea dre parallel, the disparity of a match between\nleft and right images is zero for a point :tt infinity, 36 in our test\nsystem for the nearest point. Range is computed from disparity.\nOur two main problems were, first, how to design this robot vision\nso that it will reject "spurious" matches between areas of the left\n\n57\n\nand right views and, second, \' p ow to enable it to interpret part of a\nscene that is ambiguous because only one camera can view it. The\nmethod we devised of determining whether or not a match is\nspurious is to assume that every surface is opaque and that it\ntherefore hides different areas of the background from each camera.\nBy forming left-view and right-view match spaces and comparing\nthem, spurious matches are eliminated.\nThis first approach was favored in the work reported here\nbecause Julesz had performed experiments in which disparity had\nbeen computed this way automatically. We repeated Julesz\' experiments with random gray-value dot patterns instead of the random\ngray-value dot patterns he used. We defined a "match space" in\nwhich we employed Julesz\' methods of both removing spurious\nmatches and of filling ambiguous regions. To demonstrate the\nresults of this processing we devised the range map. Employing\nnext, random square patterns we devised means of eliminating\nspurious surfaces and of reducing parallelopipeds of matches to\nplanes of matches. Finally, we aimed a TV camera at a Mars-like\nj\t\n\nscene from two positions, with the axes parallel between positions,\nfed the output of the camera through an analog-to-digital computer\ninto a computer where the images from the camera in its two\npositions were compared, line by line. The result of matching,\n\nI\t\n\neliminating spurious matches and eliminating ambiguities was again\na range map.\n\nj\t\n\nThese steps, when followed by recognition of form, are models\nof what Julesz calls local and global stereopsis. To take the next\nstep beyond the recognition of form, namely, the automatic recognition of objects, it appears that the second approach is needed,\nnamely, one in which edges are detected and localized in space. In\nhigher vertebrate animals and in our second approach edges are\ndetected prior to comparison between left and right views. In b-,ne\n\n58\n\nti\n\na.\n\ne\n\ncases, however, it may be more efficient to detect edges after\ncomparison of left and right views, for example, by searching the\nrange map.\nSo that the above operations can be performed rapidly, we\nhave devised a means of shifting as many lines of the images at a\ntime as are needed in computation past a single time-shared computing element. Matched points or edges can be plotted in a single\nmatch space equipped with two sets of wiring to provide the two\nviews required of this space for the rerroval of ambiguities.\nSo that a stereo TV camera assembly can make a sequence of\nobservations of a scene, we have designed camera assemblies\nvariable in vergence, * p itch, roll and yaw. We have built the C1\nassembly that has these properties. A stereo camera assembly,\nwe conclude, must employ both short and long focal length lenses\nin each camera, the short to provide the finder lenses, the long to\nprovide for identification of features. The one stereo TV camera\nassembly we ha\xe2\x80\xa2\xe2\x80\xa2e both built and operated (Type C3b) has only\nshort focal-length lenses and is adjustable only in focus and vergence.\nWe have derived formulas for range accuracy and employed\nthem to predict the effects of different focal lengths, interocular\ndistances and sensor arrays.\nFeatures on the surfaces of objects and spatial relations among\nthese features, transmitted by a robot on Mars to an earth station,\n,tan be reconstruci . ] there into the appearance of objects. We have\nconstructed, from such data, the appearance of a cylinder in sunlight.\n\nJergence denotes the convergence -divergence adjustment of\nthe two optic axes (Ref. 33).\n\n59\n\n1\n\nP RECEDING PAGE BLANK NOT FILMED\nNote to Sections II G and II N\n\nAPPENDIX A\nEQUATIONS FOR RANGE AND UNCERTAINTY IN RANGE\nA. 1 The Geometry of Stereo TV Optics with Parallel Axes\nFigure A-1 is a redrawing of Fig. 6a to show the uncertainty in\nrange measurement + 1z corresponding to the uncertainty of point S on\nthe face of the camera tube. Positive uncertainty is + Xz or PM,\nnegative uncertainty - Nz or PR. The angle of uncertainty a is also\nintroduced in preparation for a discussion of the first strategy of II B.\nr\n\nIn addition to the axis z, there is a parallel axis q in the x-z plane,\nwhich bisects the interocular distance, 2b. The range, z, or P is\n\n\'I\n\nmeasured from the optical centers of the lenses, L, which are assumed\nto behave like pinhole lenses.\n\nT\'\n\nFrom the similarity of triangles POL and LAS in Fig. A. 1,\nz_f\nE s\nz = bs\n\n\t\n\n(A-1)\n\nbut in Fig. 6,\ns = dL = d\n\nZ\n\n=d\n\nR 2\n\nbf 2bf\n\n(1) on p. 1`)\n\t\n\nThus, for a system with parallel optical axes where range is measured\nj\nii\t\n\nfrom the optical centers of the lenses, the nominal value of the range\nis the product of the interocular distance and the focal length, divided\nby the disparity between the positions of the image on the image plane.\n\n61\n\ny\n\n\t\n\nA. 2 Derivation of Equation of Range Uncertainty for Stereo TV\nCameras with Parallel Axes\nIf the uncc-rtainty of the position S on the image plane is either 1s or\n- \\s, then the uncertainty in the range is \\z or - \\z, respectively.\nFrom Fig. A-1,\nf\t 1s _ z+\\z\t\n__G\xe2\x80\x94\nS-\n\n(A-2)\n\n\t\n\n(A-3)\n\nz\n\n+\n\nAz = sb;s\n\nSubtracting Eq (A-1) from (A-3), we obtain the range uncertainty 1z\ndue to uncertainty in a position S on the image plane (or equivalently\nan angalar uncertainty b in determining (3):\nAz - bf(s - \\s) - bfs_ bf \xe2\x80\xa2\n- -\n\n_1\n\ns\n\n^.^\xe2\x80\x94 - s s - ;1s\n\n(A-4)\n\nRearranging Eq. (A-1) and substituting into Eq. (A-4),\n^s z 2\nAz - b\n\n(2) cal page 38\n\n= \xe2\x80\x94 \xe2\x80\xa2\xe2\x80\xa2 z\n. ^s\n\n62\n\nQ\n\ns\ni\n\n-L.\nSYMBOL DEFINITIONS\nZ = RANGE OF A POINT P FROM 0\n- POINT wNFRE O P TIC AX II PlFarfS\n\nIMAGE PLANE\n\ns = DISTANCE ON THE IMAGE PLANE FROM POINT A TO IMAGE OF POINT P\n7\ni\n\nA r = PM = UNCERTAINTY IN RANGE CORRESPONDING TO UNCERTAINTY\nIN THE POSITION OF POINT S,As\n-At - PR\n\ni\n.,\n\nFig. A-1. Geometry of pai \xe2\x80\xa2 aliei o p tics for range finding.\n0\n\n63\n\nI\n\nPRECEDING PACE BLANK NOT FILMED\n\nREFERENCES\n1. Sutro, L. L. , D. B. Moulton, et al. , "1963 Advanced Sensor\nInvestigations", Report R-470, Charles Stark Draper Laboratory, MIT, 1964, pp. 1-35.\n2. Sutro, ice. L. , "Advanced Sensor and Control System Studies,\n1964 to September 1965", Report R-519, Charles Stark Draper\nLaboratory, MIT, 1966, pp. 16-33.\n3. 1\\1orcno -Diaz, R. , "An Analytical Model of the Group 2 Ganglion\nCell in the Frog\'s Retina", Report E-1858, Charles Stark\nDraper Laboratory, MIT, 1965.\ni\n\n4. Moreno-Diaz, R. , "An Analytical Model of the \'Bug Detector\'\nGanglion Cell di the Frog\'s Retina", Cyberne tic Problems in\nBionics, H. L. Oestreicher and D. R. Rlcore, Eds. , Gordon\nand Breach, N. Y. , 1968, pp. 481-491.\n5. Sutro, L. L. , "Information Processing and Data Compression\nfor Exobiology Missions", Adv ances in Astrona ut ical Science,\nVol. 22, The Search for Extraterre strial Life, Scholarly\nPublications, Inc. , Sun Valley, Cal, , 1967, pp. 347 -378, and\nReport R-545, Charles Stark Draper U-boratory, MIT, 1966.\n6. Sutro, L. L. , "Proposed Electronics to Represent the Properties\nof a Frog\'s Eye", Cy berne tic P roblem s in Bio nics, H. L.\nOestreicher and D. R. Moore, Eds. , Gordon and Breach, N. Y. ,\n1968, pp. 811-819.\n7. Sutro, L. L. , "First Steps Toward a Model of The Vertebrate\nCentral Nervous System", Cybernetics, Artificial Intelli gence\na nd Ecolog y, Proceedings of the Fourth Annual Symposium of\nthe Americal Society for Cybernetics, H. W. Robinson and\nD. E. Knight, Eds. , Spartan Books, N. Y. , 1972, pp. 225-252.\n\n65\n\n8. Lettvin, J. L., H. R. Maturana, W. S. McCulloch and W. H. Pitts,\n"What a Frog\'s Eye Tells a Frog\'s Brain", Proceedings of the\nI. R. E. , Vol. 47, No. 11, November, 1959.\n\nt\t\n\n9. Sutro, L. L. , et al. , "Development of Visual, Contact and\nDecision Subsystems for a Mars Rover", Report R-565, Charles\nStark Draper Laboratory , MIT, 1967, pp. 15-16 (out of print).\n10. Fukishima, K. , "Visual Feature Extract.on by a Multilayered\nNetwork of Analog Threshold Elements", IEEE Transactions on\nSystem Science and Cybernetics, Vol. SSC-5, No. 4 (Oct. , 1969),\npp. 332-333.\n\ni\t\n!\t\n\n{\t\n\n11. Fukishima, K., "A Feature Extractor for Curvilinear Patterns:\nA Design Suggested by the Mammalian Visual System",\nKybernetik 7 (1970), pp. 153-160.\n12. Sutro, L. L. and J. B. Lerman, "Robot Vision", Remotely Manned\nSystems in Space, Proceedings of First National Conference,\nedited by Ewald Heer, California Institute of Technology, Pasadena,\nCal., 1973.\n13. Held, R. , et al. , "Locating and Identifying: Two Modes of Visual\nProcessing. A Symposium", Psychologiscne Forschung 31,\n42/43 (1967).\n14. Julesz, B., Foundations of Cyclopean Perception, The University\nof Chicago Press, Chicago, 1971, p. 144.\n15. Sutro, L. L., and W. L. Kilmer, "An Assembly of Computers\nto Command and Control a RoboO, first published in the Proceedings of the 1969 Spring Joint Computer Conference, AFIPS\nPress, Montvale, N.J., pp. 113-137. This Pape -- was revised\nand republished both in ANALOG-Science Fiction, Science Fact,\nMay 1970, and as Report R-582-1, Charles Stark Draper Laboratory, MIT, 1969.\n\n66\n\ni\n\n16. Sutro, L. L. and W. L. Kilmer, "Development of DecisionMaking Devices Inspired by the Animal Nervous System,"\nReport R-636, Charles Stark Draper Laboratory, MIT,\nCambridge, Mass. (in preparation).\n17. Sutro, L. L. and W. L. Kilmer, "Development of DecisionMaking Network Based Upon a Model of the Reticular Formation"\nReport R-736, Charles Stark Draper Laboratory, MIT,\nCambridge, Mass. (in preparation).\n18. Sutro, L. L. and W. L. Kilmer, op. cit. , Ref. 15, p. 131,\nAlso in Report R-582-1, Charles Stark Draper Laboratory,\nMIT, 1969, P. 52.\n19. Julesz, B. , "Towards the Automation of Binocular Depth Perception", Proceedings of the IFIP Congress, 1962, North\nHolland Publishing Co., Amsterdam, 1963, pp. 439-443.\n20. Lerman, J. B. , "Computer Processing of Stereo Images for\nthe Automatic Extraction of Range", thesis submitted in partial\nfulfillment for the degrees of bachelor of science and master\nof science, Department of Electrical Engineering, MIT,\nReport T-531, Charles Stark Draper Laboratory, MIT, 1970\n(out of print).\n21. Nitzan, D. , "Stereopsis Error Analysis", Technical Note 71,\nArtificial Intelligence Center, Stanford Research Institute,\nMenlo Park, California, 1972.\n22. Julesz, B. , op, cit. , Ref. 14, p. 150.\n23. Pettigrew, J. D., "The Neurophysiology of Binocular Vision",\nScientific American, Vol. 227, No. 2, August, 1972, pp. 84-96.\n24. Julesz, B. , op, cit. , Ref. 14, p. 1.\n\n67\n\nr\n\n25. Julesz, B. , "Texture and Visual Perception", Scientific\nAmerican, Vol. 212, No. 2, February 1965, p. 46.\n26. Sutro, L. L. , Notes of Conversations with W. S. McCulloch,\n1958-1969.\n27. Prewitt, J. , M. S. , "Object Enhancement and Extraction",\nin Picture Processing and Psyehopictorics, edited by\nB. S. Lipkin and A. Rosenfeld, Academic Press, New York,\n1970, pp. 75-149,\n\n\\\t\n\n28. Rosenfeld, A. , "A Nonlinear Edge Detection Technique",\nProceedings of the I. E. E. E. , May 1970, pp. 814-816,\n:i\n\n\t\n\n29. Sutro, L. L. and W. L, Kilmer, op, cit. , Ref. 15, pp. 123\nand 124. Also in Report R-582-1, Charles Stark Draper\nLaboratory, MIT, 1969, pp. 28-31,\n30, Sutro, L. L. , "A Model of Visual Space", Biological Prototypes and Syntnetic Systems, Vol, 1, E. E. Bernard and\nM. R. Kare, eds. , Plenum Press, Inc., New York, 1962,\npp. 75-87. In reading this paper, substitute the word\n"luminance" for the word "brightness".\n31. Sutro, L. L. and W. L. Kilmer, op, cit. , Ref. 15, p, 130.\nAlso in Report R-582-1, Charles Stark Draper Laboratory,\nMIT, 1969, pp. 47 and 48.\t\n32. Sutro, L. L. , W. L. Kilmer, R. Moreno-Diaz, W. S. McCulloch\net al, , op, cit. , Ref. 9, pp. 23 -30 and 85-86.\n33, Julesz, B. , op. cit. , Ref. 22, p. 52.\n34. Ogle, K. N. , "Researches n Binocular Vision", Hafner\nPublishing Co. , New York, 1964, pp, 166, 173ff a--I 281.\n\n68\n\ni\n\n1\n\n'